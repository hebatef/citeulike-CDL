doc_id	citeulike_id	type	journal	booktitle	series	publisher	pages	volume	number	year	month	postedat	address	title	abstract
0	42	article	proceedings of the national academy of sciences of the united states of america	\N	\N	\N	4	101	6	2004	feb	2004-11-04 02:25:05	department of computational biology, graduate school of frontier sciences, university of tokyo, japan. arita@k.u-tokyo.ac.jp	the metabolic world of escherichia coli is not small.	to elucidate the organizational and evolutionary principles of the metabolism of living organisms, recent studies have addressed the graph-theoretic analysis of large biochemical networks responsible for the synthesis and degradation of cellular building blocks [jeong, h., tombor, b., albert, r., oltvai, z. n. \\& barab\\{\\'a\\}si, a. l. (2000) nature 407, 651-654; wagner, a. \\& fell, d. a. (2001) proc. r. soc. london ser. b 268, 1803-1810; and ma, h.-w. \\& zeng, a.-p. (2003) bioinformatics 19, 270-277]. in such studies, the global properties of the network are computed by considering enzymatic reactions as links between metabolites. however, the pathways computed in this manner do not conserve their structural moieties and therefore do not correspond to biochemical pathways on the traditional metabolic map. in this work, we reassessed earlier results by digitizing carbon atomic traces in metabolic reactions annotated for escherichia coli. our analysis revealed that the average path length of its metabolism is much longer than previously thought and that the metabolic world of this organism is not small in terms of biosynthesis and degradation.
1	43	article	science	\N	\N	american association for the advancement of science	5	295	5560	2002	mar	2004-11-04 02:25:51	department of anesthesiology, university of michigan medical school, ann arbor, mi 48109, usa.	reverse engineering of biological complexity	advanced technologies and biology have extremely different physical implementations, but they are far more alike in systems-level organization than is widely appreciated. convergent evolution in both domains produces modular architectures that are composed of elaborate hierarchies of protocols and layers of feedback regulation, are driven by demand for robustness to uncertain environments, and use often imprecise components. this complexity may be largely hidden in idealized laboratory settings and in normal operation, becoming conspicuous only when contributing to rare cascading failures. these puzzling and paradoxical features are neither accidental nor artificial, but derive from a deep and necessary interplay between complexity and robustness, modularity, feedback, and fragility. this review describes insights from engineering theory and practice that can shed some light on biological complexity.
2	60	article	nat rev neurosci	\N	\N	nature publishing group	12	5	11	2004	nov	2004-11-04 21:20:59	institute for learning and brain sciences and the department of speech and hearing sciences, university of washington, seattle, washington 98195, usa. pkkuhl@u.washington.edu	early language acquisition: cracking the speech code	infants learn language with remarkable speed, but how they do it remains a mystery. new data show that infants use computational strategies to detect the statistical and prosodic patterns in language input, and that this leads to the discovery of phonemes and words. social interaction with another human being affects speech learning in a way that resembles communicative learning in songbirds. the brain's commitment to the statistical and prosodic patterns that are experienced early in life might help to explain the long-standing puzzle of why infants are better language learners than adults. successful learning by infants, as well as constraints on that learning, are changing theories of language acquisition.
3	61	article	trends in cognitive sciences	\N	\N	\N	7	8	9	2004	sep	2004-11-04 21:29:27	department of psychology and programs in cognitive and neural science, indiana university, bloomington, in 47405, usa. osporns@indiana.edu	organization, development and function of complex brain networks	recent research has revealed general principles in the structural and functional organization of complex networks which are shared by various natural, social and technological systems. this review examines these principles as applied to the organization, development and function of complex brain networks. specifically, we examine the structural properties of large-scale anatomical and functional brain networks and discuss how they might arise in the course of network growth and rewiring. moreover, we examine the relationship between the structural substrate of neuroanatomy and more dynamic functional and effective connectivity patterns that underlie human cognition. we suggest that network analysis offers new fundamental insights into global and integrative aspects of brain function, including the origin of flexible and coherent cognitive states within the neural architecture.
4	62	article	plos biology	\N	\N	public library of science	\N	2	11	2004	nov	2004-11-04 21:34:10	department of psychology and programs in cognitive and neural science, indiana university, bloomington, indiana, united states of america. osporns@indiana.edu.	motifs in brain networks.	complex brains have evolved a highly efficient network architecture whose structural connectivity is capable of generating a large repertoire of functional states. we detect characteristic network building blocks (structural and functional motifs) in neuroanatomical data sets and identify a small set of structural motifs that occur in significantly increased numbers. our analysis suggests the hypothesis that brain networks maximize both the number and the diversity of functional motifs, while the repertoire of structural motifs remains small. using functional motif number as a cost function in an optimization algorithm, we obtain network topologies that resemble real brain networks across a broad spectrum of structural measures, including small-world attributes. these results are consistent with the hypothesis that highly evolved neural architectures are organized to maximize functional repertoires and to support highly efficient integration of information.
5	98	electronic	\N	\N	\N	\N	\N	\N	\N	2004	may	2004-11-07 13:58:29	\N	topological generalizations of network motifs	biological and technological networks contain patterns, termed network motifs, which occur far more often than in randomized networks. {n}etwork motifs were suggested to be elementary building blocks that carry out key functions in the network. {i}t is of interest to understand how network motifs combine to form larger structures. {t}o address this, we present a systematic approach to define "motif generalizations": families of motifs of different sizes that share a common architectural theme. {t}o define motif generalizations, we first define "roles" in a subgraph according to structural equivalence. {f}or example, the feedforward loop triad--a motif in transcription, neuronal, and some electronic networks--has three roles: an input node, an output node, and an internal node. {t}he roles are used to define possible generalizations of the motif. {t}he feedforward loop can have three simple generalizations, based on replicating each of the three roles and their connections. {w}e present algorithms for efficiently detecting motif generalizations. {w}e find that the transcription networks of bacteria and yeast display only one of the three generalizations, the multi-output feedforward generalization. {i}n contrast, the neuronal network of {c}. elegans mainly displays the multi-input generalization. {f}orward-logic electronic circuits display a multi-input, multi-output hybrid. {t}hus, networks which share a common motif can have very different generalizations of that motif. {u}sing mathematical modeling, we describe the information processing functions of the different motif generalizations in transcription, neuronal, and electronic networks.
6	99	article	nature	\N	\N	nature publishing group	2	393	6684	1998	jun	2004-11-07 13:58:29	department of theoretical and applied mechanics, cornell university, ithaca, new york 14853, usa. djw24@columbia.edu	collective dynamics of 'small-world' networks	networks of coupled dynamical systems have been used to model biological oscillators1, 2, 3, 4, josephson junction arrays5,6, excitable media7, neural networks8, 9, 10, spatial games11, genetic control networks12 and many other self-organizing systems. ordinarily, the connection topology is assumed to be either completely regular or completely random. but many biological, technological and social networks lie somewhere between these two extremes. here we explore simple models of networks that can be tuned through this middle ground: regular networks 'rewired' to introduce increasing amounts of disorder. we find that these systems can be highly clustered, like regular lattices, yet have small characteristic path lengths, like random graphs. we call them 'small-world' networks, by analogy with the small-world phenomenon13,14 (popularly known as six degrees of separation15). the neural network of the worm caenorhabditis elegans, the power grid of the western united states, and the collaboration graph of film actors are shown to be small-world networks. models of dynamical systems with small-world coupling display enhanced signal-propagation speed, computational power, and synchronizability. in particular, infectious diseases spread more easily in small-world networks than in regular lattices.
7	101	article	science	\N	\N	american association for the advancement of science	3	298	5594	2002	oct	2004-11-08 11:27:57	departments of physics of complex systems and molecular cell biology, weizmann institute of science, rehovot, israel 76100.	network motifs: simple building blocks of complex networks	complex networks are studied across many fields of science. to uncover their structural design principles, we defined  ” network motifs,” patterns of interconnections occurring in complex networks at numbers that are significantly higher than those in randomized networks. we found such motifs in networks from biochemistry, neurobiology, ecology, and engineering. the motifs shared by ecological food webs were distinct from the motifs shared by the genetic networks of escherichia coli and saccharomyces cerevisiae or from those found in the world wide web. similar motifs were found in networks that perform information processing, even though they describe elements as different as biomolecules within a cell and synaptic connections between neurons in caenorhabditis elegans. motifs may thus define universal classes of networks. this approach may uncover the basic building blocks of most networks.
8	102	article	j. acm	\N	\N	acm	28	46	5	1999	sep	2004-11-08 14:13:12	new york, ny, usa	authoritative sources in a hyperlinked environment	the network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. we develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their effectiveness in a variety of context on the world wide web. the central issue we address within our framework is the distillation of broad search topics, through the discovery of  ” authorative” information sources on such topics. we propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of  ” hub pages” that join them together in  the link structure. our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristrics for link-based analysis.
9	114	article	computer networks and isdn systems	proceedings of the seventh international conference on world wide web 7	www7	elsevier science publishers b. v.	10	30	1-7	1998	apr	2004-11-08 15:54:40	amsterdam, the netherlands, the netherlands	the anatomy of a large-scale hypertextual web search engine	in this paper, we present google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. google is designed to crawl and index the web efficiently and produce much more satisfying search results than existing systems. the prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/ to engineer a search engine is a challenging task. search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. they answer tens of millions of queries every day. despite the importance of large-scale search engines on the web, very little academic research has been done on them. furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from three years ago. this paper provides an in-depth description of our large-scale web search engine — the first such detailed public description we know of to date. apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. this paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.
10	122	article	phys rev e stat nonlin soft matter phys	\N	\N	\N	\N	64	5 Pt 1	2001	nov	2004-11-08 16:34:35	school of physics, center for theoretical physics, seoul national university, seoul 151-747, korea.	spectra and eigenvectors of scale-free networks.	we study the spectra and eigenvectors of the adjacency matrices of scale-free networks when bidirectional interaction is allowed, so that the adjacency matrix is real and symmetric. the spectral density shows an exponential decay around the center, followed by power-law long tails at both spectrum edges. the largest eigenvalue lambda1 depends on system size n as lambda1 approximately n1/4 for large n, and the corresponding eigenfunction is strongly localized at the hub, the vertex with largest degree. the component of the normalized eigenfunction at the hub is of order unity. we also find that the mass gap scales as n(-0.68).
11	123	article	proceedings of the national academy of sciences of the united states of america	\N	\N	\N	5	100	11	2003	may	2004-11-08 16:35:18	department of mathematics, university of california at san diego, la jolla 92093-0112, usa. fan@ucsd.edu	spectra of random graphs with given expected degrees	in the study of the spectra of power-law graphs, there are basically two  competing approaches. one is to prove analogues of wigner's semicircle law,  whereas the other predicts that the eigenvalues follow a power-law  distribution. although the semicircle law and the power law have nothing in  common, we will show that both approaches are essentially correct if one  considers the appropriate matrices. we will prove that (under certain mild  conditions) the eigenvalues of the (normalized) laplacian of a random  power-law graph follow the semicircle law, whereas the spectrum of the  adjacency matrix of a power-law graph obeys the power law. our results are  based on the analysis of random graphs with given expected degrees and their  relations to several key invariants. of interest are a number of (new) values  for the exponent ?, where phase transitions for eigenvalue distributions  occur. the spectrum distributions have direct implications to numerous graph  algorithms such as, for example, randomized algorithms that involve rapidly  mixing markov chains.
12	127	article	proceedings of the national academy of sciences	\N	\N	national academy of sciences	5	99	22	2002	oct	2004-11-08 17:12:25	department of management sciences, university of iowa, iowa city, ia 52242, usa. filippo-menzer@uiowa.edu	growing and navigating the small world web by local content	can we model the scale-free distribution of web hypertext degree under realistic assumptions about the behavior of page authors? can a web crawler efficiently locate an unknown relevant page? these questions are receiving much attention due to their potential impact for understanding the structure of the web and for building better search engines. here i investigate the connection between the linkage and content topology of web pages. the relationship between a text-induced distance metric and a link-based neighborhood probability distribution displays a phase transition between a region where linkage is not determined by content and one where linkage decays according to a power law. this relationship is used to propose a web growth model that is shown to accurately predict the distribution of web page degree, based on textual content and assuming only local knowledge of degree for existing pages. a qualitatively similar phase transition is found between linkage and semantic distance, with an exponential decay tail. both relationships suggest that efficient paths can be discovered by decentralized web navigation algorithms based on textual and/or categorical cues.
13	128	article	proceedings of the national academy of sciences	\N	\N	national academy of sciences	5	98	2	2001	jan	2004-11-08 17:14:49	santa fe institute, 1399 hyde park road, santa fe, nm 87501, usa. mark@santafe.edu	the structure of scientific collaboration networks	the structure of scientific collaboration networks is investigated. two scientists are considered connected if they have authored a paper together and explicit networks of such connections are constructed by using data drawn from a number of databases, including {medline} (biomedical research), the los alamos {e-print} archive (physics), and {ncstrl} (computer science). i show that these collaboration networks form  ” small worlds,” in which randomly chosen pairs of scientists are typically separated by only a short path of intermediate acquaintances. i further give results for mean and distribution of numbers of collaborators of authors, demonstrate the presence of clustering in the networks, and highlight a number of apparent differences in the patterns of collaboration between the fields studied.
14	129	article	proceedings of the national academy of sciences	\N	\N	national academy of sciences	3	97	21	2000	oct	2004-11-08 17:15:37	center for polymer studies and department of physics, boston university, boston, ma 02215, usa. amaral@buphy.bu.edu	classes of small-world networks	we study the statistical properties of a variety of diverse real-world networks. we present evidence of the occurrence of three classes of small-world networks: (a) scale-free networks, characterized by a vertex connectivity distribution that decays as a power law; (b) broad-scale networks, characterized by a connectivity distribution that has a power law regime followed by a sharp cutoff; and (c) single-scale networks, characterized by a connectivity distribution with a fast decaying tail. moreover, we note for the classes of broad-scale and single-scale networks that there are constraints limiting the addition of new links. our results suggest that the nature of such constraints may be the controlling factor for the emergence of different classes of networks.
15	148	article	nature	\N	\N	nature publishing group	3	427	6976	2004	feb	2004-11-10 05:48:07	department of physics, massachusetts institute of technology, cambridge, massachusetts 02139, usa.	multistability in the lactose utilization network of escherichia coli.	multistability, the capacity to achieve multiple internal states in response to a single set of external inputs, is the defining characteristic of a switch. biological switches are essential for the determination of cell fate in multicellular organisms, the regulation of cell-cycle oscillations during mitosis and the maintenance of epigenetic traits in microbes. the multistability of several natural and synthetic systems has been attributed to positive feedback loops in their regulatory networks. however, feedback alone does not guarantee multistability. the phase diagram of a multistable system, a concise description of internal states as key parameters are varied, reveals the conditions required to produce a functional switch. here we present the phase diagram of the bistable lactose utilization network of escherichia coli. we use this phase diagram, coupled with a mathematical model of the network, to quantitatively investigate processes such as sugar uptake and transcriptional regulation in vivo. we then show how the hysteretic response of the wild-type system can be converted to an ultrasensitive graded response. the phase diagram thus serves as a sensitive probe of molecular interactions and as a powerful tool for rational network design.
16	154	article	physical review e	\N	\N	american physical society	\N	69	2	2003	aug	2004-11-10 17:09:17	\N	finding and evaluating community structure in networks	we propose and study a set of algorithms for discovering community structure in networksÂ—natural divisions of network nodes into densely connected subgroups. our algorithms all share two definitive features: first, they involve iterative removal of edges from the network to split it into communities, the edges removed being identified using any one of a number of possible "betweenness" measures, and second, these measures are, crucially, recalculated after each removal. we also propose a measure for the strength of the community structure found by our algorithms, which gives us an objective metric for choosing the number of communities into which a network should be divided. we demonstrate that our algorithms are highly effective at discovering community structure in both computer-generated and real-world network data, and show how they can be used to shed light on the sometimes dauntingly complex structure of networked systems.we propose and study a set of algorithms for discovering community structure in networksÂ—natural divisions of network nodes into densely connected subgroups. our algorithms all share two definitive features: first, they involve iterative removal of edges from the network to split it into communities, the edges removed being identified using any one of a number of possible "betweenness" measures, and second, these measures are, crucially, recalculated after each removal. we also propose a measure for the strength of the community structure found by our algorithms, which gives us an objective metric for choosing the number of communities into which a network should be divided. we demonstrate that our algorithms are highly effective at discovering community structure in both computer-generated and real-world network data, and show how they can be used to shed light on the sometimes dauntingly complex structure of networked systems.
17	155	misc	\N	\N	\N	\N	\N	\N	\N	2003	mar	2004-11-10 17:13:30	\N	the structure and function of complex networks	inspired by empirical studies of networked systems such as the internet, social networks, and bio- logical networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks.
18	156	article	j. mach. learn. res.	\N	\N	jmlr.org	28	3	\N	2003	mar	2004-11-10 17:52:19	cambridge, ma, usa	matching words and pictures	we present a new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text. learning the joint distribution of image regions and words has many applications. we consider in detail predicting words associated with whole images (auto-annotation) and corresponding to particular image regions (region naming). auto-annotation might help organize and access large collections of images. region naming is a model of object recognition as a process of translating image regions to words, much as one might translate from one language to another. learning the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining, particularly because it is typically hard to apply data mining techniques to collections of images. we develop a number of models for the joint distribution of image regions and words, including several which explicitly learn the correspondence between regions and words. we study multi-modal and correspondence extensions to hofmann's hierarchical clustering/aspect model, a translation model adapted from statistical machine translation (brown et al.), and a multi-modal extension to mixture of latent dirichlet allocation ({mom}-{lda}). all models are assessed using a large collection of annotated images of real scenes. we study in depth the difficult problem of measuring performance. for the annotation task, we look at prediction performance on held out data. we present three alternative measures, oriented toward different types of task. measuring the performance of correspondence methods is harder, because one must determine whether a word has been placed on the right region of an image. we can use annotation performance as a proxy measure, but accurate measurement requires hand labeled data, and thus must occur on a smaller scale. we show results using both an annotation proxy, and manually labeled data.
19	175	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2004-11-11 01:04:06	\N	an overview of audio information retrieval	abstract.   the problem of audio information retrieval is familiar to anyone who has returned from vacation to find an answering machine full of messages. while there is not yet an ?altavista? for the audio data type, many workers are finding ways to automatically locate, index, and browse audio using recent advances in speech recognition and machine listening. this paper reviews the state of the art in audio information retrieval, and presents recent advances in automatic speech recognition, word spotting, speaker and music identification, and audio similarity with a view towards making audio less ?opaque?. a special section addresses intelligent interfaces for navigating and browsing audio and multimedia documents, using automatically derived information to go beyond the tape recorder metaphor.
20	186	article	nature	\N	\N	nature publishing group	3	428	6985	2004	apr	2004-11-11 10:22:43	division of chemistry and chemical engineering, california institute of technology, pasadena, california 91125, usa.	programmed population control by cell-cell communication and regulated killing.	de novo engineering of gene circuits inside cells is extremely difficult, and efforts to realize predictable and robust performance must deal with noise in gene expression and variation in phenotypes between cells. here we demonstrate that by coupling gene expression to cell survival and death using cell-cell communication, we can programme the dynamics of a population despite variability in the behaviour of individual cells. specifically, we have built and characterized a 'population control' circuit that autonomously regulates the density of an escherichia coli population. the cell density is broadcasted and detected by elements from a bacterial quorum-sensing system, which in turn regulate the death rate. as predicted by a simple mathematical model, the circuit can set a stable steady state in terms of cell density and gene expression that is easily tunable by varying the stability of the cell-cell communication signal. this circuit incorporates a mechanism for programmed death in response to changes in the environment, and allows us to probe the design principles of its more complex natural counterparts.
21	205	inproceedings	\N	cscw	\N	acm	9	\N	\N	1998	\N	2004-11-11 13:27:05	new york, ny, usa	mobility in collaboration	note: {ocr} errors may be found in this reference list extracted from the full text article.  {acm} has opted to expose the complete list rather than only correct and linked references.
22	214	article	cyberpsychol behav	\N	\N	\N	17	5	6	2002	dec	2004-11-11 16:32:16	applied technology for neuro-psychology laboratory, istituto auxologico italiano, centre for studies and research in communication psychology, universit\\`{a} cattolica del sacro cuore, milan, italy. auxo.psylab@auxologico.it	the sociocognitive psychology of computer-mediated communication: the present and future of technology-based interactions.	the increased diffusion of the internet has made computer-mediated communication ({cmc}) very popular. however, a difficult question arises for psychologists and communication researchers: "what are the communicative characteristics of {cmc}?" according to the "cues-filtered-out" approach, {cmc} lacks the specifically relational features (social cues), which enable the interlocutors to identify correctly the kind of interpersonal situations they find themselves in. this paper counters this vision by integrating in its theoretical frame the different psycho-social approaches available in current literature. in particular, the paper describes the characteristics of the socio-cognitive processes-emotional expression, context definition, and identity creation-used by the interlocutors to make order and create relationships out of the miscommunication processes typical of {cmc}. moreover, it presents the emerging forms of {cmc}-instant messaging, shared hypermedia, weblogs, and graphical chats-and their possible social and communicative effects.
23	226	article	proceedings of the national academy of sciences of the united states of america	\N	\N	\N	5	101	8	2004	feb	2004-11-12 02:17:30	department of chemical engineering, university of california, los angeles, ca 90095, usa.	design of artificial cell–cell communication using gene and metabolic networks	artificial transcriptional networks have been used to achieve novel, nonnative behavior in bacteria. typically, these artificial circuits are isolated from cellular metabolism and are designed to function without intercellular communication. to attain concerted biological behavior in a population, synchronization through intercellular communication is highly desirable. here we demonstrate the design and construction of a gene-metabolic circuit that uses a common metabolite to achieve tunable artificial cell–cell communication. this circuit uses a threshold concentration of acetate to induce gene expression by acetate kinase and part of the nitrogen-regulation two-component system. as one application of the cell–cell communication circuit we created an artificial quorum sensor. engineering of carbon metabolism in escherichia coli made acetate secretion proportional to cell density and independent of oxygen availability. in these cells the circuit induced gene expression in response to a threshold cell density. this threshold can be tuned effectively by controlling {?ph} over the cell membrane, which determines the partition of acetate between medium and cells. mutagenesis of the enhancer sequence of the {glnap2} promoter produced variants of the circuit with changed sensitivity demonstrating tunability of the circuit by engineering of its components. the behavior of the circuit shows remarkable predictability based on a mathematical design model.
24	230	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2004-11-12 05:26:24	\N	dynamic conditional random fields	conditional random fields (crfs) for sequence modeling have several  advantages over joint models such as hmms, including the ability to  relax strong independence assumptions made in those models, and the  ability to incorporate arbitrary overlapping features. previous work has  focused on linear-chain crfs, which correspond to finite-state machines,  and have efficient exact inference algorithms. often, however, we wish  to label sequence data in multiple interacting ways---for example,...
25	239	article	proteomics	\N	\N	wiley-vch verlag	14	4	4	2004	apr	2004-11-12 15:04:26	department of physics, university of notre dame, notre dame, in 46556, usa.	functional and topological characterization of protein interaction networks	the elucidation of the cell's large-scale organization is a primary challenge for post-genomic biology, and understanding the structure of protein interaction networks offers an important starting point for such studies. we compare four available databases that approximate the protein interaction network of the yeast, saccharomyces cerevisiae, aiming to uncover the network's generic large-scale properties and the impact of the proteins' function and cellular localization on the network topology. we show how each database supports a scale-free, topology with hierarchical modularity, indicating that these features represent a robust and generic property of the protein interactions network. we also find strong correlations between the network's structure and the functional role and subcellular localization of its protein constituents, concluding that most functional and/or localization classes appear as relatively segregated subnetworks of the full protein interaction network. the uncovered systematic differences between the four protein interaction databases reflect their relative coverage for different functional and localization classes and provide a guide for their utility in various bioinformatics studies.
26	271	article	curr opin microbiol	\N	\N	\N	11	7	3	2004	jun	2004-11-12 20:07:36	department of chemistry, umist, faraday building, sackville st, po box 88, manchester m60 1qd, uk. dbk@umist.ac.uk	metabolomics and systems biology: making sense of the soup.	novel techniques for acquiring metabolomics data continue to emerge. such data require proper storage in suitably configured databases, which then permit one to establish the size of microbial metabolomes (hundreds of major metabolites) and allow the nature, organisation and control of metabolic networks to be investigated. a variety of algorithms for metabolic network reconstruction coupled to suitable modelling algorithms are the ground substances for the development of metabolic network and systems biology. even qualitative models of metabolic networks, when subject to stoichiometric constraints, can prove highly informative, and are the first step to the quantitative models, which alone can allow the true representation of complex biochemical systems.
27	306	article	eur j biochem	\N	\N	\N	10	269	16	2002	aug	2004-11-13 13:28:15	institute of biological sciences, university of wales, aberystwyth, uk.	schemes of flux control in a model of saccharomyces cerevisiae glycolysis.	we used parameter scanning to emulate changes to the limiting rate for steps in a fitted model of glucose-derepressed yeast glycolysis. three flux-control regimes were observed, two of which were under the dominant control of hexose transport, in accordance with various experimental studies and other model predictions. a third control regime in which phosphofructokinase exerted dominant glycolytic flux control was also found, but it appeared to be physiologically unreachable by this model, and all realistically obtainable flux control regimes featured hexose transport as a step involving high flux control.
28	310	article	nat rev mol cell biol	\N	\N	nature publishing group	8	2	12	2001	dec	2004-11-13 17:42:44	department of biology, virginia polytechnic institute and state university, blacksburg, virginia 24061, usa. tyson@vt.edu	network dynamics and cell physiology	complex assemblies of interacting proteins carry out most of the interesting jobs in a cell, such as metabolism, {dna} synthesis, movement and information processing. these physiological properties play out as a subtle molecular dance, choreographed by underlying regulatory networks. to understand this dance, a new breed of theoretical molecular biologists reproduces these networks in computers and in the mathematical language of dynamical systems.
29	311	article	nature	\N	\N	nature publishing group	3	403	6767	2000	jan	2005-05-02 02:23:00	department of molecular biology and physics, princeton university, new jersey 08544, usa. melowitz@princeton.edu	a synthetic oscillatory network of transcriptional regulators	networks of interacting biomolecules carry out many essential functions in living cells1, but the 'design principles' underlying the functioning of such intracellular networks remain poorly understood, despite intensive efforts including quantitative analysis of relatively simple systems2. here we present a complementary approach to this problem: the design and construction of a synthetic network to implement a particular function. we used three transcriptional repressor systems that are not part of any natural biological clock3, 4, 5 to build an oscillating network, termed the repressilator, in escherichia coli. the network periodically induces the synthesis of green fluorescent protein as a readout of its state in individual cells. the resulting oscillations, with typical periods of hours, are slower than the cell-division cycle, so the state of the oscillator has to be transmitted from generation to generation. this artificial clock displays noisy behaviour, possibly because of stochastic fluctuations of its components. such 'rational network design' may lead both to the engineering of new cellular behaviours and to an improved understanding of naturally occurring networks.
30	448	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2004-11-15 03:25:07	\N	information foraging	information foraging theory is an approach to understanding how strategies and technologies for information seeking, gathering, and consumption are adapted to the flux of information in the environment. the theory assumes that people, when possible, will modify their strategies or the structure of the environment to maximize their rate of gaining valuable information. the theory is developed by (a) adaptation (rational) analysis of information foraging problems and (b) a detailed process model (adaptive control of thought in information foraging [act-if]). the adaptation analysis develops (a) information patch models, which deal with time allocation and information filtering and enrichment activities in environments in which information is encountered in clusters; (b) information scent models, which address the identification of information value from proximal cues; and (c) information diet models, which address decisions about the selection and pursuit of information items. act-if is instantiated as a production system model of people interacting with complex information technology. humans actively seek, gather, share, and consume information to a degree unapproached by other organisms. ours might properly be characterized as a species of informavores (dennett, 1991). our adaptive success depends to a large extent on a vast and complex
31	459	article	genome biology	\N	\N	\N	\N	3	9	2002	aug	2004-11-15 14:02:17	department of cell and molecular biology, university of california at berkeley, berkeley, ca 94720-3206, usa. spellman@fruitfly.org	design and implementation of microarray gene expression markup language ({mage}-{ml}).	{background}: meaningful exchange of microarray data is currently difficult because it is rare that published data provide sufficient information depth or are even in the same format from one publication to another. only when data can be easily exchanged will the entire biological community be able to derive the full benefit from such microarray studies. {results}: to this end we have developed three key ingredients towards standardizing the storage and exchange of microarray data. first, we have created a minimal information for the annotation of a microarray experiment ({miame})-compliant conceptualization of microarray experiments modeled using the unified modeling language ({uml}) named {mage}-{om} (microarray gene expression object model). second, we have translated {mage}-{om} into an {xml}-based data format, {mage}-{ml}, to facilitate the exchange of data. third, some of us are now using {mage} (or its progenitors) in data production settings. finally, we have developed a freely available software tool kit ({mage}-{stk}) that eases the integration of {mage}-{ml} into end users' systems. {conclusions}: {mage} will help microarray data producers and users to exchange information by providing a common platform for data exchange, and {mage}-{stk} will make the adoption of {mage} easier.
32	487	article	social psychology quarterly	\N	\N	\N	20	56	3	1993	\N	2004-11-15 16:54:25	\N	a theory of {problem-solving} behavior	in this paper we develop a formal, testable theory of problem-solving behavior with special relevance to individuals and small groups. the theory is consistent with principles drawn from operant behavior and social exchange theories but also incorporates elements of cognitive psychology. problem solving is defined as a nonroutine activity oriented toward changing an undesirable state of affairs. the focus on change differentiates problem solving from coping, which is oriented toward relieving feelings of stress. a decision-making model is presented, which takes the problem-solving process through its latter stages. the theory is based on two axioms and three theorems pertaining to the process of decision making. these axioms and theorems serve as the foundation for deriving 14 theorems that establish the antecedent conditions affecting decisions relevnat to each of four stages in the problem-solving process. this theory is distinguished from other problem-solving theories in its effort to account for conditions leading to awareness of problems and in its emphasis on generic problem-solving processes rather than on the effectiveness of problem-solving outcomes.
33	501	article	nucleic acids research	\N	\N	\N	4	27	1	1999	jan	2004-11-15 17:04:25	gsf-forschungszentrum f\\"{u}r umwelt und gesundheit, munich information center for protein sequences, am max-planck-instut f\\"{u}r biochemie, am klopferspitz 18, d-82152 martinsried, germany. mewes@mips.biochem.mpg.de	{mips}: a database for genomes and protein sequences	the munich information center for protein sequences ({mips}-{gsf}), martinsried near munich, germany, develops and maintains genome oriented databases. it is commonplace that the amount of sequence data available increases rapidly, but not the capacity of qualified manual annotation at the sequence databases. therefore, our strategy aims to cope with the data stream by the comprehensive application of analysis tools to sequences of complete genomes, the systematic classification of protein sequences and the active support of sequence analysis and functional genomics projects. this report describes the systematic and up-to-date analysis of genomes ({pedant}), a comprehensive database of the yeast genome ({mygd}), a database reflecting the progress in sequencing the arabidopsis thaliana genome ({matd}), the database of assembled, annotated human {est} clusters ({mest}), and the collection of protein sequence data within the framework of the {pir}-international protein sequence database (described elsewhere in this volume). {mips} provides access through its {www} server (http://www.mips.biochem.mpg.de) to a spectrum of generic databases, including the above mentioned as well as a database of protein families ({protfam}), the {mitop} database, and the all-against-all {fasta} database.
34	516	electronic	\N	\N	\N	\N	\N	\N	\N	2004	jun	2004-11-15 17:12:26	\N	random networks with tunable degree distribution and clustering	we present an algorithm for generating random networks with arbitrary degree distribution and clustering (frequency of triadic closure). we use this algorithm to generate networks with exponential, power law, and poisson degree distributions with variable levels of clustering. such networks may be used as models of social networks and as a testable null hypothesis about network structure. finally, we explore the effects of clustering on the point of the phase transition where a giant component forms in a random network, and on the size of the giant component. some analysis of these effects is presented.
35	644	misc	\N	\N	\N	\N	\N	\N	\N	2004	jul	2004-11-15 21:22:54	\N	social structure and opinion formation	we present a dynamical theory of opinion formation that takes explicitly into account the structure of the social network in which in- dividuals are embedded. the theory predicts the evolution of a set of opinions through the social network and establishes the existence of a martingale property, i.e. that the expected weighted fraction of the population that holds a given opinion is constant in time. most importantly, this weighted fraction is not either zero or one, but corresponds to a non-trivial distribution of opinions in the long time limit. this co-existence of opinions within a social network is in agreement with the often observed locality effect, in which an opinion or a fad is localized to given groups without infecting the whole society. we verified these predictions, as well as those concerning the fragility of opinions and the importance of highly connected individuals in opinion formation, by performing computer experiments on a number of social networks.
36	722	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2004-11-16 18:50:09	\N	{edutella}: a {p2p} networking infrastructure based on {rdf}	metadata for the world wide web is important, but metadata for peer-to-peer (p2p) networks is absolutely crucial. in this paper we discuss the open source project edutella which builds upon metadata standards defined for the www and aims to provide an rdf-based metadata infrastructure for p2p applications, building on the recently announced jxta framework. we describe the goals and main services this infrastructure will provide and the architecture to connect edutella peers based on exchange of rdf metadata. as the query service is one of the core services of edutella, upon which other services are built, we specify in detail the edutella common data model (ecdm) as basis for the edutella query exchange language (rdf-qel-i) and format implementing distributed queries over the edutella network. finally, we shortly discuss registration and mediation services, and introduce the prototype and application scenario for our current edutella aware peers.
37	769	article	physica d: nonlinear phenomena	\N	\N	\N	19	143	1-4	2000	sep	2004-11-18 00:14:50	\N	from kuramoto to crawford: exploring the onset of synchronization in populations of coupled oscillators	the kuramoto model describes a large population of coupled limit-cycle oscillators whose natural frequencies are drawn from some prescribed distribution. if the coupling strength exceeds a certain threshold, the system exhibits a phase transition: some of the oscillators spontaneously synchronize, while others remain incoherent. the mathematical analysis of this bifurcation has proved both problematic and fascinating. we review 25 years of research on the kuramoto model, highlighting the false turns as well as the successes, but mainly following the trail leading from kuramoto's work to crawford's recent contributions. it is a lovely winding road, with excursions through mathematical biology, statistical physics, kinetic theory, bifurcation theory, and plasma physics.
38	781	article	bioinformatics	\N	\N	oxford university press	7	19	4	2003	mar	2004-11-18 17:38:08	control and dynamical systems, mc 107-81, california institute of technology, pasadena, ca 91125, usa. sysbio-team@caltech.edu	the systems biology markup language ({sbml}): a medium for representation and exchange of biochemical network models	motivation: molecular biotechnology now makes it possible to build elaborate systems models, but the systems biology community needs information standards if models are to be shared, evaluated and developed cooperatively.
39	816	article	\N	\N	\N	\N	\N	\N	\N	2004	aug	2004-11-19 18:16:33	\N	introduction to random boolean networks	the goal of this tutorial is to promote interest in the study of random boolean networks (rbns). these can be very interesting models, since one does not have to assume any functionality or particular connectivity of the networks to study their generic properties. like this, rbns have been used for exploring the configurations where life could emerge. the fact that rbns are a generalization of cellular automata makes their research a very important topic. the tutorial, intended for a broad audience, presents the state of the art in rbns, spanning over several lines of research carried out by different groups. we focus on research done within artificial life, as we cannot exhaust the abundant research done over the decades related to rbns.
40	841	article	nature	\N	\N	nature publishing group	3	397	6715	1999	jan	2004-11-20 04:48:16	department of molecular biology, princeton university, new jersey 08544, usa.	robustness in bacterial chemotaxis.	networks of interacting proteins orchestrate the responses of living cells to a variety of external stimuli, but how sensitive is the functioning of these protein networks to variations in their biochemical parameters? one possibility is that to achieve appropriate function, the reaction rate constants and enzyme concentrations need to be adjusted in a precise manner, and any deviation from these 'fine-tuned' values ruins the network's performance. an alternative possibility is that key properties of biochemical networks are robust; that is, they are insensitive to the precise values of the biochemical parameters. here we address this issue in experiments using chemotaxis of escherichia coli, one of the best-characterized sensory systems. we focus on how response and adaptation to attractant signals vary with systematic changes in the intracellular concentration of the components of the chemotaxis network. we find that some properties, such as steady-state behaviour and adaptation time, show strong variations in response to varying protein concentrations. in contrast, the precision of adaptation is robust and does not vary with the protein concentrations. this is consistent with a recently proposed molecular mechanism for exact adaptation, where robustness is a direct consequence of the network's architecture.
41	943	article	journal of documentation	\N	\N	emerald group publishing limited	19	59	3	2003	may	2004-11-23 06:24:11	\N	serendipity and information seeking: an empirical study	"serendipity" has both a classical origin in literature and a more modern manifestation where it is found in the descriptions of the problem solving and knowledge acquisition of humanities and science scholars. studies of information retrieval and information seeking have also discussed the utility of the notion of serendipity. some have implied that it may be stimulated, or that certain people may "encounter" serendipitous information more than others. all to some extent accept the classical definition of serendipity as a "fortuitous" accident. the analysis presented here is part of a larger study concerning the information-seeking behaviour of interdisciplinary scholars. this paper considers the nature of serendipity in information-seeking contexts, and reinterprets the notion of serendipity as a phenomenon arising from both conditions and strategies - as both a purposive and a non-purposive component of information seeking and related knowledge acquisition.
42	1019	article	bioinformatics	\N	\N	\N	\N	20 Suppl 1	\N	2004	aug	2004-11-24 20:16:44	school of computer science \\& engineering, hebrew university, jerusalem 91904, israel.	inferring quantitative models of regulatory networks from expression data.	{motivation}: genetic networks regulate key processes in living cells. various methods have been suggested to reconstruct network architecture from gene expression data. however, most approaches are based on qualitative models that provide only rough approximations of the underlying events, and lack the quantitative aspects that are critical for understanding the proper function of biomolecular systems. {results}: we present fine-grained dynamical models of gene transcription and develop methods for reconstructing them from gene expression data within the framework of a generative probabilistic model. unlike previous works, we employ quantitative transcription rates, and simultaneously estimate both the kinetic parameters that govern these rates, and the activity levels of unobserved regulators that control them. we apply our approach to expression datasets from yeast and show that we can learn the unknown regulator activity profiles, as well as the binding affinity parameters. we also introduce a novel structure learning algorithm, and demonstrate its power to accurately reconstruct the regulatory network from those datasets.
43	1021	article	journal of computational biology	\N	\N	mary ann liebert, inc., publishers	19	7	3-4	2000	aug	2004-11-24 20:22:42	school of computer science and engineering, hebrew university, jerusalem, israel. nir@cs.huji.ac.il	using bayesian networks to analyze expression data	{dna} hybridization arrays simultaneously measure the expression level for thousands of genes. these measurements provide a "snapshot" of transcription levels within the cell. a major challenge in computational biology is to uncover, from such measurements, gene/protein interactions and key biological features of cellular systems. in this paper, we propose a new framework for discovering interactions between genes based on multiple expression measurements. this framework builds on the use of bayesian networks for representing statistical dependencies. a bayesian network is a graph-based model of joint multivariate probability distributions that captures properties of conditional independence between variables. such models are attractive for their ability to describe complex stochastic processes and because they provide a clear methodology for learning from (noisy) observations. we start by showing how bayesian networks can describe interactions between genes. we then describe a method for recovering gene interactions from microarray data using tools for learning bayesian networks. finally, we demonstrate this method on the s. cerevisiae cell-cycle measurements of spellman et al. (1998).
44	1022	article	pacific symposium on biocomputing. pacific symposium on biocomputing	\N	\N	\N	12	\N	\N	2002	\N	2004-11-24 20:28:30	duke university, department of computer science, box 90129, durham, nc 27708-0129, usa.	combining location and expression data for principled discovery of genetic regulatory network models.	we develop principled methods for the automatic induction (discovery) of genetic regulatory network models from multiple data sources and data modalities. models of regulatory networks are represented as bayesian networks, allowing the models to compactly and robustly capture probabilistic multivariate statistical dependencies between the various cellular factors in these networks. we build on previous bayesian network validation results by extending the validation framework to the context of model induction, leveraging heuristic simulated annealing search algorithms and posterior model averaging. using expression data in isolation yields results inconsistent with location data so we incorporate genomic location data to guide the model induction process. we combine these two data modalities by allowing location data to influence the model prior and expression data to influence the model likelihood. we demonstrate the utility of this approach by discovering genetic regulatory models of thirty-three variables involved in s. cerevisiae pheromone response. the models we automatically generate are consistent with the current understanding regarding this regulatory network, but also suggest new directions for future experimental investigation.
45	1023	article	pac symp biocomput	\N	\N	\N	11	\N	\N	2001	\N	2004-11-24 20:48:19	mit laboratory for computer science, 545 technology square, cambridge, ma 02139, usa.	using graphical models and genomic expression data to statistically validate models of genetic regulatory networks.	we propose a model-driven approach for analyzing genomic expression data that permits genetic regulatory networks to be represented in a biologically interpretable computational form. our models permit latent variables capturing unobserved factors, describe arbitrarily complex (more than pair-wise) relationships at varying levels of refinement, and can be scored rigorously against observational data. the models that we use are based on bayesian networks and their extensions. as a demonstration of this approach, we utilize 52 genomes worth of affymetrix {genechip} expression data to correctly differentiate between alternative hypotheses of the galactose regulatory network in s. cerevisiae. when we extend the graph semantics to permit annotated edges, we are able to score models describing relationships at a finer degree of specification.
46	1064	misc	\N	\N	\N	\N	\N	\N	\N	-1	\N	2004-11-25 19:17:58	\N	integrated {semantic-syntactic} video modeling for search and browsing	video processing and computer vision communities usually employ shot-based or object-based structural video models and associate low-level (color, texture, shape, and motion) and semantic descriptions (textual annotations) with these structural (syntactic) elements. database and information retrieval communities, on the other hand, employ entity-relation or object-oriented models to model the semantics of multimedia documents. this paper proposes a new generic integrated semantic-syntactic video model to include all of these elements within a single framework to enable structured video search and browsing combining textual and low-level descriptors. the proposed model includes semantic entities (video objects and events) and the relations between them. we introduce a new "actor" entity to enable grouping of object roles in specific events. this context-dependent classification of attributes of an object allows for more efficient browsing and retrieval. the model also allows for decomposition of events into elementary motion units and elementary reaction/interaction units in order to access mid-level semantics and low-level video features. the instantiations of the model are expressed as graphs. users can formulate flexible queries that can be translated into such graphs. alternatively, users can input query graphs by editing an abstract model (model template). search and retrieval is accomplished by matching the query graph with those instantiated models in the database. examples and experimental results are provided to demonstrate the effectiveness of the proposed integrated modeling and querying framework.
47	1164	inproceedings	\N	icfp	\N	acm	11	37	9	2002	sep	2004-11-30 02:55:27	new york, ny, usa	composable and compilable macros: you want it when?	many macro systems, especially for lisp and scheme, allow macro transformers to perform general computation. moreover, the language for implementing compile-time macro transformers is usually the same as the language for implementing run-time functions. as a side effect of this sharing, implementations tend to allow the mingling of compile-time values and run-time values, as well as values from separate compilations. such mingling breaks programming tools that must parse code without executing it. macro implementors avoid harmful mingling by obeying certain macro-definition protocols and by inserting phase-distinguishing annotations into the code. however, the annotations are fragile, the protocols are not enforced, and programmers can only reason about the result in terms of the compiler's implementation. {mzscheme}---the language of the {plt} scheme tool suite---addresses the problem through a macro system that separates compilation without sacrificing the expressiveness of macros.
48	1380	inproceedings	\N	popl	\N	acm	13	\N	\N	1995	\N	2004-12-01 23:49:23	new york, ny, usa	a call-by-need lambda calculus	the mismatch between the operational semantics of the lambda calculus and the actual behavior of implementations is a major obstacle for compiler writers. they cannot explain the behavior of their evaluator in terms of source level syntax, and they cannot easily compare distinct implementations of different lazy strategies. in this paper we derive an equational characterization of call-by-need and prove it correct with respect to the original lambda calculus. the theory is a strictly smaller theory than the lambda calculus. immediate applications of the theory concern the correctness proofs of a number of implementation strategies,  e.g. , the call-by-need continuation passing transformation and the realization of sharing via assignments.
49	2100	article	science	science	\N	american association for the advancement of science	5	292	5518	2001	may	2004-12-06 13:33:12	the institute for systems biology, 4225 roosevelt way ne, suite 200, seattle, wa 98105, usa. tideker@systemsbiology.org	integrated genomic and proteomic analyses of a systematically perturbed metabolic network	we demonstrate an integrated approach to build, test, and refine a model of a cellular pathway, in which perturbations to critical pathway components are analyzed using {dna} microarrays, quantitative proteomics, and databases of known physical interactions. using this approach, we identify 997 messenger {rnas} responding to 20 systematic perturbations of the yeast galactose-utilization pathway, provide evidence that approximately 15 of 289 detected proteins are regulated posttranscriptionally, and identify explicit physical interactions governing the cellular response to each perturbation. we refine the model through further iterations of perturbation and global measurements, suggesting hypotheses about the regulation of galactose utilization and physical interactions between this and a variety of other metabolic pathways.
50	2118	article	journal of economic dynamics and control	\N	\N	\N	18	28	8	2004	jun	2004-12-06 16:53:10	\N	network structure and the diffusion of knowledge	this paper models knowledge diffusion as a barter process in which agents exchange different types of knowledge. this is intended to capture the observed practice of informal knowledge trading. agents are located on a network and are directly connected with a small number of other agents. agents repeatedly meet those with whom direct connections exist and trade if mutually profitable trades exist. in this way knowledge diffuses throughout the economy. we examine the relationship between network architecture and diffusion performance. we consider the space of structures that fall between, at one extreme, a network in which every agent is connected to  n  nearest neighbours, and at the other extreme a network with each agent being connected to, on average,  n  randomly chosen agents. we find that the performance of the system exhibits clear 'small world' properties, in that the steady-state level of average knowledge is maximal when the structure is a small world (that is, when most connections are local, but roughly 10 percent of them are long distance). the variance of knowledge levels among agents is maximal in the small world region, whereas the coefficient of variation is minimal. we explain these results as reflecting the dynamics of knowledge transmission as affected by the architecture of connections among agents.
51	2837	article	nat biotech	\N	\N	nature publishing group	5	22	12	2004	dec	2005-05-19 20:29:47	\N	improved monomeric red, orange and yellow fluorescent proteins derived from discosoma sp. red fluorescent protein	fluorescent proteins are genetically encoded, easily imaged reporters crucial in biology and biotechnology1, 2. when a protein is tagged by fusion to a fluorescent protein, interactions between fluorescent proteins can undesirably disturb targeting or function3. unfortunately, all wild-type yellow-to-red fluorescent proteins reported so far are obligately tetrameric and often toxic or disruptive4, 5. the first true monomer was {mrfp1}, derived from the discosoma sp. fluorescent protein  ” {dsred}” by directed evolution first to increase the speed of maturation6, then to break each subunit interface while restoring fluorescence, which cumulatively required 33 substitutions7. although {mrfp1} has already proven widely useful, several properties could bear improvement and more colors would be welcome. we report the next generation of monomers. the latest red version matures more completely, is more tolerant of n-terminal fusions and is over tenfold more photostable than {mrfp1}. three monomers with distinguishable hues from yellow-orange to red-orange have higher quantum efficiencies.
52	3123	misc	\N	\N	\N	\N	\N	\N	\N	-1	\N	2004-12-08 20:32:41	\N	traits: composable units of behavior	inheritance is the fundamental reuse mechanism in object-oriented programming languages; its most prominent variants are single inheritance, multiple inheritance, and mixin inheritance. in the first part of this paper, we identify and illustrate the conceptual and practical reusability problems that arise with these forms of inheritance. we then present a simple compositional model for structuring object-oriented programs, which we call traits. traits are essentially groups of methods that...
53	3200	article	the journal of academic librarianship	\N	\N	\N	5	30	6	2004	nov	2004-12-09 18:34:45	\N	distance teaching: comparing two online information literacy courses	this article explores the similarities and differences between two asynchronous online information literacy courses. details of the courses and how the {acrl} information literacy standards are incorporated will be outlined. in exploring distance learning and distance teaching, the article will discuss issues related to online information literacy learning experiences and suggest ways to address those issues and improve teaching and learning.
54	3271	article	science	\N	\N	\N	2	280	5360	1998	\N	2004-12-09 23:42:21	\N	searching the world wide web	the coverage and recency of the major world wide web search engines was analyzed, yielding some surprising results. the coverage of any one engine is significantly limited: no single engine indexes more than about one-third of the \\&quot;indexable web,\\&quot; the coverage of the six engines investigated varies by an order of magnitude, and combining the results of the six engines yields about 3.5 times as many documents on average as compared with the results from only one engine. analysis of the overlap...
55	3298	article	lisp and symbolic computation	\N	\N	\N	\N	4	3	1991	\N	2004-12-10 19:44:48	\N	organizing programs without classes	. all organizational functions carried out by classes can be accomplished in a simple and natural way by object inheritance in classless languages, with no need for special mechanisms. a single model---dividing types into prototypes and traits---supports sharing of behavior and extending or replacing representations. a natural extension, dynamic object inheritance, can model behavioral modes. object inheritance can also be used to provide structured name spaces for well-known objects. classless ...
56	3450	misc	\N	\N	\N	\N	\N	\N	\N	2004	\N	2004-12-13 09:24:49	\N	propagation of trust and distrust	a (directed) network of people connected by ratings or trust scores, and a model for propagating those trust scores, is a fundamental building block in many of today's most successful e-commerce and recommendation systems. we develop a framework of trust propagation schemes, each of which may be appropriate in certain circumstances, and evaluate the schemes on a large trust network consisting of 800k trust scores expressed among 130k people. we show that a small number of expressed trusts/distrust per individual allows us to predict trust between any two people in the system with high accuracy. our work appears to be the first to incorporate distrust in a computational trust propagation setting.
57	3521	misc	\N	\N	\N	\N	\N	\N	\N	2001	\N	2004-12-13 13:22:56	\N	a conceptual framework and a toolkit for supporting the rapid prototyping of context-aware applications	computing devices and applications are now used beyond the desktop, in diverse environments, and this trend toward ubiquitous computing is accelerating. one challenge that remains in this emerging research field is the ability to enhance the behavior of any application by informing it of the context of its use. by context, we refer to any information that characterizes a situation related to the interaction between humans, applications, and the surrounding environment. context-aware applications promise richer and easier interaction, but the current state of research in this field is still far removed from that vision. this is due to 3 main problems: (a) the notion of context is still ill defined, (b) there is a lack of conceptual models and methods to help drive the design of context-aware applications, and (c) no tools are available to jump-start the development of context-aware applications. in this anchor article, we address these 3 problems in turn. we first define context, identify categories of contextual information, and characterize context-aware application behavior. though the full impact of context-aware computing requires understanding very subtle and high-level notions of context, we are focusing our efforts on the pieces of context that can be inferred automatically from sensors in a physical environment. we then present a conceptual framework that separates the acquisition and representation of context from the delivery and reaction to context by a context-aware application. we have built a toolkit, the context toolkit, that instantiates this conceptual framework and supports the rapid development of a rich space of context-aware applications. we illustrate the usefulness of the conceptual framework by describing a number of context-aware applications that have been prototyped using the context toolkit. we also demonstrate how such a framework can support the investigation of important research challenges in the area of context-aware computing.
58	3532	proceedings	\N	proceedings of the fourteenth international joint conference on artificial intelligence (ijcai-95)	\N	morgan kaufmann publishers inc.: san mateo, ca, usa	5	\N	\N	1995	\N	2004-12-13 17:33:49	montreal, quebec, canada	letizia: an agent that assists web browsing	letizia is a user interface agent that assists a user browsing the world wide web. as the user operates a conventional web browser such as netscape, the agent tracks user behavior and attempts to anticipate items of interest by doing concurrent, autonomous exploration of links from the user's current position. the agent automates a browsing strategy consisting of a best-first search augmented by heuristics inferring user interest from browsing behavior.  1 introduction  letizia Ãlvarez de...
59	3560	inproceedings	\N	proceedings of the 15th annual international acm sigir conference on research and development in information retrieval	sigir	acm	11	\N	\N	1992	\N	2004-12-14 01:50:42	new york, ny, usa	{scatter/gather}: a cluster-based approach to browsing large document collections	document clustering has not been well received as an information retrieval tool. objections to its use fall into two main categories: first, that clustering is too slow for large corpora (with running time often quadratic in the number of documents); and second, that clustering does not appreciably improve {retrieval.we} argue that these problems arise only when clustering is used in an attempt to improve conventional search techniques. however, looking at clustering as an information access tool in its own right obviates these objections, and provides a powerful new access paradigm. we present a document browsing technique that employs document clustering as its primary operation. we also present fast (linear time) clustering algorithms which support this interactive browsing paradigm.
60	3562	proceedings	\N	knowledge discovery and data mining	\N	\N	4	\N	\N	1998	\N	2004-12-14 01:51:26	\N	initialization of iterative refinement clustering algorithms	iterative refinement clustering algorithms (e.g. {k-means
61	4104	article	library collections, acquisitions, and technical services	\N	\N	\N	12	28	4	2004	{	2004-12-16 18:47:30	\N	collection development for new librarians: advice from the trenches	there are many challenges facing new librarians in the academic environment, including collection development. this article analyzes the topic of collection development and how it relates to new professionals in the field of librarianship. the article contains a literature review of papers discussing the collection development curriculum in library and information science programs, expected skills required of collection development offices, and library training programs for new librarians. the article also provides practical advise by recent graduates and their collection development experiences. topics of discussion include acclimation to a new environment, collection development policies and procedures, liaison work, resource selection, and time management.
62	4119	article	european management journal	\N	\N	\N	9	19	6	2001	dec	2004-12-17 04:25:48	\N	knowledge management::  the benefits and limitations of computer systems	much organisational effort has been put into knowledge management initiatives in recent years, and information and communication technologies ({icts}) have been central to many of these initiatives. however, organisations have found that levering knowledge through {icts} is often hard to achieve. this paper addresses the question of why this is the case, and what we can learn of value to the future practice of knowledge management. the analysis in the paper is based on a human-centred view of knowledge, emphasising the deep tacit knowledge which underpins human thought and action, and the complex sense-reading and sense-giving processes which human beings carry out in communicating with each other and 'sharing' knowledge. the paper concludes that computer-based systems can be of benefit in knowledge-based activities, but only if we are careful in using such systems to support the development and communication of human meaning.
63	4280	article	plos biol	\N	\N	public library of science	\N	2	12	2004	dec	2004-12-19 17:14:50	laboratory of populations, rockefeller and columbia universities, new york, new york, usa. cohen@rockefeller.edu	mathematics is biology's next microscope, only better; biology is mathematics' next physics, only better	joel cohen offers a historical and prospective analysis of the relationship between mathematics and biology.
64	4297	article	science	\N	\N	american association for the advancement of science	4	306	5704	2004	dec	2004-12-20 03:28:01	department of chemistry and biochemistry, university of california, santa barbara, ca 93106-9510, usa.	building programmable jigsaw puzzles with {rna}	one challenge in supramolecular chemistry is the design of versatile, self-assembling building blocks to attain total control of arrangement of matter at a molecular level. we have achieved reliable prediction and design of the three-dimensional structure of artificial {rna} building blocks to generate molecular jigsaw puzzle units called tectosquares. they can be programmed with control over their geometry, topology, directionality, and addressability to algorithmically self-assemble into a variety of complex nanoscopic fabrics with predefined periodic and aperiodic patterns and finite dimensions. this work emphasizes the modular and hierarchical characteristics of {rna} by showing that small {rna} structural motifs can code the precise topology of large molecular architectures. it demonstrates that fully addressable materials based on {rna} can be synthesized and provides insights into self-assembly processes involving large populations of {rna} molecules.
65	4303	inproceedings	\N	proceedings of the twenty-ninth annual acm symposium on theory of computing	stoc	acm	9	\N	\N	1997	\N	2004-12-20 08:42:35	new york, ny, usa	consistent hashing and random trees: distributed caching protocols for relieving hot spots on the world wide web	an abstract is not available.
66	4304	misc	\N	\N	\N	\N	\N	\N	\N	2001	\N	2004-12-20 08:45:22	\N	{peer-to-peer} architecture case study: gnutella network	despite recent excitement generated by the p2p paradigm and despite surprisingly fast deployment of some p2p applications, there are few quantitative evaluations of p2p system behavior. due to its open architecture and achieved scale, gnutella is an interesting p2p architecture case study. gnutella, like most other p2p applications, builds at the application level a virtual network with its own routing mechanisms. the topology of this overlay network and the routing mechanisms used have a significant influence on application properties such as performance, reliability, and scalability. we built a 'crawler' to extract the topology of gnutella's application level network, we analyze the topology graph and evaluate generated network traffic. we find that although gnutella is not a pure power-law network, its current configuration has the benefits and drawbacks of a power-law structure. these findings lead us to propose changes to the gnutella protocol and implementations that bring significant performance and scalability improvements
67	4448	inproceedings	\N	proceedings of the fifteenth annual acm symposium on principles of distributed computing	podc	acm	8	\N	\N	1996	\N	2004-12-21 16:42:41	new york, ny, usa	simple, fast, and practical non-blocking and blocking concurrent queue algorithms	an abstract is not available.
68	8851	article	technology in society	\N	\N	elsevier	12	25	4	2003	nov	2005-11-29 02:56:24	\N	wireless broadband drivers and their social implications	wireless local area networks now offer high-speed internet access at numerous locations in both public and private environments. associated with this rapid growth, numerous social implications come to the fore, especially relating to practices, such as the free use and shar- ing of bandwidth. using case-based comparative analysis, we examine three primary strate- gies involved in providing wireless broadband access. based on this research, we discuss the future of wi-fi growth, emergent competing technologies, and the broad social implications of this phenomenon.
69	32678	article	journal of management	\N	\N	elsevier science	22	29	6	2003	dec	2005-03-29 10:53:48	\N	the network paradigm in organizational research: a review and typology	in this paper, we review and analyze the emerging network paradigm in organizational research. we begin with a conventional review of recent research organized around recognized research streams. next, we analyze this research, developing a set of dimensions along which network studies vary, including direction of causality, levels of analysis, explanatory goals, and explanatory mechanisms. we use the latter two dimensions to construct a 2-by-2 table cross-classifying studies of network consequences into four canonical types: structural social capital, social access to resources, contagion, and environmental shaping. we note the rise in popularity of studies with a greater sense of agency than was traditional in network research. 10.1016/s0149-2063_03_00087-4
70	45311	article	higher education management and policy	\N	\N	oecd - organisation for economic co-operation and development	\N	\N	2	-1	\N	2004-12-29 20:31:50	\N	higher education management and policy: volume 16 issue 2 complete edition	the journal of oecd&#39;s programme on institutional management in higher education. this issue features article on fair access, assessment of personnel, a merger, private university policy initiatives, accessibility and equity, internationalisation, and enrollment management.  - a faustian bargain&#63; institutional responses to national and international rankings -  - &#34;standards will drop&#34; - and other fears about the equality agenda in higher education -
71	52185	article	educational media international	\N	\N	routledge	17	41	3	2004	\N	2006-05-09 02:51:55	\N	becoming an online teacher: adapting to a changed environment for teaching and learning in higher education	advancements in online technologies have facilitated a convergence of distance and campus-based learning and, thus, offer new opportunities for all students through better access to resources, increased interaction between staff and students and greater flexibility in place and time. however, the transition to online teaching and learning presents new challenges as the roles and expectations of both staff and students evolve. an online teacher must create a coherent learning experience for students with whom they may not meet face-to-face and, therefore, must develop new support strategies that maintain motivation and encourage interaction. adapting student-centred approaches to the online environment has required the development of new skills and changes to teaching practices. this paper presents an analysis of the changed environment for teachers and learners in a post-graduate coursework programme based on constructivist principles that has moved from predominately on-campus delivery to online mode. the authors examine the impact of changes to teaching and learning over the past 5 years of the programme's development and reflect on the implications of these for becoming an online teacher. <{b>devenir} un professeur en ligne : adaptation \\`{a} un environnement vari\\'{e} d'enseignement et de formation sup\\'{e}rieure.</b> les progr\\`{e}s de technologie en ligne ont facilit\\'{e} une convergence de formation par correspondance et de formation au campus et ceci offre de nouvelles possibilit\\'{e}s pour tous les \\'{e}tudiants par un meilleur acc\\`{e}s aux ressources, une interaction augment\\'{e}e entre le corps enseignant et les \\'{e}tudiants et une plus grande flexibilit\\'{e} temporelle et coh\\'{e}rente. cependant, la transition vers l'enseignement et la formation en ligne repr\\'{e}sente de nouveaux d\\'{e}fis \\'{e}tant donn\\'{e} que les r\\^{o}les et les expectatives du corps enseignant ainsi que des \\'{e}tudiants alt\\`{e}rent. un professeur en ligne doit cr\\'{e}er une exp\\'{e}rience de formation coh\\'{e}rente pour des \\'{e}tudiants qu'il ne rencontre pas personnellement et par cons\\'{e}quent il doit d\\'{e}velopper une nouvelle strat\\'{e}gie de soutien qui maintient la motivation et encourage l'interaction. l'adaption des approches concentr\\'{e}es sur les \\'{e}tudiants \\`{a} un environnement en ligne exige un d\\'{e}veloppement de nouveaux talents et des changements de pratiques d'enseignement. cet expos\\'{e} pr\\'{e}sente une analyse de la convergence d'environnements de formation qui a tourn\\'{e} d'une assistance pr\\'{e}dominante du c\\^{o}t\\'{e} du campus \\`{a} un mode en ligne et de l'effet de cette convergence sur les professeurs et les \\'{e}tudiants d'un programme d'\\'{e}tudes bas\\'{e} sur des principes constructifs. les auteurs examinent l'impact des changements d'enseignement et de formation pendant les cinq derni\\`{e}res ann\\'{e}es du d\\'{e}veloppement de ce programme et consid\\`{e}rent ses r\\'{e}percussions sur l'\\'{e}ducation d'un professeur en ligne. <{b>ausbildung} zur {online-lehrkraft}: anpassung an ein ver\\"{a}ndertes lern- und lehrumfeld im bereich der hochschulausbildung.</b> die fortschritte in der {online-technologie} haben eine ann\\"{a}herung zwischen fernstudium und hochschulbasiertem lernen erm\\"{o}glicht, und dies er\\"{o}ffnet neue m\\"{o}glichkeiten f\\"{u}r alle studenten durch einen besseren zugang zu ressourcen, einer verbesserten interaktion zwischen lehrerkollegium und studenten und einer gr\\"{o}{\\ss}eren r\\"{a}umlichen und zeitlichen flexibilit\\"{a}t. der \\"{u}bergang zum {online-unterricht} und ?lernen stellt au{\\ss}erdem neue herausforderungen dar, da sich die rolle und die erwartungen sowohl des lehrerkollegiums als auch der studenten ver\\"{a}ndern. eine {online-lehrkraft} muss ein schl\\"{u}ssiges lernerlebnis f\\"{u}r studenten schaffen, die sie gegebenenfalls nie pers\\"{o}nlich kennen lernt. aus diesem grunde muss sie neue betreuungsstrategien entwickeln, die die motivation aufrechterhalten und die interaktion f\\"{o}rdern. durch die anpassung der studentischen lernans\\"{a}tze an ein {online-umfeld} ist die entwicklung neuer f\\"{a}higkeiten und eine \\"{a}nderung der lehrpraktiken erforderlich geworden. dieser bericht stellt eine analyse des ver\\"{a}nderten lernumfelds f\\"{u}r lehrer und studenten in einem auf konstruktivistischen prinzipien basierten weiterf\\"{u}hrenden studienprogramm vor, das von einer \\"{u}berwiegend hochschulbasierten wissensvermittlung zu {online-methoden} \\"{u}bergeht. die autoren untersuchen den einfluss der lehr- und lernver\\"{a}nderungen in den vergangenen f\\"{u}nf jahren der programmentwicklung und betrachten deren auswirkungen auf die ausbildung einer {online-lehrkraft}.
72	70786	article	studies in higher education	\N	\N	carfax publishing, part of the taylor \\&amp; francis group	\N	\N	1	-1	\N	2005-08-23 22:48:59	\N	an actor-network critique of community in higher education: implications for networked learning	this article provides an actor-network critique of ideas on community that are influential in higher education and draws implications for networked learning theory and practice. networked learning is examined as an educational movement which contains alternative models of learning but which offers to create a sense of virtual community within the structures of mass higher education. benedict andersonâ€™s work is drawn upon to understand the notion of community and to examine â€˜the nationâ€™ as  prototypical community. aspects of actor-network theory are discussed and illustrated, from which a critique of the idea of community in higher education is developed.
73	70816	unpublished	\N	\N	\N	\N	\N	\N	\N	1999	\N	2004-12-29 16:04:10	\N	codata and comonads in haskell	haskell, a wide spectrum, functional programming language, provides means to define and use an extremely rich variety of data including free, polymorphic datatypes, type classes, and data with additional computational structure abstracted by monads. somewhat less attention has been given to supporting abstract data types, which we shall call codata types. monomorphic, parameterless versions of codata types can be defined by modules, but haskell's module system is not comparably powerful with the class system.
74	71724	article	ieee\\slash acm transactions on networking	\N	\N	\N	16	1	4	1993	\N	2005-01-01 18:46:30	\N	random early detection gateways for congestion avoidance	this paper presents random early detection (red) gateways for congestion avoidance in packet-switched networks. the gateway detects incipient congestion by computing the average queue size. the gateway could notify connections of congestion either by dropping packets arriving at the gateway or by setting a bit in packet headers. when the average queue size exceeds a preset threshold, the gateway drops or marks each arriving packet with a certain probability, where the exact probability is a function of the average queue size. red gateways keep the average queue size low while allowing occasional bursts of packets in the queue. during congestion, the probability that the gateway notifies a particular connection to reduce its window is roughly proportional to that connection's share of the bandwidth through the gateway. red gateways are designed to accompany a transport-layer congestion control protocol such as tcp. the red gateway has no bias against bursty traffic and avoids the global synchronization of many connections decreasing their window at the same time. simulations of a tcp/ip network are used to illustrate the performance of red gateways.
75	71746	misc	\N	\N	\N	\N	\N	\N	\N	2004	\N	2005-01-02 15:05:35	\N	a measurement study of the bittorrent peer-to-peer file-sharing system	p2p systems for sharing content have become very popular over the last few  years. however, despite the increasing attention of both the research community  and large numbers of users, the actual behavior of these systems over prolonged periods  of time is still poorly understood. this paper presents a detailed measurement  study over a period of eight months of bittorrent/suprnova, a p2p file-sharing  system that is quickly gaining in popularity. in particular, we show measurement  results of...
76	71783	techreport	\N	\N	\N	\N	\N	\N	NOTTCS-TR-96-4	1996	\N	2005-01-02 19:21:50	\N	monadic parser combinators	in functional programming, a popular approach to building recursive descent parsers is to model parsers as functions, and to define higher-order functions (or combinators) that implement grammar constructions such as sequencing, choice, and repetition. such parsers form an instance of a monad , an algebraic structure from mathematics that has proved useful for addressing a number of computational problems. the purpose of this article is to provide a step-by-step tutorial on the monadic approach to building functional parsers, and to explain some of the benefits that result from exploiting monads. no prior knowledge of parser combinators or of monads is assumed. indeed, this article can also be viewed as a first introduction to the use of monads in programming.  2 graham hutton and erik meijer  contents  1 introduction 3 2 combinator parsers 4 2.1 the type of parsers 4 2.2 primitive parsers 4 2.3 parser combinators 5 3 parsers and monads 8 3.1 the parser monad 8 3.2 monad comprehension ...
77	73383	proceedings	\N	sigir	\N	acm press	7	\N	\N	2004	\N	2005-01-07 14:50:45	\N	document clustering by concept factorization	in this paper, we propose a new data clustering method called concept factorization that models each concept as a linear combination of the data points, and each data point as a linear combination of the concepts. with this model, the data clustering task is accomplished by computing the two sets of linear coefficients, and this linear coefficients computation is carried out by finding the non-negative solution that minimizes the reconstruction error of the data points. the cluster label of each data point can be easily derived from the obtained linear coefficients. this method differs from the method of clustering based on non-negative matrix factorization (nmf) in that it can be applied to data containing negative values and the method can be implemented in the kernel space. our experimental results show that the proposed data clustering method and its variations performs best among 11 algorithms and their variations that we have evaluated on both tdt2 and reuters-21578 corpus. in addition to its good performance, the new method also has the merit in its easy and reliable derivation of the clustering results
78	74514	article	current topics in medicinal chemistry	\N	\N	\N	13	4	7	2004	mar	2005-01-10 13:41:30	\N	guided docking approaches to {structure-based} design and screening	with the number of protein-ligand complexes available in the protein data bank constantly growing, structure-based approaches to drug design and screening have become increasingly important. alongside this explosion of structural information, a number of molecular docking methods have been developed over the last years with the aim of maximally exploiting all available structural and chemical information that can be derived from proteins, from ligands, and from protein-ligand complexes. in this respect, the term \\&\\#039;guided docking\\&\\#039; is introduced to refer to docking approaches that incorporate some degree of chemical information to actively guide the orientation of the ligand into the binding site. to reflect the focus on the use of chemical information, a classification scheme for guided docking approaches is proposed. in general terms, guided docking approaches can be divided into indirect and direct approaches. indirect approaches incorporate chemical information implicitly, having an effect on scoring but not on orienting the ligand during sampling. in contrast, direct approaches incorporate chemical information explicitly, thus actively guiding the orientation of the ligand during sampling. direct approaches can be further divided into protein-based, mapping-based, and ligandbased approaches to reflect the source used to derive the features capturing the chemical information inside the protein cavity. within each category, a representative list of docking approaches is discussed. in view of the limitations of current scoring functions, it was generally found that making optimal use of chemical information represents an efficient knowledge-based strategy for improving binding affinity estimations, ligand binding-mode predictions, and virtual screening enrichments obtained from protein-ligand docking.
79	76440	article	trends in cognitive sciences	\N	\N	\N	6	9	1	2005	jan	2005-01-12 20:11:10	center for cognitive neuroscience, university of pennsylvania, philadelphia, pa 19104, usa.	neuroethics: the practical and the philosophical	in comparison with the ethical issues surrounding molecular genetics, there has been little public awareness of the ethical implications of neuroscience. yet recent progress in cognitive neuroscience raises a host of ethical issues of at least comparable importance. some are of a practical nature, concerning the applications of neurotechnology and their likely implications for individuals and society. others are more philosophical, concerning the way we think about ourselves as persons, moral agents and spiritual beings. this article reviews key examples of each type of issue, including the relevant advances in science and technology and their accompanying social and philosophical problems.
80	76468	article	nature	\N	\N	nature publishing group	4	433	7022	2005	jan	2005-03-16 03:27:40	\N	simultaneous determination of protein structure and dynamics	we present a protocol for the experimental determination of ensembles of protein conformations that represent simultaneously the native structure and its associated dynamics. the procedure combines the strengths of nuclear magnetic resonance spectroscopyâ€”for obtaining experimental information at the atomic level about the structural and dynamical features of proteinsâ€”with the ability of molecular dynamics simulations to explore a wide range of protein conformations. we illustrate the method for human ubiquitin in solution and find that there is considerable conformational heterogeneity throughout the protein structure. the interior atoms of the protein are tightly packed in each individual conformation that contributes to the ensemble but their overall behaviour can be described as having a significant degree of liquid-like character. the protocol is completely general and should lead to significant advances in our ability to understand and utilize the structures of native proteins.
81	78006	misc	\N	\N	\N	\N	\N	\N	\N	2002	\N	2005-01-13 19:58:40	\N	the location stack: a layered model for location in ubiquitous computing	based on five design principles extracted from a survey of location systems, we present the location stack, a layered software engineering model for location in ubiquitous computing. our model is similar in spirit to the seven-layer open system interconnect (osi) model for computer networks. we map two existing ubiquitous computing systems to the model to illustrate the leverage the location stack provides. by encouraging system designers to think of their applications in this way, we hope to drive location-based computing toward a common vocabulary and standard infrastructure, permitting members of the ubiquitous computing community to easily evaluate and build on each otherâ€™s work.
82	79158	article	journal of artificial intelligence research	\N	\N	\N	41	7	\N	1997	\N	2005-01-17 15:15:15	\N	towards flexible teamwork	many ai researchers are today striving to build agent teams for complex, dynamic multi-agent domains, with intended applications in arenas such as education, training, entertainment, information integration, and collective robotics. unfortunately, uncertainties in these complex, dynamic domains obstruct coherent teamwork. in particular, team members often encounter differing, incomplete, and possibly inconsistent views of their environment. furthermore, team members can unexpectedly fail in...
83	79713	article	nature reviews. genetics	\N	\N	\N	8	5	6	2004	jun	2005-01-18 16:52:17	\N	integrating ethics and science in the international {hapmap} project.	genomics resources that use samples from identified populations raise scientific, social and ethical issues that are, in many ways, inextricably linked. scientific decisions about which populations to sample to produce the hapmap, an international genetic variation resource, have raised questions about the relationships between the social identities used to recruit participants and the biological findings of studies that will use the hapmap. the sometimes problematic implications of those complex relationships have led to questions about how to conduct genetic variation research that uses identified populations in an ethical way, including how to involve members of a population in evaluating the risks and benefits posed for everyone who shares that identity. the ways in which these issues are linked is increasingly drawing the scientific and ethical spheres of genomics research closer together.
84	80418	article	nature	\N	\N	nature publishing group	4	433	7023	2005	jan	2005-06-20 17:33:48	\N	evolutionary dynamics on graphs	evolutionary dynamics have been traditionally studied in the context of homogeneous or spatially extended populations1, 2, 3, 4. here we generalize population structure by arranging individuals on a graph. each vertex represents an individual. the weighted edges denote reproductive rates which govern how often individuals place offspring into adjacent vertices. the homogeneous population, described by the moran process3, is the special case of a fully connected graph with evenly weighted edges. spatial structures are described by graphs where vertices are connected with their nearest neighbours. we also explore evolution on random and scale-free networks5, 6, 7. we determine the fixation probability of mutants, and characterize those graphs for which fixation behaviour is identical to that of a homogeneous population7. furthermore, some graphs act as suppressors and others as amplifiers of selection. it is even possible to find graphs that guarantee the fixation of any advantageous mutant. we also study frequency-dependent selection and show that the outcome of evolutionary games can depend entirely on the structure of the underlying graph. evolutionary graph theory has many fascinating applications ranging from ecology to multi-cellular organization and economics.
85	81500	article	physical review letters	\N	\N	\N	\N	89	20	2002	nov	2005-01-21 15:59:24	department of physics, university of michigan, ann arbor, mi 48109-1120, usa.	assortative mixing in networks.	a network is said to show assortative mixing if the nodes in the network that have many connections tend to be connected to other nodes with many connections. here we measure mixing patterns in a variety of networks and find that social networks are mostly assortatively mixed, but that technological and biological networks tend to be disassortative. we propose a model of an assortatively mixed network, which we study both analytically and numerically. within this model we find that networks percolate more easily if they are assortative and that they are also more robust to vertex removal.
86	82207	article	nucleic acids research	\N	\N	oxford university press	2	33	Database issue	2005	jan	2010-07-07 00:03:52	\N	{arrayexpress}--a public repository for microarray gene expression data at the {ebi}.	arrayexpress is a new public database of microarray gene expression data at the ebi, which is a generic gene expression database designed to hold data from all microarray platforms. arrayexpress uses the annotation standard minimum information about a microarray experiment (miame) and the associated xml data exchange format microarray gene expression markup language (mage-ml) and it is designed to store well annotated data in a structured way. the arrayexpress infrastructure consists of the database itself, data submissions in mage-ml format or via an online submission tool miamexpress, online database query interface, and the expression profiler online analysis tool. arrayexpress accepts three types of submission, arrays, experiments and protocols, each of these is assigned an accession number. help on data submission and annotation is provided by the curation team. the database can be queried on parameters such as author, laboratory, organism, experiment or array types. with an increasing number of organisations adopting mage-ml standard, the volume of submissions to arrayexpress is increasing rapidly. the database can be accessed at http://www.ebi.ac.uk/arrayexpress.
87	82216	article	nucleic acids research	\N	\N	oxford university press	3	33	suppl 1	2005	jan	2010-01-30 20:26:10	\N	online mendelian inheritance in man ({omim}), a knowledgebase of human genes and genetic disorders	online mendelian inheritance in man ({omim}™) is a comprehensive, authoritative and timely knowledgebase of human genes and genetic disorders compiled to support human genetics research and education and the practice of clinical genetics. started by dr victor a. {mckusick} as the definitive reference mendelian inheritance in man, {omim} (http://www.ncbi.nlm.nih.gov/omim/) is now distributed electronically by the national center for biotechnology information, where it is integrated with the entrez suite of databases. derived from the biomedical literature, {omim} is written and edited at johns hopkins university with input from scientists and physicians around the world. each {omim} entry has a full-text summary of a genetically determined phenotype and/or gene and has numerous links to other genetic databases such as {dna} and protein sequence, {pubmed} references, general and locus-specific mutation databases, {hugo} nomenclature, {mapviewer}, {genetests}, patient support groups and many others. {omim} is an easy and straightforward portal to the burgeoning information in human genetics.
88	82275	article	nucleic acids research	\N	\N	oxford university press	3	33	Database issue	2005	jan	2009-06-08 13:11:55	\N	{e-msd}: an integrated data resource for bioinformatics.	the macromolecular structure database (msd) group (http://www.ebi.ac.uk/msd/) continues to enhance the quality and consistency of macromolecular structure data in the worldwide protein data bank (wwpdb) and to work towards the integration of various bioinformatics data resources. one of the major obstacles to the improved integration of structural databases such as msd and sequence databases like uniprot is the absence of up to date and well-maintained mapping between corresponding entries. we have worked closely with the uniprot group at the ebi to clean up the taxonomy and sequence cross-reference information in the msd and uniprot databases. this information is vital for the reliable integration of the sequence family databases such as pfam and interpro with the structure-oriented databases of scop and cath. this information has been made available to the efamily group (http://www.efamily.org.uk/) and now forms the basis of the regular interchange of information between the member databases (msd, uniprot, pfam, interpro, scop and cath). this exchange of annotation information has enriched the structural information in the msd database with annotation from wider sequence-oriented resources. this work was carried out under the  structure integration with function, taxonomy and sequences (sifts)' initiative (http://www.ebi.ac.uk/msd-srv/docs/sifts) in the msd group. 10.1093/nar/gki058
89	82938	techreport	\N	\N	\N	massachusetts institute of technology	\N	\N	\N	1978	\N	2005-01-24 18:58:15	cambridge, ma, usa	rabbit: a compiler for scheme	we have developed a compiler for the lexically-scoped dialect of {lisp} known as {scheme}. the compiler knows relatively little about specific data manipulation primitives such as arithmetic operators, but concentrates on general issues of environment and control. rather than having specialized knowledge about a large variety of control and environment constructs, the compiler handles only a small basis set which reflects the semantics of lambda-calculus. all of the traditional imperative constructs, such as sequencing, assignment, looping, {go} {to}, as well as many standard {lisp} constructs such as {and}, {or} and {cond}, are expressed as macros in terms of the applicative basis set. a small number of optimization techniques, coupled with the treatment of function calls as {go} {to} statements, serves to produce code as good as that produced by more traditional compilers.
90	83492	proceedings	\N	proceedings of the 7th usenix security symposium	\N	\N	\N	\N	\N	1998	\N	2005-01-25 18:43:16	san antonio, tx	data mining approaches for intrusion detection	in this paper we discuss our research in developing general and systematic methods for intrusion detection. the key ideas are to use data mining techniques to discover consistent and useful patterns of system features that describe program and user behavior, and use the set of relevant system features to compute (inductively learned) classifiers that can recognize anomalies and known intrusions. using experiments on the sendmail system call data and the network tcpdump data, we demonstrate that ...
91	83540	article	nature	\N	\N	nature publishing group	3	401	6755	1999	oct	2005-01-26 03:28:02	bell laboratories, lucent technologies, murray hill, new jersey 07974, usa.	learning the parts of objects by non-negative matrix factorization	is perception of the whole based on perception of its parts? there is psychological1 and physiological2, 3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4, 5. but little is known about how brains or computers might learn the parts of objects. here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. this is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. these constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. when non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.
92	85519	article	annual review of psychology	\N	\N	\N	22	48	1	1997	feb	2005-01-29 05:49:02	\N	{human-computer} interaction: psychology as a science of design	human-computer interaction (hci) is the area of intersection between psychology and the social sciences, on the one hand, and computer science and technology, on the other. hci researchers analyse and design-specific user-interface technologies (e.g. three-dimensional pointing devices, interactive video). they study and improve the processes of technology development (e.g. usability evaluation, design rationale). they develop and evaluate new applications of technology (e.g. computer conferencing, software design environments). through the past two decades, hci has progressively integrated its scientific concerns with the engineering goal of improving the usability of computer systems and applications, thus establishing a body of technical knowledge and methodology. hci continues to provide a challenging test domain for applying and developing psychology and social science in the context of technology development and use.
93	86440	article	j. med. chem.	journal of medicinal chemistry	\N	american chemical society	10	47	7	2004	mar	2005-01-31 13:48:08	department of chemistry, columbia university, new york, new york 10036, usa. rich@chem.columbia.edu	glide:? a new approach for rapid, accurate docking and scoring. 1. method and assessment of docking accuracy	unlike other methods for docking ligands to the rigid {3d} structure of a known protein receptor, glide approximates a complete systematic search of the conformational, orientational, and positional space of the docked ligand. in this search, an initial rough positioning and scoring phase that dramatically narrows the search space is followed by torsionally flexible energy optimization on an {opls}-{aa} nonbonded potential grid for a few hundred surviving candidate poses. the very best candidates are further refined via a monte carlo sampling of pose conformation; in some cases, this is crucial to obtaining an accurate docked pose. selection of the best docked pose uses a model energy function that combines empirical and force-field-based terms. docking accuracy is assessed by redocking ligands from 282 cocrystallized {pdb} complexes starting from conformationally optimized ligand geometries that bear no memory of the correctly docked pose. errors in geometry for the top-ranked pose are less than 1 \\aa{} in nearly half of the cases and are greater than 2 \\aa{} in only about one-third of them. comparisons to published data on rms deviations show that glide is nearly twice as accurate as {gold} and more than twice as accurate as {flexx} for ligands having up to 20 rotatable bonds. glide is also found to be more accurate than the recently described surflex method.
94	86776	misc	\N	\N	\N	\N	\N	\N	\N	2002	\N	2005-02-01 17:10:59	\N	early aspects: a model for {aspect-oriented} requirements engineering	effective re must reconcile the need to achieve separation of concerns with the need to satisfy broadly scoped requirements and constraints. techniques such as use cases and viewpoints help achieve separation of stakeholders' concerns but ensuring their consistency with global requirements and constraints is largely unsupported. we build on recent work that has emerged from the aspect-oriented programming (aop) community to propose a general model for aspect oriented requirements engineering (aore). the model supports separation of crosscutting functional and non-functional properties at the requirements level. we argue that early separation of such crosscutting properties supports effective determination of their mapping and influence on artefacts at later development stages. a realisation of the model based on a case study of a toll collection system is presented.
95	87024	article	plos biology	\N	\N	\N	\N	2	10	2004	oct	2005-02-02 22:42:48	sobell department of motor neuroscience, institute of neurology, university college london, london, united kingdom. konrad@koerding.de	a neuroeconomics approach to inferring utility functions in sensorimotor control.	making choices is a fundamental aspect of human life. for over a century experimental economists have characterized the decisions people make based on the concept of a utility function. this function increases with increasing desirability of the outcome, and people are assumed to make decisions so as to maximize utility. when utility depends on several variables, indifference curves arise that represent outcomes with identical utility that are therefore equally desirable. whereas in economics utility is studied in terms of goods and services, the sensorimotor system may also have utility functions defining the desirability of various outcomes. here, we investigate the indifference curves when subjects experience forces of varying magnitude and duration. using a two-alternative forced-choice paradigm, in which subjects chose between different magnitude-duration profiles, we inferred the indifference curves and the utility function. such a utility function defines, for example, whether subjects prefer to lift a 4-kg weight for 30 s or a 1-kg weight for a minute. the measured utility function depends nonlinearly on the force magnitude and duration and was remarkably conserved across subjects. this suggests that the utility function, a central concept in economics, may be applicable to the study of sensorimotor control.
96	87035	article	bmc evolutionary biology	\N	\N	\N	\N	3	1	2003	aug	2005-02-03 03:29:52	graduate group in biophysics, university of california, berkeley, ca 94720, usa. amoses@lbl.gov	position specific variation in the rate of evolution in transcription factor binding sites	{background}:the binding sites of sequence specific transcription factors are an important and relatively well-understood class of functional non-coding {dnas}. although a wide variety of experimental and computational methods have been developed to characterize transcription factor binding sites, they remain difficult to identify. comparison of non-coding {dna} from related species has shown considerable promise in identifying these functional non-coding sequences, even though relatively little is known about their {evolution.results}:here we analyse the genome sequences of the budding yeasts saccharomyces cerevisiae, s. bayanus, s. paradoxus and s. mikatae to study the evolution of transcription factor binding sites. as expected, we find that both experimentally characterized and computationally predicted binding sites evolve slower than surrounding sequence, consistent with the hypothesis that they are under purifying selection. we also observe position-specific variation in the rate of evolution within binding sites. we find that the position-specific rate of evolution is positively correlated with degeneracy among binding sites within s. cerevisiae. we test theoretical predictions for the rate of evolution at positions where the base frequencies deviate from background due to purifying selection and find reasonable agreement with the observed rates of evolution. finally, we show how the evolutionary characteristics of real binding motifs can be used to distinguish them from artefacts of computational motif finding {algorithms.conclusion}:as has been observed for protein sequences, the rate of evolution in transcription factor binding sites varies with position, suggesting that some regions are under stronger functional constraint than others. this variation likely reflects the varying importance of different positions in the formation of the {protein-dna} complex. the characterization of the pattern of evolution in known binding sites will likely contribute to the effective use of comparative sequence data in the identification of transcription factor binding sites and is an important step toward understanding the evolution of functional non-coding {dna}.
97	90414	article	science	\N	\N	american association for the advancement of science	3	261	5124	1993	aug	2005-02-08 16:18:57	department of psychology, university of arizona, tucson 85724.	dynamics of the hippocampal ensemble code for space	ensemble recordings of 73 to 148 rat hippocampal neurons were used to predict accurately the animals' movement through their environment, which confirms that the hippocampus transmits an ensemble code for location. in a novel space, the ensemble code was initially less robust but improved rapidly with exploration. during this period, the activity of many inhibitory cells was suppressed, which suggests that new spatial information creates conditions in the hippocampal circuitry that are conducive to the synaptic modification presumed to be involved in learning. development of a new population code for a novel environment did not substantially alter the code for a familiar one, which suggests that the interference between the two spatial representations was very small. the parallel recording methods outlined here make possible the study of the dynamics of neuronal interactions during unique behavioral events.
98	90415	article	science	\N	\N	american association for the advancement of science	3	265	5172	1994	jul	2005-02-08 16:20:51	division of neural systems, memory, and aging, university of arizona, tucson 85724.	reactivation of hippocampal ensemble memories during sleep	simultaneous recordings were made from large ensembles of hippocampal "place cells" in three rats during spatial behavioral tasks and in slow-wave sleep preceding and following these behaviors. cells that fired together when the animal occupied particular locations in the environment exhibited an increased tendency to fire together during subsequent sleep, in comparison to sleep episodes preceding the behavioral tasks. cells that were inactive during behavior, or that were active but had non-overlapping spatial firing, did not show this increase. this effect, which declined gradually during each post-behavior sleep session, may result from synaptic modification during waking experience. information acquired during active behavior is thus re-expressed in hippocampal circuits during sleep, as postulated by some theories of memory consolidation.
99	90416	article	nature neuroscience	\N	\N	\N	6	5	8	2002	aug	2005-02-08 16:25:33	division of physics, mathematics \\& astronomy, california institute of technology, pasadena, california 91125, usa.	temporal structure in neuronal activity during working memory in macaque parietal cortex.	many cortical structures have elevated firing rates during working memory, but it is not known how the activity is maintained. to investigate whether reverberating activity is important, we studied the temporal structure of local field potential ({lfp}) activity and spiking from area {lip} in two awake macaques during a memory-saccade task. using spectral analysis, we found spatially tuned elevated power in the gamma band (25-90 hz) in {lfp} and spiking activity during the memory period. spiking and {lfp} activity were also coherent in the gamma band but not at lower frequencies. finally, we decoded {lfp} activity on a single-trial basis and found that {lfp} activity in parietal cortex discriminated between preferred and anti-preferred direction with approximately the same accuracy as the spike rate and predicted the time of a planned movement with better accuracy than the spike rate. this finding could accelerate the development of a cortical neural prosthesis.
100	90466	article	annual review of neuroscience	\N	\N	\N	29	26	1	2003	\N	2005-02-08 23:51:20	department of brain and cognitive sciences, meliora hall, university of rochester, rochester, ny 14627, usa. alex@bcs.rochester.edu	inference and computation with population codes.	in the vertebrate nervous system, sensory stimuli are typically encoded through the concerted activity of large populations of neurons. classically, these patterns of activity have been treated as encoding the value of the stimulus (e.g., the orientation of a contour), and computation has been formalized in terms of function approximation. more recently, there have been several suggestions that neural computation is akin to a bayesian inference process, with population activity patterns representing uncertainty about stimuli in the form of probability distributions (e.g., the probability density function over the orientation of a contour). this paper reviews both approaches, with a particular emphasis on the latter, which we see as a very promising framework for future modeling and experimental work.
101	92730	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2005-02-10 19:08:22	\N	{bbc} news | {health} | couple 'choose' to have deaf baby	africa   bbc wap use flourishing in africa bbc wap use flourishing in africa  mobile use in africa is leapfrogging pc technology africa, in particular nigeria, is dominating international mobile phone access to the bbc's website. according to july's statistics, 61% of the bbc's international wap users came from nigeria and 19% from south africa.  "wap is the one platform where african countries continue to appear in the top five in our statistics," said bbc developer gareth owen.  africa is the world's largest-growing mobile phone market with unreliable landlines encouraging the growth.  wap technology - which stands for wireless application protocol - allows people to access basic information on the internet, like news summaries, through their mobile phone handset.   i'm in uganda and the only access i have 2 the outside world is this pinhole 2 info ugandan texter to the bbc  do you need a computer? ringing in changes in nigeria  according to the bbc's statistics, page views for wap usage are growing at 100% year on year.  uk users account for 65% of wap traffic; and international usage for 35%. mobile phone providers in many african countries have only recently begun rolling out wap-enabled handsets.  and the large take up of bbc news via mobiles in nigeria contrasts starkly with the relatively small number of users accessing the internet via pcs - hampered by slow and unreliable landlines.  the bbc's technology correspondent mark ward says that in many places on the continent pc ownership is low but pc literacy surprisingly high.  internet cafes tend to be very popular, as much a meeting place as well as a place where people access their email, he says.    the bbc receives regular messages of thanks from people in africa, who say the only access they have to news is via their mobiles.  "i'm in uganda and the only access i have 2 the outside world is this pinhole 2 info cause i don't have access to tv. thanx," said one texter from uganda. the country accounted for 7% of bbc wap usage in july.  other top countries helping account for the 58m wap page views in july were jamaica, singapore and israel.  in the uk, the bbc has about a 20% share of the market with a reach of 1.2m users monthly.
102	92789	article	acm trans. comput. syst.	\N	\N	acm	36	16	2	1998	may	2005-02-10 21:33:01	new york, ny, usa	the part-time parliament	recent archaeological discoveries on the island of paxos reveal that the parliament functioned despite the peripatetic propensity of its part-time legislators. the legislators maintained consistent copies of the parliamentary record, despite their frequent forays from the chamber and the forgetfulness of their messengers. the paxon parliament's protocol provides a new way of implementing the state machine approach to the design of distributed systems.
103	92797	inproceedings	\N	proceedings of the 10th international workshop on distributed algorithms	wdag	springer-verlag	16	\N	\N	1996	\N	2005-02-10 22:14:31	london, uk, uk	how to build a highly available system using consensus	an abstract is not available.
104	93116	article	journal of monetary economics	\N	\N	\N	18	16	3	1985	nov	2005-02-11 10:19:26	\N	indivisible labor and the business cycle	a growth model with shocks to technology is studied. labor is indivisible, so all variability in hours worked is due to fluctuations in the number employed. we find that, unlike previous equilibrium models of the business cycle, this economy displays large fluctuations in hours worked and relatively small fluctuations in productivity. this finding is independent of individuals' willingness to substitute leisure across time. this and other findings are the result of studying and comparing summary statistics describing this economy, an economy with divisible labor, and post-war {u.s}. time series.
105	93163	article	econometrica	\N	\N	the econometric society	7	18	2	1950	\N	2005-02-11 11:33:55	\N	the bargaining problem	a new treatment is presented of a classical economic problem, one which occurs in many forms, as bargaining, bilateral monopoly, etc. it may also be regarded as a nonzero-sum two-person game. in this treatment a few general assumptions are made concerning the behavior of a single individual and of a group of two individuals in certain economic environments. from these, the solution (in the sense of this paper) of the classical problem may be obtained. in the terms of game theory, values are found for the game.
106	93166	article	econometrica	\N	\N	the econometric society	12	50	1	1982	\N	2005-02-11 11:35:50	\N	perfect equilibrium in a bargaining model	two players have to reach an agreement on the partition of a pie of size 1. each has to make in turn, a proposal as to how it should be divided. after one player has made an offer, the other must decide either to accept it, or to reject it and continue the bargaining. several properties which the players' preferences possess are assumed. the perfect equilibrium partitions ({p.e}.p.) are characterized in all the modesls satisfying these assumptions. specially, it is proved that when every player bears a fixed bargaining cost for each period (c"1 and c"2), then: (i) if c"1 < c"2 the only {p.e}.p. gives all the pie to 1; (ii) if c"1 > c"2 the only {p.e}.p. gives to 1; only c"2. in the case where each player has a fixed discounting factor (@d"1 and @d"2) the only {p.e}.p. is (1 - @d"2)/(1 - @d"1@d"2).
107	93513	article	plos biol	\N	\N	public library of science	\N	3	1	2004	nov	2005-07-24 15:23:32	medical research council rosalind franklin centre for genomics research hinxton, cambridge, united kingdom.	highly conserved {non-coding} sequences are associated with vertebrate development	in addition to protein coding sequence, the human genome contains a significant amount of regulatory {dna}, the identification of which is proving somewhat recalcitrant to both in silico and functional methods. an approach that has been used with some success is comparative sequence analysis, whereby equivalent genomic regions from different organisms are compared in order to identify both similarities and differences. in general, similarities in sequence between highly divergent organisms imply functional constraint. we have used a whole-genome comparison between humans and the pufferfish, fugu rubripes, to identify nearly 1,400 highly conserved non-coding sequences. given the evolutionary divergence between these species, it is likely that these sequences are found in, and furthermore are essential to, all vertebrates. most, and possibly all, of these sequences are located in and around genes that act as developmental regulators. some of these sequences are over 90\\% identical across more than 500 bases, being more highly conserved than coding sequence between these two species. despite this, we cannot find any similar sequences in invertebrate genomes. in order to begin to functionally test this set of sequences, we have used a rapid in vivo assay system using zebrafish embryos that allows tissue-specific enhancer activity to be identified. functional data is presented for highly conserved non-coding sequences associated with four unrelated developmental regulators ({sox21}, {pax6}, {hlxb9}, and {shh}), in order to demonstrate the suitability of this screen to a wide range of genes and expression patterns. of 25 sequence elements tested around these four genes, 23 show significant enhancer activity in one or more tissues. we have identified a set of non-coding sequences that are highly conserved throughout vertebrates. they are found in clusters across the human genome, principally around genes that are implicated in the regulation of development, including many transcription factors. these highly conserved non-coding sequences are likely to form part of the genomic circuitry that uniquely defines vertebrate development.
108	94299	article	journal of computer-mediated communication	\N	\N	\N	\N	10	2	2005	\N	2005-02-14 05:10:31	\N	gender, identity, and language use in teenage blogs	this study examines issues of online identity and language use among male and female teenagers who created and maintained weblogs, personal journals made publicly accessible on the world wide web. online identity and language use were examined in terms of the disclosure of personal information, sexual identity, emotive features, and semantic themes. male and female teenagers presented themselves similarly in their blogs, often revealing personal information such as their real names, ages, and locations. males more so than females used emoticons, employed an active and resolute style of language, and were more likely to present themselves as gay. the results suggest that teenagers stay closer to reality in their online expressions of self than has previously been suggested, and that these explorations involve issues, such as learning about their sexuality, that commonly occur during the adolescent years.
109	94348	article	nucleic acids research	\N	\N	oxford university press	7	22	22	1994	nov	2005-02-14 12:22:28	european molecular biology laboratory, heidelberg, germany.	{clustal} w: improving the sensitivity of progressive multiple sequence alignment through sequence weighting, position-specific gap penalties and weight matrix choice.	the sensitivity of the commonly used progressive multiple sequence alignment method has been greatly improved for the alignment of divergent protein sequences. firstly, individual weights are assigned to each sequence in a partial alignment in order to down-weight near-duplicate sequences and up-weight the most divergent ones. secondly, amino acid substitution matrices are varied at different alignment stages according to the divergence of the sequences to be aligned. thirdly, residue-specific gap penalties and locally reduced gap penalties in hydrophilic regions encourage new gaps in potential loop regions rather than regular secondary structure. fourthly, positions in early alignments where gaps have been opened receive locally reduced gap penalties to encourage the opening up of new gaps at these positions. these modifications are incorporated into a new program, {clustal} w which is freely available.
110	94461	article	nucleic acids res	\N	\N	\N	3	30	1	2002	jan	2005-02-14 17:46:42	swiss institute of bioinformatics, swiss institute for experimental cancer research (isrec), ch-1066 epalinges /lausanne, switzerland.	the {prosite} database, its status in 2002.	{prosite} [bairoch and bucher (1994) nucleic acids res., 22, 3583-3589; hofmann et al. (1999) nucleic acids res., 27, 215-219] is a method of identifying the functions of uncharacterized proteins translated from genomic or {cdna} sequences. the {prosite} database (http://www.expasy.org/prosite/) consists of biologically significant patterns and profiles designed in such a way that with appropriate computational tools it can rapidly and reliably help to determine to which known family of proteins (if any) a new sequence belongs, or which known domain(s) it contains.
111	94463	article	nucleic acids research	\N	\N	oxford university press	4	26	2	1998	jan	2005-02-14 17:49:32	the institute for genomic research, 9712 medical center drive, rockville, md 20850, usa. salzberg@tigr.org	microbial gene identification using interpolated markov models.	this paper describes a new system, {glimmer}, for finding genes in microbial genomes. in a series of tests on haemophilus influenzae , helicobacter pylori and other complete microbial genomes, this system has proven to be very accurate at locating virtually all the genes in these sequences, outperforming previous methods. a conservative estimate based on experiments on h.pylori and h. influenzae is that the system finds >97\\% of all genes. {glimmer} uses interpolated markov models ({imms}) as a framework for capturing dependencies between nearby nucleotides in a {dna} sequence. an {imm}-based method makes predictions based on a variable context; i.e., a variable-length oligomer in a {dna} sequence. the context used by {glimmer} changes depending on the local composition of the sequence. as a result, {glimmer} is more flexible and more powerful than fixed-order markov methods, which have previously been the primary content-based technique for finding genes in microbial {dna}.
112	94493	article	j. acm	\N	\N	acm	20	34	1	1987	jan	2005-02-14 19:21:16	new york, ny, usa	on the minimal synchronism needed for distributed consensus	reaching agreement is a primitive of distributed computing. whereas this poses no problem in an ideal, failure-free environment, it imposes certain constraints on the capabilities of an actual system: a system is viable only if it permits the existence of consensus protocols tolerant to some number of failures. fischer et al. have shown that in a completely asynchronous model, even one failure cannot be tolerated. in this paper their work is extended: several critical system parameters, including various synchrony conditions, are identified and how varying these affects the number of faults that can be tolerated is examined. the proofs expose general heuristic principles that explain why consensus is possible in certain models but not possible in others.
113	94495	article	j. acm	\N	\N	acm	37	43	4	1996	jul	2005-02-14 19:22:36	new york, ny, usa	the weakest failure detector for solving consensus	we determine what information about failures is necessary and sufficient to solve consensus in asynchronous distributed systems subject to crash failures. in chandra and toueg [1996], it is shown that w, a failure detector that provides surprisingly little information about which processes have crashed, is sufficient to solve consensus in asynchronous systems with a majority of correct processes. in this paper, we prove that to solve consensus, any failure detector has to provide at least as much information as w. thus, w is indeed the weakest failure detector for solving consensus in asynchronous systems with a majority of correct processes.
114	95902	article	annual review of sociology	\N	\N	\N	31	24	\N	1998	\N	2005-02-15 16:30:49	\N	social dilemmas: the anatomy of cooperation	the study of social dilemmas is the study of the tension between individual and collective rationality. in a social dilemma, individually reasonable behavior leads to a situation in which everyone is worse off. the first part of this review is a discussion of categories of social dilemmas and how they are modeled. the key two-person social dilemmas (prisoner\\~{a}­s dilemma, assurance, chicken) and multiple-person social dilemmas (public goods dilemmas and commons dilemmas) are examined. the second part is an extended treatment of possible solutions for social dilemmas. these solutions are organized into three broad categories based on whether the solutions assume egoistic actors and whether the structure of the situation can be changed: motivational solutions assume actors are not completely egoistic and so give some weight to the outcomes of their partners. strategic solutions assume egoistic actors, and neither of these categories of solutions involve changing the fundamental structure of the situation. solutions that do involve changing the rules of the game are considered in the section on structural solutions. i conclude the review with a discussion of current research and directions for future work.
115	95939	misc	\N	\N	\N	\N	\N	\N	\N	2005	feb	2005-02-15 17:42:28	\N	{np}-complete problems and physical reality	can np-complete problems be solved efficiently in the physical universe? i survey proposals including soap bubbles, protein folding, quantum computing, quantum advice, quantum adiabatic algorithms, quantum-mechanical nonlinearities, hidden variables, relativistic time dilation, analog computing, malament-hogarth spacetimes, quantum gravity, closed timelike curves, and â€œanthropic computing. â€ the section on soap bubbles even includes some â€œexperimental â€ results. while i do not believe that any of the proposals will let us solve np-complete problems efficiently, i argue that by studying them, we can learn something not only about computation but also about physics. 1
116	96165	article	proceedings of the national academy of sciences of the united states of america	\N	\N	national academy of sciences	5	101	41	2004	oct	2005-02-15 23:15:41	institut f\\"{u}r theoretische physik, universit\\"{a}t zu k\\"{o}ln, z\\"{u}lpicherstrasse 77, 50937 cologne, germany. berg@thp.uni-koeln.de	local graph alignment and motif search in biological networks	interaction networks are of central importance in postgenomic molecular biology, with increasing amounts of data becoming available by high-throughput methods. examples are gene regulatory networks or protein interaction maps. the main challenge in the analysis of these data is to read off biological functions from the topology of the network. topological motifs, i.e., patterns occurring repeatedly at different positions in the network, have recently been identified as basic modules of molecular information processing. in this article, we discuss motifs derived from families of mutually similar but not necessarily identical patterns. we establish a statistical model for the occurrence of such motifs, from which we derive a scoring function for their statistical significance. based on this scoring function, we develop a search algorithm for topological motifs called graph alignment, a procedure with some analogies to sequence alignment. the algorithm is applied to the gene regulation network of escherichia coli.
117	96396	article	knowledge acquisition	\N	\N	\N	\N	\N	\N	1993	\N	2005-02-16 11:14:41	\N	a translation approach to portable ontology specifications	to support the sharing and reuse of formally represented knowledge among ai systems, it is useful to define the common vocabulary in which shared knowledge is represented. a specification of a representational vocabulary for a shared domain of discourseâ€“â€“definitions of classes, relations, functions, and other objectsâ€“â€“is called an ontology. this paper describes a mechanism for defining ontologies that are portable over representation systems. definitions written in a standard format for predicate calculus are translated by a system called ontolingua into specialized representations, including frame-based systems as well as relational languages. this allows researchers to share and reuse ontologies, while retaining the computational benefits of specialized implementations. we discuss how the translation approach to portability addresses several technical problems. one problem is how to accommodate the stylistic and organizational differences among representations while preserving declarative content. another is how to translate from a very expressive language into restricted languages, remaining system-independent while preserving the computational efficiency of implemented systems. we describe how these problems are addressed by basing ontolingua itself on an ontology of domain-independent, representational idioms.
118	96402	proceedings	\N	proceedings of the 9th workshop on job scheduling strategies for parallel processing	\N	\N	\N	\N	\N	2003	jun	2005-02-16 11:38:22	\N	{ourgrid}: an approach to easily assemble grids with equitable resource sharing	available grid technologies like the globus toolkit make possible for one to run a parallel application on resources distributed across several administrative domains. most grid computing users, however, don't have access to more than a handful of resources onto which they can use this technologies. this happens mainly because gaining access to resources still depends on personal negotiations between the user and each resource owner. to address this problem, we are developing the {ourgrid}...
119	97088	electronic	\N	\N	\N	\N	\N	\N	\N	2005	feb	2005-02-17 12:45:22	\N	decoding by linear programming	this paper considers a natural error correcting problem with real valued input/output. we wish to recover an input vector fâˆˆr n  from corrupted measurements y=af+e. here, a is an m by n (coding) matrix and e is an arbitrary and unknown vector of errors. is it possible to recover f exactly from the data y? we prove that under suitable conditions on the coding matrix a, the input f is the unique solution to the â„“ 1 -minimization problem (  x   â„“1 :=Î£ i  x i  ) min(gâˆˆr n )   y - ag   â„“1  provided that the support of the vector of errors is not too large,   e   â„“0 := {i:e i  â‰  0} â‰¤ÏÂ·m for some Ï>0. in short, f can be recovered exactly by solving a simple convex optimization problem (which one can recast as a linear program). in addition, numerical experiments suggest that this recovery procedure works unreasonably well; f is recovered exactly even in situations where a significant fraction of the output is corrupted. this work is related to the problem of finding sparse solutions to vastly underdetermined systems of linear equations. there are also significant connections with the problem of recovering signals from highly incomplete measurements. in fact, the results introduced in this paper improve on our earlier work. finally, underlying the success of â„“ 1  is a crucial property we call the uniform uncertainty principle that we shall describe in detail.
120	99601	article	psychological review	\N	\N	\N	29	98	2	1991	\N	2005-02-20 13:34:09	\N	culture and the self: implications for cognition, emotion, and motivation	people in different cultures have strikingly different construals of the self, of others, and of the interdependence of the 2. these construals can influence, and in many cases determine, the very nature of individual experience, including cognition, emotion, and motivation. many asian cultures have distinct conceptions of individuality that insist on the fundamental relatedness of individuals to each other. the emphasis is on attending to others, fitting in, and harmonious interdependence with them. american culture neither assumes nor values such an overt connectedness among individuals. in contrast, individuals seek to maintain their independence from others by attending to the self and by discovering and expressing their unique inner attributes. as proposed herein, these construals are even more powerful than previously imagined. theories of the self from both psychology and anthropology are integrated to define in detail the difference between a construal of the self as independent and a construal of the self as interdependent. each of these divergent construals should have a set of specific consequences for cognition, emotion, and motivation; these consequences are proposed and relevant empirical literature is reviewed. focusing on differences in self-construals enables apparently inconsistent empirical findings to be reconciled, and raises questions about what have been thought to be culture-free aspects of cognition, emotion, and motivation.
121	99849	article	acm trans. inf. syst. secur.	\N	\N	acm	32	3	4	2000	nov	2005-02-20 23:17:04	new york, ny, usa	testing intrusion detection systems: a critique of the 1998 and 1999 {darpa} intrusion detection system evaluations as performed by lincoln laboratory	in 1998 and again in 1999, the lincoln laboratory of {mit} conducted a comparative evaluation of intrusion detection systems ({idss}) developed under {darpa} funding. while this evaluation represents a significant and monumental undertaking, there are a number of issues associated with its design and execution that remain unsettled. some methodologies used in the evaluation are questionable and may have biased its results. one problem is that the evaluators have published relatively little concerning some of the more critical aspects of their work, such as validation of their test data. the appropriateness of the evaluation techniques used needs further investigation. the purpose of this article is to attempt to identify the shortcomings of the lincoln lab effort in the hope that future  efforts of this kind will be placed on a sounder footing. some of the problems that the article points out might well be resolved if the evaluators were to publish a detailed description of their procedures and the rationale that led to their adoption, but other problems would clearly remain./par>
122	99888	article	journal of neurophysiology	\N	\N	american physiological society	15	93	2	2005	feb	2005-02-21 04:03:36	neuroscience department, brown university, providence, ri, usa. wilson\\_truccolo@brown.edu	a point process framework for relating neural spiking activity to spiking history, neural ensemble, and extrinsic covariate effects.	multiple factors simultaneously affect the spiking activity of individual neurons. determining the effects and relative importance of these factors is a challenging problem in neurophysiology. we propose a statistical framework based on the point process likelihood function to relate a neuron's spiking probability to three typical covariates: the neuron's own spiking history, concurrent ensemble activity, and extrinsic covariates such as stimuli or behavior. the framework uses parametric models of the conditional intensity function to define a neuron's spiking probability in terms of the covariates. the discrete time likelihood function for point processes is used to carry out model fitting and model analysis. we show that, by modeling the logarithm of the conditional intensity function as a linear combination of functions of the covariates, the discrete time point process likelihood function is readily analyzed in the generalized linear model ({glm}) framework. we illustrate our approach for both {glm} and {non-glm} likelihood functions using simulated data and multivariate single-unit activity data simultaneously recorded from the motor cortex of a monkey performing a visuomotor pursuit-tracking task. the point process framework provides a flexible, computationally efficient approach for maximum likelihood estimation, goodness-of-fit assessment, residual analysis, model selection, and neural decoding. the framework thus allows for the formulation and analysis of point process models of neural spiking activity that readily capture the simultaneous effects of multiple covariates and enables the assessment of their relative importance.
123	99930	electronic	phys rev lett.	\N	\N	\N	\N	94	1	2005	jan	2005-02-21 12:11:47	\N	scale-free brain functional networks	functional magnetic resonance imaging ({fmri}) is used to extract <em> functional networks</em> connecting correlated human brain sites. analysis of the resulting networks in different tasks shows that: (a) the distribution of functional connections, and the probability of finding a link vs. distance are both scale-free, (b) the characteristic path length is small and comparable with those of equivalent random networks, and (c) the clustering coefficient is orders of magnitude larger than those of equivalent random networks. all these properties, typical of scale-free small world networks, reflect important functional information about brain states.
124	99931	article	scientific american	\N	\N	\N	9	284	4	2001	\N	2005-02-21 12:13:02	\N	the semantic web	the entertainment system was belting out the beatles' "we can work it out" when the phone rang. when pete answered, his phone turned the sound down by sending a message to all the other local devices that had a volume control. his sister, lucy, was on the line from the doctor's office: "mom needs to see a specialist and then has to have a series of physical therapy sessions. biweekly or something. i'm going to have my agent set up the appointments." pete immediately agreed to share the chauffeuring.
125	100015	article	science	\N	new series	american association for the advancement of science	7	185	4157	1974	\N	2005-02-21 15:34:17	\N	judgment under uncertainty: heuristics and biases	the thirty-five chapters in this book describe various judgmental heuristics and the biases they produce, not only in laboratory experiments but in important social, medical, and political situations as well. individual chapters discuss the representativeness and availability heuristics, problems in judging covariation and control, overconfidence, multistage inference, social perception, medical diagnosis, risk perception, and methods for correcting and improving judgments under uncertainty. about half of the chapters are edited versions of classic articles; the remaining chapters are newly written for this book. most review multiple studies or entire subareas of research and application rather than describing single experimental studies. this book will be useful to a wide range of students and researchers, as well as to decision makers seeking to gain insight into their judgments and to improve them.
126	100050	article	sigplan not.	popl	\N	acm	10	38	1	2003	jan	2005-02-21 16:03:11	new york, ny, usa	ownership types for object encapsulation	ownership types provide a statically enforceable way of specifying object encapsulation and enable local reasoning about program correctness in object-oriented languages. however, a type system that enforces strict object encapsulation is too constraining: it does not allow efficient implementation of important constructs like iterators. this paper argues that the right way to solve the problem is to allow objects of classes defined in the same module to have privileged access to each other's representations; we show how to do this for inner classes. this approach allows programmers to express constructs like iterators and yet supports local reasoning about the correctness of the classes, because a class and its inner classes together can be reasoned about as a module. the paper also sketches how we use our variant of ownership types to enable efficient software upgrades in persistent object stores.
127	100145	inproceedings	3rd international workshop on agents and peer-to-peer computing (ap2pc 2004)	\N	\N	\N	\N	\N	\N	2004	jul	2005-02-21 18:05:03	\N	how social structure improves distributed reputation systems - three hypotheses	reputation systems provide an incentive for cooperation in artificial societies by keeping track of the behavior of autonomous entities. the self-organization of p2p systems demands for the distribution of the reputation system to the autonomous entities themselves. they may cooperate by issuing recommendations of other entitiesâ€™ trustworthiness. the recipient of a recommendation has to assess its truthfulness and consistency before taking it into account. the current assessment methods are based on plausibility considerations that have several inherent limitations. in our previous work, we have suggested the application of non-repudiable tokens that overcome most of the limitations. however, there remain limitations that are not overcome or only partly  overcome. therefore, in this paper, we propose social structure as a complementary means of overcoming the remaining limitations of plausibility considerations. for this purpose, we examine the properties of social structure and discuss how distributed reputation systems can make use of them. this leads us to the formulation of three hypotheses of how social structure overcomes the limitations of plausibility considerations. the hypotheses are tested by the means of simulation. the simulation results corroborate two hypotheses and indicate the validity of the third hypothesis.
128	100166	article	science	\N	\N	american association for the advancement of science	6	303	5659	2004	feb	2005-02-21 19:04:19	school of computer science and engineering, hebrew university, 91904 jerusalem, israel. nir@cs.huji.ac.il	inferring cellular networks using probabilistic graphical models	high-throughput genome-wide molecular assays, which probe cellular networks from different perspectives, have become central to molecular biology. probabilistic graphical models are useful for extracting meaningful biological insights from the resulting data sets. these models provide a concise representation of complex cellular networks by composing simpler submodels. procedures based on well-understood principles for inferring such models from data facilitate a model-based methodology for analysis and discovery. this methodology and its capabilities are illustrated by several recent applications to gene expression data.
129	100184	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2005-02-21 20:12:29	\N	{peer-to-peer} communication across network address translators	jâ€™fais des trous, des petits trous... toujours des petits trous- s. gainsbourg network address translation (nat) causes well-known difficulties for peer-to-peer (p2p) communication, since the peers involved may not be reachable at any globally valid ip address. several nat traversal techniques are known, but their documentation is slim, and data about their robustness or relative merits is slimmer. this paper documents and analyzes one of the simplest but most robust and practical nat traversal techniques, commonly known as â€œhole punching. â€ hole punching is moderately well-understood for udp communication, but we show how it can be reliably used to set up peer-to-peer tcp streams as well. after gathering data on the reliability of this technique on a wide variety of deployed nats, we find that about 82 % of the nats tested support hole punching for udp, and about 64 % support hole punching for tcp streams. as nat vendors become increasingly conscious of the needs of important p2p applications such as voice over ip and online gaming protocols, support for hole punching is likely to increase in the future. 1
130	100185	article	internet computing, ieee	internet computing, ieee	\N	ieee	7	6	2	2002	mar	2005-02-21 20:28:00	\N	unraveling the web services web: an introduction to {soap}, {wsdl}, and {uddi}	this tutorial explores the most salient and stable specifications in each of the three major areas of the emerging web services framework. they are the simple object access protocol, the web services description language and the universal description, discovery, and integration directory, which is a registry of web services descriptions.
131	100219	article	applied ergonomics	\N	\N	\N	11	19	1	1988	mar	2005-02-21 23:53:28	\N	the psychology of personal information management	a requirement of 'the office of the future' is that it provides us with an effective way of storing and retrieving information. but existing {it} products go nowhere near supporting the variety of activities which can be observed in paper-based offices, and it is not surprising that concepts of the 'paperless office' are as far off as they were when the idea was first mooted. this paper illustrates how many of the issues involved in the automation of information management are essentially psychological in nature. these principally devolve upon the processes of recall, recognition and categorisation. examples of existing information management techniques show how there is a trend to automate with a view to simulating office practices, or to develop according to the availability of technological solutions. both of these are inefficient with respect to the user's psychological needs. a framework for developing user-oriented information management systems is discussed and relevant research issues presented.
132	100359	article	current opinion in neurobiology	\N	\N	\N	8	14	2	2004	apr	2005-02-22 21:51:54	department of anatomy, university of cambridge, downing street, cambridge cb2 3dy, uk. ws234@cam.ac.uk	neural coding of basic reward terms of animal learning theory, game theory, microeconomics and behavioural ecology.	neurons in a small number of brain structures detect rewards and reward-predicting stimuli and are active during the expectation of predictable food and liquid rewards. these neurons code the reward information according to basic terms of various behavioural theories that seek to explain reward-directed learning, approach behaviour and decision-making. the involved brain structures include groups of dopamine neurons, the striatum including the nucleus accumbens, the orbitofrontal cortex and the amygdala. the reward information is fed to brain structures involved in decision-making and organisation of behaviour, such as the dorsolateral prefrontal cortex and possibly the parietal cortex. the neural coding of basic reward terms derived from formal theories puts the neurophysiological investigation of reward mechanisms on firm conceptual grounds and provides neural correlates for the function of rewards in learning, approach behaviour and decision-making.
133	100476	article	wirel. netw.	\N	\N	springer-verlag new york, inc.	17	11	1-2	2005	jan	2008-02-28 18:43:45	secaucus, nj, usa	ariadne: a secure on-demand routing protocol for ad hoc networks	an ad hoc network is a group of wireless mobile computers (or nodes), in which individual nodes cooperate by forwarding packets for each other to allow nodes to communicate beyond direct wireless transmission range. prior research in ad hoc networking has generally studied the routing problem in a non-adversarial setting, assuming a trusted environment. in this paper, we present attacks against routing in ad hoc networks, and we present the design and performance evaluation of a new secure on-demand ad hoc network routing protocol, called ariadne. ariadne prevents attackers or compromised nodes from tampering with uncompromised routes consisting of uncompromised nodes, and also prevents many types of {denial-of-service} attacks. in addition, ariadne is efficient, using only highly efficient symmetric cryptographic primitives.
134	101942	proceedings	system sciences, 2001. proceedings of the 34th annual hawaii international conference on	\N	\N	\N	\N	\N	\N	2001	\N	2005-02-23 16:45:11	\N	knowledge management in action: integrating knowledge across communities	this paper offers a brief overview and critique of dominant approaches to knowledge management ({km}) and its links with innovation. it then draws upon a case study example to offer a closer analysis of the link between {km} and the development of communities of practice during processes of innovation. the paper argues, first, that in many cases innovation is an interactive process requiring knowledge and expertise from different functions and layers across the organization. in such cases critical problems concern the integration of knowledge across disparate communities, rather than the sharing of knowledge within communities. second that if knowledge integration across communities is to develop, a more action-oriented perspective on {km} and the development of {km} tools is needed.
135	101983	article	nature	\N	\N	nature publishing group	5	433	7028	2005	feb	2005-02-25 12:51:29	\N	functional cartography of complex metabolic networks	high-throughput techniques are leading to an explosive growth in the size of biological databases and creating the opportunity to revolutionize our understanding of life and disease. interpretation of these data remains, however, a major scientific challenge. here, we propose a methodology that enables us to extract and display information contained in complex networks1, 2, 3. specifically, we demonstrate that we can find functional modules4, 5 in complex networks, and classify nodes into universal roles according to their pattern of intra- and inter-module connections. the method thus yields a 'cartographic representation' of complex networks. metabolic networks6, 7, 8 are among the most challenging biological networks and, arguably, the ones with most potential for immediate applicability9. we use our method to analyse the metabolic networks of twelve organisms from three different superkingdoms. we find that, typically, 80\\% of the nodes are only connected to other nodes within their respective modules, and that nodes with different roles are affected by different evolutionary constraints and pressures. remarkably, we find that metabolites that participate in only a few reactions but that connect different modules are more conserved than hubs whose links are mostly within a single module.
136	103166	article	organizational behavior and human decision processes	\N	\N	\N	32	50	\N	1991	\N	2005-02-24 20:30:06	\N	the theory of planned behavior	research dealing with various aspects of the theory of planned behavior ( ajzen, 1985 and ajzen, 1987 ) is reviewed, and some unresolved issues are discussed. in broad terms, the theory is found to be well supported by empirical evidence. intentions to perform behaviors of different kinds can be predicted with high accuracy from attitudes toward the behavior, subjective norms, and perceived behavioral control; and these intentions, together with perceptions of behavioral control, account for considerable variance in actual behavior. attitudes, subjective norms, and perceived behavioral control are shown to be related to appropriate sets of salient behavioral, normative, and control beliefs about the behavior, but the exact nature of these relations is still uncertain. expectancy-value formulations are found to be only partly successful in dealing with these relations. optimal rescaling of expectancy and value measures is offered as a means of dealing with measurement limitations. finally, inclusion of past behavior in the prediction equation is shown to provide a means of testing the theory's sufficiency, another issue that remains unresolved. the limited available evidence concerning this question shows that the theory is predicting behavior quite well in comparison to the ceiling imposed by behavioral reliability.
137	103180	article	annual review of sociology	\N	\N	\N	28	26	\N	2000	\N	2005-02-24 20:30:06	\N	framing processes and social movements: an overview and assessment	the recent proliferation of scholarship on collective action frames and framing processes in relation to social movements indicates that framing processes have come to be regarded, alongside resource mobilization and political opportunity processes, as a central dynamic in understanding the character and course of social movements. this review examines the analytic utility of the framing literature for understanding social movement dynamics. we first review how collective action frames have been conceptualized, including their characteristic and variable features. we then examine the literature related to framing dynamics and processes. next we review the literature regarding various contextual factors that constrain and facilitate framing processes. we conclude with an elaboration of the consequences of framing processes for other movement processes and outcomes. we seek throughout to provide clarification of the linkages between framing concepts/processes and other conceptual and theoretical formulations relevant to social movements, such as schemas and ideology. copyright Â© 2000 by annual reviews. all rights reserved.
138	103243	book	\N	\N	\N	hobart press	\N	\N	\N	1993	\N	2005-02-24 20:30:07	summit, nj	visualizing data	enormous quantities of data go unused or underused today, simply because people can't visualize the quantities and relationships in it. using a downloadable programming environment developed by the author, visualizing data demonstrates methods for representing data accurately on the web and elsewhere, complete with user interaction, animation, and more. how do the 3.1 billion a, c, g and t letters of the human genome compare to those of a chimp or a mouse? what do the paths that millions of visitors take through a web site look like? with visualizing data, you learn how to answer complex questions like these with thoroughly interactive displays. we're not talking about cookie-cutter charts and graphs. this book teaches you how to design entire interfaces around large, complex data sets with the help of a powerful new design and prototyping tool called "processing". used by many researchers and companies to convey specific data in a clear and understandable manner, the processing beta is available free. with this tool and visualizing data as a guide, you'll learn basic visualization principles, how to choose the right kind of display for your purposes, and how to provide interactive features that will bring users to your site over and over. this book teaches you:        the seven stages of visualizing data -- acquire, parse, filter, mine, represent, refine, and interact   how all data problems begin with a question and end with a narrative construct that provides a clear answer without extraneous details   several example projects with the code to make them work   positive and negative points of each representation discussed. the focus is on customization so that each one best suits what you want to convey about your data set  the book does not provide ready-made "visualizations" that can be plugged into any data set. instead, with chapters divided by types of data rather than types of display, you'll learn how each visualization conveys the unique properties of the data it represents -- why the data was collected, what's interesting about it, and what stories it can tell. visualizing data teaches you how to answer questions, not simply display information.
139	103291	book	\N	\N	\N	edward arnold	\N	\N	\N	1999	\N	2005-02-24 20:30:08	london	multilevel statistical models	{this new edition of this classic incorporates the most recent thinking on methodology and software, as well as the latest literature on multilevel statistical models. topics covered by multilevel models have increased in recent years, and the methods are widely applied in the social sciences<br>as well as in areas such as epidemiology, geography, education, surveys, and medicine. this third edition includes chapters on meta analysis, factor analysis and structural equation models, and has expanded sections on mcmc methods.}
140	103567	book	\N	\N	\N	princeton university press	\N	\N	\N	2002	\N	2005-02-24 21:00:55	\N	the theory of incentives: the principal-agent model	{economics has much to do with incentives--not least, incentives to work hard, to produce quality products, to study, to invest, and to save. although adam smith amply confirmed this more than two hundred years ago in his analysis of sharecropping contracts, only in recent decades has a theory begun to emerge to place the topic at the heart of economic thinking. in this book, jean-jacques laffont and david martimort present the most thorough yet accessible introduction to incentives theory to date. central to this theory is a simple question as pivotal to modern-day management as it is to economics research: what makes people act in a particular way in an economic or business situation? in seeking an answer, the authors provide the methodological tools to design institutions that can ensure good incentives for economic agents.<p>this book focuses on the principal-agent model, the "simple" situation where a principal, or company, delegates a task to a single agent through a contract--the essence of management and contract theory. how does the owner or manager of a firm align the objectives of its various members to maximize profits? following a brief historical overview showing how the problem of incentives has come to the fore in the past two centuries, the authors devote the bulk of their work to exploring principal-agent models and various extensions thereof in light of three types of information problems: adverse selection, moral hazard, and non-verifiability. offering an unprecedented look at a subject vital to industrial organization, labor economics, and behavioral economics, this book is set to become the definitive resource for students, researchers, and others who might find themselves pondering what contracts, and the incentives they embody, are really all about.}
141	103573	book	\N	\N	\N	cambridge university press	\N	\N	\N	2003	\N	2005-02-24 21:00:55	\N	public choice {iii}	{this book represents a considerable revision and expansion of public choice ii (1989). as in the previous editions, all of the major topics of public choice are covered.  these include: why the state exists, voting rules, federalism, the theory of clubs, two-party and multiparty electoral systems, rent seeking, bureaucracy, interest groups, dictatorship, the size of government, voter participation, and political business cycles.  normative issues in public choice are also examined.  the book is suitable for upper level courses in economics dealing with politics, and political science courses emphasizing rational actor models.} {this book represents a considerable revision and expansion of public choice ii (1989). as in the previous editions, all of the major topics of public choice are covered.  these include: why the state exists, voting rules, federalism, the theory of clubs, two-party and multiparty electoral systems, rent seeking, bureaucracy, interest groups, dictatorship, the size of government, voter participation, and political business cycles.  normative issues in public choice are also examined.  the book is suitable for upper level courses in economics dealing with politics, and political science courses emphasizing rational actor models.}
142	104275	article	commun. acm	\N	\N	acm	11	39	1	1996	jan	2005-02-25 20:27:54	new york, ny, usa	information extraction	an abstract is not available.
143	104324	article	theoretical computer science	\N	\N	\N	34	\N	\N	1975	\N	2005-02-26 03:28:54	\N	call-by-name, call-by-value, and the \\$\\lambda\\$-calculus	this paper examines the old question of the relationship between iswim and the Î»-calculus, using the distinction between call-by-value and call-by-name. it is held that the relationship should be mediated by a standardisation theorem. since this leads to difficulties, a new Î»-calculus is introduced whose standardisation theorem gives a good correspondence with iswim as given by the secd machine, but without the  letrec  feature. next a call-by-name variant of iswim is introduced which is in an analogous correspondence withthe usual Î»-calculus. the relation between call-by-value and call-by-name is then studied by giving simulations of each language by the other and interpretations of each calculus in the other. these are obtained as another application of the continuation technique. some emphasis is placed throughout on the notion of operational equality (or contextual equality). if terms can be proved equal in a calculus they are operationally equal in the corresponding language. unfortunately, operational equality is not preserved by either of the simulations.
144	104354	misc	\N	\N	\N	\N	\N	\N	\N	2003	\N	2005-02-26 03:48:14	\N	geographic routing without location information	for many years, scalable routing for wireless communication systems was a compelling but elusive goal. recently, several routing algorithms that exploit geographic information (e.g. {gpsr)} have been proposed to achieve this goal. these algorithms refer to nodes by their location, not address, and use those coordinates to route greedily, when possible, towards the destination. however, there are many situations where location information is not available at the nodes, and so geographic methods cannot be used. in this paper we define a scalable coordinate-based routing algorithm that does not rely on location information, and thus can be used in a wide variety of ad hoc and sensornet environments.
145	104769	article	inf. process. manage.	\N	\N	pergamon press, inc.	15	40	5	2004	sep	2005-02-26 16:04:34	tarrytown, ny, usa	combining the language model and inference network approaches to retrieval	the inference network retrieval model, as implemented in the {inquery} search engine, allows for richly structured queries. however, it incorporates a form of ad hoc tf.idf estimates for word probabilities. language modeling offers more formal estimation techniques. in this paper we combine the language modeling and inference network approaches into a single framework. the resulting model allows structured queries to be evaluated using language modeling estimates. we explore the issues involved, such as combining beliefs and smoothing of proximity nodes. experimental results are presented comparing the query likelihood model, the {inquery} system, and our new model. the results reaffirm that high quality structured queries outperform unstructured queries and show that our system consistently achieves higher average precision than {inquery}.
146	104811	article	british journal of educational technology	\N	\N	blackwell publishing ltd.	18	36	2	2005	mar	2005-06-02 11:20:24	\N	an instructional model for web-based e-learning education with a blended learning process approach	web-based e-learning education research and development now focuses on the inclusion of new technological features and the exploration of software standards. however, far less effort is going into finding solutions to psychopedagogical problems in this new educational category. this paper proposes a psychopedagogical instructional model based on content structure, the latest research into information processing psychology and social contructivism, and defines a blended approach to the learning process. technologically speaking, the instructional model is supported by learning objects, a concept inherited from the object-oriented paradigm.
147	104965	article	autonomous robots	\N	\N	\N	38	8	3	2000	\N	2005-02-26 17:58:32	\N	multiagent systems: a survey from a machine learning perspective	distributed artificial intelligence (dai) has existed as a subfield of ai for less than two decades. dai is concerned with systems that consist of multiple independent entities that interact in a domain. traditionally, dai has been divided into two sub-disciplines: distributed problem solving (dps) focuses on the information management aspects of systems with several components working together towards a common goal; multiagent systems (mas) deals with behavior management in collections of several independent entities, or agents. this survey of mas is intended to serve as an introduction to the field and as an organizational framework. a series of general multiagent scenarios are presented. for each scenario, the issues that arise are described along with a sampling of the techniques that exist to deal with them. the presented techniques are not exhaustive, but they highlight how multiagent systems can be and have been used to build complex systems. when options exist, the techniques presented are biased towards machine learning approaches. additional opportunities for applying machine learning to mas are highlighted and robotic soccer is presented as an appropriate test bed for mas. this survey does not focus exclusively on robotic systems. however, we believe that much of the prior research in non-robotic mas is relevant to robotic mas, and we explicitly discuss several robotic mas, including all of those presented in this issue.
148	105195	article	bioinformatics	\N	\N	\N	\N	\N	\N	2005	feb	2005-02-26 21:32:59	center for information biology and dna data bank of japan, national institute of genetics, mishima, 411-8540, japan; department of genetics, the graduate university for advanced studies (sokendai), mishima, 411-8540, japan.	recoverable one-dimensional encoding of protein three-dimensional structures.	protein one-dimensional ({1d}) structures such as secondary structure and contact number provide intuitive pictures to understand how the native three-dimensional ({3d}) structure of a protein is encoded in the amino acid sequence. however, it has not been clear whether a given set of {1d} structures contains sufficient information for recovering the underlying {3d} structure. here we show that the {3d} structure of a protein can be recovered from a set of three types of {1d} structures, namely, secondary structure, contact number and residue-wise contact order which is introduced here for the first time. using simulated annealing molecular dynamics simulations, the structures satisfying the given native {1d} structural restraints were sought for 16 proteins of various structural classes and of sizes ranging from 56 to 146 residues. by selecting the structures best satisfying the restraints, all the proteins showed a coordinate {rms} deviation of less than {4a} from the native structure, and for most of them, the deviation was even less than {2a}. the present result opens a new possibility to protein structure prediction and our understanding of the sequence-structure relationship.
149	105205	misc	\N	\N	\N	\N	\N	\N	\N	2003	\N	2005-02-26 21:37:11	\N	splitstream: high-bandwidth multicast in cooperative environments	in tree-based multicast systems, a relatively small number of interior nodes carry the load of forwarding multicast messages. this works well when the interior nodes are highly-available, dedicated infrastructure routers but it poses a problem for application-level multicast in peer-to-peer systems. splitstream addresses this problem by striping the content across a forest of interior-node-disjoint multicast trees that distributes the forwarding load among all participating peers. for example, it is possible to construct efficient splitstream forests in which each peer contributes only as much forwarding bandwidth as it receives. furthermore, with appropriate content encodings, splitstream is highly robust to failures because a node failure causes the loss of a single stripe on average. we present the design and implementation of splitstream and show experimental results obtained on an internet testbed and via large-scale network simulation. the results show that splitstream distributes the forwarding load among all peers and can accommodate peers with different bandwidth capacities while imposing low overhead for forest construction and maintenance.
150	105569	book	\N	\N	\N	{psychology press (uk)}	\N	\N	\N	2000	aug	2005-02-27 00:46:20	\N	cognitive psychology: a student's handbook	{this is a thorough revision and updating of the extremely successful third edition. as in previous editions, the following three perspectives are considered in depth: experimental cognitive psychology; cognitive science, with its focus on cognitive modelling; and cognitive neuropsychology with its focus on cognition following brain damage. in addition, and new to this edition, is detailed discussion of the cognitive neuroscience perspective, which uses advanced brain-scanning techniques to clarify the functioning of the human brain. there is detailed coverage of the dynamic impact of these four perspectives on the main areas of cognitive psychology, including perception, attention, memory, knowledge representation, categorisation, language, problem-solving, reasoning, and judgement.<br> the aim is to provide comprehensive coverage that is up-to-date, authoritative, and accessible. all existing chapters have been extensively revised and re-organised. some of the topics receiving much greater coverage in this edition are: brain structures in perception, visual attention, implicit learning, brain structures in memory, prospective memory, exemplar theories of categorisation, language comprehension, connectionist models in perception, neuroscience studies of thinking, judgement, and decision making.<br> cognitive psychology: a students handbook will be essential reading for undergraduate students of psychology. it will also be of interest to students taking related courses in computer science, education, linguistics, physiology, and medicine.}
151	105594	article	system	\N	\N	\N	16	29	2	2001	jun	2005-02-27 02:15:19	\N	e-mail writing as a cross-cultural learning experience	this study looks into the cultural dimension involved in the e-mail correspondence between university {efl} students in taiwan and pre-service {bilingual/esl} teachers in the {usa}. e-mail entries and end-of-project reports were analyzed to yield insights into the cross-cultural communication process. the data analysis focused on the types of cultural information transmitted and effects of cultural assumptions and values on communication effectiveness. the findings revealed perceived fundamental characteristics of both chinese and american cultures by the two groups of participants. it was also found that curiosity toward the other culture was a motivating factor for on-going correspondence, but cultural presumptions were sometimes a hindrance for communication; positive interpretations of cultural differences and empathy were key factors contributing to the removal of communication obstacles. although there is no substitute for actual experiences of immersing into the target culture, cross-cultural e-mail correspondence sensitized the participants to cultural differences and served as a learning experience for better cross-cultural understanding.
152	105835	book	\N	\N	\N	wiley-interscience	\N	\N	\N	2001	may	2005-02-27 12:41:27	\N	independent component analysis	{a comprehensive introduction to ica for students and practitioners<br>   independent component analysis (ica) is one of the most exciting new topics in fields such as neural networks, advanced statistics, and signal processing. this is the first book to provide a comprehensive introduction to this new technique complete with the fundamental mathematical background needed to understand and utilize it. it offers a general overview of the basics of ica, important solutions and algorithms, and in-depth coverage of new applications in image processing, telecommunications, audio signal processing, and more.<br>   independent component analysis is divided into four sections that cover:<br>   * general mathematical concepts utilized in the book<br>   * the basic ica model and its solution<br>   * various extensions of the basic ica model<br>   * real-world applications for ica models<br>   authors hyvarinen, karhunen, and oja are well known for their contributions to the development of ica and here cover all the relevant theory, new algorithms, and applications in various fields. researchers, students, and practitioners from a variety of disciplines will find this accessible volume both helpful and informative.}
153	105906	book	\N	\N	\N	the mit press	\N	\N	\N	1999	jun	2005-02-27 13:16:43	\N	foundations of statistical natural language processing	{"statistical natural-language processing is, in my estimation, one of the most fast-moving and exciting areas of computer science these days. anyone who wants to learn this field would be well advised to get this book. for that matter, the same goes for anyone who is already in the field. i know that it is going to be one of the most well-thumbed books on my bookshelf." -- eugene charniak, department of computer science, brown university  <p>statistical approaches to processing natural language text have become dominant in recent years. this foundational text is the first comprehensive introduction to statistical natural language processing (nlp) to appear. the book contains all the theory and algorithms needed for building nlp tools. it provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. the book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications.  <p>more on this book}
154	106131	book	\N	\N	\N	wolfram media	\N	\N	\N	2002	may	2005-02-27 18:21:03	\N	a new kind of science	{physics and computer science genius stephen wolfram, whose mathematica computer language launched a multimillion-dollar company, now sets his sights on a more daunting goal: understanding the universe. wolfram lets the world see his work in <i>a new kind of science</i>, a gorgeous, 1,280-page tome more than a decade in the making. with patience, insight, and self-confidence to spare, wolfram outlines a fundamental new way of modeling complex systems.<p>  on the frontier of complexity science since he was a boy, wolfram is a  champion of cellular automata--256 "programs" governed by simple  nonmathematical rules. he points out that even the most complex  equations fail to accurately model biological systems, but the simplest  cellular automata can produce results straight out of nature--tree  branches, stream eddies, and leopard spots, for instance. the graphics  in <i>a new kind of science</i> show striking resemblance to the  patterns we see in nature every day.<p>  wolfram wrote the book in a distinct style meant to make it easy to read,   even for nontechies; a basic familiarity with logic is helpful but  not essential. readers will find themselves swept away by the elegant  simplicity of wolfram's ideas and the accidental artistry of the  cellular automaton models. whether or not wolfram's revolution  ultimately gives us the keys to the universe, his new science is  absolutely awe-inspiring. <i>--therese littleton</i>} {this long-awaited work from one of the world's most respected scientists presents a series of dramatic discoveries never before made public. starting from a collection of simple computer experiments---illustrated in the book by striking computer graphics---wolfram shows how their unexpected results force a whole new way of looking at the operation of our universe.  <p>wolfram uses his approach to tackle a remarkable array of fundamental problems in science: from the origin of the second law of thermodynamics, to the development of complexity in biology, the computational limitations of mathematics, the possibility of a truly fundamental theory of physics, and the interplay between free will and determinism.  <p>written with exceptional clarity, and illustrated by more than a thousand original pictures, this seminal book allows scientists and non-scientists alike to participate in what promises to be a major intellectual revolution.}
155	106318	book	\N	\N	\N	oxford university press, usa	\N	\N	\N	2005	dec	2005-02-28 00:41:26	\N	electric fields of the brain: the neurophysics of {eeg},  2nd edition	electroencephalography (eeg) is practiced by neurologists, cognitive neuroscientists, and others interested in functional brain imaging. whether for clinical or experimental purposes, all studies share a common purpose-to relate scalp potentials to the underlying neurophysiology. electrical potentials on the scalp exhibit spatial and temporal patterns that depend on the nature and location of the sources and the way that currents and fields spread through tissue. because these dynamic patterns are correlated with behavior and cognition, eeg provides a "window on the mind," correlating physiology and psychology. this classic and widely acclaimed text, originally published in 1981, filled the large gap between eeg and the physical sciences. it has now been brought completely up to date and will again serve as an invaluable resource for understanding the principles of electric fields in living tissue and for using hard science to study human consciousness and cognition. no comparable volume exists for it is no easy task to explain the problems of eeg in clear language, with mathematics presented mainly in appendices. among the many topics covered by the second edition are micro and meso (intermediate scale) synaptic sources, electrode placement, choice of reference, volume conduction, power and coherence measures, projection of scalp potentials to dura surface, dynamic signatures of conscious experience, neural networks immersed in global fields of synaptic action, and physiological bases for brain source dynamics. the second edition is an invaluable resource for neurologists, neuroscientists (especially cognitive neuroscientists), biomedical engineers, and their students and trainees. it will also appeal to physicists, mathematicians, computer scientists, psychiatrists, and industrial engineers interested in eeg.
156	106560	book	\N	\N	\N	{harvard university press}	\N	\N	\N	2004	apr	2005-02-28 14:46:11	\N	the success of open source	{<p> much of the innovative programming that powers the internet, creates operating systems, and produces software is the result of "open source" code, that is, code that is freely distributed--as opposed to being kept secret--by those who write it. leaving source code open has generated some of the most sophisticated developments in computer technology, including, most notably, linux and apache, which pose a significant challenge to microsoft in the marketplace. as steven weber discusses, open source's success in a highly competitive industry has subverted many assumptions about how businesses are run, and how intellectual products are created and protected. </p><p> traditionally, intellectual property law has allowed companies to control knowledge and has guarded the rights of the innovator, at the expense of industry-wide cooperation. in turn, engineers of new software code are richly rewarded; but, as weber shows, in spite of the conventional wisdom that innovation is driven by the promise of individual and corporate wealth, ensuring the free distribution of code among computer programmers can empower a more effective process for building intellectual products. in the case of open source, independent programmers--sometimes hundreds or thousands of them--make unpaid contributions to software that develops organically, through trial and error. </p><p> weber argues that the success of open source is not a freakish exception to economic principles. the open source community is guided by standards, rules, decisionmaking procedures, and sanctioning mechanisms. weber explains the political and economic dynamics of this mysterious but important market development.  </p>}
157	106572	book	\N	\N	\N	{edward elgar publishing}	\N	\N	\N	2004	nov	2010-09-01 13:20:28	\N	the network society: a {cross-cultural} perspective	{manuel castells - one of the world&#146;s pre-eminent social scientists - has drawn together a stellar group of contributors to explore the patterns and dynamics of the network society in its cultural and institutional diversity. the book analyzes the technological, cultural and institutional transformation of societies around the world in terms of the critical role of electronic communication networks in business, everyday life, public services, social interaction and politics. the contributors demonstrate that the network society is the new form of social organization in the information age, replacing the industrial society. the book analyzes processes of technological transformation in interaction with social culture in different cultural and institutional contexts: the united states of america, the united kingdom, finland, russia, china, india, canada, and catalonia. the topics examined include business productivity, global financial markets, cultural identity, the uses of the internet in education and health, the anti-globalization movement, political processes, media and identity, and public policies to guide technological development. taken together these studies show that the network society adopts very different forms, depending on the cultural and institutional environments in which it evolves. the network society, now available in paperback, is an outstanding and original volume of direct interest in academia - particularly in the fields of social sciences, communication studies, and business schools - as well as for policymakers engaged in technological policy and economic development. business and management experts will also discover much of value to them within this book.}
158	106629	article	bioinformatics (oxford, england)	bioinformatics	\N	oxford university press	12	21	5	2005	mar	2006-06-12 13:48:52	\N	a comprehensive evaluation of multicategory classification methods for microarray gene expression cancer diagnosis.	motivation: cancer diagnosis is one of the most important emerging clinical applications of gene expression microarray technology. we are seeking to develop a computer system for powerful and reliable cancer diagnostic model creation based on microarray data. to keep a realistic perspective on clinical applications we focus on multicategory diagnosis. to equip the system with the optimum combination of classifier, gene selection and cross-validation methods, we performed a systematic and comprehensive evaluation of several major algorithms for multicategory classification, several gene selection methods, multiple ensemble classifier methods and two cross-validation designs using 11 datasets spanning 74 diagnostic categories and 41 cancer types and 12 normal tissue types. results: multicategory support vector machines (mc-svms) are the most effective classifiers in performing accurate cancer diagnosis from gene expression data. the mc-svm techniques by crammer and singer, weston and watkins and one-versus-rest were found to be the best methods in this domain. mc-svms outperform other popular machine learning algorithms, such as k-nearest neighbors, backpropagation and probabilistic neural networks, often to a remarkable degree. gene selection techniques can significantly improve the classification performance of both mc-svms and other non-svm learning algorithms. ensemble classifiers do not generally improve performance of the best non-ensemble models. these results guided the construction of a software system gems (gene expression model selector) that automates high-quality model construction and enforces sound optimization and performance estimation procedures. this is the first such system to be informed by a rigorous comparative analysis of the available algorithms and datasets. availability: the software system gems is available for download from http://www.gems-system.org for non-commercial use. contact: alexander.statnikov@vanderbilt.edu.
159	106735	proceedings	computer communications and networks, 1999. proceedings. eight international conference on	computer communications and networks, 1999. proceedings. eight international conference on	\N	\N	6	\N	\N	1999	\N	2005-03-01 02:36:07	\N	on-demand multipath routing for mobile ad hoc networks	mobile ad hoc networks are characterized by multi-hop wireless links, absence of any cellular infrastructure, and frequent host mobility. design of efficient routing protocols in such networks is a challenging issue. a class of routing protocols called on-demand protocols has recently attracted attention because of their low routing overhead. the on-demand protocols depend on query floods to discover routes whenever a new route is needed. such floods take up a substantial portion of network bandwidth. we focus on a particular on-demand protocol, called dynamic source routing, and show how intelligent use of multipath techniques can reduce the frequency of query floods. we develop an analytic modeling framework to determine the relative frequency of query floods for various techniques. results show that while multipath routing is significantly better than single path routing, the performance advantage is small beyond a few paths and for long path lengths. it also shows that providing all intermediate nodes in the primary (shortest) route with alternative paths has a significantly better performance than providing only the source with alternate paths
160	106736	inproceedings	sigcomm comput. commun. rev.	proceedings of the conference on communications architectures, protocols and applications	sigcomm	acm	10	24	4	1994	oct	2005-03-01 02:40:44	new york, ny, usa	highly dynamic {destination-sequenced} {distance-vector} routing ({dsdv}) for mobile computers	an ad-hoc network is the cooperative engagement of a collection of mobile hosts without the required intervention of any centralized access point. in this paper we present an innovative design for the operation of such ad-hoc networks. the basic idea of the design is to operate each mobile host as a specialized router, which periodically advertises its view of the interconnection topology with other mobile hosts within the network. this amounts to a new sort of routing protocol. we have investigated modifications to the basic {bellman-ford} routing mechanisms, as specified by {rip} [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize ad hoc networks. our modifications address some of the previous  objections  to the use of {bellman-ford}, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the mobile hosts. finally, we describe the ways in which the basic network-layer routing can be modified to provide {mac}-layer support for ad-hoc networks.
161	107519	inproceedings	\N	the semantic web --- iswc 2002. proceedings of the first international semantic web conference	lecture notes in computer science	springer-verlag: heidelberg, germany	14	2348	\N	2002	jun	2005-03-01 10:11:45	\N	semantic matching of web services capabilities	the web is moving from being a collection of pages toward a collection of services that interoperate through the internet. the first step toward this interoperation is the location of other services that can help toward the solution of a problem. in this paper we claim that lo- cation of web services should be based on the semantic match between a declarative description of the service being sought, and a description of the service being offered. furthermore, we claim that this match is outside the representation capabilities of registries such as uddi and languages such as wsdl. we propose a solution based on daml-s, a daml-based language for service description, and we show how service capabilities are presented in the profile section of a daml-s description and how a semantic match between advertisements and requests is performed.
162	108491	inproceedings	\N	proceedings of the 4th annual acm/ieee international conference on mobile computing and networking	mobicom	acm	12	\N	\N	1998	\N	2005-03-01 17:58:36	new york, ny, usa	a performance comparison of multi-hop wireless ad hoc network routing protocols	an abstract is not available.
163	109109	article	pattern analysis and machine intelligence, ieee transactions on	\N	\N	\N	13	15	9	1993	\N	2005-03-02 00:37:20	\N	comparing images using the hausdorff distance	the hausdorff distance measures the extent to which each point of a model set lies near some point of an image set and vice versa. thus, this distance can be used to determine the degree of resemblance between two objects that are superimposed on one another. efficient algorithms for computing the hausdorff distance between all possible relative positions of a binary image and a model are presented. the focus is primarily on the case in which the model is only allowed to translate with respect to the image. the techniques are extended to rigid motion. the hausdorff distance computation differs from many other shape comparison methods in that no correspondence between the model and the image is derived. the method is quite tolerant of small position errors such as those that occur with edge detectors and other feature extraction methods. it is shown that the method extends naturally to the problem of comparing a portion of a model against an image
164	109748	inproceedings	\N	proceedings of the 1998 acm sigmod international conference on management of data	\N	acm press	11	\N	\N	1998	\N	2005-03-02 06:23:27	\N	automatic subspace clustering of high dimensional data for data mining applications	data mining applications place special requirements on clustering algorithms including: the ability to find clusters embedded in subspaces of high dimensional data, scalability, end-user comprehensibility of the results, non-presumption of any canonical data distribution, and insensitivity to the order of input records. we present clique, a clustering algorithm that satisfies each of these requirements. clique identifies dense clusters in subspaces of maximum dimensionality. it generates cluster descriptions in the form of dnf expressions that are minimized for ease of comprehension. it produces identical results irrespective of the order in which input records are presented and does not presume any specific mathematical form for data distribution. through experiments, we show that clique efficiently finds accurate clusters in large high dimensional datasets.
165	109791	inproceedings	\N	proceedings of the ninth acm sigkdd international conference on knowledge discovery and data mining	\N	acm press	9	\N	\N	2003	\N	2005-03-02 06:23:28	\N	information-theoretic co-clustering	two-dimensional contingency or co-occurrence tables arise frequently in important applications such as text, web-log and market-basket data analysis. a basic problem in contingency table analysis is  co-clustering: simultaneous clustering  of the rows and columns. a novel theoretical formulation views the contingency table as an empirical joint probability distribution of two discrete random variables and poses the co-clustering problem as an optimization problem in  information theory ---the optimal co-clustering maximizes the mutual information between the clustered random variables subject to constraints on the number of row and column clusters. we present an innovative co-clustering algorithm that monotonically increases the preserved mutual information by intertwining both the row and column clusterings at all stages. using the practical example of simultaneous word-document clustering, we demonstrate that our algorithm works well in practice, especially in the presence of sparsity and high-dimensionality.
166	109806	article	sigmod record	\N	\N	\N	15	27	3	1998	\N	2005-03-02 06:23:29	\N	database techniques for the world-wide web: {a} survey	the primary goal of this survey is to classify the different tasks to which database concepts have been applied, and to emphasize the technical innovations that are required to do so. we focus on three classes of tasks related to information management on the www: (1) modeling and querying the web, (2) information extraction and integration, and (3) web site construction and restructuring. the survey showed that powerful abstractions developed in the database community may prove to be the key...
167	109996	article	nature genetics	\N	\N	\N	5	23	1	1999	\N	2005-03-02 06:33:08	\N	genome-wide analysis of {dna} copy-number changes using c{dna} microarrays	gene amplifications and deletions frequently contribute to tumorigenesis. characterization of these {dna} copy-number changes is important for both the basic understanding of cancer and its diagnosis. comparative genomic hybridization ({cgh}) was developed to survey {dna} copy-number variations across a whole genome1. with {cgh}, differentially labelled test and reference genomic {dnas} are co-hybridized to normal metaphase chromosomes, and fluorescence ratios along the length of chromosomes provide a cytogenetic representation of {dna} copy-number variation. {cgh}, however, has a limited (\\~{}20 mb) mapping resolution, and higher-resolution techniques, such as fluorescence in situ hybridization ({fish}), are prohibitively labour-intensive on a genomic scale. array-based {cgh}, in which fluorescence ratios at arrayed {dna} elements provide a locus-by-locus measure of {dna} copy-number variation, represents another means of achieving increased mapping resolution2-4. published array {cgh} methods have relied on large genomic clone (for example {bac}) array targets and have covered only a small fraction of the human genome. {cdnas} representing over 30,000 radiation-hybrid ({rh})?mapped human genes5, 6 provide an alternative and readily available genomic resource for mapping {dna} copy-number changes. although {cdna} microarrays have been used extensively to characterize variation in human gene expression7-9, human genomic {dna} is a far more complex mixture than the {mrna} representation of human cells. therefore, analysis of {dna} copy-number variation using {cdna} microarrays would require a sensitivity of detection an order of magnitude greater than has been routinely reported7. we describe here a {cdna} microarray-based {cgh} method, and its application to {dna} copy-number variation analysis in breast cancer cell lines and tumours. using this assay, we were able to identify gene amplifications and deletions genome-wide and with high resolution, and compare alterations in {dna} copy number and gene expression
168	110014	article	proceedings of the national academy of sciences	\N	\N	\N	5	96	6	1999	\N	2005-03-02 06:33:08	\N	interpreting patterns of gene expression with self-organizing maps: methods and application to hematopoietic differentiation	array technologies have made it straightforward to monitor simultaneously the expression pattern of thousands of genes. the challenge now is to interpret such massive data sets. the first step is to extract the fundamental patterns of gene expression inherent in the data. this paper describes the application of self-organizing maps, a type of mathematical cluster analysis that is particularly well suited for recognizing and classifying features in complex, multidimensional data. the method has been implemented in a publicly available computer package, , that performs the analytical calculations and provides easy data visualization. to illustrate the value of such analysis, the approach is applied to hematopoietic differentiation in four well studied models (hl-60, u937, jurkat, and nb4 cells). expression patterns of some 6,000 human genes were assayed, and an online database was created. was used to organize the genes into biologically relevant clusters that suggest novel hypotheses about hematopoietic differentiationâ€š{Ã„}{\\\\^\\\\i}for example, highlighting certain genes and pathways involved in â€š{Ã„}{Ãº}differentiation therapyâ€š{Ã„}{Ã¹} used in the treatment of acute promyelocytic leukemia.
169	110425	techreport	\N	\N	\N	\N	\N	\N	\N	2001	\N	2005-03-02 11:31:57	\N	annual report 2000-2001	the report summarizes the activities of ioc carried out in 1997 and is presented under the following major headings: 1) implementation of ioc governing body resolutions -- resolutions adopted by the nineteenth session of the assembly, resolutions adopted by the twenty-ninth session of the executive council, overview of ioc programme structure; 2) programme activities -- ocean sciences, ocean services, global ocean observing system, capacity building in marine sciences, services and observations: tema, regional activities; and, 3) cooperation and development -- cooperation with other organizations of the united nations system and other bodies, follow-up to unced and unclos, the 1998 international year of the ocean, development of ioc within unesco, and list of publications.
170	111657	article	sigir forum	\N	\N	acm	12	32	1	1998	apr	2006-04-25 21:08:22	new york, ny, usa	real life information retrieval: a study of user queries on the web	we analyzed transaction logs of a set of 51,473 queries posed by 18,113 users of excite, a major internet search service. we provide data on: (i) queries --- the number of search terms, and the use of logic and modifiers, (ii) sessions --- changes in queries during a session, number of pages viewed, and use of relevance feedback, and (iii) terms --- their rank/frequency distribution and the most highly used search terms. common mistakes are also observed. implications are discussed.
171	111761	inproceedings	\N	arist 37	\N	\N	\N	\N	\N	2002	\N	2005-03-02 17:56:20	\N	visualizing {k}nowledge {d}omains	this chapter reviews visualization techniques that can not only be utilized to map the ever- growing domain structure of scientific disciplines but that also support information retrieval and classification. in contrast to the comprehensive surveys done in a traditional way by howard white and katherine mccain (1997; 1998), the current survey not only reviews emerging techniques in interactive data analysis and information visualization, but also visualizes bibliographical structures of the field as an integral part of our methodology. the chapter starts with a review of the history of knowledge domain visualizations. we then introduce a general process flow for the visualization of knowledge domains and explain commonly used techniques. in the interest of visualizing the domain this article reviews, we introduce a bibliographic data set of considerable size, which includes articles from the citation analysis, bibliometrics, semantics, and visualization literatures. using a tutorial style, we then apply various algorithms to demonstrate the visualization effects produced by different approaches and compare the different visualization results. at the same time, the domain visualizations reveal the relationships within and between the four fields that together form the topic of this chapter, domain visualization. we conclude with a discussion of promising new avenues of research and a general discussion.
172	112856	inproceedings	\N	proceedings of the 11th acm sigact-sigplan symposium on principles of programming languages	popl	acm	5	\N	\N	1984	\N	2005-03-03 09:04:49	new york, ny, usa	efficient implementation of the smalltalk-80 system	the smalltalk-80* programming language includes dynamic storage allocation, full upward funargs, and universally polymorphic procedures; the smalltalk-80 programming system features interactive execution with incremental compilation, and implementation portability. these features of modern programming systems are among the most difficult to implement efficiently, even individually. a new implementation of the smalltalk-80 system, hosted on a small microprocessor-based computer, achieves high performance while retaining complete (object code) compatibility with existing implementations. this paper discusses the most significant optimization techniques developed over the course of the project, many of which are applicable to other languages. the key idea is to represent certain runtime state (both code and data) in more than one form, and to convert between forms when needed.
173	112873	article	bioinformatics	\N	\N	\N	8	20	suppl 1	2004	aug	2005-03-03 10:21:32	center for biomolecular science and engineering, baskin school of engineering university of california in santa cruz, 1156 high street, santa cruz, ca 95064.	into the heart of darkness: large-scale clustering of human non-coding {dna}	motivation: it is currently believed that the human genome contains about twice as much non-coding functional regions as it does protein-coding genes, yet our understanding of these regions is very {limited.results}: we examine the intersection between syntenically conserved sequences in the human, mouse and rat genomes, and sequence similarities within the human genome itself, in search of families of non-protein-coding elements. for this purpose we develop a graph theoretic clustering algorithm, akin to the highly successful methods used in elucidating protein sequence family {relationships.the} algorithm is applied to a highly filtered set of about 700 000 human–rodent evolutionarily conserved regions, not resembling any known coding sequence, which encompasses 3.7\\% of the human genome. from these, we obtain roughly 12 000 non-singleton clusters, dense in significant sequence similarities. further analysis of genomic location, evidence of transcription and {rna} secondary structure reveals many clusters to be significantly homogeneous in one or more characteristics. this subset of the highly conserved non-protein-coding elements in the human genome thus contains rich family-like structures, which merit in-depth {analysis.availability}: supplementary material to this work is available at http://www.soe.ucsc.edu/\\~{}jill/dark.html
174	112877	article	genome research	\N	\N	\N	11	13	12	2003	dec	2005-03-03 10:51:27	genome technology branch, national human genome research institute, national institutes of health, bethesda, maryland 20892, usa.	identification and characterization of {multi-species} conserved sequences	comparative sequence analysis has become an essential component of studies aiming to elucidate genome function. the increasing availability of genomic sequences from multiple vertebrates is creating the need for computational methods that can detect highly conserved regions in a robust fashion. towards that end, we are developing approaches for identifying sequences that are conserved across multiple species; we call these  ” multi-species conserved sequences” (or {mcss}). here we report two strategies for {mcs} identification, demonstrating their ability to detect virtually all known actively conserved sequences (specifically, coding sequences) but very little neutrally evolving sequence (specifically, ancestral repeats). importantly, we find that a substantial fraction of the bases within {mcss} (?70\\%) resides within non-coding regions; thus, the majority of sequences conserved across multiple vertebrate species has no known function. initial characterization of these {mcss} has revealed sequences that correspond to clusters of transcription factor-binding sites, non-coding {rna} transcripts, and other candidate functional elements. finally, the ability to detect {mcss} represents a valuable metric for assessing the relative contribution of a species' sequence to identifying genomic regions of interest, and our results indicate that the currently available genome sequences are insufficient for the comprehensive identification of {mcss} in the human genome.
175	113339	book	\N	\N	\N	the mit press	\N	\N	\N	1997	may	2005-03-03 18:23:07	\N	the definition of standard {ml} - revised	standard {ml} is a general-purpose programming language designed for large projects. this book provides a formal definition of standard {ml} for the benefit of all concerned with the language, including users and implementers. because computer programs are increasingly required to withstand rigorous analysis, it is all the more important that the language in which they are written be defined with full rigor.<br /> <br /> one purpose of a language definition is to establish a theory of meanings upon which the understanding of particular programs may rest. to properly define a programming language, it is necessary to use some form of notation other than a programming language. given a concern for rigor, mathematical notation is an obvious choice. the authors have defined their semantic objects in mathematical notation that is completely independent of standard {ml}.<br /> <br /> in defining a language one must also define the rules of evaluation precisely--that is, define what meaning results from evaluating any phrase of the language. the definition thus constitutes a formal specification for an implementation. the authors have developed enough of their theory to give sense to their rules of evaluation.<br /> <br /> <{i>the} definition of standard {ml}</i> is the essential point of reference for standard {ml}. since its publication in 1990, the implementation technology of the language has advanced enormously and the number of users has grown. the revised edition includes a number of new features, omits little-used features, and corrects mistakes of definition.
176	113926	article	sigkdd explorations	\N	\N	\N	9	5	1	2003	\N	2005-03-04 12:54:10	\N	a survey of kernels for structured data.	kernel methods in general and support vector machines in particular have been successful in various learning tasks on data represented in a single table. much 'real-world' data, however, is structured - it has no natural representation in a single table. usually, to apply kernel methods to 'real-world' data, extensive pre-processing is performed to embed the data into areal vector space and thus in a single table. this survey describes several approaches of defining positive definite kernels on structured instances directly.
177	113948	inproceedings	\N	sosp	\N	acm press	15	21	5	1987	nov	2005-03-04 15:17:51	\N	exploiting virtual synchrony in distributed systems	we describe applications of a  virtually synchronous  environment for distributed programming, which underlies a collection of distributed programming tools in the  isis 2  system. a virtually synchronous environment allows processes to be structured into  process groups , and makes events like broadcasts to the group as an entity, group membership changes, and even migration of an activity from one place to another appear to occur instantaneously &mdash; in other words, synchronously. a major advantage to this approach is that many aspects of a distributed application can be treated independently without compromising correctness. moreover, user code that is designed as if the system were synchronous can often be executed concurrently. we argue that this approach to building distributed and fault-tolerant software is more straightforward, more flexible, and more likely to yield correct solutions than alternative approaches.
178	114027	article	signal processing	\N	\N	\N	27	36	\N	1994	\N	2005-03-04 18:15:45	\N	independent component analysis - a new concept?	the independent component analysis (ica) of a random vector consists of searching for a linear transformation that minimizes the statistical dependence between its components. in order to define suitable search criteria, the expansion of mutual information is utilized as a function of cumulants of increasing orders. an efficient algorithm is proposed, which allows the computation of the ica of a data matrix within a polynomial time. the concept of ica may actually be seen as an extension of the principal component analysis (pca), which can only impose independence up to the second order and, consequently, defines directions that are orthogonal. potential applications of ica include data analysis and compression, bayesian detection, localization of sources, and blind identification and deconvolution.
179	114040	article	neural networks	\N	\N	\N	19	13	4-5	2000	\N	2005-03-04 18:15:45	\N	independent component analysis: algorithms and applications	a fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation of multivariate data, i.e. random vectors. for reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. in other words, each component of the representation is a linear combination of the original variables. well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. independent component analysis (ica) is a recently developed method in which the goal is to find a linear representation of non-gaussian data so that the components are statistically independent, or as independent as possible. such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. in this paper, we present the basic theory and applications of ica, and our recent work on the subject.
180	114177	book	\N	\N	\N	basic books	\N	\N	\N	1980	\N	2005-03-04 19:54:24	\N	mindstorms: children, computers and powerful ideas	the gears of my childhood   before i was two years old i had developed an intense involvement with automobiles. the names of car parts made up a very substantial portion of my vocabulary: i was particularly proud of knowing about the parts of the transmission system, the gearbox, and most especially the differential. it was, of course, many years later before i understood how gears work; but once i did, playing with gears became a favorite pastime. i loved rotating circular objects against one another in gearlike motions and, naturally, my first "erector set" project was a crude gear system.   i became adept at turning wheels in my head and at making chains of cause and effect: "this one turns this way so that must turn that way so . . . " i found particular pleasure in such systems as the differential gear, which does not follow a simple linear chain of causality since the motion in the transmission shaft can be distributed in many different ways to the two wheels depending on what resistance they encounter. i remember quite vividly my excitement at discovering that a system could be lawful and completely comprehensible without being rigidly deterministic.   i believe that working with differentials did more for my mathematical development than anything i was taught in elementary school. gears, serving as models, carried many otherwise abstract ideas into my head. i clearly remember two examples from school math. i saw multiplication tables as gears, and my first brush with equations in two variables (e.g., 3 x  + 4 y  = 10) immediately evoked the differential. by the time i had made a mental gear model of the relation between  x  and  y , figuring how many teeth each gear needed, the equation had become a comfortable friend.   many years later when i read piaget this incident served me as a model for his notion of assimilation, except i was immediately struck by the fact that his discussion does not do full justice to his own idea. he talks almost entirely about cognitive aspects of assimilation. but there is also an affective component. assimilating equations to gears certainly is a powerful way to bring old knowledge to bear on a new object. but it does more as well. i am sure that such assimilations helped to endow mathematics, for me, with a positive affective tone that can be traced back to my infantile experiences with cars. i believe piaget really agrees. as i came to know him personally i understood that his neglect of the affective comes more from a modest sense that little is known about it than from an arrogant sense of its irrelevance. but let me return to my childhood.   one day i was surprised to discover that some adults---even most adults---did not understand or even care about the magic of the gears. i no longer think much about gears, but i have never turned away from the questions that started with that discovery: how could what was so simple for me be incomprehensible to other people? my proud father suggested "being clever" as an explanation. but i was painfully aware that some people who could not understand the differential could easily do things i found much more difficult. slowly i began to formulate what i still consider the fundamental fact about learning: anything is easy if you can assimilate it to your collection of models. if you can't, anything can be painfully difficult. here too i was developing a way of thinking that would be resonant with piaget's.  the understanding of learning must be genetic . it must refer to the genesis of knowledge. what an individual can learn, and how he learns it, depends on what models he has available. this raises, recursively, the question of how he learned these models. thus the "laws of learning" must be about how intellectual structures grow out of one another and about how, in the process, they acquire both logical and emotional form.   this book is an exercise in an applied genetic epistemology expanded beyond piaget's cognitive emphasis to include a concern with the affective. it develops a new perspective for education research focused on creating the conditions under which intellectual models will take root. for the last two decades this is what i have been trying to do. and in doing so i find myself frequently reminded of several aspects of my encounter with the differential gear. first, i remember that no one told me to learn about differential gears. second, i remember that there was  feeling, love , as well as understanding in my relationship with gears. third, i remember that my first encounter with them was in my second year. if any "scientific" educational psychologist had tried to "measure" the effects of this encounter, he would probably have failed. it had profound consequences but, i conjecture, only very many years later. a "pre- and post-" test at age two would have missed them.   piaget's work gave me a new framework for looking at the gears of my childhood. the gear can be used to illustrate many powerful "advanced" mathematical ideas, such as groups or relative motion. but it does more than this. as well as connecting with the formal knowledge of mathematics, it also connects with the "body knowledge," the sensorimotor schemata of a child. you can be the gear, you can understand how it turns by projecting yourself into its place and turning with it. it is this double relationship---both abstract and sensory---that gives the gear the power to carry powerful mathematics into the mind. in a terminology i shall develop in later chapters, the gear acts here as a  transitional object .   a modern-day montessori might propose, if convinced by my story, to create a gear set for children. thus every child might have the experience i had. but to hope for this would be to miss the essence of the story.  i fell in love with the gears . this is something that cannot be reduced to purely "cognitive" terms. something very personal happened, and one cannot assume that it would be repeated for other children in exactly the same form.   my thesis could be summarized as: what the gears cannot do the computer might. the computer is the proteus of machines. its essence is its universality, its power to simulate. because it can take on a thousand forms and can serve a thousand functions, it can appeal to a thousand tastes. this book is the result of my own attempts over the past decade to turn computers into instruments flexible enough so that many children can each create for themselves something like what the gears were for me.
181	114190	book	\N	\N	\N	addison-wesley publishing co.	\N	\N	\N	1989	\N	2005-03-04 19:54:25	\N	genetic algorithms in search, optimization, and machine learning	{david goldberg's <i>genetic algorithms in search, optimization and machine learning</i> is by far the bestselling introduction to genetic algorithms. goldberg is one of the preeminent researchers in the field--he has published over 100 research articles on genetic algorithms and is a student of john holland, the father of genetic algorithms--and his deep understanding of the material shines through. the book contains a complete listing of a simple genetic algorithm in pascal, which c programmers can easily understand. the book covers all of the important topics in the field, including crossover, mutation, classifier systems, and fitness scaling, giving a novice with a computer science background enough information to implement a genetic algorithm and describe genetic algorithms to a friend.}
182	114194	book	\N	\N	\N	ann arbor, mi: university of michigan press	\N	\N	\N	1975	\N	2005-03-04 19:54:25	\N	adaptation in natural and artificial systems	genetic algorithms are playing an increasingly important role in studies of complex adaptive systems, ranging from adaptive agents in economic theory to the use of machine learning techniques in the design of complex devices such as aircraft turbines and integrated circuits. adaptation in natural and artificial systems is the book that initiated this field of study, presenting the theoretical foundations and exploring applications.  in its most familiar form, adaptation is a biological process, whereby organisms evolve by rearranging genetic material to survive in environments confronting them. in this now classic work, holland presents a mathematical model that allows for the nonlinearity of such complex interactions. he demonstrates the model's universality by applying it to economics, physiological psychology, game theory, and artificial intelligence and then outlines the way in which this approach modifies the traditional views of mathematical genetics.  initially applying his concepts to simply defined artificial systems with limited numbers of parameters, holland goes on to explore their use in the study of a wide range of complex, naturally occuring processes, concentrating on systems having multiple factors that interact in nonlinear ways. along the way he accounts for major effects of coadaptation and coevolution: the emergence of building blocks, or schemata, that are recombined and passed on to succeeding generations to provide, innovations and improvements.
183	114199	article	science	\N	\N	\N	5	162	3859	1968	dec	2005-03-04 20:10:44	\N	the tragedy of the commons	hardin, professor of biology, university of california, santa barbara presented this as a presidential address to the pacific division of the american association for the advancement of science at utah state university, logan, 25 june 1968. garrett hardin described the pathogenic effects of conscience when asking someone to desist exploiting the commons in the name of conscience. one is faced with a batesonâ€™s â€œdouble bindâ€ of being accused of being an irresponsible citizen for exploiting the commons or a simpleton who stands aside while everyone else exploits the commons.
184	114704	article	science (new york, n.y.)	\N	\N	\N	6	307	5709	2005	jan	2005-06-27 12:53:26	department of chemistry and biochemistry, university of california, 607 charles e. young drive east, los angeles, ca 90095, usa. michalet@chem.ucla.edu	quantum dots for live cells, in vivo imaging, and diagnostics.	research on fluorescent semiconductor nanocrystals (also known as quantum dots or qdots) has evolved over the past two decades from electronic materials science to biological applications. we review current approaches to the synthesis, solubilization, and functionalization of qdots and their applications to cell and animal biology. recent examples of their experimental use include the observation of diffusion of individual glycine receptors in living neurons and the identification of lymph nodes in live animals by near-infrared emission during surgery. the new generations of qdots have far-reaching potential for the study of intracellular processes at the single-molecule level, high-resolution cellular imaging, long-term in vivo observation of cell trafficking, tumor targeting, and diagnostics.
185	114769	book	\N	\N	\N	wiley-blackwell	\N	\N	\N	2003	feb	2005-03-05 11:53:48	\N	molecular motors	{the latest knowledge on molecular motors is vital for the understanding of a wide range of biological and medical topics: cell motility, organelle movement, virus transport, developmental asymmetry, myopathies, and sensory defects are all related to the function or malfunction of these minute molecular machines. since there is a vast amount of information on motor mechanisms and potential biomedical and nanobiotechnological applications, this handbook fulfills the need for a collection of current research results on the functionality, regulation, and interactions of cytoskeletal, dna, and rotary motors. here, leading experts present a concise insight, ranging from atomic structure, biochemistry, and biophysics to cell biology, developmental biology and pathology. basic principles and applications make this book a valuable reference tool for researchers, professionals, and clinicians alike - all set to become a "classic" in the years to come.}
186	114954	article	communications of the acm	\N	\N	\N	\N	35	9	1992	\N	2005-03-05 19:27:54	\N	process modeling	traditionally, the modeling of information systems has focused on analyzing data flows and transformations. this modeling accounted only for the organization's data and that portion of its processes that interacted with data. newer uses of in- formation technology extend computer use beyond transaction processing into commu- nication and coordination. successfully integrating these systems into the enterprise often requires modeling even the manual organizational processes into which these systems intervene. the following are three such applications: ...
187	114960	misc	\N	\N	\N	\N	\N	\N	\N	2004	\N	2005-03-05 19:27:54	\N	inversion of control containers and the dependency injection pattern	in the java community there's been a rush of lightweight containers that help to assemble components from different projects into a cohesive application. underlying these containers is a common pattern to how they perform the wiring, a concept they refer under the very generic name of "inversion of control". in this article i dig into how this pattern works, under the more specific name of "dependency injection", and contrast it with the service locator alternative. the choice between them is less important than the principle of separating configuration from use.
188	115003	article	journal of web semantics	\N	\N	\N	19	1	4	2004	\N	2005-03-05 19:27:54	\N	{htn} planning for web service composition using {shop2}	automated composition of web services can be achieved by using ai planning techniques. hierarchical task network (htn) planning is especially well-suited for this task. in this paper, we describe how htn planning system shop2 can be used with owl-s web service descriptions. we provide a sound and complete algorithm to translate owl-s service descriptions to a shop2 domain. we prove the correctness of the algorithm by showing the correspondence to the situation calculus semantics of owl-s. we implemented a system that plans over sets of owl-s descriptions using shop2 and then executes the resulting plans over the web. the system is also capable of executing information-providing web services during the planning process. we discuss the challenges and difficulties of using planning in the information-rich and human-oriented context of web services.
189	115155	book	\N	\N	\N	springer	\N	\N	\N	2003	oct	2005-03-06 00:42:05	\N	web services - concepts, architectures and applications	like many other incipient technologies, web services are still surrounded by a tremendous level of noise. this noise results from the always dangerous combination of wishful thinking on the part of research and industry and of a lack of clear understanding of how web services came to be. on the one hand, multiple contradictory interpretations are created by the many attempts to realign existing technology and strategies with web services. on the other hand, the emphasis on what could be done with web services in the future often makes us lose track of what can be really done with web services today and in the short term. these factors make it extremely difficult to get a coherent picture of what web services are, what they contribute, and where they will be applied. alonso and his co-authors deliberately take a step back. based on their academic and industrial experience with middleware and enterprise application integration systems, they describe the fundamental concepts behind the notion of web services and present them as the natural evolution of conventional middleware, necessary to meet the challenges of the web and of {b2b} application integration. rather than providing a reference guide or a "how to write your first web service" kind of book, they discuss the main objectives of web services, the challenges that must be faced to achieve them, and the opportunities that this novel technology provides. established, as well as recently proposed, standards and techniques (e.g., {wsdl}, {uddi}, {soap}, {ws}-coordination, {ws}-transactions, and {bpel}), are then examined in the context of this discussion in order to emphasize their scope, benefits, and shortcomings. thus, the book is ideally suited both for professionals considering the development of application integration solutions and for research and students interesting in understanding and contributing to the evolution of enterprise application technologies.
190	115168	article	biochemistry	\N	\N	\N	10	33	22	1994	\N	2005-03-06 01:40:19	\N	structural mechanisms for domain movements in proteins	we survey all the known instances of domain movements in proteins for which there is crystallographic evidence for the movement. we explain these domain movements in terms of the repertoire of low-energy conformation changes that are known to occur in proteins. we first describe the basic elements of this repertoire, hinge and shear motions, and then show how the elements of the repertoire can be combined to produce domain movements. we emphasize that the elements used in particular proteins are determined mainly by the structure of the interfaces between the domains.
191	115283	book	\N	\N	\N	penguin	\N	\N	\N	2003	\N	2005-03-06 15:23:46	london	freedom evolves	daniel dennett's latest book _freedom evolves_ continues the themes that havebecome his trademark in previous titles such as _consciousness explained_ and_darwin's dangerous idea_. his task is to give a thorough account of how we--and our minds--evolved and to calm fears that such an account presents athreat to the concept of free will.in one of the most arresting and important chapters in the book, dennett laysbare several common misconceptions about determinism and introduces a toymodel which demonstrates how simple, mindlessly deterministic automata appearto make rational 'choices' to avoid harm in their limited environment. dennettclaims that misunderstanding of determinism is still prevalent amongscientists and philosophers who subsequently misrepresent his views as theycontinue to resist a materialistic treatment of mind. their fear is that if weshould ever be revealed to be 'mere machines' this will bring with it a deathsentence to consciousness and free-will. such fears resist dennett's argumentas wrong and an insult to our sense of human dignity. after carefullyaddressing those fears, dennett goes on to show how we humans can be both acreation of and a creator of culture; arguing that we are of course a speciesof animal but the emergence of human culture is a major innovation inevolutionary history providing our species with new tools to use, new topicsto think about and new perspectives to think from.what makes dennett such an unforgettably stimulating philosopher is not justthe breadth of his inter-disciplinary knowledge or his boldness andoriginality, it is that--knowing how difficult it is to get people to acceptcounter-intuitive ideas--he helps the reader visualise hismaterialistic/naturalistic world-view. there is undoubtedly still work to doto reconcile the philosophical implications of darwinian materialism and whatmakes dennett genuinely important is that he is set on trying to bring ourprecious values, including the notion of freedom, into line with darwin andnew found scientific discoveries.he is encouraging us to drop the self-image we inherited from christianity andthe western philosophical tradition with all its argument about a specialextra added ingredient called consciousness that is unique to humans. sure wehave consciousness, but there's no magic in it, says dennett. what we need,what dennett is offering us, is a new improved self-image. just because thereisn't a self to be found sitting inside our brains looking out into the worldand making decisions doesn't mean the self is an illusion.there are other, better ways to think about the self, he stresses. he alsoargues that even though we are made of tiny mindless little robots that areoblivious to our hopes and needs, there's no shame in that and no reason foralarm. what we are made of and what we can hope and strive for are differentthings. _freedom evolves_ is the culmination of three decades worth ofresearch. --_larry brown_
192	115371	misc	\N	\N	\N	\N	\N	\N	\N	2003	\N	2005-03-06 18:10:27	\N	localization from mere connectivity	it is often useful to know the geographic positions of nodes in a communications network, but adding gps receivers or other sophisticated sensors to every node can be expensive. we present an algorithm that uses connectivity information---  who is within communications range of whom---to derive the locations of the nodes in the network. the method can take advantage of additional information, such as estimated distances between neighbors or known positions for certain anchor nodes, if it is...
193	115458	inproceedings	\N	ijcai 1993	\N	aaai press	5	\N	\N	1993	\N	2005-03-06 19:27:13	\N	multi-interval discretization of continuous-valued attributes for classification learning	since most real-world applications of classification learning involve continuous-valued attributes, properly addressing the discretization process is an important problem. this paper addresses the use of the entropy minimization heuristic for discretizing the range of a continuous-valued attribute into multiple intervals. we briefly present theoretical evidence for the appropriateness of this heuristic for use in the binary discretization algorithm used in id3, c4, cart, and other learning algorithms. the results serve to justify extending the algorithm to derive multiple intervals. we formally derive a criterion based on the minimum description length principle for deciding the partitioning of intervals. we demonstrate via empirical evaluation on several real-world data sets that better decision trees are obtained using the new multi-interval algorithm.
194	115575	article	trans. of the asme -- journal of basic engineering	\N	\N	\N	10	82	Series D	1960	\N	2005-03-06 19:27:14	\N	a new approach to linear filtering and prediction problems	the classical filtering and prediction problem is re-examined using the bode- shannon representation of random processes and the â€œstate transitionâ€ method of analysis of dynamic systems.  new results are: (1) the formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing-memory and infinite- memory filters. (2) a nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error.  from the solution of this equation the coefficients of the difference (or differential) equation of the optimal linear filter are obtained without further calculations. (3) the filtering problem is shown to be the dual of the noise-free regulator problem. the new method developed here is applied to two well-known problems, confirming and extending earlier results. the discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the appendix.
195	115578	article	machine learning	\N	\N	\N	25	1	1	1986	\N	2005-03-06 19:27:14	\N	induction of decision trees	the technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. this paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, id3, in detail. results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. a reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. the paper concludes with illustrations of current research directions.
196	115613	techreport	\N	\N	\N	\N	\N	\N	649	2003	sep	2005-03-06 19:27:14	\N	graphical models, exponential families, and variational inference	the formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables, and building large-scale multivariate statistical models. graphical models have become a focus of research in many statistical, computational and mathematical fields, including bioinformatics, communication theory, statistical physics, combinatorial optimization, signal and image processing, information retrieval and statistical machine learning. many problems that arise in specific instances â€” including the key problems of computing marginals and modes of probability distributions â€” are best studied in the general setting. working with exponential family representations, and exploiting the conjugate duality between the cumulant function and the entropy for exponential families, we develop general variational representations of the problems of computing likelihoods, marginal probabilities and most probable configurations. we describe how a wide variety of algorithms â€” among them sum-product, cluster variational methods, expectation-propagation, mean field methods, max-product and linear programming relaxation, as well as conic programming relaxations â€” can all be understood in terms of exact or approximate forms of these variational representations. the variational approach provides a complementary alternative to markov chain monte carlo as a general source of approximation methods for inference in large-scale statistical models.
197	115640	article	journal of the royal statistical society b	\N	\N	\N	21	58	\N	1996	\N	2005-03-06 19:27:14	\N	regression shrinkage and selection via the lasso	we propose a new method for estimation in linear models. the &#034;lasso&#034; minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. because of the nature of this constraint it tends to produce some coefficients that are exactly zero and hence gives interpretable models. our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. it produces interpretable models like subset selection and exhibits the stability of ridge regression. there is also an interesting relationship with recent work in adaptive function estimation by donoho and johnstone. the lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described. keywords: regression, subset selection, shrinkage, quadratic programming. 1 introduction consider the usual regression situation: we h...
198	115652	incollection	\N	learning in graphical models	\N	kluwer academic publishers	14	\N	\N	1998	\N	2005-03-06 19:27:14	\N	a view of the {em} algorithm that justifies incremental, sparse, and other variants	. the em algorithm performs maximum likelihood estimation for data in which some variables are unobserved. we present a function that resembles negative free energy and show that the m step maximizes this function with respect to the model parameters and the e step maximizes it with respect to the distribution over the unobserved variables. from this perspective, it is easy to justify an incremental variant of the em algorithm in which the distribution for only one of the unobserved variables is recalculated in each e step. this variant is shown empirically to give faster convergence in a mixture estimation problem. a variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible. 1. introduction the expectation-maximization (em) algorithm finds maximum likelihood parameter estimates in problems where some variables were unobserved. special cases of the algorithm date back several dec...
199	115676	book	\N	\N	\N	wadsworth	\N	\N	\N	1984	\N	2005-03-06 19:27:15	belmont, ca	classification and regression trees	{the methodology used to construct tree structured rules is the focus of this monograph. unlike many other statistical procedures, which moved from pencil and paper to calculators, this text's use of trees was unthinkable before computers. both the practical and theoretical sides have been developed in the authors' study of tree methods. classification and regression trees reflects these two sides, covering the use of trees as a data analysis method, and in a more mathematical framework, proving some of their fundamental properties.}
200	115695	article	journal of the royal statistical society, series b	\N	\N	\N	11	57	\N	1995	\N	2005-03-06 19:27:15	\N	controlling the false discovery rate: a practical and powerful approach to multiple testing	the common approach to the multiplicity problem calls for controlling the familywise error rate (fwer). this approach, though, has faults, and we point out a few. a different approach to problems of multiple significance testing is presented. it calls for controlling the expected proportion of falsely rejected hypotheses - the false discovery rate. this error rate is equivalent to the fwer when all hypotheses are true but is smaller otherwise. therefore, in problems where the control of the false discovery rate rather than that of the fwer is desired, there is potential for a gain in power. a simple sequential bonferroni-type procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. the use of the new procedure and the appropriateness of the criterion are illustrated with examples.
201	115711	inproceedings	\N	proceedings of the 6th conference on natural language learning (conll-2002)	\N	\N	6	\N	\N	2002	\N	2005-03-06 19:27:15	taipei, taiwan	a comparison of algorithms for maximum entropy parameter estimation	a comparison of algorithms for maximum entropy parameter estimation conditional maximum entropy (me) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used for a wide variety of classification problems in natural language processing. however, the flexibility of me models is not without cost. while parameter estimation for me models is conceptually straightforward, in practice me models for typical natural language tasks are very large, and may well contain many thousands of free parameters. in this paper, we consider a number of algorithms for estimating the parameters of me models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods. surprisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limited-memory variable metric algorithm outperformed the other choices.
202	115751	article	machine learning	\N	\N	\N	17	24	2	1996	\N	2005-03-06 19:27:15	\N	bagging predictors	bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. the aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. the multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. the vital element is the instability of the prediction method. if perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy. 1. introduction a learning set of l consists of data f(y n ; x n ), n = 1; : : : ; ng where the y's are either class labels or a numerical response. we have a procedure for using this learning set to form a predictor '(x; l) --- if the input is x we ...
203	115777	inproceedings	\N	aaai-98 workshop on `learning for text categorization'	\N	\N	\N	\N	\N	1998	\N	2005-03-06 19:27:15	\N	a comparison of event models for naive {b}ayes text classification	recent approaches to text classification have used two different first-order probabilistic models for classification, both of which make the naive bayes assumption. some use a multi-variate bernoulli model, that is, a bayesian network with no dependencies between words and binary word features (e.g. larkey and croft 1996; koller and sahami 1997). others use a multinomial model, that is, a uni-gram language model with integer word counts (e.g. lewis and gale 1994; mitchell 1997). this paper aims to clarify the confusion by describing the differences and details of these two models, and by empirically comparing their classification performance on five text corpora. we find that the multi-variate bernoulli performs well with small vocabulary sizes, but that the multinomial performs usually performs even better at larger vocabulary sizesâ€”providing on average a 27 % reduction in error over the multi-variate bernoulli model at any vocabulary size.
204	115791	article	journal of computational and graphical statistics	\N	\N	\N	16	9	\N	2000	\N	2005-03-06 19:27:15	\N	markov chain sampling methods for {d}irichlet process mixture models	this article reviews markov chain methods for sampling from the posterior distribution of a dirichlet process mixture model and presents two new classes of methods. one new approach is to make metropolis-hastings updates of the indicators specifying which mixture component is associated with each observation, perhaps supplemented with a partial form of gibbs sampling. the other new approach extends gibbs sampling for these indicators by using a set of auxiliary parameters. these methods are simple to implement and are more efficient than previous ways of handling general dirichlet process mixture models with non-conjugate priors.
205	115830	inproceedings	\N	{ecml} 2002	lncs	springer-verlag	\N	2430	\N	2002	aug	2005-03-06 19:27:16	\N	variational extensions to {em} and multinomial {pca}	several authors in recent years have proposed discrete analogues to principle component analysis intended to handle discrete or positive only data, for instance suited to analyzing sets of documents. methods include non-negative matrix factorization, probabilistic latent semantic analysis, and latent dirichlet allocation. this paperbegins with a review of the basic theory of the variational extension to the expectation-maximization algorithm, and then presents discrete component finding algorithms in that light. experiments are conducted on both bigram word data and document bag-of-word to expose some of the subtleties of this new class of algorithms.
206	115881	article	\N	\N	\N	\N	\N	\N	\N	2005	mar	2005-03-06 20:54:27	\N	toward alternative metrics of journal impact: a comparison of download and citation data	we generated networks of journal relationships from citation and download data, and determined journal impact rankings from these networks using a set of social network centrality metrics. the resulting journal impact rankings were compared to the {isi} {if.} results indicate that, although social network metrics and {isi} {if} rankings deviate moderately for citation-based journal networks, they differ considerably for journal networks derived from download data. we believe the results represent a unique aspect of general journal impact that is not captured by the {isi} {if.} these results furthermore raise questions regarding the validity of the {isi} {if} as the sole assessment of journal impact, and suggest the possibility of devising impact metrics based on usage information in general.
207	116378	proceedings	information visualization, 2003. infovis 2003. ieee symposium on	\N	\N	\N	7	\N	\N	2003	\N	2005-03-07 16:10:08	\N	between aesthetics and utility: designing ambient information visualizations	unlike traditional information visualization, ambient information visualizations reside in the environment of the user rather than on the screen of a desktop computer. currently, most dynamic information that is displayed in public places consists of text and numbers. we argue that information visualization can be employed to make such dynamic data more useful and appealing. however, visualizations intended for non-desktop spaces will have to both provide valuable information and present an attractive addition to the environment - they must strike a balance between aesthetical appeal and usefulness. to explore this, we designed a real-time visualization of bus departure times and deployed it in a public space, with about 300 potential users. to make the presentation more visually appealing, we took inspiration from a modern abstract artist. the visualization was designed in two passes. first, we did a preliminary version that was presented to and discussed with prospective users. based on their input, we did a final design. we discuss the lessons learned in designing this and previous ambient information visualizations, including how visual art can be used as a design constraint, and how the choice of information and the placement of the display affect the visualization.
208	116388	article	american journal of political science	\N	\N	midwest political science association	29	44	2	2000	\N	2005-03-07 18:06:03	\N	estimation and inference via bayesian simulation: an introduction to markov chain monte carlo	bayesian statistics have made great strides in recent years, developing a class of methods for estimation and inference via stochastic simulation known as markov chain monte carlo ({mcmc}) methods. {mcmc} constitutes a revolution in statistical practice with effects beginning to be felt in the social sciences: models long consigned to the "too hard" basket are now within reach of quantitative researchers. i review the statistical pedigree of {mcmc} and the underlying statistical concepts. i demonstrate some of the strengths and weaknesses of {mcmc} and offer practical suggestions for using {mcmc} in social-science settings. simple, illustrative examples include a probit model of voter turnout and a linear regression for time-series data with autoregressive disturbances. i conclude with a more challenging application, a multinomial probit model, to showcase the power of {mcmc} methods.
209	117527	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2005-03-08 17:56:55	\N	a model of {saliency-based} visual attention for rapid scene analysis	a visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. multiscale image features are combined into a single topographical saliency map. a dynamical neural network then selects attended locations in order of decreasing saliency. the system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail. index terms: visual ...
210	117555	article	methods of information in medicine	\N	\N	\N	9	41	4	2002	\N	2005-03-08 21:37:18	decision systems group, brigham and women's hospital, harvard medical school, boston, ma, usa. qzeng@dsg.bwh.harvard.edu	characteristics of consumer terminology for health information retrieval.	{objectives}: as millions of consumers perform health information retrieval online, the mismatch between their terminology and the terminologies of the information sources could become a major barrier to successful retrievals. to address this problem, we studied the characteristics of consumer terminology for health information retrieval. {methods}: our study focused on consumer queries that were used on a consumer health service web site and a consumer health information web site. we analyzed data from the site-usage logs and conducted interviews with patients. {results}: our findings show that consumers' information retrieval performance is very poor. there are significant mismatches at all levels (lexical, semantic and mental models) between the consumer terminology and both the information source terminology and standard medical vocabularies. {conclusions}: comprehensive terminology support on all levels is needed for consumer health information retrieval.
211	118608	article	first monday	\N	\N	\N	\N	\N	\N	2005	mar	2005-03-09 19:05:10	\N	re–approaching nearness: online communication and its place in praxis	an interesting transposition has happened. it used to be that the farther things were, the more difficult it was to know them. today, thanks to communication technologies, we often develop relationships with what is far at the expense of what is immediately around us. this paper explores the increased irrelevancy that the near acquires through our use of online technologies. but by proposing a model of praxis that incorporates our actions online as well as offline, this paper also argues that online technologies can play an important part in bringing the epistemologically far near to us, and making the physically near relevant again.
212	118690	article	nature	\N	\N	nature publishing group	9	430	6995	2004	jul	2005-03-10 21:12:16	unit\\'{e} de g\\'{e}n\\'{e}tique mol\\'{e}culaire des levures, ura 2171 cnrs and ufr 927 universit\\'{e} pierre et marie curie. bdujon@pasteur.fr	genome evolution in yeasts.	identifying the mechanisms of eukaryotic genome evolution by comparative genomics is often complicated by the multiplicity of events that have taken place throughout the history of individual lineages, leaving only distorted and superimposed traces in the genome of each living organism. the hemiascomycete yeasts, with their compact genomes, similar lifestyle and distinct sexual and physiological properties, provide a unique opportunity to explore such mechanisms. we present here the complete, assembled genome sequences of four yeast species, selected to represent a broad evolutionary range within a single eukaryotic phylum, that after analysis proved to be molecularly as diverse as the entire phylum of chordates. a total of approximately 24,200 novel genes were identified, the translation products of which were classified together with saccharomyces cerevisiae proteins into about 4,700 families, forming the basis for interspecific comparisons. analysis of chromosome maps and genome redundancies reveal that the different yeast lineages have evolved through a marked interplay between several distinct molecular mechanisms, including tandem gene repeat formation, segmental duplication, a massive genome duplication and extensive gene loss.
213	118691	article	bmc bioinformatics	\N	\N	\N	\N	3	1	2002	jul	2005-03-10 21:12:22	howard hughes medical institute \\& department of genetics, washington university school of medicine, saint louis, missouri 63110 usa. eddy@genetics.wustl.edu	a memory-efficient dynamic programming algorithm for optimal alignment of a sequence to an {rna} secondary structure.	optimal ribosomal {rna} structural alignments that previously required up to 150 {gb} of memory now require less than 270 {mb}.
214	118692	article	curr opin genet dev	\N	\N	\N	4	9	6	1999	dec	2005-03-10 21:12:25	department of genetics, washington university school of medicine, st louis 63110, usa. eddy@genetics.wustl.edu	noncoding {rna} genes.	some genes produce {rnas} that are functional instead of encoding proteins. noncoding {rna} genes are surprisingly numerous. recently, active research areas include small nucleolar {rnas}, antisense riboregulator {rnas}, and {rnas} involved in x-dosage compensation. genome sequences and new algorithms have begun to make systematic computational screens for noncoding {rna} genes possible.
215	118693	article	nucleic acids research	\N	\N	\N	9	22	11	1994	jun	2005-03-10 21:12:29	mrc laboratory of molecular biology, cambridge, uk.	{rna} sequence analysis using covariance models.	we describe a general approach to several {rna} sequence analysis problems using probabilistic models that flexibly describe the secondary structure and primary sequence consensus of an {rna} sequence family. we call these models 'covariance models'. a covariance model of {trna} sequences is an extremely sensitive and discriminative tool for searching for additional {trnas} and {trna}-related sequences in sequence databases. a model can be built automatically from an existing sequence alignment. we also describe an algorithm for learning a model and hence a consensus secondary structure from initially unaligned example sequences and no prior structural information. models trained on unaligned {trna} examples correctly predict {trna} secondary structure and produce high-quality multiple alignments. the approach may be applied to any family of small {rna} sequences.
216	118699	article	rna	\N	\N	\N	11	7	2	2001	feb	2005-03-10 21:13:02	institut f\\"{u}r theoretische chemie und molekulare strukturbiologie, universit\\"{a}t wien, austria.	design of multistable {rna} molecules.	we show that the problem of designing {rna} sequences that can fold into multiple stable secondary structures can be transformed into a combinatorial optimization problem that can be solved by means of simple heuristics. hence it is feasible to design {rna} switches with prescribed structural alternatives. we discuss the theoretical background and present an efficient tool that allows the design of various types of switches. we argue that both the general properties of the sequence structure map of {rna} secondary structures and the ease with which our design tool finds bistable {rnas} strongly indicates that {rna} switches are easily accessible in evolution. thus conformational switches are yet another function for which {rna} can be employed.
217	118725	article	bioinformatics	\N	\N	\N	4	20	2	2004	jan	2005-03-10 21:15:01	institut f\\"{u}r theoretische chemie und molekulare strukturbiologie, universit\\"{a}t wien, w\\"{a}hringerstrasse 17, vienna, a-1090, austria. ivo@tbi.univie.ac.at	prediction of locally stable {rna} secondary structures for genome-wide surveys.	{motivation}: recently novel classes of functional {rnas}, most prominently the {mirnas} have been discovered, strongly suggesting that further types of functional {rnas} are still hidden in the recently completed genomic {dna} sequences. only few techniques are known, however, to survey genomes for such {rna} genes. when sufficiently similar sequences are not available for comparative approaches the only known remedy is to search directly for structural features. {results}: we present here efficient algorithms for computing locally stable {rna} structures at genome-wide scales. both the minimum energy structure and the complete matrix of base pairing probabilities can be computed in {theta(n} x l2) time and {theta(n} + l2) memory in terms of the length n of the genome and the size l of the largest secondary structure motifs of interest. in practice, the 100 mb of the complete genome of caenorhabditis elegans can be folded within about half a day on a modern {pc} with a search depth of l = 100. this is sufficient example for a survey for {mirnas}. {availability}: the software described in this contribution will be available for download at {http://www.tbi.univie.ac.at/\\~{}ivo/rna}/ as part of the vienna {rna} package.
218	118737	article	science	\N	\N	\N	3	296	5569	2002	may	2005-03-10 21:15:58	affymetrix, santa clara, ca 95051, usa., national cancer institute, bethesda, md 20892, usa.	{large-scale} transcriptional activity in chromosomes 21 and 22	10.1126/science.1068597
219	118738	article	nature	\N	\N	nature publishing group	13	423	6937	2003	may	2005-03-10 21:16:02	whitehead/mit center for genome research, nine cambridge center, cambridge, massachusetts 02142, usa. manoli@mit.edu	sequencing and comparison of yeast species to identify genes and regulatory elements	identifying the functional elements encoded in a genome is one of the principal challenges in modern biology. comparative genomics should offer a powerful, general approach. here, we present a comparative analysis of the yeast saccharomyces cerevisiae based on high-quality draft sequences of three related species (s. paradoxus, s. mikatae and s. bayanus). we first aligned the genomes and characterized their evolution, defining the regions and mechanisms of change. we then developed methods for direct identification of genes and regulatory motifs. the gene analysis yielded a major revision to the yeast gene catalogue, affecting approximately 15\\% of all genes and reducing the total count by about 500 genes. the motif analysis automatically identified 72 genome-wide elements, including most known regulatory motifs and numerous new motifs. we inferred a putative function for most of these motifs, and provided insights into their combinatorial interactions. the results have implications for genome analysis of diverse organisms, including the human.
220	118739	article	nature	\N	\N	nature publishing group	7	428	6983	2004	apr	2005-03-10 21:16:06	the broad institute, massachusetts institute of technology and harvard university, cambridge, massachusetts 02138, usa. manoli@mit.edu	proof and evolutionary analysis of ancient genome duplication in the yeast saccharomyces cerevisiae.	whole-genome duplication followed by massive gene loss and specialization has long been postulated as a powerful mechanism of evolutionary innovation. recently, it has become possible to test this notion by searching complete genome sequence for signs of ancient duplication. here, we show that the yeast saccharomyces cerevisiae arose from ancient whole-genome duplication, by sequencing and analysing kluyveromyces waltii, a related yeast species that diverged before the duplication. the two genomes are related by a 1:2 mapping, with each region of k. waltii corresponding to two regions of s. cerevisiae, as expected for whole-genome duplication. this resolves the long-standing controversy on the ancestry of the yeast genome, and makes it possible to study the fate of duplicated genes directly. strikingly, 95% of cases of accelerated evolution involve only one member of a gene pair, providing strong support for a specific model of evolution, and allowing us to distinguish ancestral and derived functions.
221	118757	article	science	\N	\N	\N	3	283	5405	1999	feb	2005-03-10 21:17:29	department of genetics, washington university school of medicine, 4566 scott avenue, st. louis, mo 63110, usa. lowe@genetics.wustl.edu	a computational screen for methylation guide {snornas} in yeast.	small nucleolar {rnas} ({snornas}) are required for ribose {2'-o}-methylation of eukaryotic ribosomal {rna}. many of the genes for this {snorna} family have remained unidentified in saccharomyces cerevisiae, despite the availability of a complete genome sequence. probabilistic modeling methods akin to those used in speech recognition and computational linguistics were used to computationally screen the yeast genome and identify 22 methylation guide {snornas}, {snr50} to {snr71}. gene disruptions and other experimental characterization confirmed their methylation guide function. in total, 51 of the 55 ribose methylated sites in yeast ribosomal {rna} were assigned to 41 different guide {snornas}.
222	118759	article	journal of computational biology	\N	\N	\N	18	7	3-4	2000	aug	2005-03-10 21:17:38	baskin center for computer science and engineering, university of california, santa cruz 95064, usa. rlyngsoe@cse.ucsc.edu	{rna} pseudoknot prediction in {energy-based} models	{rna} molecules are sequences of nucleotides that serve as more than mere intermediaries between {dna} and proteins, e.g., as catalytic molecules. computational prediction of {rna} secondary structure is among the few structure prediction problems that can be solved satisfactorily in polynomial time. most work has been done to predict structures that do not contain pseudoknots. allowing pseudoknots introduces modeling and computational problems. in this paper we consider the problem of predicting {rna} secondary structures with pseudoknots based on free energy minimization. we first give a brief comparison of energy-based methods for predicting {rna} secondary structures with pseudoknots. we then prove that the general problem of predicting {rna} secondary structures containing pseudoknots is {np} complete for a large class of reasonable models of pseudoknots.
223	118767	article	rna (new york, n.y.)	\N	\N	\N	12	10	8	2004	aug	2005-03-10 21:18:10	center for human genetics and molecular pediatric disease, aab institute of biomedical sciences, university of rochester medical center, 601 elmwood avenue, box 703, ny 14642, usa. david\\_mathews@urmc.rochester.edu	using an {rna} secondary structure partition function to determine confidence in base pairs predicted by free energy minimization	a partition function calculation for {rna} secondary structure is presented that uses a current set of nearest neighbor parameters for conformational free energy at 37 degrees c, including coaxial stacking. for a diverse database of {rna} sequences, base pairs in the predicted minimum free energy structure that are predicted by the partition function to have high base pairing probability have a significantly higher positive predictive value for known base pairs. for example, the average positive predictive value, 65.8\\%, is increased to 91.0\\% when only base pairs with probability of 0.99 or above are considered. the quality of base pair predictions can also be increased by the addition of experimentally determined constraints, including enzymatic cleavage, flavin mono-nucleotide cleavage, and chemical modification. predicted secondary structures can be color annotated to demonstrate pairs with high probability that are therefore well determined as compared to base pairs with lower probability of pairing.
224	118786	article	proc natl acad sci u s a	\N	\N	\N	4	77	11	1980	nov	2005-03-10 21:19:35	\N	fast algorithm for predicting the secondary structure of single-stranded {rna}.	a computer method is presented for finding the most stable secondary structures in long single-stranded {rnas}. it is 1-2 orders of magnitude faster than existing codes. the time required for its application increases as n3 for a chain n nucleotides long. as many as 1000 nucleotides can be searched in a single run. the approach is systematic and builds an optimal structure in a straightforward inductive procedure based on an exact mathematical algorithm. two simple half-matrices are constructed and the best folded form is read directly from the second matrix by a simple back-tracking procedure. the program utilizes published values for base-pairing energies to compute one structure with the lowest free energy.
225	118801	article	bioinformatics	\N	\N	oxford university press	22	16	7	2000	jul	2005-03-10 21:20:39	department of genetics, washington university, st. louis, mo 63110, usa.	secondary structure alone is generally not statistically significant for the detection of noncoding {rnas}	motivation: several results in the literature suggest that biologically interesting {rnas} have secondary structures that are more stable than expected by chance. based on these observations, we developed a scanning algorithm for detecting noncoding {rna} genes in genome sequences, using a fully probabilistic version of the zuker minimum-energy folding algorithm.
226	118802	article	bmc bioinformatics	\N	\N	\N	\N	2	1	2001	\N	2005-03-10 21:20:43	howard hughes medical institute and department of genetics, washington university school of medicine, saint louis, missouri, usa. elena@genetics.wustl.edu	noncoding {rna} gene detection using comparative sequence analysis.	{background}: noncoding {rna} genes produce transcripts that exert their function without ever producing proteins. noncoding {rna} gene sequences do not have strong statistical signals, unlike protein coding genes. a reliable general purpose computational genefinder for noncoding {rna} genes has been elusive. {results}: we describe a comparative sequence analysis algorithm for detecting novel structural {rna} genes. the key idea is to test the pattern of substitutions observed in a pairwise alignment of two homologous sequences. a conserved coding region tends to show a pattern of synonymous substitutions, whereas a conserved structural {rna} tends to show a pattern of compensatory mutations consistent with some base-paired secondary structure. we formalize this intuition using three probabilistic "pair-grammars": a pair stochastic context free grammar modeling alignments constrained by structural {rna} evolution, a pair hidden markov model modeling alignments constrained by coding sequence evolution, and a pair hidden markov model modeling a null hypothesis of position-independent evolution. given an input pairwise sequence alignment (e.g. from a {blastn} comparison of two related genomes) we classify the alignment into the coding, {rna}, or null class according to the posterior probability of each class. {conclusions}: we have implemented this approach as a program, {qrna}, which we consider to be a prototype structural noncoding {rna} genefinder. tests suggest that this approach detects noncoding {rna} genes with a fair degree of reliability.
227	118803	article	current biology : cb	\N	\N	\N	4	11	17	2001	sep	2005-03-10 21:20:47	howard hughes medical institute and department of genetics, washington university school of medicine, saint louis, mo 63110, usa.	computational identification of noncoding {rnas} in e. coli by comparative genomics.	some genes produce noncoding transcripts that function directly as structural, regulatory, or even catalytic {rnas} [1, 2]. unlike protein-coding genes, which can be detected as open reading frames with distinctive statistical biases, noncoding {rna} ({ncrna}) gene sequences have no obvious inherent statistical biases [3]. thus, genome sequence analyses reveal novel protein-coding genes, but any novel {ncrna} genes remain invisible. here, we describe a computational comparative genomic screen for {ncrna} genes. the key idea is to distinguish conserved {rna} secondary structures from a background of other conserved sequences using probabilistic models of expected mutational patterns in pairwise sequence alignments. we report the first whole-genome screen for {ncrna} genes done with this method, in which we applied it to the "intergenic" spacers of escherichia coli using comparative sequence data from four related bacteria. starting from >23,000 conserved interspecies pairwise alignments, the screen predicted 275 candidate structural {rna} loci. a sample of 49 candidate loci was assayed experimentally. at least 11 loci expressed small, apparently noncoding {rna} transcripts of unknown function. our computational approach may be used to discover structural {ncrna} genes in any genome for which appropriate comparative genome sequence data are available.
228	118842	article	nucleic acids res	\N	\N	\N	6	27	24	1999	dec	2005-03-10 21:23:32	center for biological sequence analysis, technical university of denmark, building 208, 2800 lyngby, denmark.	no evidence that {mrnas} have lower folding free energies than random sequences with the same dinucleotide distribution.	this work investigates whether {mrna} has a lower estimated folding free energy than random sequences. the free energy estimates are calculated by the mfold program for prediction of {rna} secondary structures. for a set of 46 {mrnas} it is shown that the predicted free energy is not significantly different from random sequences with the same dinucleotide distribution. for random sequences with the same mononucleotide distribution it has previously been shown that the native {mrna} sequences have a lower predicted free energy, which indicates a more stable structure than random sequences. however, dinucleotide content is important when assessing the significance of predicted free energy as the physical stability of {rna} secondary structure is known to depend on dinucleotide base stacking energies. even known {rna} secondary structures, like {trnas}, can be shown to have predicted free energies indistinguishable from randomized sequences. this suggests that the predicted free energy is not always a good determinant for {rna} folding.
229	119509	article	\N	\N	\N	mit press	57	\N	\N	1999	\N	2005-03-10 09:54:29	\N	distributed rational decision making	introduction  automated negotiation systems with self-interested agents are becoming increasingly important. one reason for this is the technology push of a growing standardized communication infrastructure---internet, www, nii, edi, kqml, fipa, concordia, voyager, odyssey, telescript, java, etc---over which separately designed agents belonging to different organizations can interact in an open environment in realtime and safely carry out transactions. the second reason is strong application pull for computer support for negotiation at the operative decision making level. for example, we are witnessing the advent of small transaction electronic commerce on the internet for purchasing goods, information, and communication bandwidth [29]. there is also an industrial trend toward virtual enterprises: dynamic alliances of small, agile enterprises which together can take advantage of economies of scale when available (e.g., respond to mor
230	120137	inproceedings	\N	ijcai-99 workshop on ontologies and problem-solving methods (krr5)	\N	\N	\N	\N	\N	1999	\N	2005-03-10 21:10:58	stockholm, sweden	a {f}ramework for {u}nderstanding and {c}lassifying {o}ntology {a}pplications	in this paper, we draw attention to common goals and supporting technologies of several relatively distinct communities to facilitate closer cooperation and faster progress. the common thread is the need for sharing the meaning of terms in a given domain, which is a central role of ontologies. the different communities include ontology research groups, software developers and standards organizations. using a broad definition of ?ontology?, we show that much of the work being done by those communities may be viewed as practical applications of ontologies. to achieve this, we present a framework for understanding and classifying ontology applications. we identify three main categories of ontology applications: 1) neutral authoring, 2) common access to information, and 3) indexing for search. in each category, we identify specific ontology application scenarios. for each, we indicate their intended purpose, the role of the ontology, the supporting technologies and who the principal actors are and what they do. we illuminate the similarities and differences between scenarios.
231	120141	article	biopolymers	\N	\N	\N	14	29	6-7	1990	may	2005-03-10 21:18:15	max-planck institut f\\"{u}r biophysikalische chemie, g\\"{o}ttingen, federal republic of germany.	the equilibrium partition function and base pair binding probabilities for {rna} secondary structure	a novel application of dynamic programming to the folding problem for {rna} enables one to calculate the full equilibrium partition function for secondary structure and the probabilities of various substructures. in particular, both the partition function and the probabilities of all base pairs are computed by a recursive scheme of polynomial order n3 in the sequence length n. the temperature dependence of the partition function gives information about melting behavior for the secondary structure. the pair binding probabilities, the computation of which depends on the partition function, are visually summarized in a "box matrix" display and this provides a useful tool for examining the full ensemble of probable alternative equilibrium structures. the calculation of this ensemble representation allows a proper application and assessment of the predictive power of the secondary structure method, and yields important information on alternatives and intermediates in addition to local information about base pair opening and slippage. the results are illustrated for representative {trna}, {5s} {rna}, and self-replicating and self-splicing {rna} molecules, and allow a direct comparison with enzymatic structure probes. the effect of changes in the thermodynamic parameters on the equilibrium ensemble provides a further sensitivity check to the predictions.
232	121025	article	psychological review	\N	\N	\N	25	84	4	1977	\N	2005-03-11 10:12:57	\N	features of similarity	questions the metric and dimensional assumptions that underlie the geometric representation of similarity on both theoretical and empirical grounds. a new set-theoretical approach to similarity is developed in which objects are represented as collections of features and similarity is described as a feature-matching process. specifically, a set of qualitative assumptions is shown to imply the contrast model, which expresses the similarity between objects as a linear combination of the measures of their common and distinctive features. several predictions of the contrast model are tested in studies of similarity with both semantic and perceptual stimuli. the model is used to uncover, analyze, and explain a variety of empirical phenomena such as the role of common and distinctive features, the relations between judgments of similarity and difference, the presence of asymmetric similarities, and the effects of context on judgments of similarity. the contrast model generalizes standard representations of similarity data in terms of clusters and trees. it is also used to analyze the relations of prototypicality and family resemblance. (39 ref) {(psycinfo} database record (c) 2009 {apa,} all rights reserved)
233	121079	article	acm transactions on computer systems	\N	\N	\N	51	19	3	2001	\N	2005-03-11 10:12:57	\N	design and evaluation of a wide-area event notification service	the components of a loosely coupled system are typically designed to operate by generating and responding to asynchronous events. an  event notification service  is an application-independent infrastructure that supports the construction of event-based systems, whereby generators of events publish event notifications to the infrastructure and consumers of events subscribe with the infrastructure to receive relevant notifications. the two primary services that should be provided to components by the infrastructure are notification selection (i. e., determining which notifications match which subscriptions) and notification delivery (i.e., routing matching notifications from publishers to subscribers). numerous event notification services have been developed for local-area networks, generally based on a centralized server to select and deliver event  notifications. therefore, they suffer from an inherent inability to scale to wide-area networks, such as the internet, where the number and physical distribution of the service's clients can quickly overwhelm a centralized solution. the critical challenge in the setting of a wide-area network is to maximize the expressiveness in the selection mechanism without sacrificing scalability in the delivery mechanism. this paper presents siena, an event notification service that we have designed and implemented to exhibit both expressiveness and scalability. we describe the service's interface to applications, the algorithms used by networks of servers to select and deliver event notifications, and the strategies used to optimize performance. we also present results of simulation studies that  examine the scalability and performance of the service.
234	121098	inproceedings	\N	proceedings of the conference on applications, technologies, architectures, and protocols for computer communication	\N	\N	11	\N	\N	1999	\N	2005-03-11 10:12:58	\N	on {power-law} relationships of the internet topology	despite the apparent randomness of the internet, we discover some surprisingly simple power-laws of the internet topology. these power-laws hold for three snapshots of the internet, between november 1997 and december 1998, despite a 45% growth of its size during that period. we show that our power-laws fit the real data very well resulting in correlation coefficients of 96% or higher.our observations provide a novel perspective of the structure of the internet. the power-laws describe concisely skewed distributions of graph properties such as the node outdegree. in addition, these power-laws can be used to estimate important parameters such as the average neighborhood size, and facilitate the design and the performance analysis of protocols. furthermore, we can use them to generate and select realistic topologies for simulation purposes.
235	121161	techreport	\N	\N	\N	\N	\N	\N	HPL-2002-57	2002	mar	2005-03-11 10:12:58	\N	{peer-to-peer} computing	the term "peer-to-peer refers to a class of systems and applications that employ distributed resources to perform a critical function in a decentralized manner. with the pervasive deployment of computers, p2p is increasingly receiving attention in research, product development, and investment circles. this interest ranges from enthusiasm, through hype, to disbelief in its potential. some of the benefits of a p2p approach include: improving scalability by avoiding dependency on centralized points; eliminating the need for costly infrastructure by enabling direct communication among clients; and enabling resource aggregation. this survey reviews the field of p2p systems and applications by summarizing the key concepts and giving an overview of the most important systems. design and implementation issues of p2p systems are analyzed in general, and then revisited for each of the case studies described in section 6. this survey will help people understand the potential benefits of p2p in the research community and industry. for people unfamiliar with the field it provides a general overview, as well as detailed case studies. it is also intended for users, developers, and information technologies maintaining systems, in particular comparison of p2p solutions with alternative architectures and models.
236	121192	article	ieee transactions on systems, man and cybernetics	\N	\N	\N	13	19	1	1989	jan	2005-03-11 10:12:58	\N	development and application of a metric on semantic nets	motivated by the properties of spreading activation and conceptual distance, the authors propose a metric, called distance, on the power set of nodes in a semantic net. distance is the average minimum path length over all pairwise combinations of nodes between two subsets of nodes. distance can be successfully used to assess the conceptual distance between sets of concepts when used on a semantic net of hierarchical relations. when other kinds of relationships, like `cause', are used, distance must be amended but then can again be effective. the judgements of distance significantly correlate with the distance judgements that people make and help to determine whether one semantic net is better or worse than another. the authors focus on the mathematical characteristics of distance that presents novel cases and interpretations. experiments in which distance is applied to pairs of concepts and to sets of concepts in a hierarchical knowledge base show the power of hierarchical relations in representing information about the conceptual distance between concepts
237	121196	techreport	\N	\N	\N	\N	\N	\N	TR-00-010	2000	\N	2005-03-11 10:12:58	berkeley, ca	a scalable content addressable network	hash tables - which map "keys" onto "values" - are an essential building block in modern software systems. we believe a similar functionality would be equally valuable to large distributed systems. in this paper, we introduce the concept of a content-addressable network (can) as a distributed infrastructure that provides hash table-like functionality on internet-like scales. the can is scalable, fault-tolerant and completely self-organizing, and we demonstrate its scalability, robustness and low-latency properties through simulation.
238	121199	inproceedings	\N	proceedings of the acm sigraph	\N	\N	\N	\N	\N	1987	jul	2005-03-11 10:12:58	anaheim, california	flocks, herds, and schools: a distributed behavioral model	the aggregate motion of a flock of birds, a herd of land animals, or a school of fish is a beautiful and familiar part of the natural world. but this type of complex motion is rarely seen in computer animation. this paper explores an approach based on simulation as an alternative to scripting the paths of each bird individually. the simulated flock is an elaboration of a particle system, with the simulated birds being the particles. the aggregate motion of the simulated flock is created by a distributed behavioral model much like that at work in a natural flock; the birds choose their own course. each simulated bird is implemented as an independent actor that navigates according to its local perception of the dynamic environment, the laws of simulated physics that rule its motion, and a set of behaviors programmed into it by the &#034;animator.&#034; the aggregate motion of the simulated flock is the result of the dense interaction of the relatively simple behaviors of the individual simulated b...
239	121209	inproceedings	\N	proceedings of the 5th symposium on operating systems design and implementation (osdi) 2002	\N	\N	\N	\N	\N	2002	dec	2005-03-11 10:12:58	boston, ma, usa	an analysis of internet content delivery systems	in the span of only a few years, the internet has experienced an astronomical increase in the use of specialized content delivery systems, such as content delivery networks and peer-to-peer file sharing systems. therefore, an understanding of content delivery on the internet now requires a detailed understanding of how these systems are used in practice. this paper examines content delivery from the point of view of four content delivery systems: http web traffic, the akamai content delivery network, and kazaa and gnutella peer-to-peer file sharing traffic. we collected a trace of all incoming and outgoing network traffic at the university of washington, a large university with over 60,000 students, faculty, and staff. from this trace, we isolated and characterized traffic belonging to each of these four delivery classes. our results (1) quantify the rapidly increasing importance of new content delivery systems, particularly peerto-peer networks, (2) characterize the behavior of these systems from the perspectives of clients, objects, and servers, and (3) derive implications for caching in these systems. 1
240	121542	inproceedings	\N	22nd annual int'l acm-sigir conf. research and development in information retrieval	\N	acm press	\N	\N	\N	1999	\N	2005-03-11 13:55:09	new york	visualization of {s}earch {r}esults: {a} {c}omparative {e}valuation of {t}ext, 2{d}, and 3{d} {i}nterfaces	although there have been many prototypes of visualization in support of information retrieval, there has been little systematic evaluation that distinguishes the benefits of the visualization per se from that of various accompanying features. the current study focuses on such an evaluation of {nirve}, a tool that supports visualization of search results. insofar as possible, functionally equivalent {3d}, {2d}, and text versions of {nirve} were implemented. nine novices and six professional users..
241	121580	article	ieee transactions on software engineering	\N	\N	\N	\N	SE-12	2	1986	\N	2005-03-11 13:56:54	\N	a rational design process: how and why to fake it	software engineers have been searching for the ideal software development process: a process in which programs are derived from specifications in the same way that lemmas and theorems are derived from axioms in published proofs. after explaining why we can never achieve it, this paper describes such a process. the process is described in terms of a sequence of documents that should be produced on the way to producing the software. we show that such documents can serve several purposes. they provide a basis for preliminary design review, serve as reference material during the coding, and guide the maintenance programmer in his work. we discuss how these documents can be constructed using the same principles that should guide the software design. the resulting documentation is worth much more than the "afterthought" documentation that is usually produced. if we take the care to keep all of the documents up-to-date, we can create the appearance of a fully rational design process.
242	121608	book	\N	\N	cognition and categorization	lawrence erlbaum publishers	21	\N	\N	1978	\N	2005-03-11 13:56:55	hillside, nj	principles of {c}ategorization	the chapter is divided into five parts. the first part presents the two general principles that are proposed to underlie categorization systems. the second part shows the way in which these principles appear to result in a basic and primary level of categorization in the levels of abstraction in a taxonomy. it is essentially a summary of the research already reported on basic level objects (rosch et al., 1976). thus the second section may be omitted by the reader already sufficiently familiar with that material. the third part relates the principles of categorization to the formation of prototypes in those categories that are at the same level of abstraction in a taxonomy. in particular, this section attempts to clarify the operational concept of prototypicality and to separate that concept from claims concerning the role of prototypes in cognitive processing, representation, and learning for which there is little evidence. the fourth part presents two issues that are problematical for the abstract principles of categorization stated in part i: (1) the relation of context to basic level objects and prototypes; and (2) assumptions about the nature of the attributes of real-world objects that underlie the claim that there is structure in the world. the fifth part is a report of initial attempts to base an analysis of the attributes, functions, and contexts of objects on a consideration of objects as props in culturally defined events.
243	121779	article	ieee intelligent systems	\N	\N	\N	7	18	2	2003	\N	2005-03-11 15:28:23	\N	ontologies for {e}nterprise {k}nowledge {m}anagement	ontologies are a key technology for enabling semantics-driven knowledge processing, and it is widely accepted that the next generation of knowledge management system will rely on conceptual models in the form of ontologies. unfortunately, the development of real-world enterprise-wide ontology-based knowledge management systems is still in an early stage. the authors present an integrated enterprise knowledge management architecture developed within the ontologging project dealing with several challenges related to applying ontologies in real-world environments. they focus on two important ontology management problems--namely, supporting multiple ontologies and managing ontology evolution.
244	121870	book	\N	\N	\N	cambridge university press	\N	\N	\N	2003	apr	2005-03-11 15:32:39	cambridge, uk	probability theory: the logic of science	{going beyond the conventional mathematics of probability theory, this study views the subject in a wider context. it discusses new results, along with applications of probability theory to a variety of problems. the book contains many exercises and is suitable for use as a textbook on graduate-level courses involving data analysis. aimed at readers already familiar with applied mathematics at an advanced undergraduate level or higher, it is of interest to scientists concerned with inference from incomplete information.}
245	122299	inproceedings	\N	proceedings of the {usenix} winter 1990 technical conference	\N	usenix association	11	\N	\N	1990	\N	2005-03-11 18:24:36	berkeley, ca	{cvs} {ii}: parallelizing software development	the program described in this paper fills a need in the unix community for a freely available tool to manage software revision and release control in a multi-developer,multi-directory,multi-group environment. this tool also addresses the increasing need for tracking third-party vendor source distributions while trying to maintain local modifications to earlier releases. 1. background  in large software development projects, it is usually necessary for more than one software developer to be...
246	123095	book	\N	\N	\N	oxford university press	\N	\N	\N	1989	\N	2005-03-12 04:20:54	\N	computability and logic	computability and logic has become a classic because of its accessibility to students without a mathematical background and because it covers not simply the staple topics of an intermediate logic course, such as godel's incompleteness theorems, but also a large number of optional topics, from turing's theory of computability to ramsey's theorem. including a selection of exercises, adjusted for this edition, at the end of each chapter, it offers a new and simpler treatment of the representability of recursive functions, a traditional stumbling block for students on the way to the godel incompleteness theorems.
247	123644	incollection	\N	handbook of philosophical logic	\N	reidel	\N	2	\N	1984	\N	2005-03-12 04:21:03	dordrecht	dynamic logic	dynamic logic (dl) is a formal system for reasoning about programs. traditionally, this has meant formalizing correctness specifications and proving rigorously that those specifications are met by a particular program. other activities fall into this category as well: determining the equivalence of programs, comparing the expressive power of various programming constructs, synthesizing programs from specifications, etc. formal systems too numerous to mention have been proposed for these purposes, each with its own peculiarities. dl can be described as a blend of three complementary classical ingredients: first-order predicate logic, modal logic, and the algebra of regular events. these components merge to form a system of remarkable unity that is theoretically rich as well as practical. the name dynamic logic emphasizes the principal feature distinguishing it from classical predicate logic. in the latter, truth is static: the truth value of a formula ' is determined by a valuation of its free variables over some structure. the valuation and the truth value of ' it induces are regarded as immutable; there is no formalism relating them to any other valuations or truth values. in dynamic logic, there are explicit syntactic constructs called programs whose main role is to change the values of variables, thereby changing the truth values of formulas. for example, the program x: = x + 1 over the natural numbers changes the truth value of the formula x is even. such changes occur on a metalogical level in classical predicate logic. for example, in tarski's definition of truth of a formula, if u: fx; y; : : : g! n is a valuation of variables over the natural numbers n, then the formula 9x x 2 = y is defined to be true under the valuation u iff there exists an a 2 n such that the formula x
248	123866	book	\N	\N	\N	north-holland	\N	\N	\N	1952	\N	2005-03-12 04:21:06	\N	introduction to metamathematics	stephen cole kleene was one of the greatest logicians of the twentieth century and this book is the influential textbook he wrote to teach the subject to the next generation. it was first published in 1952, some twenty years after the publication of gÃ¶del's paper on the incompleteness of arithmetic, which marked, if not the beginning of modern logic, at least a turning point after which â€œnothing was ever the same.â€ kleene was an important figure in logic, and lived a long full life of scholarship and teaching. the 1930s was a time of creativity and ferment in the subject, when the notion of â€œcomputableâ€ moved from the realm of philosophical speculation to the realm of science. this was accomplished by the work of kurt gÃ¶de1, alan turing, and alonzo church, who gave three apparently different precise definitions of â€œcomputableâ€. when they all turned out to be equivalent, there was a collective realization that this was indeed the â€œright notionâ€. kleene played a key role in this process. one could say that he was â€œthere at the beginningâ€ of modern logic. he showed the equivalence of lambda calculus with turing machines and with gÃ¶del's recursion equations, and developed the modern machinery of partial recursive functions. this textbook played an invaluable part in educating the logicians of the present. it played an important role in their own logical education.
249	123906	book	\N	\N	\N	harvard university press	\N	\N	\N	1972	\N	2005-03-12 04:21:07	cambridge, ma	naming and necessity	{<p>  if there is such a thing as essential reading in metaphysics or in philosophy of language, this is it.  </p><p>  ever since the publication of its original version, <i>naming and necessity</i> has had great and increasing influence. it redirected philosophical attention to neglected questions of natural and metaphysical necessity and to the connections between these and theories of reference, in particular of naming, and of identity. from a critique of the dominant tendency to assimilate names to descriptions and more generally to treat their reference as a function of their fregean sense, surprisingly deep and widespread consequences may be drawn. the largely discredited distinction between accidental and essential properties, both of individual things (including people) and of kinds of things, is revived. so is a consequent view of science as what seeks out the essences of natural kinds. traditional objections to such views are dealt with by sharpening distinctions between epistemic and metaphysical necessity; in particular by the startling admission of necessary a posteriori truths. from these, in particular from identity statements using rigid designators whether of things or of kinds, further remarkable consequences are drawn for the natures of things, of people, and of kinds; strong objections follow, for example to identity versions of materialism as a theory of the mind.  </p><p>  this seminal work, to which today's thriving essentialist metaphysics largely owes its impetus, is here published with a substantial new preface by the author.  </p>}
250	125979	book	\N	\N	\N	a bradford book	\N	\N	\N	1992	apr	2005-03-13 07:21:00	\N	adaptation in natural and artificial systems: an introductory analysis with applications to biology, control, and artificial intelligence	{john holland's <i>adaptation in natural and artificial systems</i> is one of the classics in the field of complex adaptive systems. holland is known as the father of genetic algorithms and classifier systems and in this tome he describes the theory behind these algorithms. drawing on ideas from the fields of biology and economics, he shows how computer programs can evolve. the book contains mathematical proofs that are accessible only to those with strong backgrounds in engineering or science.} {genetic algorithms are playing an increasingly important role in studies of complex adaptive systems, ranging from adaptive agents in economic theory to the use of machine learning techniques in the design of complex devices such as aircraft turbines and integrated circuits. <i>adaptation in natural and artificial systems</i> is the book that initiated this field of study, presenting the theoretical foundations and exploring applications.<br /> <br /> in its most familiar form, adaptation is a biological process, whereby organisms evolve by rearranging genetic material to survive in environments confronting them. in this now classic work, holland presents a mathematical model that allows for the nonlinearity of such complex interactions. he demonstrates the model's universality by applying it to economics, physiological psychology, game theory, and artificial intelligence and then outlines the way in which this approach modifies the traditional views of mathematical genetics.<br /> <br /> initially applying his concepts to simply defined artificial systems with limited numbers of parameters, holland goes on to explore their use in the study of a wide range of complex, naturally occuring processes, concentrating on systems having multiple factors that interact in nonlinear ways. along the way he accounts for major effects of coadaptation and coevolution: the emergence of building blocks, or schemata, that are recombined and passed on to succeeding generations to provide, innovations and improvements.<br /> <br /> john h. holland is professor of psychology and professor of electrical engineering and computer science at the university of michigan. he is also maxwell professor at the santa fe institute and is director of the university of michigan/santa fe institute advanced research program.}
251	126681	book	\N	\N	\N	graphics press	\N	\N	\N	1997	feb	2005-03-14 15:54:53	\N	visual explanations: images and quantities, evidence and narrative	{with <i>visual explanations</i>, edward r. tufte adds a third volume to his indispensable series on information display. the first, <i>the visual display of quantitative information,</i> which focuses on charts and graphs that display numerical information, virtually defined the field. the second, <i>envisioning information,</i> explores similar territory but with an emphasis on maps and cartography. <i>visual explanations</i> centers on dynamic data--information that changes over time. (tufte has described the three books as being about, respectively, "pictures of numbers, pictures of nouns, and pictures of verbs.") <p> like its predecessors, <i>visual explanations</i> is both intellectually stimulating and beautiful to behold. tufte, a self-publisher, takes extraordinary pains with design and production. the book ranges through a variety of topics, including the explosion of the space shuttle <i>challenger</i> (which could have been prevented, tufte argues, by better information display on the part of the rocket's engineers), magic tricks, a cholera epidemic in 19th-century london, and the principle of using "the smallest effective difference" to display distinctions in data. throughout, tufte presents ideas with crystalline clarity and illustrates them in exquisitely rendered samples.}
252	127079	article	bioinformatics	\N	\N	\N	3	19	8	2003	may	2005-03-15 04:34:04	bioinfobank institute, limanowskiego 24a, 60-744 poznan, poland.	{3d}-jury: a simple approach to improve protein structure predictions	motivation: consensus structure prediction methods (meta-predictors) have higher accuracy than individual structure prediction algorithms (their  components). the goal for the development of the {3d}-jury system is to create  a simple but powerful procedure for generating meta-predictions using  variable sets of models obtained from diverse sources. the resulting  protocol should help to improve the quality of structural annotations of  novel {proteins.results}: the {3d}-jury system generates meta-predictions from sets of models  created using variable methods. it is not necessary to know prior  characteristics of the methods. the system is able to utilize immediately  new components (additional prediction providers). the accuracy of the system  is comparable with other well-tuned prediction servers. the algorithm  resembles methods of selecting models generated using ab initio folding simulations.  it is simple and offers a portable solution to improve the accuracy of other  protein structure prediction {protocols.availability}: the {3d}-jury system is available via the structure prediction meta server ({http://bioinfo}.{pl}/meta/) to the academic {community.contact}: {leszek@bioinfo.plsupplementary} information:  {3d}-jury is coupled to the continuous online server evaluation program, {livebench} ({http://bioinfo}.{pl}/{livebench}/)
253	128099	article	neuroimage	\N	\N	\N	11	12	4	2000	oct	2005-03-15 16:25:10	the wellcome department of cognitive neurology, institute of neurology, queen square, london wc1n 3bg, united kingdom.	nonlinear responses in {fmri}: the balloon model, volterra kernels, and other hemodynamics	there is a growing appreciation of the importance of nonlinearities in evoked responses in {fmri}, particularly with the advent of event-related {fmri}. these nonlinearities are commonly expressed as interactions among stimuli that can lead to the suppression and increased latency of responses to a stimulus that are incurred by a preceding stimulus. we have presented previously a model-free characterization of these effects using generic techniques from nonlinear system identification, namely a volterra series formulation. at the same time buxton et al. (1998) described a plausible and compelling dynamical model of hemodynamic signal transduction in {fmri}. subsequent work by mandeville et al. (1999) provided important theoretical and empirical constraints on the form of the dynamic relationship between blood flow and volume that underpins the evolution of the {fmri} signal. in this paper we combine these system identification and model-based approaches and ask whether the balloon model is sufficient to account for the nonlinear behaviors observed in real time series. we conclude that it can, and furthermore the model parameters that ensue are biologically plausible. this conclusion is based on the observation that the balloon model can produce volterra kernels that emulate empirical kernels. to enable this evaluation we had to embed the balloon model in a hemodynamic input-state-output model that included the dynamics of perfusion changes that are contingent on underlying synaptic activation. this paper presents (i) the full hemodynamic model (ii), how its associated volterra kernels can be derived, and (iii) addresses the model's validity in relation to empirical nonlinear characterisations of evoked responses in {fmri} and other neurophysiological constraints.
254	128240	article	cell	\N	\N	\N	10	113	5	2003	may	2005-03-15 21:41:12	\N	{development of genetic circuitry exhibiting toggle switch or oscillatory behavior in escherichia coli}	analysis of the system design principles of signaling systems requires model systems where all components and regulatory interactions are known. components of the lac and ntr systems were used to construct genetic circuits that display toggle switch or oscillatory behavior. both devices contain an â€œactivator moduleâ€ consisting of a modified  glna  promoter with  lac  operators, driving the expression of the activator, nri. since nri activates the  glna  promoter, this creates an autoactivated circuit repressible by laci. the oscillator contains a â€œrepressor moduleâ€ consisting of the nri-activated  glnk  promoter driving laci expression. this circuitry produced synchronous damped oscillations in turbidostat cultures, with periods much longer than the cell cycle. for the toggle switch, laci was provided constitutively; the level of active repressor was controlled by using a  lacy  mutant and varying the concentration of iptg. this circuitry provided nearly discontinuous expression of activator.
255	128303	article	trends biotechnol	\N	\N	\N	-395	20	11	2002	nov	2005-03-15 21:41:14	\N	gene networks: how to put the function in genomics.	an increasingly popular model of regulation is to represent networks of genes as if they directly affect each other. although such gene networks are phenomenological because they do not explicitly represent the proteins and metabolites that mediate cell interactions, they are a logical way of describing phenomena observed with transcription profiling, such as those that occur with popular microarray technology. the ability to create gene networks from experimental data and use them to reason about their dynamics and design principles will increase our understanding of cellular function. we propose that gene networks are also a good way to describe function unequivocally, and that they could be used for genome functional annotation. here, we review some of the concepts and methods associated with gene networks, with emphasis on their construction based on experimental data.
256	128334	article	genome res	\N	\N	\N	8	11	8	2001	aug	2005-03-15 21:41:16	\N	{creating the gene ontology resource: design and implementation}	the exponential growth in the volume of accessible biological information has generated a confusion of voices surrounding the annotation of molecular information about genes and their products. the gene ontology {(go)} project seeks to provide a set of structured vocabularies for specific biological domains that can be used to describe gene products in any organism. this work includes building three extensive ontologies to describe molecular function, biological process, and cellular component, and providing a community database resource that supports the use of these ontologies. the {go} consortium was initiated by scientists associated with three model organism databases: {sgd,} the saccharomyces genome database; {flybase,} the drosophila genome database; and {mgd/gxd,} the mouse genome informatics databases. additional model organism database groups are joining the project. each of these model organism information systems is annotating genes and gene products using {go} vocabulary terms and incorporating these annotations into their respective model organism databases. each database contributes its annotation files to a shared {go} data resource accessible to the public at http://www.geneontology.org/. the {go} site can be used by the community both to recover the {go} vocabularies and to access the annotated gene product data sets from the model organism databases. the {go} consortium supports the development of the {go} database resource and provides tools enabling curators and researchers to query and manipulate the vocabularies. we believe that the shared development of this molecular annotation resource will contribute to the unification of biological information.
257	128343	article	bioinformatics	\N	\N	\N	-681	16	8	2000	aug	2005-03-15 21:41:17	\N	genetic network inference: from co-expression clustering to reverse engineering.	{motivation}: advances in molecular biological, analytical and computational technologies are enabling us to systematically investigate the complex molecular processes underlying biological systems. in particular, using high-throughput gene expression assays, we are able to measure the output of the gene regulatory network. we aim here to review datamining and modeling approaches for conceptualizing and unraveling the functional relationships implicit in these datasets. clustering of co-expression profiles allows us to infer shared regulatory inputs and functional pathways. we discuss various aspects of clustering, ranging from distance measures to clustering algorithms and multiple-cluster memberships. more advanced analysis aims to infer causal connections between genes directly, i.e. who is regulating whom and how. we discuss several approaches to the problem of reverse engineering of genetic networks, from discrete boolean networks, to continuous linear and non-linear models. we conclude that the combination of predictive modeling with systematic experimental verification will be required to gain a deeper insight into living organisms, therapeutic targeting and bioengineering.
258	128350	article	science	\N	\N	\N	-1591	295	5560	2002	mar	2005-03-15 21:41:17	\N	a genomic regulatory network for development.	development of the body plan is controlled by large networks of regulatory genes. a gene regulatory network that controls the specification of endoderm and mesoderm in the sea urchin embryo is summarized here. the network was derived from large-scale perturbation analyses, in combination with computational methodologies, genomic data, cis-regulatory analysis, and molecular embryology. the network contains over 40 genes at present, and each node can be directly verified at the {dna} sequence level by cis-regulatory analysis. its architecture reveals specific and general aspects of development, such as how given cells generate their ordained fates in the embryo and why the process moves inexorably forward in developmental time.
259	128354	article	science	\N	\N	\N	6	278	5338	1997	oct	2005-03-15 21:41:17	\N	{exploring the metabolic and genetic control of gene expression on a genomic scale}	dna microarrays containing virtually every gene of saccharomyces cerevisiae were used to carry out a comprehensive investigation of the temporal program of gene expression accompanying the metabolic shift from fermentation to respiration. the expression profiles observed for genes with known metabolic functions pointed to features of the metabolic reprogramming that occur during the diauxic shift, and the expression patterns of many previously uncharacterized genes provided clues to their possible functions. the same dna microarrays were also used to identify genes whose expression was affected by deletion of the transcriptional co-repressor tup1 or overexpression of the transcriptional activator yap1. these results demonstrate the feasibility and utility of this approach to genomewide exploration of gene expression patterns.
260	128391	article	nature	\N	\N	\N	3	405	6788	2000	jun	2005-03-15 21:41:18	\N	{protein function in the post-genomic era}	faced with the avalanche of genomic sequences and data on messenger {rna} expression, biological scientists are confronting a frightening prospect: piles of information but only flakes of knowledge. {h}ow can the thousands of sequences being determined and deposited, and the thousands of expression profiles being generated by the new array methods, be synthesized into useful knowledge? {w}hat form will this knowledge take? {t}hese are questions being addressed by scientists in the field known as 'functional genomics'.
261	128425	article	bmc bioinformatics	\N	\N	\N	\N	5	1	2004	mar	2005-03-15 21:41:19	\N	defining transcriptional networks through integrative modeling of {mrna} expression and transcription factor binding data.	{background}: functional genomics studies are yielding information about regulatory processes in the cell at an unprecedented scale. in the yeast s. cerevisiae, {dna} microarrays have not only been used to measure the {mrna} abundance for all genes under a variety of conditions but also to determine the occupancy of all promoter regions by a large number of transcription factors. the challenge is to extract useful information about the global regulatory network from these data. {results}: we present {ma}-networker, an algorithm that combines microarray data for {mrna} expression and transcription factor occupancy to define the regulatory network of the cell. multivariate regression analysis is used to infer the activity of each transcription factor, and the correlation across different conditions between this activity and the {mrna} expression of a gene is interpreted as regulatory coupling strength. applying our method to s. cerevisiae, we find that, on average, 58\\% of the genes whose promoter region is bound by a transcription factor are true regulatory targets. these results are validated by an analysis of enrichment for functional annotation, response for transcription factor deletion, and over-representation of cis-regulatory motifs. we are able to assign directionality to transcription factors that control divergently transcribed genes sharing the same promoter region. finally, we identify an intrinsic limitation of transcription factor deletion experiments related to the combinatorial nature of transcriptional control, to which our approach provides an alternative. {conclusion}: our reliable classification of {chip} positives into functional and non-functional {tf} targets based on their expression pattern across a wide range of conditions provides a starting point for identifying the unknown sequence features in non-coding {dna} that directly or indirectly determine the context dependence of transcription factor action. complete analysis results are available for browsing or download at {http://bussemaker.bio.columbia.edu/papers/ma}-networker/.
262	128429	article	mol biol cell	\N	\N	\N	16	11	12	2000	dec	2005-03-15 21:41:19	\N	{genomic expression programs in the response of yeast cells to environmental changes}	we explored genomic expression patterns in the yeast saccharomyces cerevisiae responding to diverse environmental transitions. dna microarrays were used to measure changes in transcript levels over time for almost every yeast gene, as cells responded to temperature shocks, hydrogen peroxide, the superoxide-generating drug menadione, the sulfhydryl-oxidizing agent diamide, the disulfide-reducing agent dithiothreitol, hyper- and hypo-osmotic shock, amino acid starvation, nitrogen source depletion, and progression into stationary phase. a large set of genes (approximately 900) showed a similar drastic response to almost all of these environmental changes. additional features of the genomic responses were specialized for specific conditions. promoter analysis and subsequent characterization of the responses of mutant strains implicated the transcription factors yap1p, as well as msn2p and msn4p, in mediating specific features of the transcriptional response, while the identification of novel sequence elements provided clues to novel regulators. physiological themes in the genomic responses to specific environmental stresses provided insights into the effects of those stresses on the cell.
263	128436	article	nat genet	\N	\N	\N	-476	29	4	2001	dec	2005-03-15 21:41:19	\N	correlation between transcriptome and interactome mapping data from saccharomyces cerevisiae.	genomic and proteomic approaches can provide hypotheses concerning function for the large number of genes predicted from genome sequences. because of the artificial nature of the assays, however, the information from these high-throughput approaches should be considered with caution. although it is possible that more meaningful hypotheses could be formulated by integrating the data from various functional genomic and proteomic projects, it has yet to be seen to what extent the data can be correlated and how such integration can be achieved. we developed a 'transcriptome-interactome correlation mapping' strategy to compare the interactions between proteins encoded by genes that belong to common expression-profiling clusters with those between proteins encoded by genes that belong to different clusters. using this strategy with currently available data sets for saccharomyces cerevisiae, we provide the first global evidence that genes with similar expression profiles are more likely to encode interacting proteins. we show how this correlation between transcriptome and interactome data can be used to improve the quality of hypotheses based on the information from both approaches. the strategy described here may help to integrate other functional genomic and proteomic data, both in yeast and in higher organisms.
264	128478	article	nature	\N	\N	\N	-696	425	6959	2003	oct	2005-03-15 21:41:20	\N	global analysis of protein expression in yeast.	the availability of complete genomic sequences and technologies that allow comprehensive analysis of global expression profiles of messenger {rna} have greatly expanded our ability to monitor the internal state of a cell. yet biological systems ultimately need to be explained in terms of the activity, regulation and modification of proteins--and the ubiquitous occurrence of post-transcriptional regulation makes {mrna} an imperfect proxy for such information. to facilitate global protein analyses, we have created a saccharomyces cerevisiae fusion library where each open reading frame is tagged with a high-affinity epitope and expressed from its natural chromosomal location. through immunodetection of the common tag, we obtain a census of proteins expressed during log-phase growth and measurements of their absolute levels. we find that about 80\\% of the proteome is expressed during normal growth conditions, and, using additional sequence information, we systematically identify misannotated genes. the abundance of proteins ranges from fewer than 50 to more than 10(6) molecules per cell. many of these molecules, including essential proteins and most transcription factors, are present at levels that are not readily detectable by other proteomic techniques nor predictable by {mrna} levels or codon bias measurements.
265	128479	article	nature	\N	\N	\N	4	418	6896	2002	jul	2005-03-15 21:41:20	\N	{functional profiling of the saccharomyces cerevisiae genome}	determining the effect of gene deletion is a fundamental approach to understanding gene function. conventional genetic screens exhibit biases, and genes contributing to a phenotype are often missed. we systematically constructed a nearly complete collection of gene-deletion mutants (96% of annotated open reading frames, or orfs) of the yeast saccharomyces cerevisiae. dna sequences dubbed 'molecular bar codes' uniquely identify each strain, enabling their growth to be analysed in parallel and the fitness contribution of each gene to be quantitatively assessed by hybridization to high-density oligonucleotide arrays. we show that previously known and new genes are necessary for optimal growth under six well-studied conditions: high salt, sorbitol, galactose, ph 8, minimal medium and nystatin treatment. less than 7% of genes that exhibit a significant increase in messenger rna expression are also required for optimal growth in four of the tested conditions. our results validate the yeast gene-deletion collection as a valuable resource for functional genomics.
266	128483	article	science	\N	\N	\N	4	274	5287	1996	oct	2005-03-15 21:41:20	\N	{life with 6000 genes}	the genome of the yeast saccharomyces cerevisiae has been completely sequenced through a worldwide collaboration. the sequence of 12,068 kilobases defines 5885 potential protein-encoding genes, approximately 140 genes specifying ribosomal rna, 40 genes for small nuclear rna molecules, and 275 transfer rna genes. in addition, the complete sequence provides information about the higher order organization of yeast's 16 chromosomes and allows some insight into their evolutionary history. the genome shows a considerable amount of apparent genetic redundancy, and one of the major problems to be tackled during the next stage of the yeast genome project is to elucidate the biological functions of all of these genes.
267	128554	article	annu rev genomics hum genet	\N	\N	\N	29	2	\N	2001	\N	2005-03-15 21:41:22	\N	{a new approach to decoding life: systems biology}	systems biology studies biological systems by systematically perturbing them (biologically, genetically, or chemically); monitoring the gene, protein, and informational pathway responses; integrating these data; and ultimately, formulating mathematical models that describe the structure of the system and its response to individual perturbations. the emergence of systems biology is described, as are several examples of specific systems approaches.
268	128558	article	trends biotechnol	\N	\N	\N	7	21	6	2003	jun	2005-03-15 21:41:22	\N	{building with a scaffold: emerging strategies for high- to low-level cellular modeling}	computational cellular models are becoming crucial for the analysis of complex biological systems. an important new paradigm for cellular modeling involves building a comprehensive scaffold of molecular interactions and then mining this scaffold to reveal a hierarchy of signaling, regulatory and metabolic pathways. we review the important trends that make this approach feasible and describe how they are spurring the development of models at multiple levels of abstraction. pathway maps can be extracted from the scaffold using &#x2018;high-level&#x2019; computational models, which identify the key components, interactions and influences required for more detailed &#x2018;low-level&#x2019; models. large-scale experimental measurements validate high-level models, whereas targeted experimental manipulations and measurements test low-level models.
269	128561	article	proc natl acad sci usa	\N	\N	\N	5	100	13	2003	jun	2005-03-15 21:41:22	\N	{prediction and measurement of an autoregulatory genetic module}	10.1073/pnas.1332628100 the deduction of phenotypic cellular responses from the structure and  behavior of complex gene regulatory networks is one of the defining challenges  of systems biology. this goal will require a quantitative understanding of the  modular components that constitute such networks. we pursued an integrated  approach, combining theory and experiment, to analyze and describe the  dynamics of an isolated genetic module, an in vivo autoregulatory  gene network. as predicted by the model, temperature-induced protein  destabilization led to the existence of two expression states, thus  elucidating the trademark bistability of the positive feedback-network  architecture. after sweeping the temperature, observed population  distributions and coefficients of variation were in quantitative agreement  with those predicted by a stochastic version of the model. because model  fluctuations originated from small molecule-number effects, the experimental  validation underscores the importance of internal noise in gene expression.  this work demonstrates that isolated gene networks, coupled with proper  quantitative descriptions, can elucidate key properties of functional genetic  modules. such an approach could lead to the modular dissection of naturally  occurring gene regulatory networks, the deduction of cellular processes such  as differentiation, and the development of engineered cellular control.
270	128570	article	genome res	\N	\N	\N	9	12	1	2002	jan	2005-03-15 21:41:22	\N	{relating whole-genome expression data with protein-protein interactions}	we investigate the relationship of protein-protein interactions with mrna expression levels, by integrating a variety of data sources for yeast. we focus on known protein complexes that have clearly defined interactions between their subunits. we find that subunits of the same protein complex show significant coexpression, both in terms of similarities of absolute mrna levels and expression profiles, e.g., we can often see subunits of a complex having correlated patterns of expression over a time course. we classify the yeast protein complexes as either permanent or transient, with permanent ones being maintained through most cellular conditions. we find that, generally, permanent complexes, such as the ribosome and proteasome, have a particularly strong relationship with expression, while transient ones do not. however, we note that several transient complexes, such as the rna polymerase ii holoenzyme and the replication complex, can be subdivided into smaller permanent ones, which do have a strong relationship to gene expression. we also investigated the interactions in aggregated, genome-wide data sets, such as the comprehensive yeast two-hybrid experiments, and found them to have only a weak relationship with gene expression, similar to that of transient complexes. (further details on genecensus.org/expression/interactions and bioinfo.mbb.yale.edu/expression/interactions.)
271	128640	article	j mol biol	\N	\N	\N	15	307	5	2001	apr	2005-03-15 21:41:28	\N	{three-dimensional cluster analysis identifies interfaces and functional residue clusters in proteins}	three-dimensional cluster analysis offers a method for the prediction of functional residue clusters in proteins. this method requires a representative structure and a multiple sequence alignment as input data. individual residues are represented in terms of regional alignments that reflect both their structural environment and their evolutionary variation, as defined by the alignment of homologous sequences. from the overall (global) and the residue-specific (regional) alignments, we calculate the global and regional similarity matrices, containing scores for all pairwise sequence comparisons in the respective alignments. comparing the matrices yields two scores for each residue. the regional conservation score (c(r)(x)) defines the conservation of each residue x and its neighbors in 3d space relative to the protein as a whole. the similarity deviation score (s(x)) detects residue clusters with sequence similarities that deviate from the similarities suggested by the full-length sequences. we evaluated 3d cluster analysis on a set of 35 families of proteins with available cocrystal structures, showing small ligand interfaces, nucleic acid interfaces and two types of protein-protein interfaces (transient and stable). we present two examples in detail: fructose-1,6-bisphosphate aldolase and the mitogen-activated protein kinase erk2. we found that the regional conservation score (c(r)(x)) identifies functional residue clusters better than a scoring scheme that does not take 3d information into account. c(r)(x) is particularly useful for the prediction of poorly conserved, transient protein-protein interfaces. many of the proteins studied contained residue clusters with elevated similarity deviation scores. these residue clusters correlate with specificity-conferring regions: 3d cluster analysis therefore represents an easily applied method for the prediction of functionally relevant spatial clusters of residues in proteins.
272	128668	article	proc natl acad sci usa	\N	\N	\N	-15515	100	26	2003	dec	2005-03-15 21:41:30	\N	network component analysis: reconstruction of regulatory signals in biological systems.	high-dimensional data sets generated by high-throughput technologies, such as {dna} microarray, are often the outputs of complex networked systems driven by hidden regulatory signals. traditional statistical methods for computing low-dimensional or hidden representations of these data sets, such as principal component analysis and independent component analysis, ignore the underlying network structures and provide decompositions based purely on a priori statistical constraints on the computed component signals. the resulting decomposition thus provides a phenomenological model for the observed data and does not necessarily contain physically or biologically meaningful signals. here, we develop a method, called network component analysis, for uncovering hidden regulatory signals from outputs of networked systems, when only a partial knowledge of the underlying network topology is available. the a priori network structure information is first tested for compliance with a set of identifiability criteria. for networks that satisfy the criteria, the signals from the regulatory nodes and their strengths of influence on each output node can be faithfully reconstructed. this method is first validated experimentally by using the absorbance spectra of a network of various hemoglobin species. the method is then applied to microarray data generated from yeast saccharamyces cerevisiae and the activities of various transcription factors during cell cycle are reconstructed by using recently discovered connectivity information for the underlying transcriptional regulatory networks.
273	128690	article	proc natl acad sci usa	\N	\N	\N	5	100	21	2003	oct	2005-03-15 21:41:31	\N	{structure and function of the feed-forward loop network motif}	engineered systems are often built of recurring circuit modules that carry out key functions. {t}ranscription networks that regulate the responses of living cells were recently found to obey similar principles: they contain several biochemical wiring patterns, termed network motifs, which recur throughout the network. {o}ne of these motifs is the feed-forward loop ({ffl}). {t}he {ffl}, a three-gene pattern, is composed of two input transcription factors, one of which regulates the other, both jointly regulating a target gene. {t}he {ffl} has eight possible structural types, because each of the three interactions in the {ffl} can be activating or repressing. {h}ere, we theoretically analyze the functions of these eight structural types. {w}e find that four of the {ffl} types, termed incoherent {ffl}s, act as sign-sensitive accelerators: they speed up the response time of the target gene expression following stimulus steps in one direction (e.g., off to on) but not in the other direction (on to off). {t}he other four types, coherent {ffl}s, act as sign-sensitive delays. {w}e find that some {ffl} types appear in transcription network databases much more frequently than others. {i}n some cases, the rare {ffl} types have reduced functionality (responding to only one of their two input stimuli), which may partially explain why they are selected against. {a}dditional features, such as pulse generation and cooperativity, are discussed. {t}his study defines the function of one of the most significant recurring circuit elements in transcription networks.
274	128746	article	j theor biol	\N	\N	\N	11	222	2	2003	may	2005-03-15 21:41:32	\N	{evolving protein interaction networks through gene duplication}	the topology of the proteome map revealed by recent large-scale hybridization methods has shown that the distribution of protein-protein interactions is highly heterogeneous, with many proteins having few edges while a few of them are heavily connected. this particular topology is shared by other cellular networks, such as metabolic pathways, and it has been suggested to be responsible for the high mutational homeostasis displayed by the genome of some organisms. in this paper we explore a recent model of proteome evolution that has been shown to reproduce many of the features displayed by its real counterparts. the model is based on gene duplication plus re-wiring of the newly created genes. the statistical features displayed by the proteome of well-known organisms are reproduced and suggest that the overall topology of the protein maps naturally emerges from the two leading mechanisms considered by the model.
275	128750	article	bioinformatics	\N	\N	\N	10	19 Suppl 2	\N	2003	\N	2005-03-15 21:41:32	\N	gene networks inference using dynamic bayesian networks.	this article deals with the identification of gene regulatory networks from experimental data using a statistical machine learning approach. a stochastic model of gene interactions capable of handling missing variables is proposed. it can be described as a dynamic bayesian network particularly well suited to tackle the stochastic nature of gene regulation and gene expression measurement. parameters of the model are learned through a penalized likelihood maximization implemented through an extended version of {em} algorithm. our approach is tested against experimental data relative to the {s.o}.s. {dna} repair network of the escherichia coli bacterium. it appears to be able to extract the main regulations between the genes involved in this network. an added missing variable is found to model the main protein of the network. good prediction abilities on unlearned data are observed. these first results are very promising: they show the power of the learning algorithm and the ability of the model to capture gene interactions. keywords: gene regulatory networks, structure extraction, expression profiles, dynamic bayesian networks, kalman filter, penalized likelihood, {em} algorithm. contact: perrin@poleia.lip6.fr
276	128757	article	genome res	\N	\N	\N	9	14	1	2004	jan	2005-03-15 21:41:32	\N	whole-genome discovery of transcription factor binding sites by network-level conservation.	comprehensive identification of {dna} cis-regulatory elements is crucial for a predictive understanding of transcriptional network dynamics. strong evidence suggests that these {dna} sequence motifs are highly conserved between related species, reflecting strong selection on the network of regulatory interactions that underlie common cellular behavior. here, we exploit a systems-level aspect of this conservation-the network-level topology of these interactions-to map transcription factor ({tf}) binding sites on a genomic scale. using network-level conservation as a constraint, our algorithm finds 71\\% of known {tf} binding sites in the yeast saccharomyces cerevisiae, using only 12\\% of the sequence of a phylogenetic neighbor. most of the novel predicted motifs show strong features of known {tf} binding sites, such as functional category and/or expression profile coherence of their corresponding genes. network-level conservation should provide a powerful constraint for the systematic mapping of {tf} binding sites in the larger genomes of higher eukaryotes.
277	128766	article	nat rev genet	\N	\N	\N	-391	2	6	2001	jun	2005-03-15 21:41:32	\N	computational analysis of microarray data.	microarray experiments are providing unprecedented quantities of genome-wide data on gene-expression patterns. although this technique has been enthusiastically developed and applied in many biological contexts, the management and analysis of the millions of data points that result from these experiments has received less attention. sophisticated computational tools are available, but the methods that are used to analyse the data can have a profound influence on the interpretation of the results. a basic understanding of these computational tools is therefore required for optimal experimental design and meaningful data analysis.
278	128770	article	nat genet	\N	\N	\N	6	34	1	2003	may	2005-03-15 21:41:32	\N	c. elegans {orfeome} version 1.1: experimental verification of the genome annotation and resource for proteome-scale protein expression.	to verify the genome annotation and to create a resource to functionally characterize the proteome, we attempted to gateway-clone all predicted protein-encoding open reading frames ({orfs}), or the '{orfeome},' of caenorhabditis elegans. we successfully cloned approximately 12,000 {orfs} ({orfeome} 1.1), of which roughly 4,000 correspond to genes that are untouched by any {cdna} or expressed-sequence tag ({est}). more than 50\\% of predicted genes needed corrections in their intron-exon structures. notably, approximately 11,000 c. elegans proteins can now be expressed under many conditions and characterized using various high-throughput strategies, including large-scale interactome mapping. we suggest that similar {orfeome} projects will be valuable for other organisms, including humans.
279	128773	article	science	\N	\N	\N	3	290	5500	2000	dec	2005-03-15 21:41:32	\N	{genome-wide location and function of dna binding proteins}	understanding how dna binding proteins control global gene expression and chromosomal maintenance requires knowledge of the chromosomal locations at which these proteins function in vivo. we developed a microarray method that reveals the genome-wide location of dna-bound proteins and used this method to monitor binding of gene-specific transcription activators in yeast. a combination of location and expression profiles was used to identify genes whose expression is directly controlled by gal4 and ste12 as cells respond to changes in carbon source and mating pheromone, respectively. the results identify pathways that are coordinately regulated by each of the two activators and reveal previously unknown functions for gal4 and ste12. genome-wide location analysis will facilitate investigation of gene regulatory networks, gene function, and genome maintenance.
280	128791	article	nucleic acids res	\N	\N	\N	-297	32 Database issue	\N	2004	jan	2005-03-15 21:41:33	\N	{regulondb} (version 4.0): transcriptional regulation, operon organization and growth conditions in escherichia coli k-12.	{regulondb} is the primary database of the major international maintained curation of original literature with experimental knowledge about the elements and interactions of the network of transcriptional regulation in escherichia coli k-12. this includes mechanistic information about operon organization and their decomposition into transcription units ({tus}), promoters and their sigma type, binding sites of specific transcriptional regulators ({trs}), their organization into 'regulatory phrases', active and inactive conformations of {trs}, as well as terminators and ribosome binding sites. the database is complemented with clearly marked computational predictions of {tus}, promoters and binding sites of {trs}. the current version has been expanded to include information beyond specific mechanisms aimed at gathering different growth conditions and the associated induced and/or repressed genes. {regulondb} is now linked with {swiss-prot}, with microarray databases, and with a suite of programs to analyze and visualize microarray experiments. we provide a summary of the biological knowledge contained in {regulondb} and describe the major changes in the design of the database. {regulondb} can be accessed on the web at the {url}: {http://www.cifn.unam.mx/computational\\_biology}/regulondb/.
281	128799	article	proc natl acad sci usa	\N	\N	\N	-10605	93	20	1996	oct	2005-03-15 21:41:33	\N	parallel human genome analysis: microarray-based expression monitoring of 1000 genes.	microarrays containing 1046 human {cdnas} of unknown sequence were printed on glass with high-speed robotics. these 1.0-cm2 {dna} \\&quot;chips\\&quot; were used to quantitatively monitor differential expression of the cognate human genes using a highly sensitive two-color hybridization assay. array elements that displayed differential expression patterns under given experimental conditions were characterized by sequencing. the identification of known and novel heat shock and phorbol ester-regulated genes in human t cells demonstrates the sensitivity of the assay. parallel gene analysis with microarrays provides a rapid and efficient method for large-scale human gene discovery.
282	128811	article	bioinformatics	\N	\N	\N	9	19 Suppl 1	\N	2003	jul	2005-03-15 21:41:33	\N	{genome-wide discovery of transcriptional modules from dna sequence and gene expression}	in this paper, we describe an approach for understanding transcriptional regulation from both gene expression and promoter sequence data. we aim to identify transcriptional modules--sets of genes that are co-regulated in a set of experiments, through a common motif profile. using the em algorithm, our approach refines both the module assignment and the motif profile so as to best explain the expression data as a function of transcriptional motifs. it also dynamically adds and deletes motifs, as required to provide a genome-wide explanation of the expression data. we evaluate the method on two saccharomyces cerevisiae gene expression data sets, showing that our approach is better than a standard one at recovering known motifs and at generating biologically coherent modules. we also combine our results with binding localization data to obtain regulatory relationships with known transcription factors, and show that many of the inferred relationships have support in the literature. contact: eran@cs.stanford.edu keywords: probabilistic models, gene expression, transcriptional regulation.
283	128818	article	proc natl acad sci usa	\N	\N	\N	5	100	13	2003	jun	2005-03-15 21:41:33	\N	{detailed map of a cis-regulatory input function}	most genes are regulated by multiple transcription factors that bind specific sites in {dna} regulatory regions. these cis-regulatory regions perform a computation: the rate of transcription is a function of the active concentrations of each of the input transcription factors. here, we used accurate gene expression measurements from living cell cultures, bearing {gfp} reporters, to map in detail the input function of the classic {laczya} operon of escherichia coli, as a function of about a hundred combinations of its two inducers, {camp} and isopropyl {{beta}-d-thiogalactoside} {(iptg).} we found an unexpectedly intricate function with four plateau levels and four thresholds. this result compares well with a mathematical model of the binding of the regulatory proteins {camp} receptor protein {(crp)} and {laci} to the lac regulatory region. the model is also used to demonstrate that with few mutations, the same region could encode much purer {and-like} or even {or-like} functions. this possibility means that the wild-type region is selected to perform an elaborate computation in setting the transcription rate. the present approach can be generally used to map the input functions of other genes.
284	128819	article	genome res	\N	\N	\N	-1994	13	11	2003	nov	2005-03-15 21:41:33	\N	cytoscape: a software environment for integrated models of biomolecular interaction networks.	cytoscape is an open source software project for integrating biomolecular interaction networks with high-throughput expression data and other molecular states into a unified conceptual framework. although applicable to any system of molecular components and interactions, cytoscape is most powerful when used in conjunction with large databases of protein-protein, {protein-dna}, and genetic interactions that are increasingly available for humans and model organisms. cytoscape's software core provides basic functionality to layout and query the network; to visually integrate the network with expression profiles, phenotypes, and other molecular states; and to link the network to databases of functional annotations. the core is extensible through a straightforward plug-in architecture, allowing rapid development of additional computational analyses and features. several case studies of cytoscape plug-ins are surveyed, including a search for interaction pathways correlating with changes in gene expression, a study of protein complexes involved in cellular recovery to {dna} damage, inference of a combined physical/functional interaction network for halobacterium, and an interface to detailed stochastic/kinetic gene regulatory models.
285	128876	article	proc natl acad sci usa	\N	\N	\N	-5935	100	10	2003	may	2005-03-15 21:41:35	\N	reverse engineering gene networks: integrating genetic perturbations with dynamical modeling.	while the fundamental building blocks of biology are being tabulated by the various genome projects, microarray technology is setting the stage for the task of deducing the connectivity of large-scale gene networks. we show how the perturbation of carefully chosen genes in a microarray experiment can be used in conjunction with a reverse engineering algorithm to reveal the architecture of an underlying gene regulatory network. our iterative scheme identifies the network topology by analyzing the steady-state changes in gene expression resulting from the systematic perturbation of a particular node in the network. we highlight the validity of our reverse engineering approach through the successful deduction of the topology of a linear in numero gene network and a recently reported model for the segmentation polarity network in drosophila melanogaster. our method may prove useful in identifying and validating specific drug targets and in deconvolving the effects of chemical compounds.
286	128893	article	j mol biol	\N	\N	\N	13	290	1	1999	jul	2005-03-15 21:41:35	\N	{the packing density in proteins: standard radii and volumes}	the sizes of atomic groups are a fundamental aspect of protein structure. they are usually expressed in terms of standard sets of radii for atomic groups and of volumes for both these groups and whole residues. atomic groups, which subsume a heavy-atom and its covalently attached hydrogen atoms into one moiety, are used because the positions of hydrogen atoms in protein structures are generally not known. we have calculated new values for the radii of atomic groups and for the volumes of atomic groups. these values should prove useful in the analysis of protein packing, protein recognition and ligand design. our radii for atomic groups were derived from intermolecular distance calculations on a large number (approximately 30,000) of crystal structures of small organic compounds that contain the same atomic groups to those found in proteins. our radii show significant differences to previously reported values. we also use this new radii set to determine the packing efficiency in different regions of the protein interior. this analysis shows that, if the surface water molecules are included in the calculations, the overall packing efficiency throughout the protein interior is high and fairly uniform. however, if the water structure is removed, the packing efficiency in peripheral regions of the protein interior is underestimated, by approximately 3.5 \\%.
287	128931	article	bioinformatics	\N	\N	\N	1	19	13	2003	sep	2005-03-15 21:41:36	\N	{detecting putative orthologs}	summary: we developed an algorithm that improves upon the common procedure of taking reciprocal best blast hits (rbh) in the identification of orthologs. the method--reciprocal smallest distance algorithm (rsd)--relies on global sequence alignment and maximum likelihood estimation of evolutionary distances to detect orthologs between two genomes. rsd finds many putative orthologs missed by rbh because it is less likely than rbh to be misled by the presence of a close paralog.  availability: a python program and readme file are freely available from: http://charles.stanford.edu/~dennis/research.html 10.1093/bioinformatics/btg213
288	128968	article	science	\N	\N	\N	4	293	5537	2001	sep	2005-03-15 21:41:36	\N	{global analysis of protein activities using proteome chips}	to facilitate studies of the yeast proteome, we cloned 5800 open reading frames and overexpressed and purified their corresponding proteins. the proteins were printed onto slides at high spatial density to form a yeast proteome microarray and screened for their ability to interact with proteins and phospholipids. we identified many new calmodulin- and phospholipid-interacting proteins; a common potential binding motif was identified for many of the calmodulin-binding proteins. thus, microarrays of an entire eukaryotic proteome can be prepared and screened for diverse biochemical activities. the microarrays can also be used to screen protein-drug interactions and to detect posttranslational modifications.
289	129007	article	neural computation	\N	\N	\N	25	10	2	1998	\N	2005-03-15 21:52:32	\N	natural gradient works efficiently in learning	when a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction, but the natural gradient does. information geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation), and the space of linear dynamical systems (for blind source deconvolution). the dynamical behavior of natural gradient online learning is analyzed and is proved to be fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters. this suggests that the plateau phenomenon, which appears in the backpropagation learning algorithm of multilayer perceptrons, might disappear or might not be so serious when the natural gradient is used. an adaptive method of updating the learning rate is proposed and analyzed.
290	129008	article	neural networks	\N	\N	\N	6	12	6	1999	\N	2005-03-15 21:52:32	\N	improving support vector machine classifiers by modifying kernel functions	we propose a method of modifying a kernel function to improve the performance of a support  vector machine classifier. this is based on the riemannian geometrical structure induced by the  kernel function. the idea is to enlarge the spatial resolution around the separating boundary  surface by a conformal mapping such that the separability between classes is increased. examples  are given specifically for modifying gaussian radial basis function kernels. simulation results for  both artificial and real data show remarkable improvement of generalization errors, supporting our  idea.  1 introduction  support vector machine (svm) is a new promising pattern classification technique proposed recently by vapnik and co-workers (boser et al., 1992, cortes and vapnik, 1995, and vapnik, 1995). unlike traditional methods which minimize the empirical training error, svm aims at minimizing an upper bound of the generalization error through maximizing the margin between the separating hyperplane and...
291	129019	article	science	\N	\N	\N	2	285	\N	1999	\N	2005-03-15 21:52:32	\N	detecting protein function and {protein-protein} interactions from genome sequences	a computational method is proposed for inferring protein interactions from genome sequences on the basis of the observation that some pairs of interacting proteins have homologs in another organism fused into a single protein chain. {s}earching sequences from many genomes revealed 6809 such putative protein-protein interactions in {e}scherichia coli and 45,502 in yeast. {m}any members of these pairs were confirmed as functionally related; computational filtering further enriches for interactions. {s}ome proteins have links to several other proteins; these coupled links appear to represent functional interactions such as complexes or pathways. {e}xperimentally confirmed interacting pairs are documented in a {d}atabase of {i}nteracting {p}roteins.
292	129026	inproceedings	\N	proceedings of the pacific symposium on biocomputing 2002	\N	world scientific	11	\N	\N	2002	\N	2005-03-15 21:52:32	\N	the spectrum kernel: a string kernel for {svm} protein classification	we introduce a new sequence-similarity kernel, the spectrum kernel, for use with support vector machines (svms) in a discriminative approach to the protein classification problem. our kernel is conceptually simple and efficient to compute and, in experiments on the scop database, performs well in comparison with state-of-the-art methods for homology detection. moreover, our method produces an svm classifier that allows linear time classification of test sequences. our experiments provide evidence that string-based kernels, in conjunction with svms, could offer a viable and computationally efficient alternative to other methods of protein classification and homology detection.
293	129029	article	bioinformatics	\N	\N	\N	8	16	10	2000	\N	2005-03-15 21:52:32	\N	support vector machine classification and validation of cancer tissue samples using microarray expression data	motivation: dna microarray experiments generating thousands of gene expression measurements, are being used to gather information from tissue and cell samples regarding gene expression differences that will be useful in diagnosing disease. we have developed a new method to analyse this kind of data using support vector machines (svms). this analysis consists of both classification of the tissue samples, and an exploration of the data for mis-labeled or questionable tissue results.  results: we demonstrate the method in detail on samples consisting of ovarian cancer tissues, normal ovarian tissues, and other normal tissues. the dataset consists of expression experiment   results for 97802 cdnas for each tissue. as a result of computational analysis, a tissue sample is discovered and confirmed to be wrongly labeled. upon correction of this mistake and the removal of an outlier, perfect classification of tissues is achieved, but not with high confidence. we identify and analyse a subset of genes from the ovarian dataset whose expression is highly   differentiated between the types of tissues. to show robustness of the svm method, two previously published datasets from other types of tissues or cells are analysed. the results are comparable to those previously obtained. we show that other machine learning methods also perform comparably to the svm on many of those datasets.  availability: the svm software is available at http://www.cs.columbia.edu/~bgrundy/svm.  contact: booch@cse.ucsc.edu 10.1093/bioinformatics/16.10.906
294	129038	article	journal of machine learning research	\N	\N	\N	12	2	\N	2001	\N	2005-03-15 21:52:33	\N	support vector clustering	we present a novel clustering method using the approach of support vector machines. data points are mapped by means of a gaussian kernel to a high dimensional feature space, where we search for the minimal enclosing sphere. this sphere, when mapped back to data space, can separate into several components, each enclosing a separate cluster of points. we present a simple algorithm for identifying these clusters. the width of the gaussian kernel controls the scale at which the data is probed while the soft margin constant helps coping with outliers and overlapping clusters. the structure of a dataset is explored by varying the two parameters, maintaining a minimal number of support vectors to assure smooth cluster boundaries. we demonstrate the performance of our algorithm on several datasets.
295	129039	inproceedings	\N	{nips}	\N	\N	6	\N	\N	2000	\N	2005-03-15 21:52:33	\N	text classification using string kernels	we propose a novel approach for categorizing text documents based on the use of a special kernel. the kernel is an inner product in the feature space generated by all subsequences of length &lt;em&gt;k&lt;/em&gt;. a subsequence is any ordered sequence of &lt;em&gt;k&lt;/em&gt; characters occurring in the text though not necessarily contiguously. the subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences that are close to contiguous. a direct computation of this feature vector would involve a prohibitive amount of computation even for modest values of &lt;em&gt;k&lt;/em&gt;, since the dimension of the feature space grows exponentially with &lt;em&gt;k&lt;/em&gt;. the paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique. experimental comparisons of the performance of the kernel compared with a standard word feature space kernel (joachims, 1998) show positive results on modestly sized datasets. the case of contiguous subsequences is also considered for comparison with the subsequences kernel with different decay factors. for larger documents and datasets the paper introduces an approximation technique that is shown to deliver good approximations efficiently for large datasets.
296	129085	unpublished	\N	\N	\N	\N	\N	\N	\N	2001	\N	2005-03-15 21:52:33	\N	deterministic scale-free networks	scale-free networks are abundant in nature and society, describing such diverse systems as the world wide web, the web of human sexual contacts, or the chemical network of a cell. all models used to generate a scale-free topology are stochastic, that is they create networks in which the nodes appear to be randomly connected to each other. here we propose a simple model that generates scale-free networks in a deterministic fashion. we solve exactly the model, showing that the tail of the degree distribution follows a power law.
297	129100	book	\N	\N	cbms-nsf regional conference series in applied mathematics	{siam}	\N	59	\N	1990	\N	2005-03-15 21:52:33	philadelphia	spline models for observational data	for the last two decades the author has been associated with pioneer work in the area of nonparametric regression and spline smoothing. in the monograph under review this vast amount of information is made available in a consolidated form along with a brief survey of the work of other researchers. the general smoothing problem is presented using reproducing kernel (r.k.) hilbert spaces. the solution of variational problems arising in univariate and multivariate smoothing, partial spline models, additive and interaction spline models is characterized in terms of a relevant r.k. the r.k. as a covariance kernel provides the link between spline estimates and bayes estimates. the convergence rates of various estimates are obtained in terms of the rate of decay of the eigenvalues of the r.k. and the fourier-bessel coefficients of the function being estimated with respect to the eigenfunction. from a practical point of view the choice of a good basis and smoothing parameter is crucial. for the latter problem data-dependent techniques such as cross-validation and general cross-validation are discussed in detail through illustrative examples. numerical methods for computing the estimates and the available packages are also indicated. the solution of integral equations of the first kind and partial spline models in nonlinear regression are briefly considered. <p> the reviewer considers the monograph a valuable contribution and recommends it strongly to everyone with some interest in this important area of statistics.
298	129118	book	\N	\N	\N	springer-verlag	\N	\N	\N	1996	\N	2005-03-15 21:52:33	new-york	principal component analysis	principal component analysis is central to the study of multivariate data. although one of the earliest multivariate techniques it continues to be the subject of much research, ranging from new model- based approaches to algorithmic ideas from neural networks. it is extremely versatile with applications in many disciplines. the first edition of this book was the first comprehensive text written solely on principal component analysis. the second edition updates and substantially expands the original version, and is once again the definitive text on the subject. it includes core material, current research and a wide range of applications. its length is nearly double that of the first edition. researchers in statistics, or in other fields that use principal component analysis, will find that the book gives an authoritative yet accessible account of the subject. it is also a valuable resource for graduate courses in multivariate analysis. the book requires some knowledge of matrix algebra. ian jolliffe is professor of statistics at the university of aberdeen. he is author or co-author of over 60 research papers and three other books. his research interests are broad, but aspects of principal component analysis have fascinated him and kept him busy for over 30 years.
299	129240	book	\N	\N	\N	cold spring harbor laboratory press	\N	\N	\N	1989	\N	2005-03-15 21:56:05	plainview, ny	molecular cloning: a laboratory manual	the first two editions of this manual have been mainstays of molecular biology for nearly twenty years, with an unrivalled reputation for reliability, accuracy, and clarity. <p>in this new edition, authors joe sambrook and david russell have completely updated the book, revising every protocol and adding a mass of new material, to broaden its scope and maintain its unbeatable value for studies in genetics, molecular cell biology, developmental biology, microbiology, neuroscience, and immunology. <p>handsomely redesigned and presented in new bindings of proven durability, this three-volume work is essential for everyone using today&#146;s biomolecular techniques. <p>the opening chapters describe essential techniques, some well-established, some new, that are used every day in the best laboratories for isolating, analyzing and cloning dna molecules, both large and small. <p>these are followed by chapters on cdna cloning and exon trapping, amplification of dna, generation and use of nucleic acid probes, mutagenesis, and dna sequencing. <p>the concluding chapters deal with methods to screen expression libraries, express cloned genes in both prokaryotes and eukaryotic cells, analyze transcripts and proteins, and detect protein-protein interactions. <p>the appendix is a compendium of reagents, vectors, media, technical suppliers, kits, electronic resources and other essential information. <p>as in earlier editions, this is the only manual that explains how to achieve success in cloning and provides a wealth of information about why techniques work, how they were first developed, and how they have evolved.
300	129291	article	cell	\N	\N	\N	13	118	\N	2004	\N	2005-03-15 21:56:06	\N	exploration of essential gene functions via titratable promoter alleles	nearly 20% of yeast genes are required for viability, hindering genetic analysis with knockouts. we created promoter-shutoff strains for over two-thirds of all essential yeast genes and subjected them to morphological analysis, size profiling, drug sensitivity screening, and microarray expression profiling. we then used this compendium of data to ask which phenotypic features characterized different functional classes and used these to infer potential functions for uncharacterized genes. we identified genes involved in ribosome biogenesis (has1, urb1, and urb2), protein secretion (sec39), mitochondrial import (mim1), and trna charging (gsn1). in addition, apparent negative feedback transcriptional regulation of both ribosome biogenesis and the proteasome was observed. we furthermore show that these strains are compatible with automated genetic analysis. this study underscores the importance of analyzing mutant phenotypes and provides a resource to complement the yeast knockout collection.
301	129335	article	journal of knowledge management	\N	\N	\N	\N	6	2	2002	may	2005-03-15 22:54:51	\N	complex acts of knowing: paradox and descriptive self-awareness	we are reaching the end of the second generation of knowledge management, with its focus on tacit-explicit knowledge conversion. triggered by the seci model of nonaka, it replaced a first generation focus on timely information provision for decision support and in support of bpr initiatives. like bpr it has substantially failed to deliver on its promised benefits. the third generation requires the clear separation of context, narrative and content management and challenges the orthodoxy of scientific management. complex adaptive systems theory is used to create a sense-making model that utilises self-organising capabilities of the informal communities and identifies a natural flow model of knowledge creation, disruption and utilisation. however, the argument from nature of many complexity thinkers is rejected given the human capability to create order and predictability through collective and individual acts of freewill. disillusionment was creeping in, organisations knowledge is seen paradoxically, as both a thing and a flow requiring diverse management approaches.
302	129356	book	\N	\N	\N	{morgan kaufmann}	\N	\N	\N	1999	may	2005-03-15 23:20:39	\N	managing gigabytes: compressing and indexing documents and images	{of all the tasks programmers are asked to perform, storing, compressing, and retrieving information are some of the most challenging--and critical to many applications. <i>managing gigabytes: compressing and indexing documents and images</i> is a treasure trove of theory, practical illustration, and general discussion in this fascinating technical subject.<p> ian witten, alistair moffat, and timothy bell have updated their original work with this even more impressive second edition. this version adds recent techniques such as block-sorting, new indexing techniques, new lossless compression strategies, and many other elements to the mix. in short, this work is a comprehensive summary of text and image compression, indexing, and querying techniques. the history of relevant algorithm development is woven well with a practical discussion of challenges, pitfalls, and specific solutions.<p> this title is a textbook-style exposition on the topic, with its information organized very clearly into topics such as compression, indexing, and so forth. in addition to diagrams and example text transformations, the authors use "pseudo-code" to present algorithms in a language-independent manner wherever possible. they also supplement the reading with <i>mg</i>--their own implementation of the techniques. the mg c language source code is freely available on the web. <p> alone, this book is an impressive collection of information. nevertheless, the authors list numerous titles for further reading in selected topics. whether you're in the midst of application development and need solutions fast or are merely curious about how top-notch information management is done, this hardcover is an excellent investment. <i>--stephen w. plain</i><p> <b>topics covered</b>: text compression models, including huffman, lzw, and their variants; trends in information management; index creation and compression; image compression; performance issues; and overall system implementation.} {<p>in this fully updated second edition of the highly acclaimed <b>managing gigabytes</b>, authors witten, moffat, and bell continue to provide unparalleled coverage of state-of-the-art techniques for compressing and indexing data. whatever your field, if you work with large quantities of information, this book is essential reading--an authoritative theoretical resource and a practical guide to meeting the toughest storage and access challenges. it covers the latest developments in compression and indexing and their application on the web and in digital libraries. it also details dozens of powerful techniques supported by mg, the authors' own system for compressing, storing, and retrieving text, images, and textual images. mg's source code is freely available on the web.<br><br>* up-to-date coverage of new text compression algorithms such as block sorting, approximate arithmetic coding, and fat huffman coding<br>* new sections on content-based index compression and distributed querying, with 2 new data structures for fast indexing<br>* new coverage of image coding, including descriptions of de facto standards in use on the web (gif and png), information on calic, the new proposed jpeg lossless standard, and jbig2<br>* new information on the internet and www, digital libraries, web search engines, and agent-based retrieval<br>* accompanied by a public domain system called mg which is a fully worked-out operational example of the advanced techniques developed and explained in the book<br>* new appendix on an existing digital library system that uses the mg software}
303	129936	book	\N	\N	\N	oxford university press	\N	\N	\N	1988	\N	2005-03-16 14:49:19	new york	conduction of heat in solids	based on earlier " introduction to mathematical theory of conduction of heat in solids," volume gives general theory together with its application to linear flow of heat and to flow of heat in rod, rectangular parallelepiped, circular cylinder, sphere and cone; laplace transformation method of dealing with problems in conduction of heat is used instead of older method of contour integrals. eng soc lib, ny. ($8.00)
304	130030	book	\N	\N	\N	clarendon press, oxford	\N	\N	\N	1987	\N	2005-03-16 14:49:20	\N	computer simulation of liquids	{computer simulation is an essential tool in studying the chemistry and physics of liquids.  simulations allow us to develop models and to test them against experimental data. they can be used to evaluate approximate theories of liquids, and to provide detailed information on the structure and dynamics of model liquids at the molecular level. this book is an introduction and practical guide to the molecular dynamics and monte carlo methods.    the first four chapters describe these methods in detail, and provide the essential background in intermolecular forces and statistical mechanics.  chapters 5 and 6 emphasize the practical aspects of writing efficient programs and analysing the simulation results.  the remaining chapters cover advanced techniques, non-equilibrium methods, brownian dynamics, quantum simulations, and some important applications.  fortran code is presented in the text.}
305	130273	inproceedings	\N	proceedings of the 12th annual acm international conference on multimedia	multimedia	acm	7	\N	\N	2004	\N	2005-03-16 22:28:24	new york, ny, usa	on the detection of semantic concepts at {trecvid}	semantic multimedia management is necessary for the effective and widespread utilization of multimedia repositories and realizing the potential that lies untapped in the rich multimodal information content. this challenge has driven researchers to devise new algorithms and systems that enable automatic or semi-automatic tagging of large scale multimedia content with rich semantics. an emerging research area is the detection of a predetermined set of semantic concepts that can act as semantic filters and aid in search, and manipulation. the {nist} {trecvid} benchmark has responded by creating a task that has evaluated the performance of concept detection. within the scope of this benchmark task, this paper studies trends in the emerging concept detection systems, architectures and algorithms. it also analyzes strategies that have yielded reasonable success, and challenges and gaps that lie ahead.
306	130315	article	\\# ieeetc	\N	\N	\N	14	C-35	\\# 8	-1	aug	2005-03-16 23:04:53	\N	{graph-based} algorithms for boolean function manipulation	in this paper we present a new data structure for representing boolean functions and an associated set of manipulation algorithms. functions are represented by directed, acyclic graphs in a manner similar to the representations introduced by lee [1] and akers [2], but with further restrictions on the ordering of decision variables in the graph. although a function requires, in the worst case, a graph of size exponential in the number of arguments, many of the functions encountered in typical...
307	130783	article	nature biotechnology	\N	\N	\N	6	22	7	2004	jul	2005-03-17 05:41:11	center for biodynamics and department of biomedical engineering, boston university, 44 cummington street, boston, massachusetts 02215, usa.	engineered riboregulators enable post-transcriptional control of gene expression.	recent studies have demonstrated the important enzymatic, structural and regulatory roles of {rna} in the cell. here we present a post-transcriptional regulation system in escherichia coli that uses {rna} to both silence and activate gene expression. we inserted a complementary cis sequence directly upstream of the ribosome binding site in a target gene. upon transcription, this cis-repressive sequence causes a stem-loop structure to form at the 5'-untranslated region of the {mrna}. the stem-loop structure interferes with ribosome binding, silencing gene expression. a small noncoding {rna} that is expressed in trans targets the cis-repressed {rna} with high specificity, causing an alteration in the stem-loop structure that activates expression. such engineered riboregulators may lend insight into mechanistic actions of endogenous {rna}-based processes and could serve as scalable components of biological networks, able to function with any promoter or gene to directly control gene expression.
308	131324	article	commun. acm	\N	\N	acm	22	34	2	1991	feb	2005-03-17 15:06:20	new york, ny, usa	understanding fault-tolerant distributed systems	note: {ocr} errors may be found in this reference list extracted from the full text article.  {acm} has opted to expose the complete list rather than only correct and linked references.
309	131662	book	\N	\N	\N	siam: society for industrial and applied mathematics	\N	\N	\N	2005	feb	2005-03-18 03:42:03	\N	mathematical models in biology (classics in applied mathematics)	{mathematical models in biology is an introductory book for readers interested in biological applications of mathematics and modeling in biology. a favorite in the mathematical biology community since its first publication in 1988, the book shows how relatively simple mathematics can be applied to a variety of models to draw interesting conclusions. connections are made between diverse biological examples linked by common mathematical themes. a variety of discrete and continuous ordinary and partial differential equation models are explored. although great advances have taken place in many of the topics covered, the simple lessons contained in mathematical models in biology are still important and informative.   <p>shortly after the first publication of mathematical models in biology, the genomics revolution turned mathematical biology into a prominent area of interdisciplinary research. in this new millennium, biologists have discovered that mathematics is not only useful, but indispensable! as a result, there has been much resurgent interest in, and a huge expansion of, the fields collectively called mathematical biology. this book serves as a basic introduction to concepts in deterministic biological modeling.   <p>mathematical models in biology does not assume too much background knowledgeessentially some calculus and high-school algebra. it was originally written with third- and fourth-year undergraduate mathematical-biology majors in mind; however, it was picked up by beginning graduate students as well as a number of researchers in math (and some in biology) who wanted to learn about this field.}
310	132329	article	communications of the acm	\N	\N	\N	28	21	8	1978	\N	2005-03-18 14:23:36	\N	can programming be liberated from the von {n}eumann style? {a} functional style and its algebra of programs	conventional programming languages are growing ever more enormous, but not stronger. inherent defects at the most basic level cause them to be both fat and weak: their primitive word-at-a-time style of programming inherited from their common ancestor&mdash;the von neumann computer, their close coupling of semantics to state transitions, their division of programming into a world of expressions and a world of statements, their inability to effectively use powerful combining forms for building new programs from existing ones, and their lack of useful mathematical properties for reasoning about programs.   an alternative functional style of programming is founded on the use of combining forms for creating programs. functional programs deal with structured data, are often nonrepetitive and nonrecursive, are hierarchically constructed, do not name their arguments, and do not require the complex machinery of procedure declarations to become generally applicable. combining forms can use high level programs to build still higher level ones in a style not possible in conventional languages.   associated with the functional style of programming is an algebra of programs whose variables range over programs and whose operations are combining forms. this algebra can be used to transform programs and to solve equations whose &ldquo;unknowns&rdquo; are programs in much the same way one transforms equations in high school algebra. these transformations are given by algebraic laws and are carried out in the same language in which programs are written. combining forms are chosen not only for their programming power but also for the power of their associated algebraic laws. general theorems of the algebra give the detailed behavior and termination conditions for large classes of programs.   a new class of computing systems uses the functional programming style both in its programming language and in its state transition rules. unlike von neumann languages, these systems have semantics loosely coupled to states&mdash;only one state transition occurs per major computation.
311	132420	article	journal of philosophy	\N	\N	\N	24	72	\N	1975	\N	2005-03-18 14:23:39	\N	functional analysis	{this classic text is written for graduate courses in functional analysis. this text is used in modern investigations in analysis and applied mathematics. this new edition includes up-to-date presentations of topics as well as more examples and exercises. new topics include kakutani's fixed point theorem, lamonosov's invariant subspace theorem, and an ergodic theorem. <p> this text is part of the walter rudin student series in advanced mathematics.}
312	132451	article	cognitive science	\N	\N	\N	49	6	\N	1982	\N	2005-03-18 14:23:39	\N	connectionist models and their properties	much of the progress in the fields constituting cognitive science has been based upon the use of explicit information processing models, almost exclusively patterned after conventional serial computers. an extension of these ideas to massively parallel, connectionist models appears to offer a number of advantages. after a preliminary discussion, this paper introduces a general connectionist model and considers how it might be used in cognitive science. among the issues addressed are: stability and noise-sensitivity, distributed decision-making, time and sequence problems, and the representation of complex concepts.
313	132637	incollection	\N	the philosophy of artificial intelligence	\N	oxford university press	17	\N	\N	1990	\N	2005-03-18 14:23:41	oxford	a logical calculus of the ideas immanent in nervous activity	abstract&nbsp;&nbsp;because of the ï¿½? all-or-noneÃ¢Â€? character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. it is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. it is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. various applications of the calculus are discussed.
314	133546	inproceedings	\N	conference record of the seventeenth annual acm symposium on principles of programming languages, san francisco, california	\N	acm	15	\N	\N	1990	\N	2005-03-20 00:52:25	\N	explicit substitutions	the oe-calculus is a refinement of the -calculus where substitutions are manipulated explicitly. the oe-calculus provides a setting for studying the theory of substitutions, with pleasant mathematical properties. it is also a useful bridge between the classical -calculus  and concrete implementations.   digital equipment corporation, systems research center.  y  ecole normale sup'erieure; part of this work was completed while at digital equipment corporation, systems research center.  z  inria...
315	134227	article	nips 2003	\N	\N	\N	\N	\N	\N	-1	\N	2005-03-21 09:04:34	\N	link prediction in relational data	many real-world domains are relational in nature, consisting of a set of objects  related to each other in complex ways. this paper focuses on predicting the  existence and the type of links between entities in such domains. we apply the  relational markov network framework of taskar et al. to define a joint probabilistic  model over the entire link graph --- entity attributes and links. the application  of the rmn algorithm to this task requires the definition of probabilistic patterns  over subgraph structures. we apply this method to two new relational datasets,  one involving university webpages, and the other a social network. we show that  the collective classification approach of rmns, and the introduction of subgraph  patterns over link labels, provide significant improvements in accuracy over flat  classification, which attempts to predict each link in isolation.
316	134378	book	\N	\N	\N	{mit} press	\N	\N	\N	1999	\N	2005-03-21 21:43:49	\N	learning in graphical models	statistical applications in fields such as bioinformatics, information retrieval, speech processing, image processing and communications often involve large-scale models in which thousands or millions of random variables are linked in complex ways. graphical models provide a general methodology for approaching these problems, and indeed many of the models developed by researchers in these applied fields are instances of the general graphical model formalism. we review some of the basic ideas underlying graphical models, including the algorithmic ideas that allow graphical models to be deployed in large-scale data analysis problems. we also present examples of graphical models in bioinformatics, error-control coding and language processing. key words and phrases: probabilistic graphical models, junction tree algorithm, sum-product algorithm, markov chain monte carlo, variational inference, bioinformatics, error-control coding.
317	134382	phdthesis	\N	\N	\N	\N	\N	\N	\N	2001	\N	2005-03-21 21:43:49	\N	expectation propagation for approximate {b}ayesian inference	one of the major obstacles to using bayesian methods for pattern recognition has been its computational expense. this thesis presents an approximation technique that can perform bayesian inference faster and more accurately than previously possible. this method and ``expectation propagation and '' unifies and generalizes two previous techniques: assumed-density filtering and an extension of the kalman filter and and loopy belief propagation and an extension of belief propagation in bayesian networks. the unification shows how both of these algorithms can be viewed as approximating the true posterior distribution with a simpler distribution and which is close in the sense of {kl}-divergence. expectation propagation exploits the best of both algorithms: the generality of assumed-density filtering and the accuracy of loopy belief propagation. loopy belief propagation and because it propagates exact belief states and is useful for limited types of belief networks and such as purely discrete networks. expectation propagation approximates the belief states with expectations and such as means and variances and giving it much wider scope. expectation propagation also extends belief propagation in the opposite direction---propagating richer belief states which incorporate correlations between variables. this framework is demonstrated in a variety of statistical models using synthetic and real-world data. on gaussian mixture problems and expectation propagation is found and for the same amount of computation and to be convincingly better than rival approximation techniques: monte carlo and laplace's method and and variational bayes. for pattern recognition and expectation propagation provides an algorithm for training bayes point machine classifiers that is faster and more accurate than any previously known. the resulting classifiers outperform support vector machines on several standard datasets and in addition to having a comparable training time. expectation propagation can also be used to choose an appropriate feature set for classification and via bayesian model selection.
318	134409	article	genome res	\N	\N	\N	10	12	7	2002	jul	2005-03-21 23:12:34	laboratoire de biom\\'{e}trie et biologie evolutive, unit\\'{e} mixte de recherche centre national de la recherche scientifique, universit\\'{e} claude bernard - lyon 1, 69622 villeurbanne cedex, france.	a phylogenomic approach to bacterial phylogeny: evidence of a core of genes sharing a common history.	it has been claimed that complete genome sequences would clarify phylogenetic relationships between organisms, but up to now, no satisfying approach has been proposed to use efficiently these data. for instance, if the coding of presence or absence of genes in complete genomes gives interesting results, it does not take into account the phylogenetic information contained in sequences and ignores hidden paralogies by using a {blast} reciprocal best hit definition of orthology. in addition, concatenation of sequences of different genes as well as building of consensus trees only consider the few genes that are shared among all organisms. here we present an attempt to use a supertree method to build the phylogenetic tree of 45 organisms, with special focus on bacterial phylogeny. this led us to perform a phylogenetic study of congruence of tree topologies, which allows the identification of a core of genes supporting similar species phylogeny. we then used this core of genes to infer a tree. this phylogeny presents several differences with the {rrna} phylogeny, notably for the position of hyperthermophilic bacteria.
319	134506	proceedings	enabling technologies: infrastructure for collaborative enterprises, 2003. wet ice 2003. proceedings. twelfth ieee international workshops on	\N	\N	\N	5	\N	\N	2003	\N	2005-03-22 06:28:57	\N	{webdava}: an administrator-free approach to web file-sharing	collaboration over the internet depends on the ability of the members of a group to exchange data in a secure yet unobtrusive manner. {webdava} is a system that allows the users to define their own access-control policies to network resources that they control, enabling secure data sharing within the enterprise. our design allows users to selectively give fine-grain access to their resources without involving their system administrators. we accomplish this by using authorization credentials that define the users' privileges. our prototype implements a file-sharing service, where users maintain sensitive-information folders and can allow others to access parts of these. clients interact with the server over {http} via a java applet that transparently handles credential management. this mechanism allows users to share information with users not a priori known to the system, enabling administrator-free management.
320	135035	article	harvard business review	\N	\N	\N	\N	\N	September-October 1997	1997	\N	2005-03-22 16:13:00	\N	managing in an age of modularity	modularity is a familiar principle in the computer industry. different companies can independently design and produce components, suck as disk drives or operating software, and those modules will fit together into a complex and smoothly functioning product because the module makers obey a given set of design rules. modularity in manufacturing is already common in many companies. but now a number of them are beginning to extend the approach into the design of their products and services. modularity in design should tremendously boost the rate of innovation in many industries as it did in the computer industry. as businesses as diverse as auto manufacturing and financial services move toward modular designs, the authors say, competitive dynamics will change enormously. no longer will assemblers control the final product: suppliers of key modules will gain leverage and even take on responsibility for design rules. companies will compete either by specifying the dominant design rules (as microsoft does) or by producing excellent modules (as disk drive maker quantum does). leaders in a modular industry will control less, so they will have to watch the competitive environment closely for opportunities to link up with other module makers. they will also need to know more: engineering details that seemed trivial at the corporate level may now play a large part in strategic decisions. leaders will also become knowledge managers internally because they will need to coordinate the efforts of development groups in order to keep them focused on the modular strategies the company is pursuing.
321	135987	book	\N	\N	\N	springer-verlag	\N	\N	\N	1996	\N	2005-03-22 16:13:08	london	engineering design : a systematic approach.	{<p>effective engineering design must be carefully planned and systematically executed. in particular, engineering design methods must integrate the many different aspects of designing and the priorities of the end-user.</p> <p>this proven and internationally recognized text teaches the methods and ideas of engineering design as a condition of successful product development. it breaks down the design process into phases and then into distinct steps, each with its own working methods. having established itself, in earlier editions, as a key text, <em>engineering design</em> (3<sup>rd</sup> edition) is enhanced with more input from practising engineers, providing more examples of product development; it also tightens the scientific bases of its design ideas with new solution fields in composite components, building methods, mechatronics and adaptronics and pays attention to the economic aspects of design and development including quality assurance. the third edition also integrates electronic design process technology into its methods.</p> <p><em>engineering design</em> (3<sup>rd</sup> edition) is translated and edited by ken wallace, chairman of the engineering design centre at the university of cambridge and luciÃ«nne blessing, professor of engineering design at the technical university of berlin. it is translated from the sixth german edition.</p> <p>topics covered include:</p> <ul> <li>psychology of design; </li> <li>product planning and development; </li> <li>the design process including design for recycling; </li> <li>conceptual design; </li> <li>embodiment design; </li> <li>size ranges and modular products; </li> <li>design for quality and minimum cost.<u> </u></li></ul> <p>written to provide students and tutors of engineering design with all the fundamental information they require in a crucial subject, <em>engineering design</em> (3<sup>rd</sup> edition) will also be of immense value as a reference to anyone working in the area.</p>}
322	136219	article	artificial life journal	\N	\N	\N	\N	1	1	1993	\N	2005-03-22 16:13:11	\N	the artificial life roots of artificial intelligence	behavior-oriented ai is a scientific discipline that studies how behavior  of agents emerges and becomes intelligent and adaptive. success of the field  is defined in terms of success in building physical agents that are capable  of maximising their own self-preservation in interaction with a dynamically  changing environment. the paper addresses this artificial life route towards  artificial intelligence and reviews some of the results obtained so far.  1  official reference: steels, l. (1994) ...
323	136259	book	\N	\N	\N	prentice hall	\N	\N	\N	1994	\N	2005-03-22 16:13:12	\N	distributed operating systems	distributed operating systems have many aspects in common with centralized ones, but they also differ in certain ways. this paper is intended as an introduction to distributed operating systems, and especially to current university research about them. after a discussion of what constitutes a distributed operating system and how it is distinguished from a computer network, various key design issues are discussed. then several examples of current research projects are examined in some detail, namely, the cambridge distributed computing system, amoeba, v, and eden.
324	136459	book	\N	\N	\N	prentice hall	\N	\N	\N	1996	\N	2005-03-22 16:13:14	new jersey	robust and optimal control	{this book provides a comprehensive, step-by-step treatment of the state-space <i>h</i></b></u>&#133;Ã  control theory, reflecting recent theoretical developments this area, in particular, and in the area of robust and <i>h</i></b></u>&#133;  Ã  control theory in general. it offers as self-contained a  presentation as possible and, for reference sake, includes many background  results on linear systems, the theory and application of riccati equations and model reduction.}
325	136480	article	bmc bioinformatics	\N	\N	\N	\N	5	\N	2004	\N	2005-03-22 16:28:59	\N	tools for loading {medline} into a local relational database	background: researchers who use medline for text mining, information extraction, or natural language processing may benefit from having a copy of medline that they can manage locally. the national library of medicine (nlm) distributes medline in extensible markup language (xml)-formatted text files, but it is difficult to query medline in that format. we have developed software tools to parse the medline data files and load their contents into a relational database. although the task is conceptually straightforward, the size and scope of medline make the task nontrivial. given the increasing importance of text analysis in biology and medicine, we believe a local installation of medline will provide helpful computing infrastructure for researchers. results: we developed three software packages that parse and load medline, and ran each package to install separate instances of the medline database. for each installation, we collected data on loading time and disk-space utilization to provide examples of the process in different settings. settings differed in terms of commercial database-management system (ibm db2 or oracle 9i), processor (intel or sun), programming language of installation software (java or perl), and methods employed in different versions of the software. the loading times for the three installations were 76 hours, 196 hours, and 132 hours, and disk-space utilization was 46.3 gb, 37.7 gb, and 31.6 gb, respectively. loading times varied due to a variety of differences among the systems. loading time also depended on whether data were written to intermediate files or not, and on whether input files were processed in sequence or in parallel. disk-space utilization depended on the number of medline files processed, amount of indexing, and whether abstracts were stored as character large objects or truncated. conclusions: relational database (rdbms) technology supports indexing and querying of very large datasets, and can accommodate a locally stored version of medline. rdbms systems support a wide range of queries and facilitate certain tasks that are not directly supported by the application programming interface to pubmed. because there is variation in hardware, software, and network infrastructures across sites, we cannot predict the exact time required for a user to load medline, but our results suggest that performance of the software is reasonable. our database schemas and conversion software are publicly available at http://biotext.berkeley.edu.
326	136504	inbook	\N	logic and algebra of specification	\N	springer-verlag	43	\N	\N	1993	\N	2005-03-22 16:57:10	\N	the polyadic pi-calculus: a tutorial	the pi-calculus is a model of concurrent computation based upon the notion of naming. it is first presented in its simplest and original form, with the help of several illustrative applications. then it is generalized from monadic to polyadic form. semantics is done in terms of both a reduction system and a version of labelled transitions called commitment; the known algebraic axiomatization of strong bisimilarity is given in the new setting, and so also is a characterization in modal logic. some theorems about the replication operator are proved. justification for the polyadic form is provided by the concept of sort, sorting and sort discipline which it supports. several illustrations of different sortings are given. one example is the presentation of data structures as processes which respect a particular sorting; another is the sorting for a known translation of the lambda-calculus in to pi-calculus. for this translation, the equational validity of beta-conversion is proved with the help of replication theorems. the paper ends with an extension of the pi-calculus to w-order processes, and a brief account of the demonstration by davide sangiorgi that higher-order processes may be faithfully encoded at first-order. this extends and strengthens the original result of this kind given by bent thomsen for second-order processes.
327	136656	article	journal of computational biology : a journal of computational molecular cell biology	\N	\N	\N	34	10	6	2003	\N	2005-03-22 18:56:54	school of computing, queen's university, kingston, ontario, canada k7l3n6. shatkay@cs.queensu.ca	mining the biomedical literature in the genomic era: an overview.	the past decade has seen a tremendous growth in the amount of experimental and computational biomedical data, specifically in the areas of genomics and proteomics. this growth is accompanied by an accelerated increase in the number of biomedical publications discussing the findings. in the last few years, there has been a lot of interest within the scientific community in literature-mining tools to help sort through this abundance of literature and find the nuggets of information most relevant and useful for specific analysis tasks. this paper provides a road map to the various literature-mining methods, both in general and within bioinformatics. it surveys the disciplines involved in unstructured-text analysis, categorizes current work in biomedical literature mining with respect to these disciplines, and provides examples of text analysis methods applied towards meeting some of the current challenges in bioinformatics.
328	137270	article	bioinformatics (oxford, england)	\N	\N	\N	12	20	7	2004	may	2005-03-23 11:20:10	institute for infocomm research, 21 heng mui keng terrace, singapore 119613. zhougd@i2r.a-star.edu.sg	recognizing names in biomedical texts: a machine learning approach.	motivation: with an overwhelming amount of textual information in molecular biology and biomedicine, there is a need for effective and efficient literature mining and knowledge discovery that can help biologists to gather and make use of the knowledge encoded in text documents. in order to make organized and structured information available, automatically recognizing biomedical entity names becomes critical and is important for information retrieval, information extraction and automated knowledge acquisition. results: in this paper, we present a named entity recognition system in the biomedical domain, called powerbione. in order to deal with the special phenomena of naming conventions in the biomedical domain, we propose various evidential features: (1) word formation pattern; (2) morphological pattern, such as prefix and suffix; (3) part-of-speech; (4) head noun trigger; (5) special verb trigger and (6) name alias feature. all the features are integrated effectively and efficiently through a hidden markov model (hmm) and a hmm-based named entity recognizer. in addition, a k-nearest neighbor (k-nn) algorithm is proposed to resolve the data sparseness problem in our system. finally, we present a pattern-based post-processing to automatically extract rules from the training data to deal with the cascaded entity name phenomenon. from our best knowledge, powerbione is the first system which deals with the cascaded entity name phenomenon. evaluation shows that our system achieves the f-measure of 66.6 and 62.2 on the 23 classes of genia v3.0 and v1.1, respectively. in particular, our system achieves the f-measure of 75.8 on the "protein" class of genia v3.0. for comparison, our system outperforms the best published result by 7.8 on genia v1.1, without help of any dictionaries. it also shows that our hmm and the k-nn algorithm outperform other models, such as back-off hmm, linear interpolated hmm, support vector machines, c4.5, c4.5 rules and ripper, by effectively capturing the local context dependency and resolving the data sparseness problem. moreover, evaluation on genia v3.0 shows that the post-processing for the cascaded entity name phenomenon improves the f-measure by 3.9. finally, error analysis shows that about half of the errors are caused by the strict annotation scheme and the annotation inconsistency in the genia corpus. this suggests that our system achieves an acceptable f-measure of 83.6 on the 23 classes of genia v3.0 and in particular 86.2 on the "protein" class, without help of any dictionaries. we think that a f-measure of 90 on the 23 classes of genia v3.0 and in particular 92 on the "protein" class, can be achieved through refining of the annotation scheme in the genia corpus, such as flexible annotation scheme and annotation consistency, and inclusion of a reasonable biomedical dictionary. availability: a demo system is available at http://textmining.i2r.a-star.edu.sg/nls/demo.htm. technology license is available upon the bilateral agreement.
329	139333	article	acm transactions on programming languages and systems	\N	\N	\N	70	22	4	2000	\N	2005-03-24 18:42:10	\N	typed memory management via static capabilities	region-based memory management is an alternative to standard tracing garbage collection that makes operation such as memory deallocation explicit but verifiably safe. in this article, we present a new compiler intermediate language, called the capability language (cl), that supports region-based memory management and enjoys a provably safe type systems. unlike previous region-based type system, region lifetimes need not be lexically scoped, and yet the language may be checked for safety without complex analyses. therefore, our type system may be deployed in settings such as extensible operating systems where both the performance and safety of untrusted code is important. the central novelty of the language is the use of static capabilities to specify the permissibility of various  operations, such as memory access and deallocation. in order to ensure capabilities are relinquished properly, the type system tracks aliasing information using a form of bounded quantification. moreover, unlike previous work on region-based type systems, the proof of soundness of our type system is relatively simple, employing only standard syntactic techniques. in order to show how our language may be used in practice, we show how to translate a variant of tofte and talpin's high-level type-and-effects system for region-based memory management into our language. when combined with known region inference algorithms, this translation provides a way to compile source-level languages to cl.
330	139500	article	biophys j	\N	\N	\N	6	84	3	2003	mar	2005-03-24 22:33:29	donald danforth plant science center, st. louis, missouri 63132, usa.	development of unified statistical potentials describing protein-protein interactions.	a residue-based and a heavy atom-based statistical pair potential are developed for use in assessing the strength of protein-protein interactions. to ensure the quality of the potentials, a nonredundant, high-quality dimer database is constructed. the protein complexes in this dataset are checked by a literature search to confirm that they form multimers, and the pairwise amino acid preference to interact across a protein-protein interface is analyzed and pair potentials constructed. the performance of the residue-based potentials is evaluated by using four jackknife tests and by assessing the potentials' ability to select true protein-protein interfaces from false ones. compared to potentials developed for monomeric protein structure prediction, the interdomain potential performs much better at distinguishing protein-protein interactions. the potential developed from homodimer interfaces is almost the same as that developed from heterodimer interfaces with a correlation coefficient of 0.92. the residue-based potential is well suited for genomic scale protein interaction prediction and analysis, such as in a recently developed threading-based algorithm, {multiprospector}. however, the more time-consuming atom-based potential performs better in identifying near-native structures from docking generated decoys.
331	139501	article	genome research	\N	\N	\N	8	13	6a	2003	jun	2005-03-24 22:33:48	center of excellence in bioinformatics, university at buffalo, buffalo, new york 14203, usa.	multimeric {threading-based} prediction of {protein–protein}  interactions on a genomic scale: application to the saccharomyces  cerevisiae proteome	{multiprospector}, a multimeric threading algorithm for the prediction of  protein–protein interactions, is applied to the genome of  saccharomyces cerevisiae. each possible pairwise interaction among  more than 6000 encoded proteins is evaluated against a dimer database of 768  complex structures by using a confidence estimate of the fold assignment and  the magnitude of the statistical interfacial potentials. in total, 7321  interactions between pairs of different proteins are predicted, based on 304  complex structures. quality estimation based on the coincidence of subcellular  localizations and biological functions of the predicted interactors shows that  our approach ranks third when compared with all other large-scale methods.  unlike other in silico methods, {multiprospector} is able to identify the  residues that participate directly in the interaction. three hundred  seventy-four of our predictions can be found by at least one of the other  studies, which is compatible with the overlap between two different other  methods. from the analysis of the {mrna} abundance data, our method does not  bias towards proteins with high abundance. finally, several relevant  predictions involved in various functions are presented. in summary, we  provide a novel approach to predict protein–protein interactions on a  genomic scale that is a useful complement to experimental methods.
332	139503	article	journal of molecular biology	\N	\N	\N	13	345	5	2005	feb	2005-03-24 22:35:40	koc university, center for computational biology and bioinformatics, and college of engineering, rumelifeneri yolu, 34450 sariyer istanbul, turkey. okeskin@ku.edu.tr	hot regions in protein--protein interactions: the organization and contribution of structurally conserved hot spot residues.	structurally conserved residues at protein-protein interfaces correlate with the experimental alanine-scanning hot spots. here, we investigate the organization of these conserved, computational hot spots and their contribution to the stability of protein associations. we find that computational hot spots are not homogeneously distributed along the protein interfaces; rather they are clustered within locally tightly packed regions. within the dense clusters, they form a network of interactions and consequently their contributions to the stability of the complex are cooperative; however the contributions of independent clusters are additive. this suggests that the binding free energy is not a simple summation of the single hot spot residue contributions. as expected, around the hot spot residues we observe moderately conserved residues, further highlighting the crucial role of the conserved interactions in the local densely packed environment. the conserved occurrence of these organizations suggests that they are advantageous for protein-protein associations. interestingly, the total number of hydrogen bonds and salt bridges contributed by hot spots is as expected. thus, h-bond forming residues may use a "hot spot for water exclusion" mechanism. since conserved residues are located within highly packed regions, water molecules are easily removed upon binding, strengthening electrostatic contributions of charge-charge interactions. hence, the picture that emerges is that protein-protein associations are optimized locally, with the clustered, networked, highly packed structurally conserved residues contributing dominantly and cooperatively to the stability of the complex. when addressing the crucial question of "what are the preferred ways of proteins to associate", these findings point toward a critical involvement of hot regions in protein-protein interactions.
333	139978	article	genome biology	\N	\N	\N	\N	4	10	2003	\N	2005-03-25 16:54:00	laboratory of immunopathogenesis and bioinformatics, po box b, saic-frederick, inc, frederick, md 21702, usa.	identifying biological themes within lists of genes with {ease}.	ease is a customizable software application for rapid biological interpretation of gene lists that result from the analysis of microarray, proteomics, sage and other high-throughput genomic data. the biological themes returned by ease recapitulate manually determined themes in previously published gene lists and are robust to varying methods of normalization, intensity calculation and statistical selection of genes. ease is a powerful tool for rapidly converting the results of functional genomics studies from 'genes' to 'themes'.
334	140049	book	\N	\N	\N	morgan {k}aufman {p}ublishers {i}nc.	\N	\N	\N	1988	\N	2005-03-25 18:39:05	\N	probabilistic reasoning in intelligent systems: networks of plausible inference	<p><i>probabilistic reasoning in intelligent systems</i> is a complete and accessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. the author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other ai approaches to uncertainty, such as the dempster-shafer formalism, truth maintenance systems, and nonmonotonic logic.<br><p>the author distinguishes syntactic and semantic approaches to uncertainty--and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition--in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information.</p><br><p><i>probabilistic reasoning in intelligent systems</i> will be of special interest to scholars and researchers in ai, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. the book can also be used as an excellent text for graduate-level courses in ai, operations research, or applied probability.</p>
335	140455	book	\N	\N	\N	cambridge university press	\N	\N	\N	1981	\N	2005-03-25 22:43:06	cambridge	the meaning of things: domestic symbols and the self	the meaning of things is a study of the significance of material possessions in contemporary urban life, and of the ways people carve meaning out of their domestic environment. drawing on a survey of eighty families in chicago who were interviewed on the subject of their feelings about common household objects, mihaly csikszentmihalyi and eugene rochberg-halton provide a unique perspective on materialism, american culture, and the self. they begin by reviewing what social scientists and philosophers have said about the transactions between people and things. in the model of 'personhood' that the authors develop, goal-directed action and the cultivation of meaning through signs assume central importance. they then relate theoretical issues to the results of their survey. an important finding is the distinction between objects valued for action and those valued for contemplation. the authors compare families who have warm emotional attachments to their homes with those in which a common set of positive meanings is lacking, and interpret the different patterns of involvement. they then trace the cultivation of meaning in case studies of four families. finally, the authors address what they describe as the current crisis of environmental and material exploitation, and suggest that human capacities for the creation and redirection of meaning offer the only hope for survival. a wide range of scholars - urban and family sociologists, clinical, developmental and environmental psychologists, cultural anthropologists and philosophers, and many general readers - will find this book stimulating and compelling. cambridge, uk
336	140474	techreport	\N	\N	\N	\N	\N	\N	39	1989	feb	2005-03-25 22:43:07	palo alto, ca	a logic of authentication	questions of belief are essential in analyzing protocols for the authentication of principals in distributed computing systems. in this paper we motivate, set out, and exemplify a logic specifically designed for this analysis; we show how various protocols differ subtly with respect to the required initial assumptions of the participants and their final beliefs. our formalism has enabled us to isolate and express these differences with a precision that was not previously possible. it has drawn attention to features of protocols of which we and their authors were previously unaware, and allowed us to suggest improvements to the protocols. the reasoning about some protocols has been mechanically verified. this paper starts with an informal account of the problem, goes on to explain the formalism to be used, and gives examples of its application to protocols from the literature, both with shared-key cryptography and with public-key cryptography. some of the examples are chosen because of their practical importance, while others serve to illustrate subtle points of the logic and to explain how we use it. we discuss extensions of the logic motivated by actual practice---for example, in order to account for the use of hash functions in signatures. the final sections contain a formal semantics of the logic and some conclusions. src research report 39 was originally published on february 28, 1989, and revised on february 22, 1990. this is the main body of the revised version. an appendix to the revised version is available separately. c fldigital equipment corporation 1989, 1990 this work may not be copied or reproduced in whole or in part for any commercial
337	140507	book	\N	\N	\N	mcgraw-hill publishing company	\N	\N	\N	2000	\N	2005-03-25 22:43:07	london	software engineering: a practitioner's approach	for over 20 years, _software engineering: a practitioner's approach_ has been the best selling guide to software engineering for students and industry professionals alike.  the sixth edition continues to lead the way in software engineering. a new part 4 on web engineering presents a complete engineering approach for the analysis, design, and testing of web applications, increasingly important for today's students. additionally, the uml coverage has been enhanced and signficantly increased in this new edition.  the pedagogy has also been improved in the new edition to include sidebars. they provide information on relevant softare tools, specific work flow for specific kinds of projects, and additional information on various topics. additionally, pressman provides a running case study called "safe home" throughout the book, which provides the application of software engineering to an industry project.  new additions to the book also include chapters on the agile process models, requirements engineering, and design engineering. the book has been completely updated and contains hundreds of new references to software tools that address all important topics in the book.  the ancillary material for the book includes an expansion of the case study, which illustrates it with uml diagrams. the on-line learning center includes resources for both instructors and students such as checklists, 700 categorized web references, powerpoints, a test bank, and a software engineering library-containing over 500 software engineering papers. takeawy here is the following: 1. agile process methods are covered early in ch. 4 2. new part on web applications --5 chapters
338	140523	book	\N	\N	\N	the mit press	\N	\N	\N	1998	\N	2005-03-25 22:43:07	cambridge, ma	the invisible computer: why good products can fail, the personal computer is so complex, and information appliances are the solution	{while donald norman acknowledges in <i>the invisible computer </i> that the personal computer allows for "flexibility and power," he also makes its limitations perfectly clear. currently, computer users must navigate a sea of guidebooks, frequently asked questions (faqs), and wizards to perform a task such as searching the web or creating a spreadsheet. "the personal computer is perhaps the most frustrating technology ever," he writes. "it should be quiet, invisible, unobtrusive." his vision is that of the "information appliance", digital tools created to answer our specific needs, yet interconnected to allow communication between devices.<p> his solution? "design the tool to fit so well that the tool becomes a part of the task." he proposes using the pc as the infrastructure for devices hidden in walls, in car dashboards, and held in the palm of the hand. a word of caution: some of norman's zealotry leads to a certain creepiness (global positioning body implants) and goofiness (electric-power-generating plants in shoes). his message, though, is reasonably situated in the concept that the tools should bend to fit us and our goals: we sit down to write, not to word process; to balance bank accounts, not to fill in cells on a spreadsheet. in evenly measuring out the future of humanity's technological needs--and the limitations of the pc's current incarnation--norman presents a formidable argument for a renaissance of the information appliance. --<i>jennifer buckendorff </i>}
339	140584	book	\N	\N	\N	harvard business school press	\N	\N	\N	2000	\N	2005-03-25 22:43:08	cambridge, ma	the social life of information	how many times has your {pc} crashed today? while gordon moore's now famous law projecting the doubling of computer power every 18 months has more than borne itself out, it's too bad that a similar trajectory projecting the reliability and usefulness of all that power didn't come to pass, as well. advances in information technology are most often measured in the cool numbers of megahertz, throughput, and bandwidth{\\\\textendash}but, for many us, the experience of these advances may be better measured in hours of frustration. the gap between the hype of the information age and its reality is often wide and deep, and it's into this gap that john seely brown and paul duguid plunge. not that these guys are luddites{\\\\textendash}far from it. brown, the chief scientist at xerox and the director of its palo alto research center {(parc),} and duguid, a historian and social theorist who also works with {parc,} measure how information technology interacts and meshes with the social fabric. they write, {'technology} design often takes aim at the surface of life. there it undoubtedly scores lots of worthwhile hits. but such successes can make designers blind to the difficulty of more serious challenges{\\\\textendash}primarily the resourcefulness that helps embed certain ways of doing things deep in our lives.' the authors cast their gaze on the many trends and ideas proffered by infoenthusiasts over the years, such as software agents, 'still a long way from the predicted insertion into the woof and warp of ordinary life'; the electronic cottage that alvin toffler wrote about 20 years ago and has yet to be fully realized; and the rise of knowledge management and the challenges it faces trying to manage how people actually work and learn in the workplace. their aim is not to pass judgment but to help remedy the tunnel vision that prevents technologists from seeing larger the social context that their ideas must ultimately inhabit. the social life of information is a thoughtful and challenging read that belongs on the bookshelf of anyone trying to invent or make sense of the new world of information. from the chief scientist of xerox corporation and a research specialist in cultural studies at {uc-berkeley} comes a treatise that casts a critical eye at all the hype surrounding the boom of the information age. the authors' central complaint is that narrowly focusing on new ways to provide information will not create the cyber-revolution so many technology designers have visualized. the problem (or joy) is that information acquires meaning only through social context. brown and duguid add a humanist spin to this idea by arguing, for example, that 'trust' is a deep social relation among people and cannot be reduced to logic, and that a satisfying 'conversation' cannot be held in an internet chat room because too much social context is stripped away and cannot be replaced by just adding more information, such as pictures and biographies of the participants. from this standpoint, brown and duguid contemplate the future of digital agents, the home office, the paperless society, the virtual firm and the online university. though they offer many insightful opinions, they have not produced an easy read. as they point out, theirs is 'more a book of questions than answers' and they often reject 'linear thinking.' like most futurists, they are fond of long neologisms, but they are given to particularly unpronounceable ones like 'infoprefixification' (the tendency to put 'info' in front of words). the result is an intellectual gem in which the authors have polished some facets and, annoyingly, left others uncut. {(mar.)}
340	140600	article	the computer journal	\N	\N	\N	14	27	\N	1984	may	2005-03-25 22:43:08	\N	literate programming	literate programming is a programming methodology that combines a programming language with a documentation language, making programs more robust, more portable, and more easily maintained than programs written only in a high-level language. computer programmers already know both kinds of languages; they need only learn a few conventions about alternating between languages to create programs that are works of literature. a literate programmer is an essayist who writes programs for humans to understand, instead of primarily writing instructions for machines to follow. when programs are written in the recommended style they can be transformed into documents by a document compiler and into efficient code by an algebraic compiler. this anthology of essays from the inventor of literate programming includes knuth's early papers on related topics such as structured programming, as well as the computer journal article that launched literate programming itself.
341	140611	article	ieee computer	\N	\N	\N	7	\N	\N	1998	mar	2005-03-25 22:43:08	\N	scripting: higher level programming for the 21st century	a fundamental change is occurring in the way people write computer programs, away from system programming languages such as c or c++ to scripting languages such as perl or tcl. although many people are participating in the change, few realize that the change is occurring and even fewer know why it is happening. this article explains why scripting languages will handle many of the programming tasks in the next century better than system programming languages. system programming languages were designed for building data structures and algorithms from scratch, starting from the most primitive computer elements. scripting languages are designed for gluing. they assume the existence of a set of powerful components and are intended primarily for connecting components
342	140673	book	\N	\N	\N	oxford university press	\N	\N	\N	2003	\N	2005-03-25 22:43:09	oxford	evolution of networks: from biological nets to the {i}nternet and {www}	only recently did mankind realize that it resides on a world of networks. the internet and the world wide web are changing our life. our physical existence is based on various biological networks. we have recently learned that the term ""network"" turns out to be a central notion in our time, and the onsequent explosion of interest in networks is a social and cultural phenomenon. the principles of the complex organization and evolution of networks, natural and artificial are the topic of this book, which is written by physicists and is addressed to all involved researchers and students. the aim of the text is to understand networks and the basic principles of their structural organization and evolution. the ideas are presneted in a clear and pedegogical way, with minimal mathematics, so even students without a deep knowledge of mathematics and statistical physics will be able to rely on this as a reference. special attention is given to real networks, both natural and artificial. collected empirical data and numerous real applications of existing theories are discussed in detail, as well as the topical problems of communication networks.
343	140701	inproceedings	\N	proceedings of the 5th international conference on software engineering	\N	ieee press, piscataway nj	10	\N	\N	1981	mar	2005-03-25 22:43:09	san diego, ca	program slicing	program slicing is a method used by experienced computer programmers for abstracting from programs. starting from a subset of a program's behavior, slicing reduces that program to a minimal form which still produces that behavior. the reduced program, called a &ldquo;slice&rdquo;, is an independent program guaranteed to faithfully represent the original program within the domain of the specified subset of behavior.   finding a slice is in general unsolvable. a dataflow algorithm is presented for approximating slices when the behavior subset is specified as the values of a set of variables at a statement. experimental evidence is presented that these slices are used by programmers during debugging. experience with two automatic slicing tools is summarized. new measures of program complexity are suggested based on the organization of a program's slices.
344	140708	article	physical review e	\N	\N	\N	\N	68	\N	2003	\N	2005-03-25 22:43:10	\N	software systems as complex networks: structure, function, and evolvability of software collaboration graphs	software systems emerge from mere keystrokes to form intricate functional networks connecting many collaborating modules, objects, classes, methods, and subroutines. building on recent advances in the study of complex networks, i have examined software collaboration graphs contained within several open-source software systems, and have found them to reveal scale-free, small-world networks similar to those identified in other technological, sociological, and biological systems. i present several measures of these network topologies, and discuss their relationship to software engineering practices. i also present a simple model of software system evolution based on refactoring processes which captures some of the salient features of the observed systems. some implications of object-oriented design for questions about network robustness, evolvability, degeneracy, and organization are discussed in the wake of these findings.
345	140717	article	nature	\N	\N	\N	\N	401	\N	1999	sep	2005-03-25 22:43:10	\N	growth dynamics of the {w}orld-{w}ide {w}eb	the exponential growth of the world-wide web has transformed it into an ecology of knowledge in which highly diverse information is linked in an extremely complex and arbitrary manner. but even so, as we show here, there is order hidden in the web. we find that web pages are distributed among sites according to a universal power law: many sites have only a few pages, whereas very few sites have hundreds of thousands of pages. this universal distribution can be explained by using a simple stochastic dynamical growth model.
346	140728	book	\N	\N	\N	w.\\~{}h.\\~{}freeman and company	\N	\N	\N	1983	\N	2005-03-25 22:43:10	new york, ny	the fractal geometry of nature	{imagine an equilateral triangle. now, imagine smaller equilateral triangles perched in the center of each side of the original triangle--you have a star of david. now, place still smaller equilateral triangles in the center of each of the star's 12 sides. repeat this process infinitely and you have a koch snowflake, a mind-bending geometric figure with an infinitely large perimeter, yet with a finite area. this is an example of the kind of mathematical puzzles that this book addresses.<p> <i>the fractal geometry of nature</i> is a mathematics text. but buried in the deltas and lambdas and integrals, even a layperson can pick out and appreciate mandelbrot's point: that somewhere in mathematics, there is an explanation for nature. it is not a coincidence that fractal math is so good at generating images of cliffs and shorelines and capillary beds.}
347	140843	book	\N	\N	\N	addison-wesley	\N	\N	\N	2004	\N	2005-03-25 22:43:11	boston, ma	exploiting software: how to break code	{computing hardware would have no value without software; software tells hardware what to do. software therefore must have special authority within computing systems. all computer security problems stem from that fact, and <i>exploiting software: how to break code</i> shows you how to design your software so it's as resistant as possible to attack. sure, everything's phrased in offensive terms (as instructions for the attacker, that is), but this book has at least as much value in showing designers what sorts of attacks their software will face (the book could serve as a checklist for part of a pre-release testing regimen). plus, the clever reverse-engineering strategies that greg hoglund and gary mcgraw teach will be useful in many legitimate software projects. consider this a recipe book for mayhem, or a compendium of lessons learned by others. it depends on your situation.<p>  php programmers will take issue with the authors' blanket assessment of their language ("php is a study in bad security"), much of which seems based on older versions of the language that had some risky default behaviors--but those programmers will also double-check their servers' register_globals settings. users of insufficiently patched microsoft and oracle products will worry about the detailed attack instructions this book contains. responsible programmers and administrators will appreciate what amounts to documentation of attackers' rootkits for various operating systems, and will raise their eyebrows at the techniques for writing malicious code to unused eeprom chips in target systems. <i>--david wall</i><p>  <b>topics covered</b>: how to make software fail, either by doing something it wasn't designed to do, or by denying its use to its rightful users. techniques--including reverse engineering, buffer overflow, and particularly provision of unexpected input--are covered along with the tools needed to carry them out. a section on hardware viruses is detailed and frightening.}
348	140845	book	\N	\N	\N	americal mathematical society	\N	\N	\N	1997	\N	2005-03-25 22:43:11	providence, ri	introduction to probability	{an intuitive, yet precise introduction to probability theory, stochastic processes, and probabilistic models used in science, engineering, economics, and related fields. this is the currently used textbook for "probabilistic systems analysis," an introductory probability course at the massachusetts institute of technology, attended by a large number of undergraduate and graduate students.    the book covers the fundamentals of probability theory (probabilistic models, discrete and continuous random variables, multiple random variables, and limit theorems), which are typically part of a first course on the subject. it also contains, a number of more advanced topics, from which an instructor can choose to match the goals of a particular course. these topics include transforms, sums of random variables, least squares estimation, the bivariate normal distribution, and a fairly detailed introduction to bernoulli, poisson, and markov processes.    the book strikes a balance between simplicity in exposition and sophistication in analytical reasoning. some of the more mathematically rigorous analysis has been just intuitively explained in the text, but is developed in detail (at the level of advanced calculus) in the numerous solved theoretical problems.     the book has been widely adopted for classroom use in introductory probability courses within the usa and abroad.}
349	140848	book	\N	\N	\N	addison wesley	\N	\N	\N	2000	\N	2005-03-25 22:43:11	reading, ma	an introduction to database systems	from the publisher: for over 25 years, c. j. date's an introduction to database systems has been the authoritative resource for readers interested in gaining insight into and understanding of the principles of database systems. this revision continues to provide a solid grounding in the foundations of database technology and to provide some ideas as to how the field is likely to develop in the future.. "readers of this book will gain a strong working knowledge of the overall structure, concepts, and objectives of database systems and will become familiar with the theoretical principles underlying the construction of such systems.
350	140994	inproceedings	\N	proc. 14th international conference on machine learning	\N	morgan kaufmann	8	\N	\N	1997	\N	2005-03-26 00:53:30	\N	learning belief networks in the presence of missing values and hidden variables	in recent years there has been a flurry of works on learning probabilistic belief networks. current state of the art methods have been shown to be successful for two learning scenarios: learning both network structure and parameters from complete data, and learning parameters for a fixed network from incomplete data---that is, in the presence of missing values ---or hidden variables. however, no method has yet been demonstrated to effectively learn network structure from incomplete data. in this paper, we propose a new method for learning network structure from incomplete data. this method is based on an extension of the expectation-maximization (em) algorithm for model selection problems that performs search for the best structure inside the em procedure. we prove the convergence of this algorithm, and adapt it for learning belief networks. we then describe how to learn networks in two scenarios: when the data contains missing values, and in the presence of hidden variables. we provide experimental results that show the effectiveness of our procedure in both scenarios.
351	141652	article	science	\N	\N	\N	3	294	5550	2001	dec	2006-03-24 18:05:43	\N	resolution of the early placental mammal radiation using bayesian phylogenetics	molecular phylogenetic studies have resolved placental mammals into four major groups, but have not established the full hierarchy of interordinal relationships, including the position of the root. the latter is critical for understanding the early biogeographic history of placentals. we investigated placental phylogeny using bayesian and maximum-likelihood methods and a 16.4-kilobase molecular data set. interordinal relationships are almost entirely resolved. the basal split is between afrotheria and other placentals, at about 103 million years, and may be accounted for by the separation of south america and africa in the cretaceous. crown-group eutheria may have their most recent common ancestry in the southern hemisphere (gondwana).
352	141729	article	the international journal of supercomputer applications and high performance computing	\N	\N	\N	13	11	2	1997	Summer	2005-03-27 16:04:42	\N	globus: a metacomputing infrastructure toolkit	emerging high-performance applications require the ability to exploit diverse, geographically distributed resources. these applications use high-speed networks to integrate supercomputers, large databases, archival storage devices, advanced visualization devices, and/or scientific instruments to form networked virtual supercomputers or metacomputers. while the physical infrastructure to build such systems is becoming widespread, the heterogeneous and dynamic nature of the metacomputing...
353	142464	inproceedings	\N	proceedings of the ninth acm sigkdd international conference on knowledge discovery and data mining	kdd	acm	9	\N	\N	2003	\N	2005-03-29 01:32:17	new york, ny, usa	algorithms for estimating relative importance in networks	large and complex graphs representing relationships among sets of entities are an increasingly common focus of interest in data analysis---examples include social networks, web graphs, telecommunication networks, and biological networks. in interactive analysis of such data a natural query is "which entities are most important in the network relative to a particular individual or set of individuals?" we investigate the problem of answering such queries in this paper, focusing in particular on defining and computing the importance of nodes in a graph relative to one or more root nodes. we define a general framework and a number of different algorithms, building on ideas from social networks, graph theory, markov models, and web graph analysis. we experimentally evaluate the different properties of these algorithms on toy graphs and demonstrate how our approach can be used to study relative importance in real-world networks including a network of interactions among september 11th terrorists, a network of collaborative research in biotechnology among companies and universities, and a network of co-authorship relationships among computer science researchers.
354	142681	book	\N	\N	\N	{little, brown}	\N	\N	\N	2005	jan	2005-03-29 18:16:27	\N	blink : the power of thinking without thinking	{<i>blink</i> is about the first two seconds of looking--the decisive glance that knows in an instant. gladwell, the best-selling author of <i>the tipping point</i>, campaigns for snap judgments and mind reading with a gift for translating research into splendid storytelling. building his case with scenes from a marriage, heart attack triage, speed dating, choking on the golf course, selling cars, and military maneuvers, he persuades readers to think small and focus on the meaning of "thin slices" of behavior. the key is to rely on our "adaptive unconscious"--a 24/7 mental valet--that provides us with instant and sophisticated information to warn of danger, read a stranger, or react to a new idea. <p>  gladwell includes caveats about leaping to conclusions: marketers can manipulate our first impressions, high arousal moments make us "mind blind," focusing on the wrong cue leaves us vulnerable to "the warren harding effect" (i.e., voting for a handsome but hapless president). in a provocative chapter that exposes the "dark side of blink," he illuminates the failure of rapid cognition in the tragic stakeout and murder of amadou diallo in the bronx. he underlines studies about autism, facial reading and cardio uptick to urge training that enhances high-stakes decision-making.  in this brilliant, cage-rattling book, one can only wish for a thicker slice of gladwell's ideas about what blink camp might look like.  <i>--barbara mackoff</i>} {how do we make decisions--good and bad--and why are some people so much better at it than others? thats the question malcolm gladwell asks and answers in the follow-up to his huge bestseller, the tipping point. utilizing case studies as diverse as speed dating, pop music, and the shooting of amadou diallo, gladwell reveals that what we think of as decisions made in the blink of an eye are much more complicated than assumed. drawing on cutting-edge neuroscience and psychology, he shows how the difference between good decision-making and bad has nothing to do with how much information we can process quickly, but on the few particular details on which we focus. leaping boldly from example to example, displaying all of the brilliance that made the tipping point a classic, gladwell reveals how we can become better decision makers--in our homes, our offices, and in everyday life. the result is a book that is surprising and transforming. never again will you think about thinking the same way.}
355	142705	misc	\N	\N	\N	\N	\N	\N	\N	1999	\N	2005-03-29 20:03:05	\N	unsupervised models for named entity classification	this paper discusses the use of unlabeled examples for the problem of named entity classification. a large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classifier. however, we show that the use of unlabeled  data can reduce the requirements for supervision to just 7 simple seed rules. the approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling ...
356	143446	article	nature genetics	\N	\N	nature publishing group	4	37	4	2005	apr	2007-10-22 13:47:34	\N	genome-wide strategies for detecting multiple loci that influence complex diseases.	after nearly 10 years of intense academic and commercial research effort, large genome-wide association studies for common complex diseases are now imminent. although these conditions involve a complex relationship between genotype and phenotype, including interactions between unlinked loci, the prevailing strategies for analysis of such studies focus on the locus-by-locus paradigm. here we consider analytical methods that explicitly look for statistical interactions between loci. we show first that they are computationally feasible, even for studies of hundreds of thousands of loci, and second that even with a conservative correction for multiple testing, they can be more powerful than traditional analyses under a range of models for interlocus interactions. we also show that plausible variations across populations in allele frequencies among interacting loci can markedly affect the power to detect their marginal effects, which may account in part for the well-known difficulties in replicating association results. these results suggest that searching for interactions among genetic loci can be fruitfully incorporated into analysis strategies for genome-wide association studies.
357	143501	article	proceedings of the national academy of sciences of the united states of america	\N	\N	national academy of sciences	5	101	9	2004	mar	2005-03-31 08:53:10	school of computer science, tel aviv university, tel aviv 69978, israel.	revealing modularity and organization in the yeast molecular network by integrated analysis of highly heterogeneous genomewide data	the dissection of complex biological systems is a challenging task, made difficult by the size of the underlying molecular network and the heterogeneous nature of the control mechanisms involved. novel high-throughput techniques are generating massive data sets on various aspects of such systems. here, we perform analysis of a highly diverse collection of genomewide data sets, including gene expression, protein interactions, growth phenotype data, and transcription factor binding, to reveal the modular organization of the yeast system. by integrating experimental data of heterogeneous sources and types, we are able to perform analysis on a much broader scope than previous studies. at the core of our methodology is the ability to identify modules, namely, groups of genes with statistically significant correlated behavior across diverse data sources. numerous biological processes are revealed through these modules, which also obey global hierarchical organization. we use the identified modules to study the yeast transcriptional network and predict the function of >800 uncharacterized genes. our analysis framework, samba ({statistical-algorithmic} method for bicluster analysis), enables the processing of current and future sources of biological information and is readily extendable to experimental techniques and higher organisms.
358	143515	article	genome biology	\N	\N	\N	\N	4	9	2003	\N	2005-03-31 11:11:39	department of bioengineering, university of california, san diego, gilman drive, la jolla, ca 92092, usa. bpalsson@be-research.ucsd.edu	an expanded genome-scale model of escherichia coli k-12 ({ijr904} {gsm}/{gpr}).	background: diverse datasets, including genomic, transcriptomic, proteomic and metabolomic data, are becoming readily available for specific organisms. there is currently a need to integrate these datasets within an in silico modeling framework. constraint-based models of escherichia coli k-12 mg1655 have been developed and used to study the bacterium's metabolism and phenotypic behavior. the most comprehensive e. coli model to date (e. coli ije660a gsm) accounts for 660 genes and includes 627 unique biochemical reactions. results: an expanded genome-scale metabolic model of e. coli (ijr904 gsm/gpr) has been reconstructed which includes 904 genes and 931 unique biochemical reactions. the reactions in the expanded model are both elementally and charge balanced. network gap analysis led to putative assignments for 55 open reading frames (orfs). gene to protein to reaction associations (gpr) are now directly included in the model. comparisons between predictions made by ijr904 and ije660a models show that they are generally similar but differ under certain circumstances. analysis of genome-scale proton balancing shows how the flux of protons into and out of the medium is important for maximizing cellular growth. conclusions: e. coli ijr904 has improved capabilities over ije660a. ijr904 is a more complete and chemically accurate description of e. coli metabolism than ije660a. perhaps most importantly, ijr904 can be used for analyzing and integrating the diverse datasets. ijr904 will help to outline the genotype-phenotype relationship for e. coli k-12, as it can account for genomic, transcriptomic, proteomic and fluxomic data simultaneously.
359	144073	article	cognition	\N	\N	\N	27	75	1	2000	\N	2005-04-01 13:56:27	\N	metaphoric structuring: understanding time through spatial metaphors	{the present paper evaluates the claim that abstract conceptual domains are structured through metaphorical mappings from domains grounded directly in experience. in particular, the paper asks whether the abstract domain of time gets its relational structure from the more concrete domain of space. relational similarities between space and time are outlined along with several explanations of how these similarities may have arisen. three experiments designed to distinguish between these explanations are described. the results indicate that (1) the domains of space and time do share conceptual structure, (2) spatial relational information is just as useful for thinking about time as temporal information, and (3) with frequent use, mappings between space and time come to be stored in the domain of time and so thinking about time does not necessarily require access to spatial schemas. these findings provide some of the first empirical evidence for metaphoric structuring. it appears that abstract domains such as time are indeed shaped by metaphorical mappings from more concrete and experiential domains such as space.}
360	144184	book	\N	\N	\N	mit press	\N	\N	\N	1989	\N	2005-04-01 13:56:33	cambridge	speaking : from intention to articulation.	(from the jacket) in "speaking," willem "pim" levelt, director of the max-planck-institut fur psycholinguistik, accomplishes the formidable task of covering the entire process of speech production, from constraints on conversational appropriateness to articulation and self-monitoring of speech. "speaking" is unique in its balanced coverage of all major aspects of the production of speech, in the completeness of its treatment of the entire speech process, and in its strategy of exemplifying rather than formalizing theoretical issues. /// levelt provides a theoretically coherent picture of the speaker as information processor. he proposes a modular organization of relatively autonomous processors for message generation, grammatical encoding, phonological encoding, and articulation. for each processor, he distinguishes carefully between its operations and its input and output representation. the chapters are arranged so that the reader acquires more knowledge of semantics, syntax, lexical structure, and phonology as it becomes necessary in order to understand processing. /// throughout the book, levelt acknowledges the purposes served by the speaker's information processing--particularly in the sections on discourse processing, requesting, referring, monitoring, and self-repair. (psycinfo database record (c) 2005 apa, all rights reserved).
361	144289	article	bioinformatics	\N	\N	oxford university press	7	19	11	2003	jul	2005-04-01 14:23:06	department of genome analysis,gbf - german research center for biotechnology, mascheroder weg 1, 38124 braunschweig, germany.	the connectivity structure, giant strong component and centrality of metabolic networks	{motivation:structural} and functional analysis of genome-based large-scale metabolic networks is important for understanding the design principles and regulation of the metabolism at a system level. the metabolic network is conventionally considered to be highly integrated and very complex. a rational reduction of the metabolic network to its core structure and a deeper understanding of its functional modules are important.
362	147383	article	molecular biology and evolution	\N	\N	oxford university press	11	22	4	2005	apr	2006-09-29 12:14:19	\N	bayes empirical bayes inference of amino acid sites under positive selection.	codon-based substitution models have been widely used to identify amino acid sites under positive selection in comparative analysis of protein-coding dna sequences. the nonsynonymous-synonymous substitution rate ratio (dn/ds, denoted {omega}) is used as a measure of selective pressure at the protein level, with {omega} > 1 indicating positive selection. statistical distributions are used to model the variation in {omega} among sites, allowing a subset of sites to have {omega} > 1 while the rest of the sequence may be under purifying selection with {omega} < 1. an empirical bayes (eb) approach is then used to calculate posterior probabilities that a site comes from the site class with {omega} > 1. current implementations, however, use the naive eb (neb) approach and fail to account for sampling errors in maximum likelihood estimates of model parameters, such as the proportions and {omega} ratios for the site classes. in small data sets lacking information, this approach may lead to unreliable posterior probability calculations. in this paper, we develop a bayes empirical bayes (beb) approach to the problem, which assigns a prior to the model parameters and integrates over their uncertainties. we compare the new and old methods on real and simulated data sets. the results suggest that in small data sets the new beb method does not generate false positives as did the old neb approach, while in large data sets it retains the good power of the neb approach for inferring positively selected sites. 10.1093/molbev/msi097
363	148884	misc	\N	\N	\N	\N	\N	\N	\N	1998	\N	2005-04-03 20:05:18	\N	the social cost of cheap pseudonyms	we consider the problems of societal norms for cooperation and reputation when it is possible to obtain \\&quot;cheap pseudonyms\\&quot;, something which is becoming quite common in a wide variety of interactions on the internet. this introduces opportunities to misbehave without paying reputational consequences. a large degree of cooperation can still emerge, through a convention in which newcomers \\&quot;pay their dues\\&quot; by accepting poor treatment from players who have established positive reputations. one might ...
364	149346	article	ieee transactions on evolutionary computation	\N	\N	\N	13	1	1	1997	apr	2005-04-04 19:02:59	\N	ant colony system: a cooperative learning approach to the traveling salesman problem	this paper introduces the ant colony system (acs), a distributed algorithm that is applied to the traveling salesman problem (tsp). in the acs, a set of cooperating agents called ants cooperate to find good solutions to tsps. ants cooperate using an indirect form of communication mediated by a pheromone they deposit on the edges of the tsp graph while building solutions. we study the acs by running experiments to understand its operation. the results show that the acs outperforms other nature-inspired algorithms such as simulated annealing and evolutionary computation, and we conclude comparing acs-3-opt, a version of the acs augmented with a local search procedure, to some of the best performing algorithms for symmetric and asymmetric tsps
365	149347	article	acm trans. internet technol.	\N	\N	acm	35	2	2	2002	may	2005-04-04 19:05:40	new york, ny, usa	principled design of the modern web architecture	the world wide web has succeeded in large part because its software architecture has been designed to meet the needs of an internet-scale distributed hypermedia application. the modern web architecture emphasizes scalability of component interactions, generality of interfaces, independent deployment of components, and intermediary components to reduce interaction latency, enforce security, and encapsulate legacy systems. in this article we introduce the representational state transfer ({rest}) architectural style, developed as an abstract model of the web architecture and used to guide our redesign and definition of the hypertext transfer protocol and uniform resource identifiers. we describe the software engineering principles guiding {rest} and the interaction constraints chosen to retain those principles, contrasting them to the constraints of other architectural styles. we then compare the abstract model to the currently deployed web architecture in order to elicit mismatches between the existing protocols and the applications they are intended to support.
366	150208	article	j. acm	\N	\N	acm	32	45	6	1998	nov	2005-04-06 08:34:23	new york, ny, usa	an optimal algorithm for approximate nearest neighbor searching fixed dimensions	consider a set of s of n data points  in real d-dimensional space, rd, where distances are measured using any minkowski metric. in nearest neighbor searching, we preprocess s into a data structure, so that given any query point q? rd, is the closest point of s to q can be reported quickly. given any positive real \\&egr;, data point p is a (1 +\\&egr;)-approximate nearest neighbor of q if its distance from q is within a factor of (1 + \\&egr;) of the distance to the true nearest neighbor. we show that it is possible to preprocess a    set of n points in     rd in o(dn log n) time and o(dn) space, so that given a query point  q? rd, and \\&egr; > 0, a (1 + \\&egr;)-approximate nearest neighbor of q can be computed in o(cd, \\&egr; log n) time, where cd,\\&egr;?d1 + 6d/e;d is a factor depending only on dimension and \\&egr;. in general, we show that given an integer k ? 1, (1 + \\&egr;)-approximations  to the  k nearest neighbors of q can  be computed in additional o(kd log n) time.
367	152287	book	\N	\N	\N	cbms-nsf regional conference series in applied mathematics, philadelphia, pennsylvania	\N	\N	\N	1992	\N	2005-04-07 14:12:46	\N	ten lectures on wavelets	this book is both a tutorial on wavelets and a review of the most advanced research in this domain. the topics covered include the continuous wavelet transform of j. morlet and a. grossmann, several important results on time-frequency localization (such as the balian-low theorem and the landau-pollack-slepian theory), a complete study of the "frame" structure for a discrete set of wavelets, the multiresolution analysis and orthonormal bases of y. meyer, and finally the beautiful construction of compactly supported wavelets from filter banks (which is due to the author). <p> as mentioned in the introduction, this is a mathematics book that states and proves many theorems. in addition, it also gives many practical examples and describes several applications (in particular, in signal processing, image coding and numerical analysis).
368	152445	article	ieee transactions on pattern analysis and machine intelligence	\N	\N	\N	5	19	2	1997	\N	2005-04-07 14:12:48	\N	feature selection - evaluation, application, and small sample performance	a large number of algorithms have been proposed for feature subset selection. our experimental results show that the sequential forward floating selection (sffs) algorithm, proposed by pudil et al., dominates the other algorithms tested. we study the problem of choosing an optimal feature set for land use classification based on sar satellite images using four different texture models. pooling features derived from different texture models, followed by a feature selection results in a substantial improvement in the classification accuracy. we also illustrate the dangers of using feature selection in small sample size situations.
369	153816	article	ieee computer	\N	\N	\N	\N	35(7) 47-54	\N	2002	\N	2005-04-07 14:34:47	\N	genome sequence assembly: algorithms and issues	ultimately, genome sequencing seeks to provide an organism's complete dna sequence. automation of dna sequencing allowed scientists to decode entire genomes and gave birth to genomics, the analytic and comparative study of genomes. although genomes can include billions of nucleotides, the chemical reactions researchers use to decode the dna are accurate for only about 600 to 700 nucleotides at a time. the dna reads that sequencing produces must then be assembled into a complete picture of the genome. errors and certain dna characteristics complicate assembly. resolving these problems entails an additional and costly finishing phase that involves extensive human intervention. assembly programs can dramatically reduce this cost by taking into account additional information obtained during finishing. the paper considers how algorithms that can assemble millions of dna fragments into gene sequences underlie the current revolution in biotechnology, helping researchers build the growing database of complete genomes
370	153821	article	science	\N	\N	\N	\N	287: 2196-2204	\N	2000	\N	2005-04-07 14:34:47	\N	a {whole-genome} assembly of drosophila	we report on the quality of a whole-genome assembly of drosophila melanogaster and the nature of the computer algorithms that accomplished it. three independent external data sources essentially agree with and support the assembly's sequence and ordering of contigs across the euchromatic portion of the genome. in addition, there are isolated contigs that we believe represent nonrepetitive pockets within the heterochromatin of the centromeres. comparison with a previously sequenced 2.9- megabase region indicates that sequencing accuracy within nonrepetitive segments is greater than 99. 99% without manual curation. as such, this initial reconstruction of the drosophila sequence should be of substantial value to the scientific community.
371	153873	article	bioinformatics	\N	\N	\N	8	18	4	2002	\N	2005-04-07 14:34:48	\N	a comparative review of statistical methods for discovering differentially expressed genes in replicated microarray experiments	motivation: a common task in analyzing microarray data is to   determine which genes are differentially expressed across two kinds of tissue samples or samples obtained under two experimental   conditions. recently several statistical methods have been proposed to accomplish this goal when there are replicated samples under each condition. however, it may not be clear how these methods compare with each other. our main goal here is to compare three methods, the t-test, a regression modeling approach (thomas et al. , genome res. , 11, 1227-1236, 2001) and a mixture model approach (pan et al. , http://www.biostat.umn.edu/cgi-bin/rrs?print+2001,2001a,b) with particular attention to their different modeling assumptions.  results: it is pointed out that all the three methods are based   on using the two-sample t-statistic or its minor variation,   but they differ in how to associate a statistical significance level to the corresponding statistic, leading to possibly large difference in the resulting significance levels and the numbers of genes detected. in particular, we give an explicit formula for the test statistic used in the regression approach. using the leukemia data of golub et al. (science , 285,  531-537, 1999), we illustrate these points. we also briefly   compare the results with those of several other methods, including   the empirical bayesian method of efron et al. (j. am. stat. assoc. , to appear, 2001) and the significance analysis of microarray (sam) method of tusher et al. (proc. natl acad. sci. usa , 98, 5116-5121, 2001).  contact: weip@biostat.umn.edu 10.1093/bioinformatics/18.4.546
372	155094	book	\N	\N	\N	wiley-interscience	\N	\N	\N	1998	apr	2005-04-07 22:06:35	\N	applied regression analysis (wiley series in probability and statistics)	{an outstanding introduction to the fundamentals of regression analysis-updated and expanded the methods of regression analysis are the most widely used statistical tools for discovering the relationships among variables. this classic text, with its emphasis on clear, thorough presentation of concepts and applications, offers a complete, easily accessible introduction to the fundamentals of regression analysis. assuming only a basic knowledge of elementary statistics, applied regression analysis, third edition focuses on the fitting and checking of both linear and nonlinear regression models, using small and large data sets, with pocket calculators or computers. this third edition features separate chapters on multicollinearity, generalized linear models, mixture ingredients, geometry of regression, robust regression, and resampling procedures. extensive support materials include sets of carefully designed exercises with full or partial solutions and a series of true/false questions with answers. all data sets used in both the text and the exercises can be found on the companion disk at the back of the book. for analysts, researchers, and students in university, industrial, and government courses on regression, this text is an excellent introduction to the subject and an efficient means of learning how to use a valuable analytical tool. it will also prove an invaluable reference resource for applied scientists and statisticians.}
373	155605	techreport	\N	\N	\N	\N	\N	\N	674	2004	\N	2005-04-08 16:13:47	\N	variational inference for {d}irichlet process mixtures	abstract. dirichlet process (dp) mixture models are the cornerstone of nonparametric bayesian statistics, and the development of monte-carlo markov chain (mcmc) sampling methods for dp mixtures has enabled the application of nonparametric bayesian methods to a variety of practical data analysis problems. however, mcmc sampling can be prohibitively slow, and it is important to explore alternatives. one class of alternatives is provided by variational methods, a class of deterministic algorithms that convert inference problems into optimization problems (opper and saad 2001; wainwright and jordan 2003). thus far, variational methods have mainly been explored in the parametric setting, in particular within the formalism of the exponential family (attias 2000; ghahramani and beal 2001; blei et al. 2003). in this paper, we present a variational inference algorithm for dp mixtures. we present experiments that compare the algorithm to gibbs sampling algorithms for dp mixtures of gaussians and present an application to a large-scale image analysis problem.
374	155617	article	the annals of statistics	\N	\N	\N	22	2	6	1974	\N	2005-04-08 16:13:49	\N	mixtures of {d}irichlet processes with applications to {b}ayesian nonparametric problems	a random process called the dirichlet process whose sample functions are almost surely probability measures has been proposed by ferguson as an approach to analyzing nonparametric problems from a bayesian viewpoint. an important result obtained by ferguson in this approach is that if observations are made on a random variable whose distribution is a random sample function of a dirichlet process, then the conditional distribution of the random measure can be easily calculated, and is again a dirichlet process. this paper extends ferguson's result to cases where the random measure is a mixing distribution for a parameter which determines the distribution from which observations are made. the conditional distribution of the random measure, given the observations, is no longer that of a simple dirichlet process, but can be described as being a mixture of dirichlet processes. this paper gives a formal definition for these mixtures and develops several theorems about their properties, the most important of which is a closure property for such mixtures. formulas for computing the conditional distribution are derived and applications to problems in bio-assay, discrimination, regression, and mixing distributions are given.
375	155618	unpublished	\N	\N	\N	\N	\N	\N	\N	2003	\N	2005-04-08 16:13:49	\N	a tutorial on principal component analysis	this tutorial is designed to give the reader an understanding of principal components analysis (pca). pca is a useful statistical technique that has found application in fields such as face recognition and image compression, and is a common technique for finding patterns in data of high dimension. before getting to a description of pca, this tutorial first introduces mathematical concepts that will be used in pca. it covers standard deviation, covariance, eigenvec- tors and eigenvalues. this background knowledge is meant to make the pca section very straightforward, but can be skipped if the concepts are already familiar. there are examples all the way through this tutorial that are meant to illustrate the concepts being discussed. if further information is required, the mathematics textbook â€œelementary linear algebra 5eâ€ by howard anton, publisher john wiley & sons inc, isbn 0-471-85223-6 is a good source of information regarding the mathematical back- ground
376	155625	phdthesis	\N	\N	\N	\N	\N	\N	\N	2003	\N	2005-04-08 16:13:49	\N	variational algorithms for approximate {b}ayesian inference	the bayesian framework for machine learning allows for the incorporation of prior knowledge in a coherent way, avoids overfitting problems, and provides a principled basis for selecting between alternative models. unfortunately the computations required are usually intractable. this thesis presents a unified variational bayesian (vb) framework which approximates these computations in models with latent variables using a lower bound on the marginal likelihood. chapter 1 presents background material on bayesian inference, graphical models, and propaga- tion algorithms. chapter 2 forms the theoretical core of the thesis, generalising the expectation- maximisation (em) algorithm for learning maximum likelihood parameters to the vb em al- gorithm which integrates over model parameters. the algorithm is then specialised to the large family of conjugate-exponential (ce) graphical models, and several theorems are presented to pave the road for automated vb derivation procedures in both directed and undirected graphs (bayesian and markov networks, respectively). chapters 3-5 derive and apply the vb em algorithm to three commonly-used and important models: mixtures of factor analysers, linear dynamical systems, and hidden markov models. it is shown how model selection tasks such as determining the dimensionality, cardinality, or number of variables are possible using vb approximations. also explored are methods for combining sampling procedures with variational approximations, to estimate the tightness of vb bounds and to obtain more effective sampling algorithms. chapter 6 applies vb learning to a long-standing problem of scoring discrete-variable directed acyclic graphs, and compares the performance to annealed importance sampling amongst other methods. throughout, the vb approximation is compared to other methods including sampling, cheeseman-stutz, and asymptotic approximations such as bic. the thesis concludes with a discussion of evolving directions for model selection including infinite models and alternative approximations to the marginal likelihood.
377	155630	techreport	\N	\N	\N	\N	\N	\N	UCB//CSD-02-1202	2002	\N	2005-04-08 16:13:49	\N	modeling annotated data	we consider the problem of modeling annotated data-data with multiple types where the instance of one type (such as a caption) serves as a description of the other type (such as an image). we describe three hierarchical probabilistic mixture models which aim to describe such data, culminating in correspondence latent dirichlet allocation, a latent variable model that is effective at modeling the joint distribution of both types and the conditional distribution of the annotation given the primary type. we conduct experiments on the corel database of images and captions, assessing performance in terms of held-out likelihood, automatic annotation, and text-based image retrieval. sponsors: acm/sigir
378	155631	article	journal of machine learning research	\N	\N	\N	29	3	\N	2003	jan	2005-04-08 16:13:49	\N	latent {d}irichlet allocation	we describe latent dirichlet allocation (lda), a generative probabilistic model for collections of  discrete data such as text corpora. lda is a three-level hierarchical bayesian model, in which each  item of a collection is modeled as a finite mixture over an underlying set of topics. each topic is, in  turn, modeled as an infinite mixture over an underlying set of topic probabilities. in the context of  text modeling, the topic probabilities provide an explicit representation of a document. we present  efficient approximate inference techniques based on variational methods and an em algorithm for  empirical bayes parameter estimation. we report results in document modeling, text classification,  and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic lsi  model.
379	155633	inproceedings	\N	neural information processing systems ({n}{i}{p}{s}) 16	\N	\N	\N	\N	\N	2003	\N	2005-04-08 16:13:49	\N	hierarchical topic models and the nested {c}hinese restaurant process	we address the problem of learning topic hierarchies from data. the model selection problem in this domain is dauntingâ€”which of the large collection of possible trees to use? we take a bayesian approach, generating an appropriate prior via a distribution on partitions that we refer to as the nested chinese restaurant process. this nonparametric prior allows arbitrarily large branching factors and readily accommodates growing data collections. we build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent dirichlet allocation. we illustrate our approach on simulated data and with an application to the modeling of nips abstracts.
380	155650	article	the annals of statistics	\N	\N	\N	21	1	\N	1973	\N	2005-04-08 16:13:49	\N	a {b}ayesian analysis of some nonparametric problems	the bayesian approach to statistical problems, though fruitful in many ways, has been rather unsuccessful in treating nonparametric problems. this is due primarily to the difficulty in finding workable prior distributions on the parameter space, which in nonparametric ploblems is taken to be a set of probability distributions on a given sample space. there are two desirable properties of a prior distribution for nonparametric problems. (i) the support of the prior distribution should be large--with respect to some suitable topology on the space of probability distributions on the sample space. (ii) posterior distributions given a sample of observations from the true probability distribution should be manageable analytically. these properties are antagonistic in the sense that one may be obtained at the expense of the other. this paper presents a class of prior distributions, called dirichlet process priors, broad in the sense of (i), for which (ii) is realized, and for which treatment of many nonparametric statistical problems may be carried out, yielding results that are comparable to the classical theory. in section 2, we review the properties of the dirichlet distribution needed for the description of the dirichlet process given in section 3. briefly, this process may be described as follows. let $\\mathscr{x}$ be a space and $\\mathscr{a}$ a $\\sigma$-field of subsets, and let $\\alpha$ be a finite non-null measure on $(\\mathscr{x}, \\mathscr{a})$. then a stochastic process $p$ indexed by elements $a$ of $\\mathscr{a}$, is said to be a dirichlet process on $(\\mathscr{x}, \\mathscr{a})$ with parameter $\\alpha$ if for any measurable partition $(a_1, \\cdots, a_k)$ of $\\mathscr{x}$, the random vector $(p(a_1), \\cdots, p(a_k))$ has a dirichlet distribution with parameter $(\\alpha(a_1), \\cdots, \\alpha(a_k)). p$ may be considered a random probability measure on $(\\mathscr{x}, \\mathscr{a})$, the main theorem states that if $p$ is a dirichlet process on $(\\mathscr{x}, \\mathscr{a})$ with parameter $\\alpha$, and if $x_1, \\cdots, x_n$ is a sample from $p$, then the posterior distribution of $p$ given $x_1, \\cdots, x_n$ is also a dirichlet process on $(\\mathscr{x}, \\mathscr{a})$ with a parameter $\\alpha + \\sum^n_1 \\delta_{x_i}$, where $\\delta_x$ denotes the measure giving mass one to the point $x$. in section 4, an alternative definition of the dirichlet process is given. this definition exhibits a version of the dirichlet process that gives probability one to the set of discrete probability measures on $(\\mathscr{x}, \\mathscr{a})$. this is in contrast to dubins and freedman [2], whose methods for choosing a distribution function on the interval [0, 1] lead with probability one to singular continuous distributions. methods of choosing a distribution function on [0, 1] that with probability one is absolutely continuous have been described by kraft [7]. the general method of choosing a distribution function on [0, 1], described in section 2 of kraft and van eeden [10], can of course be used to define the dirichlet process on [0, 1]. special mention must be made of the papers of freedman and fabius. freedman [5] defines a notion of tailfree for a distribution on the set of all probability measures on a countable space $\\mathscr{x}$. for a tailfree prior, posterior distribution given a sample from the true probability measure may be fairly easily computed. fabius [3] extends the notion of tailfree to the case where $\\mathscr{x}$ is the unit interval [0, 1], but it is clear his extension may be made to cover quite general $\\mathscr{x}$. with such an extension, the dirichlet process would be a special case of a tailfree distribution for which the posterior distribution has a particularly simple form. there are disadvantages to the fact that $p$ chosen by a dirichlet process is discrete with probability one. these appear mainly because in sampling from a $p$ chosen by a dirichlet process, we expect eventually to see one observation exactly equal to another. for example, consider the goodness-of-fit problem of testing the hypothesis $h_0$ that a distribution on the interval [0, 1] is uniform. if on the alternative hypothesis we place a dirichlet process prior with parameter $\\alpha$ itself a uniform measure on [0, 1], and if we are given a sample of size $n \\geqq 2$, the only nontrivial nonrandomized bayes rule is to reject $h_0$ if and only if two or more of the observations are exactly equal. this is really a test of the hypothesis that a distribution is continuous against the hypothesis that it is discrete. thus, there is still a need for a prior that chooses a continuous distribution with probability one and yet satisfies properties (i) and (ii). some applications in which the possible doubling up of the values of the observations plays no essential role are presented in section 5. these include the estimation of a distribution function, of a mean, of quantiles, of a variance and of a covariance. a two-sample problem is considered in which the mann-whitney statistic, equivalent to the rank-sum statistic, appears naturally. a decision theoretic upper tolerance limit for a quantile is also treated. finally, a hypothesis testing problem concerning a quantile is shown to yield the sign test. in each of these problems, useful ways of combining prior information with the statistical observations appear. other applications exist. in his ph. d. dissertation [1], charles antoniak finds a need to consider mixtures of dirichlet processes. he treats several problems, including the estimation of a mixing distribution, bio-assay, empirical bayes problems, and discrimination problems.
381	155697	inproceedings	\N	uncertainty in artificial intelligence, proceedings of the seventeenth conference	\N	\N	\N	\N	\N	2001	\N	2005-04-08 16:13:50	\N	probabilistic models for unified collaborative and {content-based} recommendation in {sparse-data} environments	recommender systems leverage product and community information to target products to consumers. researchers have developed collaborative recommenders, content-based recommenders, and a few hybrid systems. we propose a unified probabilistic framework for merging collaborative and content-based recommendations. we extend hofmann&#039;s (1999) aspect model to incorporate three-way co-occurrence data among users, items, and item content. the relative influence of collaboration data versus content data is not imposed as an exogenous parameter, but rather emerges naturally from the given data sources. however, global probabilistic models coupled with standard em learning algorithms tend to drastically overfit in the sparsedata situations typical of recommendation applications. we show that secondary content information can often be used to overcome sparsity. experiments on data from the researchindex library of computer science publications show that appropriate mixture models incorporating secondary data produce significantly better quality recommenders than k-nearest neighbors (k-nn). global probabilistic models also allow more general inferences than local methods like k-nn.
382	155710	article	ieee transactions on pattern analysis and machine intelligence	\N	\N	\N	17	22	9	2000	\N	2005-04-08 16:13:50	\N	normalized cuts and image segmentation	we propose a novel approach for solving the perceptual grouping problem in vision.  rather than focusing on local features and their consistencies in the image data,  our approach aims at extracting the global impression of an image. we treat image  segmentation as a graph partitioning problem and propose a novel global criterion, the  normalized cut, for segmenting the graph. the normalized cut criterion measures both  the total dissimilarity between the different groups as well as the total similarity within  the groups. we show that an efficient computational technique based on a generalized  eigenvalue problem can be used to optimize this criterion. we have applied this approach  to segmenting static images as well as motion sequences and found results very  encouraging.  keywords: grouping, image segmentation, graph partition  1 introduction  nearly 75 years ago, wertheimer[24] launched the gestalt approach which laid out the importance of perceptual grouping and organization in v...
383	155728	article	machine learning	\N	\N	\N	33	34	\N	1999	\N	2005-04-08 16:13:50	\N	statistical models for text segmentation	this paper introduces a new statistical approach to automatically partitioning text into coherent segments. the approach is based on a technique that incrementally builds an exponential model to extract features that are correlated with the presence of boundaries in labeled training text. the models use two classes of features: topicality features that use adaptive language models in a novel way to detect broad changes of topic, and cue-word features that detect occurrences of specific words, which may be domain-specific, that tend to be used near segment boundaries. assessment of our approach on quantitative and qualitative grounds demonstrates its effectiveness in two very different domains, wall street journal news articles and television broadcast news story transcripts. quantitative results on these domains are presented using a new probabilistically motivated error metric, which combines precision and recall in a natural and flexible way. this metric is used to make a quantitative assessment of the relative contributions of the different feature types, as well as a comparison with decision trees and previously proposed text segmentation algorithms.
384	157135	inproceedings	\N	supercomputing	\N	\N	9	\N	\N	1991	\N	2005-04-09 13:37:07	\N	the omega test: a fast and practical integer programming algorithm for dependence analysis	the omega testi s ani nteger programmi ng algori thm that can determi ne whether a dependence exi sts between two array references, and i so, under what condi7: ns. conventi {nalwi[a} m holds thati nteger {programmib} techni:36 are far too expensi e to be used for dependence analysi6 except as a method of last resort for si:8 ti ns that cannot be {deci:a} by si[976 methods. we present {evi[77b} that suggests thiwi sdomi s wrong, and that the omega testi s competi ti ve wi th approxi mate algori thms...
385	158329	article	educational technology research \\& development	\N	\N	\N	15	44	2	1996	\N	2005-04-11 10:19:26	\N	seriously considering play: designing interactive learning environments based on the blending of microworlds, simulations, and games	abstract little attention has been given to the psychological and sociological value of play despite its many advantages to guiding the design of interactive multimedia learning environments for children and adults. this paper provides a brief overview of the history, research, and theory related to play. research from education, psychology, and anthropology suggests that play is a powerful mediator for learning throughout a person's life. the time has come to couple the ever increasing processing capabilities of computers with the advantages of play. the design of hybrid interactive learning environments is suggested based on the constructivist concept of a microworld and supported with elements of both games and simulations.
386	158597	article	nucleic acids res	\N	\N	\N	17	33	6	2005	\N	2005-04-11 22:27:47	bioinfobank institute ul. limanowskiego 24a, 60-744 pozna\\'{n}, poland.	practical lessons from protein structure prediction.	despite recent efforts to develop automated protein structure determination protocols, structural genomics projects are slow in generating fold assignments for complete proteomes, and spatial structures remain unknown for many protein families. alternative cheap and fast methods to assign folds using prediction algorithms continue to provide valuable structural information for many proteins. the development of high-quality prediction methods has been boosted in the last years by objective community-wide assessment experiments. this paper gives an overview of the currently available practical approaches to protein structure prediction capable of generating accurate fold assignment. recent advances in assessment of the prediction quality are also discussed.
387	159098	article	comput. linguist.	\N	\N	mit press	35	31	1	2005	mar	2006-03-16 12:59:31	cambridge, ma, usa	the proposition bank: an annotated corpus of semantic roles	the proposition bank project takes a practical approach to semantic representation, adding a layer of predicate-argument information, or semantic role labels, to the syntactic structures of the penn treebank. the resulting resource can be thought of as shallow, in that it does not represent coreference, quantification, and many other higher-order phenomena, but also broad, in that it covers every instance of every verb in the corpus and allows representative statistics to be {calculated.we} discuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus.  we describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty ''trace'' categories of the treebank.
388	160019	inproceedings	\N	proceedings of the conference on the future of software engineering	icse	acm	11	\N	\N	2000	\N	2005-04-13 19:38:57	new york, ny, usa	testing: a roadmap	an abstract is not available.
389	160143	article	systems, man and cybernetics, part a: systems and humans, ieee transactions on	\N	\N	ieee	11	30	3	2000	may	2005-04-14 02:36:30	\N	a model for types and levels of human interaction with automation	technical developments in computer hardware and software now make it possible to introduce automation into virtually all aspects of human-machine systems. given these technical capabilities, which system functions should be automated and to what extent? we outline a model for types and levels of automation that provides a framework and an objective basis for deciding which system functions should be automated and to what extent. appropriate selection is important because automation does not merely supplant but changes human activity and can impose new coordination demands on the human operator. we propose that automation can be applied to four broad classes of functions: 1) information acquisition; 2) information analysis; 3) decision and action selection; and 4) action implementation. within each of these types, automation can be applied across a continuum of levels from low to high, i.e., from fully manual to fully automatic. a particular system can involve automation of all four types at different levels. the human performance consequences of particular types and levels of automation constitute primary evaluative criteria for automation design using our model. secondary evaluative criteria include automation reliability and the costs of decision/action consequences, among others. examples of recommended types and levels of automation are provided to illustrate the application of the model to automation design
390	161150	article	journal of molecular biology	\N	\N	\N	6	326	1	2003	feb	2005-04-14 19:37:04	\N	an accurate, sensitive, and scalable method to identify functional sites in protein structures	functional sites determine the activity and interactions of proteins and as such constitute the targets of most drugs. however, the exponential growth of sequence and structure data far exceeds the ability of experimental techniques to identify their locations and key amino acids. to fill this gap we developed a computational evolutionary trace method that ranks the evolutionary importance of amino acids in protein sequences. studies show that the best-ranked residues form fewer and larger structural clusters than expected by chance and overlap with functional sites, but until now the significance of this overlap has remained qualitative. here, we use 86 diverse protein structures, including 20 determined by the structural genomics initiative, to show that this overlap is a recurrent and statistically significant feature. an automated {et} correctly identifies seven of ten functional sites by the least favorable statistical measure, and nine of ten by the most favorable one. these results quantitatively demonstrate that a large fraction of functional sites in the proteome may be accurately identified from sequence and structure. this should help focus structure-function studies, rational drug design, protein engineering, and functional annotation to the relevant regions of a protein.
391	161170	article	proceedings of the national academy of sciences of the united states of america	\N	\N	\N	5	101	31	2004	aug	2005-04-14 20:14:54	\N	anchor residues in protein-protein interactions	we show that the mechanism for molecular recognition requires one of the interacting proteins, usually the smaller of the two, to anchor a specific side chain in a structurally constrained binding groove of the other protein, providing a steric constraint that helps to stabilize a native-like bound intermediate. we identify the anchor residues in 39 protein-protein complexes and verify that, even in the absence of their interacting partners, the anchor side chains are found in conformations similar to those observed in the bound complex. these ready-made recognition motifs correspond to surface side chains that bury the largest solvent-accessible surface area after forming the complex (> or =100 a2). the existence of such anchors implies that binding pathways can avoid kinetically costly structural rearrangements at the core of the binding interface, allowing for a relatively smooth recognition process. once anchors are docked, an induced fit process further contributes to forming the final high-affinity complex. this later stage involves flexible (solvent-exposed) side chains that latch to the encounter complex in the periphery of the binding pocket. our results suggest that the evolutionary conservation of anchor side chains applies to the actual structure that these residues assume before the encounter complex and not just to their loci. implications for protein docking are also discussed. [journal article; in english; united states]
392	162250	article	sigcomm comput. commun. rev.	sigcomm	\N	acm	11	34	4	2004	oct	2005-04-16 04:20:36	new york, ny, usa	vivaldi: a decentralized network coordinate system	large-scale internet applications can benefit from an ability to predict round-trip times to other hosts without having to contact them first. explicit measurements are often unattractive because the cost of measurement can outweigh the benefits of exploiting proximity information. vivaldi is a simple, light-weight algorithm that assigns synthetic coordinates to hosts such that the distance between the coordinates of two hosts accurately predicts the communication latency between the hosts. vivaldi is fully distributed, requiring no fixed network infrastructure and no distinguished hosts. it is also efficient: a new host can compute good coordinates for itself after collecting latency information from only a few other hosts. because it requires little com-munication, vivaldi can piggy-back on the communication patterns of the application using it and scale to a large number of hosts. an evaluation of vivaldi using a simulated network whose latencies are based on measurements among 1740 internet hosts shows that a 2-dimensional euclidean model with  height vectors  embeds these hosts with low error (the median relative error in round-trip time prediction is 11 percent).
393	162268	inproceedings	\N	proc. of the 1st symposium on networked systems design and implementation	\N	\N	\N	\N	\N	2004	mar	2005-04-16 04:31:41	\N	designing a {dht} for low latency and high throughput	designing a wide-area distributed hash table (dht) that provides high- throughput and low-latency network storage is a challenge. existing systems have explored a range of solutions, including iterative routing, recursive routing, proximity routing and neighbor selection, erasure coding, replication, and server selection. <p>  this paper explores the design of these techniques and their interaction in a complete system, drawing on the measured performance of a new dht implementation and results from a simulator with an accurate internet latency model. new techniques that resulted from this exploration include use of latency predictions based on synthetic coordinates, efficient integration of lookup routing and data fetching, and a congestion control mechanism suitable for fetching data striped over large numbers of servers. <p>  measurements with 425 server instances running on 150 planetlab and ron hosts show that the latency optimizations reduce the time required to locate and fetch data by a factor of two. the throughput optimizations result in a sustainable bulk read throughput related to the number of dht hosts times the capacity of the slowest access link; with 150 selected planetlab hosts, the peak aggregate throughput over multiple clients is 12.8 megabytes per second.
394	162281	inproceedings	\N	proc. of the 5th {osdi}	\N	\N	\N	\N	\N	2002	dec	2005-04-16 04:31:44	\N	ivy: a {read/write} peer-to-peer file system	ivy is a multi-user read/write peer-to-peer file system. ivy has no centralized or dedicated components, and it provides useful integrity properties without requiring users to fully trust either the underlying peer-to-peer storage system or the other users of the file system. an ivy file system consists solely of a set of logs, one log per participant. ivy stores its logs in the {dhash} distributed hash table. each participant finds data by consulting all logs, but performs modifications by appending only to its own log. this arrangement allows ivy to maintain meta-data consistency without locking. ivy users can choose which other logs to trust, an appropriate arrangement in a semi-open peer-to-peer system. ivy presents applications with a conventional file system interface. when the underlying network is fully connected, ivy provides {nfs-like} semantics, such as close-to-open consistency. ivy detects conflicting modifications made during a partition, and provides relevant version information to application-specific conflict resolvers. performance measurements on a wide-area network show that ivy is two to three times slower than {nfs.}
395	162288	inproceedings	\N	proc. of the 18th ifip/acm international conference on distributed systems platforms (middleware 2001)	\N	\N	\N	\N	\N	2001	nov	2005-04-16 04:31:45	\N	pastry: scalable, distributed object location and routing for large-scale peer-to-peer systems	this paper presents the design and evaluation of pastry, a scalable, distributed object location and routing scheme for wide-area peer-to-peer applications. pastry provides application-level routing and object location in a potentially very large overlay network of nodes connected via the internet. it can be used to support a wide range of peer-to-peer applications like global data storage, global data sharing, and naming. an insert operation in pastry stores an object at a user-defined number of diverse nodes within the pastry network. a lookup operation reliably retrieves a copy of the requested object if one exists. moreover, a lookup is usually routed to the node nearest the client issuing the lookup (by some measure of proximity), among the nodes storing the requested object. pastry is completely decentralized, scalable, and self-configuring; it automatically adapts to the arrival, departure and failure of nodes. experimental results obtained with a prototype implementation on a simulated network of 100,000 nodes confirm pastry's scalability, its ability to self-configure and adapt to node failures, and its good network locality properties.
396	162289	inproceedings	\N	proc. of the 18th acm symposium on operating system principles	\N	\N	\N	\N	\N	2001	oct	2005-04-16 04:31:45	\N	storage management and caching in {past}, a large-scale, persistent peer-to-peer storage utility	this paper presents and evaluates the storage management and caching in past, a large-scale peer-to-peer persistent storage utility. past is based on a self-organizing, internet-based overlay network of storage nodes that cooperatively route file queries, store multiple replicas of files, and cache additional copies of popular files.in the past system, storage nodes and files are each assigned uniformly distributed identifiers, and replicas of a file are stored at nodes whose identifier matches most closely the file's identifier. this statistical assignment of files to storage nodes approximately balances the number of files stored on each node. however, non-uniform storage node capacities and file sizes require more explicit storage load balancing to permit graceful behavior under high global storage utilization; likewise, non-uniform popularity of files requires caching to minimize fetch distance and to balance the query load.we present and evaluate past, with an emphasis on its storage management and caching system. extensive trace-driven experiments show that the system minimizes fetch distance, that it balances the query load for popular files, and that it displays graceful degradation of performance as the global storage utilization increases beyond 95%.
397	162869	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2005-04-17 04:50:12	\N	social bookmarking tools (i): a general review	spacer introduction because, to paraphrase a pop music lyric from a certain rock and roll band of yesterday, "the web is old, the web is new, the web is all, the web is you", it seems like we might have to face up to some of these stark realities [n1]. with the introduction of new social software applications such as blogs, wikis, newsfeeds, social networks, and bookmarking tools (the subject of this paper), the claim that shelley powers makes in a burningbird blog entry [1] seems apposite: "this is the user's web now, which means it's my web and i can make the rules." reinvention is revolution â€“ it brings us always back to beginnings. we are here going to remind you of hyperlinks in all their glory, sell you on the idea of bookmarking hyperlinks, point you at other folks who are doing the same, and tell you why this is a good thing. just as long as those hyperlinks (or let's call them plain old links) are managed, tagged, commented upon, and published onto the web, they represent a user's own personal library placed on public record, which â€“ when aggregated with other personal libraries â€“ allows for rich, social networking opportunities. why spill any ink (digital or not) in rewriting what someone else has already written about instead of just pointing at the original story and adding the merest of titles, descriptions and tags for future reference? more importantly, why not make these personal 'link playlists' available to oneself and to others from whatever browser or computer one happens to be using at the time? this paper reviews some current initiatives, as of early 2005, in providing public link management applications on the web â€“ utilities that are often referred to under the general moniker of 'social bookmarking tools'. there are a couple of things going on here: 1) server-side software aimed specifically at managing links with, crucially, a strong, social networking flavour, and 2) an unabashedly open and unstructured approach to tagging, or user classification, of those links. a number of such utilities are presented here, together with an emergent new class of tools that caters more to the academic communities and that stores not only user-supplied tags, but also structured citation metadata terms wherever it is possible to glean this information from service providers. this provision of rich, structured metadata means that the user is provided with an accurate third-party identification of a document, which could be used to retrieve that document, but is also free to search on user-supplied terms so that documents of interest (or rather, references to documents) can be made discoverable and aggregated with other similar descriptions either recorded by the user or by other users.
398	163529	book	\N	\N	\N	viking adult	\N	\N	\N	2004	dec	2005-04-18 15:38:06	\N	collapse: how societies choose to fail or succeed	{jared diamond's <i>collapse: how societies choose to fail or succeed</i> is the glass-half-empty follow-up to his pulitzer prize-winning <i>guns, germs, and steel</i>. while <i>guns, germs, and steel</i> explained the geographic and environmental reasons why some human populations have flourished, <i>collapse</i> uses the same factors to examine why ancient societies, including the anasazi of the american southwest and the viking colonies of greenland, as well as modern ones such as rwanda, have fallen apart. not every collapse has an environmental origin, but an eco-meltdown is often the main catalyst, he argues, particularly when combined with society's response to (or disregard for) the coming disaster. still, right from the outset of <i>collapse</i>, the author makes clear that this is not a mere environmentalist's diatribe. he begins by setting the book's main question in the small communities of present-day montana as they face a decline in living standards and a depletion of natural resources. once-vital mines now leak toxins into the soil, while prion diseases infect some deer and elk and older hydroelectric dams have become decrepit. on all these issues, and particularly with the hot-button topic of logging and wildfires, diamond writes with equanimity.  <p>   because he's addressing such significant issues within a vast span of time, diamond can occasionally speak too briefly and assume too much, and at times his shorthand remarks may cause careful readers to raise an eyebrow. but in general, diamond provides fine and well-reasoned historical examples, making the case that many times, economic and environmental concerns are one and the same. with <i>collapse</i>, diamond hopes to jog our collective memory to keep us from falling for false analogies or forgetting prior experiences, and thereby save us from potential devastations to come. while it might seem a stretch to use medieval greenland and the maya to convince a skeptic about the seriousness of global warming, it's exactly this type of cross-referencing that makes <i>collapse</i> so compelling. <i>--jennifer buckendorff</i>} { in his million-copy bestseller <i>guns, germs, and steel</i>, jared diamond examined how and  why western civilizations developed the technologies and immunities that allowed them to  dominate much of the world. now in this brilliant companion volume, diamond probes the other  side of the equation: what caused some of the great civilizations of the past to collapse into ruin,  and what can we learn from their fates? <p> as in <i>guns, germs, and steel</i>, diamond weaves an all-encompassing global thesis  through a series of fascinating historical-cultural narratives. moving from the polynesian cultures  on easter island to the flourishing american civilizations of the anasazi and the maya and finally  to the doomed viking colony on greenland, diamond traces the fundamental pattern of  catastrophe. environmental damage, climate change, rapid population growth, and unwise  political choices were all factors in the demise of these societies, but other societies found  solutions and persisted. similar problems face us today and have already brought disaster to  rwanda and haiti, even as china and australia are trying to cope in innovative ways. despite our  own society\\&\\#146;s apparently inexhaustible wealth and unrivaled political power, ominous warning  signs have begun to emerge even in ecologically robust areas like montana.<p> brilliant, illuminating, and immensely absorbing, <i>collapse</i> is destined to take its place as  one of the essential books of our time, raising the urgent question: how can our world best avoid  committing ecological suicide?}
399	163532	book	\N	\N	\N	cambridge university press	\N	\N	\N	1998	may	2005-04-18 15:42:54	\N	biological sequence analysis: probabilistic models of proteins and nucleic acids	{probablistic models are becoming increasingly important in analyzing the huge amount of data being produced by large-scale dna-sequencing efforts such as the human genome project.  for example, hidden markov models are used for analyzing biological sequences, linguistic-grammar-based probabilistic models for identifying rna secondary structure, and probabilistic evolutionary models for inferring phylogenies of sequences from different organisms. this book gives a unified, up-to-date and self-contained account, with a bayesian slant, of such methods, and more generally to probabilistic methods of sequence analysis. written by an interdisciplinary team of authors, it is accessible to molecular biologists, computer scientists, and mathematicians with no formal knowledge of the other fields, and at the same time presents the state of the art in this new and important field.}
400	163543	article	science	\N	\N	\N	11	287	5461	2000	mar	2005-04-18 16:09:40	howard hughes medical institute, department of molecular and cell biology, berkeley drosophila genome project, university of california, berkeley, ca 94720, usa.	comparative genomics of the eukaryotes	a comparative analysis of the genomes {ofdrosophila} melanogaster, caenorhabditis elegans, and saccharomyces cerevisiae\\^{a}??and the proteins they are predicted to encode\\^{a}??was undertaken in the context of cellular, developmental, and evolutionary processes. the nonredundant protein sets of flies and worms are similar in size and are only twice that of yeast, but different gene families are expanded in each genome, and the multidomain proteins and signaling pathways of the fly and worm are far more complex than those of yeast. the fly has orthologs to 177 of the 289 human disease genes examined and provides the foundation for rapid analysis of some of the basic processes involved in human disease.
401	163559	electronic	\N	\N	\N	\N	\N	\N	\N	2005	apr	2005-04-18 17:38:27	\N	knowledge representation issues in semantic graphs for relationship detection	an important task for homeland security is the prediction of threat vulnerabilities, such as through the detection of relationships between seemingly disjoint entities. a structure used for this task is a "semantic graph", also known as a "relational data graph" or an "attributed relational graph". these graphs encode relationships as "typed" links between a pair of "typed" nodes. indeed, semantic graphs are very similar to semantic networks used in ai. the node and link types are related through an ontology graph (also known as a schema). furthermore, each node has a set of attributes associated with it (e.g., "age" may be an attribute of a node of type "person"). unfortunately, the selection of types and attributes for both nodes and links depends on human expertise and is somewhat subjective and even arbitrary. this subjectiveness introduces biases into any algorithm that operates on semantic graphs. here, we raise some knowledge representation issues for semantic graphs and provide some possible solutions using recently developed ideas in the field of complex networks. in particular, we use the concept of transitivity to evaluate the relevance of individual links in the semantic graph for detecting relationships. we also propose new statistical measures for semantic graphs and illustrate these semantic measures on graphs constructed from movies and terrorism data.
402	164761	techreport	\N	\N	\N	\N	\N	\N	UT-CS-94-230	1994	may	2007-09-21 02:18:59	knoxville, tn, usa	{mpi}: a {message-passing} interface standard	process nanling to allow libraries to describe their communication in terms suitable to their own data structures and algorithms, the ability to "adorn" a set of communicating processes with additional user-defined attributes, such as extra collective operations. this mechanism should provide a means for the user or library writer effectively to extend a message-passing notation. in addition, a unified mechanism or object is needed for conveniently denoting communication context, the group of ...
403	165116	article	mach. learn.	machine learning	\N	kluwer academic publishers	27	45	1	2001	oct	2005-04-19 20:15:21	hingham, ma, usa	random forests	random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. the generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. the generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. using a random selection of features to split each node yields error rates that compare favorably to adaboost (y. freund \\& r. schapire, machine learning: proceedings of the thirteenth international conference, ***, 148–156), but are more robust with respect to noise. internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. internal estimates are also used to measure variable importance. these ideas are also applicable to regression.
404	165344	inproceedings	\N	proceedings of the 36th conference on association for computational linguistics	\N	association for computational linguistics	6	\N	\N	1998	\N	2005-04-20 13:33:53	morristown, nj, usa	part of speech tagging using a network of linear separators	we present an architecture and an on-line learning algorithm and apply it to the problem of part-of-speech tagging. the architecture presented, {\\em snow}, is a network of linear separators in the feature space, utilizing the winnow update algorithm. multiplicative weight-update algorithms such as winnow have been shown to have exceptionally good behavior when applied to very high dimensional problems, and especially when the target concepts depend on only a small subset of the features in the feature space. in this paper we describe an architecture that utilizes this mistake-driven algorithm for multi-class prediction -- selecting the part of speech of a word. the experimental analysis presented here provides more evidence to that these algorithms are suitable for natural language problems. the algorithm used is an on-line algorithm: every example is used by the algorithm only once, and is then discarded. this has significance in terms of efficiency, as well as quick adaptation to new contexts. we present an extensive experimental study of our algorithm under various conditions; in particular, it is shown that the algorithm performs comparably to the best known algorithms for {\\sc pos}.
405	165362	article	mach. learn.	\N	\N	kluwer academic publishers	24	34	1-3	1999	\N	2005-04-20 13:44:48	hingham, ma, usa	learning to parse natural language with maximum entropy models	. this paper presents a machine learning system for parsing natural language that learns from manually parsed example sentences, and parses unseen data at state-of-the-art accuracies. its machine learning technology, based on the maximum entropy framework, is highly reusable and not specific to the parsing problem, while the linguistic hints that it uses to learn can be specified concisely. it therefore requires a minimal amount of human effort and linguistic knowledge for its construction. in...
406	165434	phdthesis	\N	\N	\N	university of california at berkeley	\N	\N	\N	1994	\N	2005-04-20 14:38:22	berkeley, ca, usa	bayesian learning of probabilistic language models	the general topic of this thesis is the probabilistic modeling of language, in particular natural language. in probabilistic language modeling, one characterizes the strings of phonemes, words, etc. of a certain domain in terms of a probability distribution over all possible strings within the domain. probabilistic language modeling has been applied to a wide range of problems in recent years, from the traditional uses in speech recognition to more recent applications in biological sequence...
407	165438	article	comput. linguist.	\N	\N	mit press	7	16	1	1990	mar	2005-04-20 14:22:07	cambridge, ma, usa	word association norms, mutual information, and lexicography	the term word association is used in a very particular sense in the psycholinguistic literature. (generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor. ) we will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word). this paper will propose an objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (the standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) the proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words.
408	166210	article	biophys j	\N	\N	cell press	21	88	4	2005	apr	2006-04-03 15:34:45	department of chemistry, stanford university, stanford, california 94305-5080.	exploring the {helix-coil} transition via {all-atom} equilibrium ensemble simulations	the ensemble folding of two 21-residue ±-helical peptides has been studied using all-atom simulations under several variants of the {amber} potential in explicit solvent using a global distributed computing network. our extensive sampling, orders of magnitude greater than the experimental folding time, results in complete convergence to ensemble equilibrium. this allows for a quantitative assessment of these potentials, including a new variant of the {amber}-99 force field, denoted {amber}-99\\~{o}, which shows improved agreement with experimental kinetic and thermodynamic measurements. from bulk analysis of the simulated {amber}-99\\~{o} equilibrium, we find that the folding landscape is pseudo-two-state, with complexity arising from the broad, shallow character of the native and unfolded regions of the phase space. each of these macrostates allows for configurational diffusion among a diverse ensemble of conformational microstates with greatly varying helical content and molecular size. indeed, the observed structural dynamics are better represented as a conformational diffusion than as a simple exponential process, and equilibrium transition rates spanning several orders of magnitude are reported. after multiple nucleation steps, on average, helix formation proceeds via a kinetic alignment phase in which two or more short, low-entropy helical segments form a more ideal, single-helix structure.
409	166220	article	genome biology	genome biology	\N	biomed central	-64	5	10	2004	\N	2005-04-21 14:38:20	department of biostatistical science, dana-farber cancer institute, 44 binney st, boston, ma 02115, usa. rgentlem@jimmy.harvard.edu	bioconductor: open software development for computational biology and bioinformatics.	the bioconductor project is an initiative for the collaborative creation of extensible software for computational biology and bioinformatics. the goals of the project include: fostering collaborative development and widespread use of innovative software, reducing barriers to entry into interdisciplinary scientific research, and promoting the achievement of remote reproducibility of research results. we describe details of our aims and methods, identify current challenges, compare bioconductor to other open bioinformatics projects, and provide working examples.
410	166875	article	nature methods	\N	\N	nature publishing group	5	2	5	2005	may	2005-06-27 19:56:45	\N	multiple-laboratory comparison of microarray platforms.	microarray technology is a powerful tool for measuring {rna} expression for thousands of genes at once. various studies have been published comparing competing platforms with mixed results: some find agreement, others do not. as the number of researchers starting to use microarrays and the number of cross-platform meta-analysis studies rapidly increases, appropriate platform assessments become more important. here we present results from a comparison study that offers important improvements over those previously described in the literature. in particular, we noticed that none of the previously published papers consider differences between labs. for this study, a consortium of ten laboratories from the washington, {dc-baltimore,} {usa,} area was formed to compare data obtained from three widely used platforms using identical {rna} samples. we used appropriate statistical analysis to demonstrate that there are relatively large differences in data obtained in labs using the same platform, but that the results from the best-performing labs agree rather well.
411	167628	inproceedings	\N	international conference on machine learning	\N	\N	8	\N	\N	1996	\N	2005-04-22 19:41:26	\N	experiments with a new boosting algorithm	in an earlier paper, we introduced a new â€œboostingâ€ algorithm called adaboost which, theoretically, can be used to significantly reduce the error of any learning algorithm that consistently generates classifiers whose performance is a little better than random guessing. we also introduced the related notion of a â€œpseudo-loss â€ which is a method for forcing a learning algorithm of multi-label conceptsto concentrate on the labels that are hardest to discriminate. in this paper, we describe experiments we carried out to assess how well adaboost with and without pseudo-loss, performs on real learning problems. we performed two sets of experiments. the first set compared boosting to breimanâ€™s â€œbagging â€ method when used to aggregate various classifiers (including decision trees and single attribute-value tests). we compared the performance of the two methods on a collection of machine-learning benchmarks. in the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an ocr problem.
412	168520	article	american sociological review	\N	\N	american sociological association	14	32	2	1967	\N	2005-04-23 20:18:09	\N	a framework for the comparative analysis of organizations	complex organizations are conceptualized in terms of their technologies, or the work done on raw materials. two aspects of technology vary independently: the number of exceptions that must be handled, and the degree to which search is an analyzable or unanalyzable procedure. if there is a large number of exceptions and search is not logical and analytic, the technology is described as nonroutine. few exceptions and analyzable search procedures describe a routine technology. two other types result from other combinations--craft and engineering technologies. task structures vary with the technology utilized, and are analyzed in terms of control and coordination and three levels of management. social structure in turn is related to technology and task structure. finally, the variations in three types of goals are weakly related to the preceding variables in this conceptualization. the perspective provides a basis for comparing organizations which avoids many problems found in other schemes utilizing structure, function or goals as the basis for comparison. furthermore, it allows one to selectively utilize competing organizational theories once it is understood that their relevance is restricted to organizations with specific kinds of technologies. the scheme makes apparent some errors in present efforts to compare organizations.
413	168529	article	american sociological review	\N	\N	american sociological association	15	49	2	1984	\N	2005-04-23 20:28:58	\N	structural inertia and organizational change	theory and research on organization-environment relations from a population ecology perspective have been based on the assumption that inertial pressures on structure are strong. this paper attempts to clarify the meaning of structural inertia and to derive propositions about structural inertia from an explicit evolutionary model. the proposed theory treats high levels of structural inertia as a consequence of a selection process rather than as a precondition for selection. it also considers how the strength of inertial forces varies with age, size, and complexity.
414	168559	book	\N	\N	\N	o'reilly	\N	\N	\N	1999	feb	2005-04-23 21:26:47	\N	open sources: voices from the open source revolution ({o'reilly} open source)	<{i>open} sources: voices from the open source {revolution</i}> is a fascinating look at the raging debate that is its namesake. filled with writings from the central players--from linux creator linus torvalds to perl creator larry wall--the book convinces the reader of the overwhelming merits of freeing up the many iterations of software's source code.<p> the open-source movement has become a cause c\\'{e}l\\`{e}bre in light of the widespread adoption of linux, perl, and apache as well as its corporate support from netscape, {ibm}, and oracle--and strongly felt opposition from microsoft. <{i>open} {sources</i}> doesn't address <{i>why</i}> these microsoft foes are throwing their weight behind the movement. instead, it focuses on the history and philosophy of open-source software (previously referred to as <{i>freeware</i}>) as an argument for shaping the future of programming. <{i>open} {sources</i}> is much larger than just a fight with any one company. instead, it is a revolutionary call to release software development from the vested interests that label new directions in software development as threatening.<p> this is not to say that opening the source code is an entirely egalitarian and communistic endeavor. these are programmers and startup owners; they want to be able to continue to program for a living. to that end, <{i>open} {sources</i}> contains strong business profiles from entrepreneurs such as apache's--and now, {o'reilly} \\& {associates'--brian} behlendorf, who discusses how to give away software in order to lure customers in for specialized versions. in many ways, this is a hands-on guide, displaying an insider's view of the development process and providing specifics on testing details and altering licensing agreements. however, interspersed with tech talk is a reader-friendly guide for those interested in the future of software development. <{i>--jennifer} {buckendorff</i}>
415	168970	article	interacting with computers	\N	\N	\N	18	13	2	2000	dec	2005-04-24 14:15:35	\N	what is beautiful is usable	an experiment was conducted to test the relationships between users' perceptions of a computerized system's beauty and usability. the experiment used a computerized application as a surrogate for an automated teller machine ({atm}). perceptions were elicited before and after the participants used the system. pre-experimental measures indicate strong correlations between system's perceived aesthetics and perceived usability. post-experimental measures indicated that the strong correlation remained intact. a multivariate analysis of covariance revealed that the degree of system's aesthetics affected the post-use perceptions of both aesthetics and usability, whereas the degree of actual usability had no such effect. the results resemble those found by social psychologists regarding the effect of physical attractiveness on the valuation of other personality attributes. the findings stress the importance of studying the aesthetic aspect of human–computer interaction ({hci}) design and its relationships to other design dimensions.
416	169545	misc	\N	\N	\N	\N	\N	\N	\N	1997	\N	2005-04-24 15:41:30	\N	a new location technique for the active office	configuration of the computing and communications systems found at home and in the workplace is a complex task that currently requires the attention of the user. researchers have begun to examine computers that would autonomously change their functionality based on observations of who or what was around them. by determining their context, using input from sensor systems distributed throughout the environment, computing devices could personalize themselves to their current user, adapt their behaviour according to their location, or react to their surroundings. the authors present a novel sensor system, suitable for large-scale deployment in indoor environments, which allows the locations of people and equipment to be accurately determined. we also describe some of the context-aware applications that might make use of this fine-grained location information
417	169549	inproceedings	\N	acm transactions on information systems	\N	\N	11	\N	\N	1992	jan	2005-04-24 15:41:31	at\\&t laboratories cambridge	the active badge location system	a novel system for the location of people in an office environment is described. members of staff wear badges that transmit signals providing information about their location to a centralized location service, through a network of sensors. the paper also examines alternative location techniques, system design issues and applications, particularly relating to telephone call routing. location systems raise concerns about the privacy of an individual and these issues are also addressed.
418	169806	article	proceedings of the national academy of sciences of the united states of america	\N	\N	\N	5	100	26	2003	dec	2005-04-25 13:13:14	european molecular biology laboratory, meyerhofstrasse 1, d-69117 heidelberg, germany.	genome evolution reveals biochemical networks and functional modules.	the analysis of completely sequenced genomes uncovers an astonishing variability between species in terms of gene content and order. {d}uring genome history, the genes are frequently rearranged, duplicated, lost, or transferred horizontally between genomes. {t}hese events appear to be stochastic, yet they are under selective constraints resulting from the functional interactions between genes. {t}hese genomic constraints form the basis for a variety of techniques that employ systematic genome comparisons to predict functional associations among genes. {t}he most powerful techniques to date are based on conserved gene neighborhood, gene fusion events, and common phylogenetic distributions of gene families. {h}ere we show that these techniques, if integrated quantitatively and applied to a sufficiently large number of genomes, have reached a resolution which allows the characterization of function at a higher level than that of the individual gene: global modularity becomes detectable in a functional protein network. {i}n {e}scherichia coli, the predicted modules can be benchmarked by comparison to known metabolic pathways. {w}e found as many as 74% of the known metabolic enzymes clustering together in modules, with an average pathway specificity of at least 84%. {t}he modules extend beyond metabolism, and have led to hundreds of reliable functional predictions both at the protein and pathway level. {t}he results indicate that modularity in protein networks is intrinsically encoded in present-day genomes.
419	171407	article	administrative science quarterly	\N	\N	johnson graduate school of management, cornell university	24	17	1	1972	mar	2005-04-26 08:56:30	\N	a garbage can model of organizational choice	organized anarchies are organizations characterized by problematic preferences, unclear technology, and fluid participation. recent studies of universities, a familiar form of organized anarchy, suggest that such organizations can be viewed for some purposes as collections of choices looking for problems, issues and feelings looking for decision situations in which they might be aired, solutions looking for issues to which they might be an answer, and decision makers looking for work. these ideas are translated into an explicit computer simulation model of a garbage can decision process. the general implications of such a model are described in terms of five major measures on the process. possible applications of the model to more narrow predictions are illustrated by an examination of the model's predictions with respect to the effect of adversity on university decision making.
420	171416	article	acm trans. internet technol.	\N	\N	acm	36	5	1	2005	feb	2005-04-26 10:16:06	new york, ny, usa	inside {pagerank}	although the interest of a web page is strictly related to its content and to the subjective readers' cultural background, a measure of the page authority can be provided that only depends on the topological structure of the web. {pagerank} is a noticeable way to attach a score to web pages on the basis of the web connectivity. in this article, we look inside {pagerank} to disclose its fundamental properties concerning stability, complexity of computational scheme, and critical role of parameters involved in the computation. moreover, we introduce a circuit analysis that allows us to understand the distribution of the page score, the way different web communities interact each other, the role of dangling pages (pages with no outlinks), and the secrets for promotion of web pages.
421	171427	inproceedings	\N	nips	\N	\N	6	\N	\N	2000	\N	2005-04-26 14:39:42	\N	feature selection for {svms}	we introduce a method of feature selection for support vector machines.
422	171716	article	the american journal of sociology	\N	\N	\N	48	92	6	1987	\N	2005-04-26 23:02:28	\N	social contagion and innovation: cohesion versus structural equivalence	two classes of network models are used to reanalyze a sociological classic often cited as evidence of social contagion in the diffusion of technological innovation: medical innovation. debate between the cohesion and structural equivalence models poses the following question for study: did the physicians resolve the uncertainty of adopting the new drug through conversations with colleagues (cohesion) or through their perception of the action proper for an occupant of their position in the social structure of colleagues (structural equivalence)? the alternative models are defined, compared, and tested. four conclusions are drawn: (a) contagion was not the dominant factor driving tetracyclene's diffusion. where there is evidence of contagion, there is evidence of personal preferences at work.
423	171730	article	american psychologist	\N	\N	\N	14	53	9	1998	\N	2005-04-26 23:02:30	\N	internet paradox: a social technology that reduces social involvement and psychological well-being?	the internet could change the lives of average citizens as much as did the telephone in the early part of the 20th century and television in the 1950s and 1960s. re- searchers and social critics are debating whether the internet is improving or harming participation in com- munity life and social relationships. this research exam- ined the social and psychological impact of the lnternet on 169 people in 73 households during their first i to 2 years on-line. we used longitudinal data to examine the effects of the internet on social involvement and psycho- logical well-being. in this sample, the internet was used extensively for communication. nonetheless, greater use of the internet was associated with declines in partici- pants'communication with family members in the house- hold, declines in the size of their social circle, and in- creases in their depression and loneliness. these findings have implications for research, for public policy, and for the design of technology.
424	172870	article	nature genetics	\N	\N	nature publishing group	5	37	5	2005	apr	2005-06-08 23:29:46	\N	combinatorial {microrna} target predictions.	micrornas are small noncoding rnas that recognize and bind to partially complementary sites in the 3â€² untranslated regions of target genes in animals and, by unknown mechanisms, regulate protein production of the target transcript1, 2, 3. different combinations of micrornas are expressed in different cell types and may coordinately regulate cell-specific target genes. here, we present pictar, a computational method for identifying common targets of micrornas. statistical tests using genome-wide alignments of eight vertebrate genomes, pictar's ability to specifically recover published microrna targets, and experimental validation of seven predicted targets suggest that pictar has an excellent success rate in predicting targets for single micrornas and for combinations of micrornas. we find that vertebrate micrornas target, on average, roughly 200 transcripts each. furthermore, our results suggest widespread coordinate control executed by micrornas. in particular, we experimentally validate common regulation of mtpn by mir-375, mir-124 and let-7b and thus provide evidence for coordinate microrna control in mammals.
425	173485	article	nat genet	\N	\N	\N	6	30	1	2002	jan	2005-04-28 13:10:19	department of chemistry, university of california los angeles, los angeles, california 90095-1570, usa.	a genomic view of alternative splicing	recent genome-wide analyses of alternative splicing indicate that 40-60\\% of human genes have alternative splice forms, suggesting that alternative splicing is one of the most significant components of the functional complexity of the human genome. here we review these recent results from bioinformatics studies, assess their reliability and consider the impact of alternative splicing on biological functions. although the 'big picture' of alternative splicing that is emerging from genomics is exciting, there are many challenges. high-throughput experimental verification of alternative splice forms, functional characterization, and regulation of alternative splicing are key directions for research. we recommend a community-based effort to discover and characterize alternative splice forms comprehensively throughout the human genome.
426	175025	book	\N	\N	\N	addison-wesley professional	\N	\N	\N	1998	may	2005-04-30 16:58:08	\N	art of computer programming, volume 3: sorting and searching (2nd edition)	the first revision of this third volume is the most comprehensive survey of classical computer techniques for sorting and searching. it extends the treatment of data structures in volume 1 to consider both large and small databases and internal and external memories. the book contains a selection of carefully checked computer methods, with a quantitative analysis of their efficiency. outstanding features of the second edition include a revised section on optimum sorting and new discussions of the theory of permutations and of universal hashing.
427	175375	article	bioinformatics	\N	\N	oxford university press	9	21	9	2005	may	2010-01-03 16:48:09	\N	co-occurrence based meta-analysis of scientific texts: retrieving biological relationships between genes	motivation: the advent of high-throughput experiments in molecular biology creates a need for methods to efficiently extract and use information for large numbers of genes. recently, the associative concept space ({acs}) has been developed for the representation of information extracted from biomedical literature. the {acs} is a euclidean space in which thesaurus concepts are positioned and the distances between concepts indicates their relatedness. the {acs} uses co-occurrence of concepts as a source of information. in this paper we evaluate how well the system can retrieve functionally related genes and we compare its performance with a simple gene co-occurrence method.  results: to assess the performance of the {acs} we composed a test set of five groups of functionally related genes. with the {acs} good scores were obtained for four of the five groups. when compared to the gene co-occurrence method, the {acs} is capable of revealing more functional biological relations and can achieve results with less literature available per gene. hierarchical clustering was performed on the {acs} output, as a potential aid to users, and was found to provide useful clusters. our results suggest that the algorithm can be of value for researchers studying large numbers of genes.  availability: the {acs} program is available upon request from the authors.  contact: r.jelier@erasmusmc.nl 10.1093/bioinformatics/bti268
428	175909	electronic	\N	\N	\N	\N	\N	\N	\N	2005	apr	2005-05-02 08:09:57	\N	classical mechanics	an overview of the foundations of classical mechanics
429	179333	article	journal of the royal statistical society b	\N	\N	\N	19	64	Part 3	2002	\N	2005-05-03 15:13:16	\N	a direct approach to false discovery rates	multiple&#150;hypothesis testing involves guarding against much more complicated errors than single&#150;hypothesis testing. whereas we typically control the type i error rate for a single&#150;hypothesis test, a compound error rate is controlled for multiple&#150;hypothesis tests. for example, controlling the false discovery rate fdr traditionally involves intricate sequential p&#150;value rejection methods based on the observed data. whereas a sequential p&#150;value method fixes the error rate and estimates its corresponding rejection region, we propose the opposite approach&#150;we fix the rejection region and then estimate its corresponding error rate. this new approach offers increased applicability, accuracy and power. we apply the methodology to both the positive false discovery rate pfdr and fdr, and provide evidence for its benefits. it is shown that pfdr is probably the quantity of interest over fdr. also discussed is the calculation of the q&#150;value, the pfdr analogue of the p&#150;value, which eliminates the need to set the error rate beforehand as is traditionally done. some simple numerical examples are presented that show that this new approach can yield an increase of over eight times in power compared with the benjamini&#150;hochberg fdr method.
430	179399	article	cell	\N	\N	\N	7	110	\N	2002	\N	2005-05-03 15:13:17	\N	prediction of plant {microrna} targets	we predict regulatory targets for 14 arabidopsis micrornas (mirnas) by identifying mrnas with near complementarity. complementary sites within predicted targets are conserved in rice. of the 49 predicted targets, 34 are members of transcription factor gene families involved in developmental patterning or cell differentiation. the near-perfect complementarity between plant mirnas and their targets suggests that many plant mirnas act similarly to small interfering rnas and direct mrna cleavage. the targeting of developmental transcription factors suggests that many plant mirnas function during cellular differentiation to clear key regulatory transcripts from daughter cell lineages.
431	179467	inproceedings	\N	proceedings of the twenty-first acm sigmod-sigact-sigart symposium on principles of database systems	pods	acm	13	\N	\N	2002	\N	2005-05-03 15:22:19	new york, ny, usa	data integration: a theoretical perspective	data integration is the problem of combining data residing at different sources, and providing the user with a unified view of these data. the problem of designing data integration systems is important in current real world applications, and is characterized by a number of issues that are interesting from a theoretical point of view. this document presents on overview of the material to be presented in a tutorial on data integration. the tutorial is focused on some of the theoretical issues that are relevant for data integration. special attention will be devoted to the following aspects: modeling a data integration application, processing queries in data integration, dealing with inconsistent data sources, and reasoning on queries.
432	179598	inproceedings	\N	proceedings of the 22nd annual conference on computer graphics and interactive techniques	siggraph	acm	9	\N	\N	1995	\N	2005-05-03 17:21:22	new york, ny, usa	fast multiresolution image querying	an abstract is not available.
433	179603	book	\N	\N	\N	oxford university press, inc.	\N	\N	\N	1995	\N	2005-05-03 18:05:39	\N	neural networks for pattern recognition	{this book provides a solid statistical foundation for neural networks from a pattern recognition  perspective. the focus is on the types of neural nets that are most widely used in practical applications,  such as the multi-layer perceptron and radial basis function networks. rather than trying to cover many  different types of neural networks, bishop thoroughly covers topics such as density estimation, error  functions, parameter optimization algorithms, data pre-processing, and bayesian methods. all topics are  organized well and all mathematical foundations are explained before being applied to neural networks.  the text is suitable for a graduate or advanced undergraduate level course on neural networks or for  practitioners interested in applying neural networks to real-world problems. the reader is assumed to have  the level of math knowledge necessary for an undergraduate science degree.} {this is the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition. after introducing the basic concepts, the book examines techniques for modelling probability density functions and the properties and merits of the multi-layer perceptron and radial basis function network models. also covered are various forms of error functions, principal algorithms for error function minimalization, learning and generalization in neural networks, and bayesian techniques and their applications.  designed as a text, with over 100 exercises, this fully up-to-date work will benefit anyone involved in the fields of neural computation and pattern recognition.}
434	180259	inproceedings	\N	siggraph	\N	acm	7	\N	\N	1996	\N	2005-05-04 21:56:08	new york, ny, usa	{sketch}: an interface for sketching {3d} scenes	note: {ocr} errors may be found in this reference list extracted from the full text article.  {acm} has opted to expose the complete list rather than only correct and linked references.
435	180994	article	journal of the american statistical association	\N	\N	\N	\N	\N	\N	1996	\N	2005-05-06 03:30:14	\N	the selection of prior distributions by formal rules	subjectivism has become the dominant philosophical foundation for bayesian inference. yet in practice, most bayesian anlyses are performed with so-called "noninformative" priors, that is, priors constructed by some formal rule. we review the plethora of techniques for constructing such priors and discuss some of the practical and philosophical issues that arise when they are used. we give special emphasis to jeffrey's rules and discuss the evolution of his viewpoint about the interpretation of priors, away from unique representation of ignorance toward teh nition that they should be chosen by convention. we conclude that the problems raised by the research on priors chosen by formal rules are serious and may not be dismissed lightly: when sample sizes are small (relative to the number of parameters being estimated), it is dangerous to put faith in any "default" solution; but when asymptitvs take over, jeffrey's rules and their variants remain reasonable choices. we also provide an annotated bibliography.
436	181740	article	communications of the acm	\N	\N	\N	9	43	8	2000	\N	2005-05-06 16:28:47	\N	automatic personalization based on web usage mining	this paper we describe an approach to usage-based web personalization taking into account the full spectrum of web mining techniques and activities. our approach is described by the architecture shown in figure 1, which heavily uses data mining techniques, thus making the personalization process both automatic and dynamic, and hence up-to-date. specifically, we have developed techniques for preprocessing of web usage logs and grouping url references into sets called user transactions [cms99]. a ...
437	181748	misc	\N	\N	\N	\N	\N	\N	\N	-1	\N	2005-05-06 16:35:46	\N	web mining	the world wide web is an enormous information repository, but in order to efficiently access the information contained in it, sophisticated software is needed. this software is to a large extent based on data mining technology. in this section we start with a brief discussion of the nature of the web and the specific problems encountered when trying to find information. we then give an overview of the many techniques that exist for web mining, illustrated with applications of these techniques....
438	186777	article	educational psychologist	\N	\N	\N	15	31	3/4	1996	\N	2005-05-09 04:21:29	\N	constructivist, emergent, and sociocultural perspectives in the context of developmental research	our overall intent is to clarify relations between the psychological constructivist, sociocultural, and emergent perspectives. we provide a grounding for the comparisons in the first part of the article by outlining an interpretive framework that we developed in the course of a classroom-based research project. at this level of classroom processes, the framework involves an emergent approach in which psychological constructivist analyses of individual activity are coordinated with interactionist analyses of classroom interactions and discourse. in the second part of the article, we describe an elaboration of the framework that locates classroom processes in school and societal contexts. the perspective taken at this level is broadly sociocultural and focuses on the influence of indlividuals' participation in culturally organized practices. in the third part of the article, we use the discussion of the framework as a backdrop against which to compare and contrast the three theoretical perspectives. we discuss how the emergent approach augments the psychological constructivist perspective by making it possible to locate analyses of individual students' constructive activities in social context. in addition, we consider the purposes for which the emergent and sociocultural perspectives might be particularly appropriate and observe that they together offer characterizations of individual students' activities, the classroom community, and broader communities of practice.
439	187152	article	nature	\N	\N	nature publishing group	9	422	6934	2003	apr	2005-05-09 18:11:46	whitehead institute center for genome research, 320 charles street, cambridge, massachusetts 02141, usa. jgalag@mit.edu	the genome sequence of the filamentous fungus neurospora crassa	neurospora crassa is a central organism in the history of twentieth-century genetics, biochemistry and molecular biology. here, we report a high-quality draft sequence of the n. crassa genome. the approximately 40-megabase genome encodes about 10,000 protein-coding genes—more than twice as many as in the fission yeast schizosaccharomyces pombe and only about 25\\% fewer than in the fruitfly drosophila melanogaster. analysis of the gene set yields insights into unexpected aspects of neurospora biology including the identification of genes potentially associated with red light photobiology, genes implicated in secondary metabolism, and important differences in ca2+ signalling as compared with plants and animals. neurospora possesses the widest array of genome defence mechanisms known for any eukaryotic organism, including a process unique to fungi called repeat-induced point mutation ({rip}). genome analysis suggests that {rip} has had a profound impact on genome evolution, greatly slowing the creation of new genes through genomic duplication and resulting in a genome with an unusually low proportion of closely related genes.
440	190986	article	journal of molecular biology	\N	\N	\N	21	267	3	1997	apr	2005-05-10 16:18:35	department of information studies and krebs institute for biomolecular research, university of sheffield, western bank, uk.	development and validation of a genetic algorithm for flexible docking.	prediction of small molecule binding modes to macromolecules of known three-dimensional structure is a problem of paramount importance in rational drug design (the "docking" problem). we report the development and validation of the program {gold} (genetic optimisation for ligand docking). {gold} is an automated ligand docking program that uses a genetic algorithm to explore the full range of ligand conformational flexibility with partial flexibility of the protein, and satisfies the fundamental requirement that the ligand must displace loosely bound water on binding. numerous enhancements and modifications have been applied to the original technique resulting in a substantial increase in the reliability and the applicability of the algorithm. the advanced algorithm has been tested on a dataset of 100 complexes extracted from the brookhaven protein {databank}. when used to dock the ligand back into the binding site, {gold} achieved a 71\\% success rate in identifying the experimental binding mode.
441	191640	article	american journal of sociology	\N	\N	\N	25	94	\N	1998	\N	2005-05-11 02:28:01	\N	social capital in the creation of human capital	in this paper, the concept of social capital is introduced and illustrated, its forms are described, thr social structural conditions under which it arises are examined, and it is using in an analysis of dropouts from high school. use of the concept of social capital is part of a general theoretical strategy discussed in the paper: taking rational action as a starting point, but rejecting the extreme individualistic premises that often accompany it. the conception of social capital as a resource for action is one way of introducing social structure into the rational action paradigm. three forms of social capital are examined: obligations and expectations, information channels, and social norms. the role of closure in the social structure in facilitating the first and third of these forms of social capital is described. an analysis of the effect of the lack of social capital available to high school sophomores on dropping out of school before graduation is carried out. the effect of social capital within the family and in the community outside the family is examined.
442	192188	article	political analysis	\N	\N	\N	19	10	\N	2002	\N	2005-05-11 02:28:12	\N	dynamic ideal point estimation via markov chain monte carlo for the {u.s}. supreme court, 1953-1999	at the heart of attitudinal and strategic explanations of judicial behavior is the assumption that justices have policy preferences. in this paper we employ markov chain monte carlo methods to fit a bayesian measurement model of ideal points for all justices serving on the u.s. supreme court from 1953 through 1999. we are particularly interested in determining to what extent ideal points of justices change throughout their tenure on the court. this is important because judicial politics scholars oftentimes invoke preference measures that are time invariant. to investigate preference change, we posit a dynamic item response model that allows ideal points to change systematically over time. additionally, we introduce bayesian methods for fitting multivariate dynamic linear models to political scientists. our results suggest that many justices do not have temporally constant ideal points. moreover, our ideal point estimates outperform existing measures and explain judicial behavior quite well across civil rights, civil liberties, economics, and federalism cases. 10.1093/pan/10.2.134
443	193082	article	science	\N	\N	\N	\N	199	\N	1978	\N	2005-05-11 13:01:12	\N	diversity in tropical rain forests and coral reefs	the commonly observed high diversity of trees in tropical rain forests and corals on tropical reefs is a nonequilibrium state which, if not disturbed further, will progress toward a low-diversity equilibrium community. this may not happen if gradual changes in climate favor different species. if equilibrium is reached, a lesser degree of diversity may be sustained by niche diversification or by a compensatory mortality that favors inferior competitors. however, tropical forests and reefs are subject to severe disturbances often enough that equilibrium may never be attained. 10.1126/science.199.4335.1302
444	197224	article	structure fold des	\N	\N	\N	13	7	9	1999	sep	2005-05-11 19:34:47	protein structure group department of biological sciences university of warwick coventry, cv4 7al, uk.	a systematic comparison of protein structure classifications: {scop}, {cath} and {fssp}.	{background}: several methods of structural classification have been developed to introduce some order to the large amount of data present in the protein data bank. such methods facilitate structural comparisons and provide a greater understanding of structure and function. the most widely used and comprehensive databases are {scop}, {cath} and {fssp}, which represent three unique methods of classifying protein structures: purely manual, a combination of manual and automated, and purely automated, respectively. in order to develop reliable template libraries and benchmarks for protein-fold recognition, a systematic comparison of these databases has been carried out to determine their overall agreement in classifying protein structures. {results}: approximately two-thirds of the protein chains in each database are common to all three databases. despite employing different methods, and basing their systems on different rules of protein structure and taxonomy, {scop}, {cath} and {fssp} agree on the majority of their classifications. discrepancies and inconsistencies are accounted for by a small number of explanations. other interesting features have been identified, and various differences between manual and automatic classification methods are presented. {conclusions}: using these databases requires an understanding of the rules upon which they are based; each method offers certain advantages depending on the biological requirements and knowledge of the user. the degree of discrepancy between the systems also has an impact on reliability of prediction methods that employ these schemes as benchmarks. to generate accurate fold templates for threading, we extract information from a consensus database, encompassing agreements between {scop}, {cath} and {fssp}.
445	197226	article	nature genetics	nat genet	\N	nature publishing group	8	36	10	2004	oct	2005-05-11 19:45:16	stanford center for biomedical ethics, 701a welch road, suite 1105, palo alto, california 94304, usa. micho@stanford.edu <micho@stanford.edu>	a module map showing conditional activity of expression modules in cancer.	dna microarrays are widely used to study changes in gene expression in tumors, but such studies are typically system-specific and do not address the commonalities and variations between different types of tumor. here we present an integrated analysis of 1,975 published microarrays spanning 22 tumor types. we describe expression profiles in different tumors in terms of the behavior of modules, sets of genes that act in concert to carry out a specific function. using a simple unified analysis, we extract modules and characterize gene-expression profiles in tumors as a combination of activated and deactivated modules. activation of some modules is specific to particular types of tumor; for example, a growth-inhibitory module is specifically repressed in acute lymphoblastic leukemias and may underlie the deregulated proliferation in these cancers. other modules are shared across a diverse set of clinical conditions, suggestive of common tumor progression mechanisms. for example, the bone osteoblastic module spans a variety of tumor types and includes both secreted growth factors and their receptors. our findings suggest that there is a single mechanism for both primary tumor proliferation and metastasis to bone. our analysis presents multiple research directions for diagnostic, prognostic and therapeutic studies. {$[$}abstract from author{$]$}
446	197257	book	\N	\N	\N	the mit press	\N	\N	\N	2003	sep	2005-05-11 21:25:50	\N	rules of play: game design fundamentals	publisher's description: as pop culture, games are as important as film or television--but game design has yet to develop a theoretical framework or critical vocabulary. in rules of play katie salen and eric zimmerman present a much-needed primer for this emerging field. they offer a unified model for looking at all kinds of games, from board games and sports to computer and video games. as active participants in game culture, the authors have written rules of play as a catalyst for innovation, filled with new concepts, strategies, and methodologies for creating and understanding games. building an aesthetics of interactive systems, salen and zimmerman define core concepts like "play," "design," and "interactivity." they look at games through a series of eighteen "game design schemas," or conceptual frameworks, including games as systems of emergence and information, as contexts for social play, as a storytelling medium, and as sites of cultural resistance. written for game scholars, game developers, and interactive designers, rules of play is a textbook, reference book, and theoretical guide. it is the first comprehensive attempt to establish a solid theoretical framework for the emerging discipline of game design.
447	197288	book	\N	\N	\N	{the mit press}	\N	\N	\N	1998	nov	2005-05-11 21:42:47	\N	from barbie to mortal kombat: gender and computer games	{this book explores the complicated issue of gender in computer games\\&\\#173;-particularly the development of video games for girls. one side is the concern that the average computer game, being attractive primarily to boys, furthers the technology access gap between the genders. yet attempts to create computer games that girls want to play brings about another set of concerns: should games be gendered at all? and does having boys' games and girls' games merely reinforce the way gender differences are socialized in play?<p> cassell and jenkins have gathered the thoughts of several feminist and media scholars to explore the issues from multiple perspectives, but this is not a work confined to ivory-tower theorizing. alongside the philosophical explorations are pragmatic investigations of the hard-nosed, real world of computer-game manufacture and sales. particularly enlightening is a section featuring interviews with several leading creators of games for girls. and while all agree that it's good to be past the days when women in computer games were limited to scantily clad background figures or damsels in distress, the visions of an appropriate future are both diverse and well defended. there is no pretense here of easy answers, but there are many excellent questions. <i>--elizabeth lewis</i>}
448	197344	article	bioinformatics (oxford, england)	\N	\N	\N	9	20	2	2004	jan	2005-05-11 22:27:25	berkeley phylogenomics group, department of bioengineering, university of california, 473 evans hall 1762, berkeley, ca 94720-1762, usa. kimmen@uclink.berkeley.edu	phylogenomic inference of protein molecular function: advances and challenges.	{motivation}: protein families evolve a multiplicity of functions through gene duplication, speciation and other processes. as a number of studies have shown, standard methods of protein function prediction produce systematic errors on these data. phylogenomic analysis--combining phylogenetic tree construction, integration of experimental data and differentiation of orthologs and paralogs--has been proposed to address these errors and improve the accuracy of functional classification. the explicit integration of structure prediction and analysis in this framework, which we call structural phylogenomics, provides additional insights into protein superfamily evolution. {results}: results of protein functional classification using phylogenomic analysis show fewer expected false positives overall than when pairwise methods of functional classification are employed. we present an overview of the motivations and fundamental principles of phylogenomic analysis, new methods developed for the key tasks, benchmark datasets for these tasks (when available) and suggest procedures to increase accuracy. we also discuss some of the methods used in the celera genomics high-throughput phylogenomic classification of the human genome. {availability}: software tools from the berkeley phylogenomics group are available at http://phylogenomics.berkeley.edu
449	197425	article	j. comput. chem.	\N	\N	wiley subscription services, inc., a wiley company	17	25	9	2004	jul	2005-05-12 10:12:01	encysive pharmaceuticals inc., 7000 fannin, houston, texas 77030; novartis institutes for biomedical research, basle, wsj-88.10.14, p.o. box, ch-4002 basle, switzerland; department of molecular biology, tpc15, the scripps research institute, 10550 n. torrey pines rd., la jolla, california 92037	development and testing of a general amber force field	we describe here a general amber force field ({gaff}) for organic molecules. {gaff} is designed to be compatible with existing amber force fields for proteins and nucleic acids, and has parameters for most organic and pharmaceutical molecules that are composed of h, c, n, o, s, p, and halogens. it uses a simple functional form and a limited number of atom types, but incorporates both empirical and heuristic models to estimate force constants and partial atomic charges. the performance of {gaff} in test cases is encouraging. in test i, 74 crystallographic structures were compared to {gaff} minimized structures, with a root-mean-square displacement of 0.26 \\aa{}, which is comparable to that of the tripos 5.2 force field (0.25 \\aa{}) and better than those of {mmff} 94 and {charmm} (0.47 and 0.44 \\aa{}, respectively). in test {ii}, gas phase minimizations were performed on 22 nucleic acid base pairs, and the minimized structures and intermolecular energies were compared to {mp2}/{6-31g}* results. the {rms} of displacements and relative energies were 0.25 \\aa{} and 1.2 kcal/mol, respectively. these data are comparable to results from {parm99/resp} (0.16 \\aa{} and 1.18 kcal/mol, respectively), which were parameterized to these base pairs. test {iii} looked at the relative energies of 71 conformational pairs that were used in development of the parm99 force field. the {rms} error in relative energies (compared to experiment) is about 0.5 kcal/mol. {gaff} can be applied to wide range of molecules in an automatic fashion, making it suitable for rational drug design and database searching. {\\copyright} 2004 wiley periodicals, inc. j comput chem 25: 1157–1174, 2004
450	197446	article	journal of physical chemistry	\N	\N	\N	11	97	40	1993	oct	2005-05-12 11:22:40	\N	a {well-behaved} electrostatic potential based method using charge restraints for deriving atomic charges: the {resp} model	we present a new approach to generating electrostatic potential (esp) derived charges for molecules. the major strength of electrostatic potential derived charges is that they optimally reproduce the intermolecular interaction properties of molecules with a simple two-body additive potential, provided, of course, that a suitably accurate level of quantum mechanical calculation is used to derive the e s p around the molecule. previously, the major weaknesses of these charges have been that they were not easily transferable between common functional groups in related molecules, they have often been conformationally dependent, and the large charges that frequently occur can be problematic for simulating intramolecular interactions. introducing restraints in the form of a penalty function into the fitting process considerably reduces the above problems, with only a minor decrease in the quality of the fit to the quantum mechanical esp. several other refinements in addition to the restrained electrostatic potential (resp) fit yield a general and algorithmic charge fitting procedure for generating atom-centered point charges. this approach can thus be recommended for general use in molecular mechanics, molecular dynamics, and free energy calculations for any organic or bioorganic system.
451	197592	article	journal of the american medical informatics association	\N	\N	\N	8	9	6	2002	nov	2005-05-12 18:52:55	department of genetics, stanford medical informatics, stanford, california 94305, usa.	creating an online dictionary of abbreviations from {medline}	objective the growth of the biomedical literature presents special challenges for both human readers and automatic algorithms. one such challenge derives from the common and uncontrolled use of abbreviations in the literature. each additional abbreviation increases the effective size of the vocabulary for a field. therefore, to create an automatically generated and maintained lexicon of abbreviations, we have developed an algorithm to match abbreviations in text with their expansions.design our method uses a statistical learning algorithm, logistic regression, to score abbreviation expansions based on their resemblance to a training set of human-annotated abbreviations. we applied it to medstract, a corpus of medline abstracts in which abbreviations and their expansions have been manually annotated. we then ran the algorithm on all abstracts in medline, creating a dictionary of biomedical abbreviations. to test the coverage of the database, we used an independently created list of abbreviations from the china medical tribune.measurements we measured the recall and precision of the algorithm in identifying abbreviations from the medstract corpus. we also measured the recall when searching for abbreviations from the china medical tribune against the database.results on the medstract corpus, our algorithm achieves up to 83\\% recall at 80\\% precision. applying the algorithm to all of medline yielded a database of 781,632 high-scoring abbreviations. of all the abbreviations in the list from the china medical tribune, 88\\% were in the database.conclusion we have developed an algorithm to identify abbreviations from text. we are making this available as a public abbreviation server at \\url{http://abbreviation.stanford.edu/}.
452	200385	electronic	\N	\N	\N	\N	\N	\N	\N	2001	jun	2005-05-15 04:17:11	\N	determining the density of states for classical statistical models: a random walk algorithm to produce a flat histogram	we describe an efficient monte carlo algorithm using a random walk in energy space to obtain a very accurate estimate of the density of states for classical statistical models. the density of states is modified at each step when the energy level is visited to produce a flat histogram. by carefully controlling the modification factor, we allow the density of states to converge to the true value very quickly, even for large systems. from the density of states at the end of the random walk, we can estimate thermodynamic quantities such as internal energy and specific heat capacity by calculating canonical averages at any temperature. using this method, we not only can avoid repeating simulations at multiple temperatures, but we can also estimate the free energy and entropy, quantities that are not directly accessible by conventional monte carlo simulations. this algorithm is especially useful for complex systems with a rough landscape since all possible energy levels are visited with the same probability. as with the multicanonical monte carlo technique, our method overcomes the tunneling barrier between coexisting phases at first-order phase transitions. in this paper, we apply our algorithm to both first- and second-order phase transitions to demonstrate its efficiency and accuracy. we obtained direct simulational estimates for the density of states for two-dimensional ten-state potts models on lattices up to  200Ã—200  and ising models on lattices up to  256Ã—256.  our simulational results are compared to both exact solutions and existing numerical data obtained using other methods. applying this approach to a three-dimensional  Â± j  spin-glass model, we estimate the internal energy and entropy at zero temperature; and, using a two-dimensional random walk in energy and order-parameter space, we obtain the (rough) canonical distribution and energy landscape in order-parameter space. preliminary data suggest that the glass transition temperature is about  1.2  and that better estimates can be obtained with more extensive application of the method. this simulational method is not restricted to energy space and can be used to calculate the density of states for any parameter by a random walk in the corresponding space.
453	200747	book	\N	\N	\N	mcgraw-hill	\N	\N	\N	1997	oct	2005-05-15 17:50:32	\N	growing up digital: the rise of the net generation	{don tapscott, author of <i>the digital  economy</i>, turns his attention to the way young people--surrounded by high-tech toys and tools from birth--will likely affect the future. in <i>growing up digital: the rise of the net generation</i>, tapscott parlays some 300 interviews into predictions on how today's 2- to 22-year-olds might reshape society. his observations about this enormously influential population, which will total 88 million in north america alone by the year 2000, range from the kind of employees they may eventually be to how they could be reached by marketers.}
454	200770	book	\N	\N	\N	{zone books}	\N	\N	\N	2002	jun	2005-05-15 18:15:07	\N	publics and counterpublics	{most of the people around us belong to our world not directly, as kin or comrades, but as strangers. how do we recognize them as members of our world? we are related to them as transient participants in common publics. indeed, most of us would find it nearly impossible to imagine a social world without publics. in the eight essays in this book, michael warner addresses the question: what is a public?<br /> <br /> according to warner, the idea of a public is one of the central fictions of modern life. publics have powerful implications for how our social world takes shape, and much of modern life involves struggles over the nature of publics and their interrelations. the idea of a public contains ambiguities, even contradictions. as it is extended to new contexts, politics, and media, its meaning changes in ways that can be difficult to uncover.<br /> <br /> combining historical analysis, theoretical reflection, and extensive case studies, warner shows how the idea of a public can reframe our understanding of contemporary literary works and politics and of our social world in general. in particular, he applies the idea of a public to the junction of two intellectual traditions: public-sphere theory and queer theory.}
455	201547	article	quarterly review of biology	\N	\N	\N	22	\N	46	1971	\N	2005-05-16 17:16:14	\N	the evolution of reciprocal altruism	a model is presented to account for the natural selection of what is termed reciprocally altruistic behavior. the model shows how selection can operate against the cheater (non-reciprocator) in the system. three instances of altruistic behavior are discussed, the evolution of which the model can explain: (1) behavior involved in cleaning symbioses; (2) warning cries in birds; and (3) human reciprocal altruism. regarding human reciprocal altruism, it is shown that the details of the psychological system that regulates this altruism can be explained by the model. specifically, friendship, dislike, moralistic aggression, gratitude, sympathy, trust, suspicion, trustworthiness, aspects of guilt, and some forms of dishonesty and hypocrisy can be explained as important adaptations to regulate the altruistic system. each individual human is seen as possessing altruistic and cheating tendencies, the expression of which is sensitive to developmental variables that were selected to set the tendencies at a balance appropriate to the local social and ecological environment.
456	201597	article	commun. acm	\N	\N	acm	9	35	12	1992	dec	2005-05-16 18:36:00	new york, ny, usa	information filtering and information retrieval: two sides of the same coin?	an abstract is not available.
457	201704	inproceedings	\N	proceedings of the 15th conference on uncertainty in artificial intelligence	\N	\N	12	\N	\N	1999	\N	2005-05-17 00:48:34	\N	loopy belief propagation for approximate inference: an empirical study	recently, researchers have demonstrated that &amp;quot;loopy belief propagation &amp;quot;--- the use of pearl&#039;s polytree algorithm in a bayesian network with loops--- can perform well in the context of error-correcting codes. the most dramatic instance of this is the near shannon-limit performance of &amp;quot;turbo codes &amp;quot;--- codes whose decoding algorithm is equivalent to loopy belief propagation in a chain-structured bayesian network. in this paper we ask: is there something special about the error-correcting code context, or does loopy propagation work as an approximate inference scheme in a more general setting? we compare the marginals computed using loopy propagation to the exact ones in four bayesian network architectures, including two real-world networks: alarm and qmr. we find that the loopy beliefs often converge and when they do, they give a good approximation to the correct marginals. however, on the qmr network, the loopy beliefs oscillated and had no obvious relationship to the correct posteriors. we present some initial investigations into the cause of these oscillations, and show that some simple methods of preventing them lead to the wrong results. 1
458	201777	article	journal of functional programming	\N	\N	cambridge university press	5	7	05	1997	sep	2005-05-17 10:39:21	new york, ny, usa	the zipper	almost every programmer has faced the problem of representing a tree together with a subtree that is the focus of attention, where that focus may move left, right, up or down the tree. the zipper is huet\\&apos;s nifty name for a nifty data structure which fulfills this need. i wish i had known of it when i faced this task, because the solution i came up with was not quite so efficient or elegant as the zipper.
459	201791	inproceedings	\N	19-th acm symposium on operating systems principles	\N	\N	\N	\N	\N	2003	oct	2005-05-17 11:02:05	bolton landing, ny, usa	measurement, modeling, and analysis of a {peer-to-peer} {file-sharing} workload	peer-to-peer (p2p) file sharing accounts for an astonishing volume of current internet traffic. this paper probes deeply into modern p2p file sharing systems and the forces that drive them. by doing so, we seek to increase our understanding of p2p file sharing workloads and their implications for future multimedia workloads. our research uses a three-tiered approach. first, we analyze a 200-day trace of over 20 terabytes of kazaa p2p traffic collected at the university of washington. second, we develop a model of multimedia workloads that lets us isolate, vary, and explore the impact of key system parameters. our model, which we parameterize with statistics from our trace, lets us confirm various hypotheses about file-sharing behavior observed in the trace. third, we explore the potential impact of locality-awareness in kazaa.our results reveal dramatic differences between p2p file sharing and web traffic. for example, we show how the immutability of kazaa's multimedia objects leads clients to fetch objects at most once; in contrast, a world-wide web client may fetch a popular page (e.g., cnn or google) thousands of times. moreover, we demonstrate that: (1) this "fetch-at-most-once" behavior causes the kazaa popularity distribution to deviate substantially from zipf curves we see for the web, and (2) this deviation has significant implications for the performance of multimedia file-sharing systems. unlike the web, whose workload is driven by document change, we demonstrate that clients' fetch-at-most-once behavior, the creation of new objects, and the addition of new clients to the system are the primary forces that drive multimedia workloads such as kazaa. we also show that there is substantial untapped locality in the kazaa workload. finally, we quantify the potential bandwidth savings that locality-aware p2p file-sharing architectures would achieve.
460	201820	article	user modeling and user-adapted interaction	\N	\N	\N	\N	\N	\N	2002	\N	2005-05-17 11:52:14	\N	hybrid recommender systems: survey and experiments	recommender systems represent user preferences for the purpose of suggesting items to purchase or examine. they have become fundamental applications in electronic commerce and information access, providing suggestions that effectively prune large information spaces so that users are directed toward those items that best meet their needs and preferences. a variety of techniques have been proposed for performing recommendation, including content-based, collaborative, knowledge-based and other techniques. to improve performance, these methods have sometimes been combined in hybrid recommenders. this paper surveys the landscape of actual and possible hybrid recommenders, and introduces a novel hybrid, entreec, a system that combines knowledge-based recommendation and collaborative filtering to recommend restaurants. further, we show that semantic ratings obtained from the knowledge-based part of the system enhance the effectiveness of collaborative filtering.
461	201830	inproceedings	\N	{aaai}/{iaai}	\N	\N	6	\N	\N	1998	\N	2005-05-17 11:52:14	\N	recommendation as classification: using social and {content-based} information in recommendation	recommendation systems make suggestions about artifacts to a user. for instance, they may predict whether a user would be interested in seeing a particular movie. social recomendation methods collect ratings of artifacts from many individuals and use nearest-neighbor techniques to make recommendations to a user concerning new artifacts. however, these methods do not use the significant amount of other information that is often available about the nature of each artifact --- such as cast lists or movie reviews, for example. this paper presents an inductive learning approach to recommendation that is able to use both ratings information and other forms of information about each artifact in predicting user preferences. we show that our method outperforms an existing social-filtering method in the domain of movie recommendations on a dataset of more than 45,000 movie ratings collected from a community of over 250 users.  introduction  recommendations are a part of everyday life. we usually...
462	202252	article	chaos	\N	\N	\N	13	11	1	2001	mar	2005-05-18 05:09:51	center for biodynamics and department of biomedical engineering, boston university, 44 cummington st., boston, massachusetts 02215.	designer gene networks: towards fundamental cellular control.	the engineered control of cellular function through the design of synthetic genetic networks is becoming plausible. here we show how a naturally occurring network can be used as a parts list for artificial network design, and how model formulation leads to computational and analytical approaches relevant to nonlinear dynamics and statistical physics. we first review the relevant work on synthetic gene networks, highlighting the important experimental findings with regard to genetic switches and oscillators. we then present the derivation of a deterministic model describing the temporal evolution of the concentration of protein in a single-gene network. bistability in the steady-state protein concentration arises naturally as a consequence of autoregulatory feedback, and we focus on the hysteretic properties of the protein concentration as a function of the degradation rate. we then formulate the effect of an external noise source which interacts with the protein degradation rate. we demonstrate the utility of such a formulation by constructing a protein switch, whereby external noise pulses are used to switch the protein concentration between two values. following the lead of earlier work, we show how the addition of a second network component can be used to construct a relaxation oscillator, whereby the system is driven around the hysteresis loop. we highlight the frequency dependence on the tunable parameter values, and discuss design plausibility. we emphasize how the model equations can be used to develop design criteria for robust oscillations, and illustrate this point with parameter plots illuminating the oscillatory regions for given parameter values. we then turn to the utilization of an intrinsic cellular process as a means of controlling the oscillations. we consider a network design which exhibits self-sustained oscillations, and discuss the driving of the oscillator in the context of synchronization. then, as a second design, we consider a synthetic network with parameter values near, but outside, the oscillatory boundary. in this case, we show how resonance can lead to the induction of oscillations and amplification of a cellular signal. finally, we construct a toggle switch from positive regulatory elements, and compare the switching properties for this network with those of a network constructed using negative regulation. our results demonstrate the utility of model analysis in the construction of synthetic gene regulatory networks. (c) 2001 american institute of physics.
463	202349	book	\N	\N	\N	oxford university press	\N	\N	\N	1988	\N	2005-05-18 08:55:30	oxford	when old technologies were new: thinking about electric communication in the late nineteenth century	{ in the history of electronic communication, the last quarter of the nineteenth century holds a special place, for it was during this period that the telephone, phonograph, electric light, wireless, and cinema were all invented.  in when old technologies were new, carolyn marvin explores how<br>two of these new inventions--the telephone and the electric light--were publicly envisioned at the end of the nineteenth century, as seen in specialized engineering journals and popular media.  marvin pays particular attention to the telephone, describing how it disrupted established social<br>relations, unsettling customary ways of dividing the private person and family from the more public setting of the community. on the lighter side, she describes how people spoke louder when calling long distance, and how they worried about catching contagious diseases over the phone. a particularly<br>powerful chapter deals with telephonic precursors of radio broadcasting--the "telephone herald" in new york and the "telefon hirmondo" of hungary--and the conflict between the technological development of broadcasting and the attempt to impose a homogenous, ethnocentric variant of anglo-saxon<br>culture on the public. while focusing on the way professionals in the electronics field tried to control the new media, marvin also illuminates the broader social impact, presenting a wide-ranging, informative, and entertaining account of the early years of electronic media. }
464	202480	article	manage. sci.	\N	\N	informs	21	35	8	1989	aug	2005-08-08 05:12:06	institute for operations research and the management sciences (informs), linthicum, maryland, usa	user acceptance of computer technology: a comparison of two theoretical models	an abstract is not available.
465	202502	article	annual review of biophysics and biomolecular structure	\N	\N	\N	24	28	1	1999	\N	2005-05-18 15:26:20	national institute of environmental health sciences, research triangle park, north carolina 27709, usa.	{molecular} {dynamics} {simulations} {of} {biomolecules}: {long-range} electrostatic effects	? abstract?current computer simulations of biomolecules typically make use of classical molecular dynamics methods, as a very large number (tens to hundreds of thousands) of atoms are involved over timescales of many nanoseconds. the methodology for treating short-range bonded and van der waals interactions has matured. however, long-range electrostatic interactions still represent a bottleneck in simulations. in this article, we introduce the basic issues for an accurate representation of the relevant electrostatic interactions. in spite of the huge computational time demanded by most biomolecular systems, it is no longer necessary to resort to uncontrolled approximations such as the use of cutoffs. in particular, we discuss the ewald summation methods, the fast particle mesh methods, and the fast multipole methods. we also review recent efforts to understand the role of boundary conditions in systems with long-range interactions, and conclude with a short perspective on future trends.
466	203297	article	trends in biotechnology	\N	\N	\N	5	22	8	2004	aug	2005-05-18 20:49:35	department of bioengineering, university of california, san diego, 9500 gilman drive \\#0412, la jolla, ca 92093-0412, usa.	comparison of network-based pathway analysis methods	network-based definitions of biochemical pathways have emerged in recent years. these pathway definitions insist on the balanced use of a whole network of biochemical reactions. two such related definitions, elementary modes and extreme pathways, have generated novel hypotheses regarding biochemical network function. the relationship between these two approaches can be illustrated by comparing and contrasting the elementary modes and extreme pathways of previously published metabolic reconstructions of the human red blood cell {(rbc)} and the human pathogen helicobacter pylori. descriptions of network properties generated by using these two approaches in the analysis of realistic metabolic networks need careful interpretation.
467	203951	inproceedings	\N	proceedings of the 23rd international conference on software engineering	icse	ieee computer society	9	\N	\N	2001	\N	2005-05-18 22:54:21	washington, dc, usa	an empirical study of global software development: distance and speed	global software development is rapidly becoming the norm for technology companies. previous qualitative research suggests that multi-site development may increase development cycle time. we use both survey data and data from the source code change management system to model the extent of delay in a multi-site software development organization, and explore several possible mechanisms for this delay. we also measure differences in same-site and cross-site communication patterns, and analyze the relationship of these variables to delay. our results show that compared to same-site work, cross-site work takes much longer, and requires more people for work of equal size and complexity. we also report a strong relationship between delay in cross-site work and the degree to which remote colleagues are perceived to help out when workloads are heavy. we discuss implications of our findings for collaboration technology for distributed software development.
468	203956	article	ieee personal communications	personal communications, ieee	\N	ieee	7	8	4	2001	aug	2005-05-18 23:23:57	\N	pervasive computing: vision and challenges	this article discusses the challenges in computer systems research posed by the emerging field of pervasive computing. it first examines the relationship of this new field to its predecessors: distributed systems and mobile computing. it then identifies four new research thrusts: effective use of smart spaces, invisibility, localized scalability, and masking uneven conditioning. next, it sketches a couple of hypothetical pervasive computing scenarios, and uses them to identify key capabilities missing from today's systems. the article closes with a discussion of the research necessary to develop these capabilities
469	205022	article	bioinformatics	\N	\N	oxford university press	5	21	10	2005	may	2008-06-24 19:48:53	\N	prediction of the phenotypic effects of non-synonymous single nucleotide polymorphisms using structural and evolutionary information	motivation: there has been great expectation that the knowledge of an individual's genotype will provide a basis for assessing susceptibility to diseases and designing individualized therapy. non-synonymous single nucleotide polymorphisms ({nssnps}) that lead to an amino acid change in the protein product are of particular interest because they account for nearly half of the known genetic variations related to human inherited diseases. to facilitate the identification of disease-associated {nssnps} from a large number of neutral {nssnps}, it is important to develop computational tools to predict the phenotypic effects of {nssnps}.
470	206013	article	etr\\&d	\N	\N	\N	16	53	2	2005	\N	2005-05-20 15:27:47	\N	engaging by design how engagement strategies in popular computer and video games can inform instructional design	computer and video games are a prevalent form of entertainment in which the purpose of the design is to engage players. game designers incorporate a number of strategies and tactics for engaging players in â€˜gameplay.â€™ these strategies and tactics may provide instructional designers with new methods for engaging learners. this investigation presents a review of game design strategies and the implications of appropriating these strategies for instructional design. specifically, this study presents an overview of the trajectory of player positioning or point of view, the role of narrative, and methods of interactive design. a comparison of engagement strategies in popular games and characteristics of engaged learning is also presented to examine how strategies of game design might be integrated into the existing framework of engaged learning. [abstract from author]
471	209097	article	ieee/acm transactions on networking	\N	\N	\N	13	5	6	1997	\N	2005-05-23 22:32:20	\N	a comparison of mechanisms for improving {tcp} performance over wireless links	reliable transport protocols such as {tcp} are tuned to perform well in traditional networks where packet losses occur mostly because of congestion. however, networks with wireless and other lossy links also suffer from significant losses due to bit errors and handoffs. {tcp} responds to all losses by invoking congestion control and avoidance algorithms, resulting in degraded end-to-end performance in wireless and lossy systems. in this paper, we compare several schemes designed to improve the...
472	209184	article	acm computer communication review	\N	\N	\N	10	27	1	1997	\N	2005-05-24 02:46:23	\N	dummynet: a simple approach to the evaluation of network protocols	network protocols are usually tested in operational networks or in simulated environments. with the former approach it is not easy to set and control the various operational parameters such as bandwidth, delays, queue sizes. simulators are easier to control, but they are often only an approximate model of the desired setting, especially for what regards the various traffic generators (both producers and consumers) and their interaction with the protocol itself.in this paper we show how a simple, yet flexible and accurate network simulator -  dummynet  - can be built with minimal modifications to an existing protocol stack, allowing experiments to be run on a standalone system.  dummynet  works by intercepting communications of the protocol layer under test and simulating the effects of finite queues, bandwidth limitations and communication delays. it runs in a fully operational system, hence allowing the use of real traffic generators and protocol implementations, while solving the problem of simulating unusual environments. with our tool, doing experiments with network protocols is as simple as running the desired set of applications on a workstation.a freebsd implementation of  dummynet , targeted to tcp, is available from the author. this implementation is highly portable and compatible with other bsd-derived systems, and takes less than 300 lines of kernel code.
473	209472	inproceedings	\N	proceedings of the 1996 acm conference on computer supported cooperative work	\N	acm press	9	\N	\N	1996	\N	2005-05-24 16:00:21	\N	re-place-ing space: the roles of place and space in collaborative systems	many collaborative and communicative environments use notions of â€œspaceâ€ and spatial organisation to facilitate and structure interaction. we argue that a focus on spatial models is misplaced. drawing on understandings from architecture and urban design, as well as from our own research findings, we highlight the critical distinction between â€œspaceâ€ and â€œplaceâ€. while designers use spatial models to support interaction, we show how it is actually a notion of â€œplaceâ€ which frames interactive behaviour. this leads us to re-evaluate spatial systems, and discuss how â€œplaceâ€, rather than â€œspaceâ€, can support cscw design.
474	209491	article	artificial intelligence	\N	\N	\N	19	177	\N	2000	\N	2005-05-24 16:00:22	\N	on agent-based software engineering	agent-based computing represents an exciting new synthesis both for artificial intelligence (ai) and, more generally, computer science. it has the potential to significantly improve the theory and the practice of modeling, designing, and implementing computer systems. yet, to date, there has been little systematic analysis of what makes the agent-based approach such an appealing and powerful computational model. moreover, even less effort has been devoted to discussing the inherent disadvantages that stem from adopting an agent-oriented view. here both sets of issues are explored. the standpoint of this analysis is the role of agent-based software in solving complex, real-world problems. in particular, it will be argued that the development of robust and scalable software systems requires autonomous agents that can complete their objectives while situated in a dynamic and uncertain environment, that can engage in rich, high-level social interactions, and that can operate within flexible organisational structures.
475	209629	inproceedings	\N	hicss	\N	\N	\N	\N	\N	2004	\N	2005-05-24 16:00:25	\N	digital artifacts for remembering and storytelling: {posthistory} and social network fragments	as part of a long-term investigation into visualizing email, we have created two visualizations of email archives.  one highlights social networks while the other depicts the temporal rhythms of interactions with individuals. while interviewing users of these systems, it became clear that the applications triggered recall of many personal events. one of the most striking and not entirely expected outcomes was that the visualizations motivated retelling stories from the usersâ€™ pasts to others. in this paper, we discuss the motivation and design of these projects and analyze their use as catalysts for personal narrative and recall.
476	209806	book	\N	\N	\N	{palgrave macmillan}	\N	\N	\N	2004	may	2005-05-24 21:51:16	\N	what video games have to teach us about learning and literacy	review {'[gee} is] a serious scholar who is taking a lead in an emerging {field.'{\\\\textendash}scott} carlson, chronicle of higher education review {'gee} astutely points out that for video game makers, unlike schools, failing to engage children is not an {option.'{\\\\textendash}this} text refers to an out of print or unavailable edition of this title. book description a controversial look at the positive things that can be learned from video games by a well known professor of education. james paul gee begins his new book with {'i} want to talk about vide games{\\\\textendash}yes, even violent video games{\\\\textendash}and say some positive things about them.' with this simple but explosive beginning, one of america's most well-respected professors of education looks seriously at the good that can come from playing video games. gee is interested in the cognitive development that can occur when someone is trying to escape a maze, find a hidden treasure and, even, blasting away an enemy with a high-powered rifle. talking about his own video-gaming experience learning and using games as diverse as lara croft and arcanum, gee looks at major specific cognitive activities: * how individuals develop a sense of identity * how one grasps meaning * how one evaluates and follow a command * how one picks a role model * how one perceives the world this is a ground-breaking book that takes up a new electronic method of education and shows the positive upside it has for learning. about the author james paul gee is one of the most well-known professors of education in the united states. he teaches at the university of wisconsin, madison and is the author of several books.
477	209845	inproceedings	\N	proc. 17th international conf. on machine learning	\N	morgan kaufmann, san francisco, ca	7	\N	\N	2000	\N	2005-05-25 04:02:54	\N	less is more: active learning with support vector machines	we describe a simple active learning heuristic which greatly enhances the generalization behav- ior of support vector machines (svms) on sev- eral practical document classiï¬cation tasks.  we observe a number of beneï¬ts, the most surpris- ing of which is that a svm trained on a well- chosen subset of the available corpus frequently performs better than one trained on all available data.  the heuristic for choosing this subset is simple to compute, and makes no use of infor- mation about the test set. given that the training time of svms depends heavily on the training set size, our heuristic not only offers better per- formance with fewer data, it frequently does so in less time than the naive approach of training on all available data.
478	210399	article	software engineering, ieee transactions on	\N	\N	\N	17	20	6	1994	\N	2005-05-25 13:57:16	\N	a metrics suite for object oriented design	given the central role that software development plays in the delivery and application of information technology, managers are increasingly focusing on process improvement in the software development area. this demand has spurred the provision of a number of new and/or improved approaches to software development, with perhaps the most prominent being object-orientation ({oo}). in addition, the focus on process improvement has increased the demand for software measures, or metrics with which to manage the process. the need for such metrics is particularly acute when an organization is adopting a new technology for which established practices have yet to be developed. this research addresses these needs through the development and implementation of a new suite of metrics for {oo} design. metrics developed in previous research, while contributing to the field's understanding of software development processes, have generally been subject to serious criticisms, including the lack of a theoretical base. following wand and weber (1989), the theoretical base chosen for the metrics was the ontology of bunge (1977). six design metrics are developed, and then analytically evaluated against weyuker's (1988) proposed set of measurement principles. an automated data collection tool was then developed and implemented to collect an empirical sample of these metrics at two field sites in order to demonstrate their feasibility and suggest ways in which managers may use these metrics for process improvement
479	211254	article	journal of experimental psychology: general	\N	\N	\N	25	130	3	2001	\N	2005-05-26 06:44:16	\N	implicit learning out of the lab: the case of orthographic regularities	children's (grades 1 to 5) implicit learning of french orthographic regularities was investigated through nonword judgment (experiments 1 and 2) and completion (experiments 3a and 3b) tasks. children were increasingly sensitive to (a) the frequency of double consonants (experiments 1, 2, and 3a), (b) the fact that vowels can never be doubled (experiment 2), and (c) the legal position of double consonants (experiments 2 and 3b). the latter effect transferred to never doubled consonants but with a decrement in performance. moreover, this decrement persisted without any trend toward fading, even after the massive amounts of experience provided by years of practice. this result runs against the idea that transfer to novel material is indicative of abstract rule-based knowledge and suggests instead the action of mechanisms sensitive to the statistical properties of the material. a connectionist model is proposed as an instantiation of such mechanisms.
480	212480	article	d-lib magazine	\N	\N	nature publishing group	\N	11	4	2005	apr	2005-05-27 00:56:39	\N	social bookmarking tools (i)	this paper reviews some current initiatives, as of early 2005, in providing public link management applications on the web - utilities that are often referred to under the general moniker of 'social bookmarking tools'. there are a couple of things going on here: 1) server-side software aimed specifically at managing links with, crucially, a strong, social networking flavour, and 2) an unabashedly open and unstructured approach to tagging, or user classification, of those links.
481	214145	article	journal of computer assisted learning	\N	\N	blackwell science ltd	12	21	3	2005	jun	2005-11-18 09:57:37	\N	mobile learning with a mobile game: design and motivational effects	abstract mobile technologies offer the opportunity to embed learning in a natural environment. this paper describes the design of the {mobilegame} prototype, exploring the opportunities to support learning through an orientation game in a university setting. the paper first introduces the scenario and then describes the general architecture of the prototype. the main part of the paper focuses on the evaluation of design issues and the effects observed in two trials. design issues include: supporting work on the move poses difficult interface questions, the accuracy of current outdoor, and indoor positioning systems is still problematic and the game requires near real-time response time. the evaluation of the effects shows that features such as 'map-navigation' and 'hunting and hiding' lead to excitement and fun. the participants immerse into a mixed reality that augments both physical and social space. the game success is based on the motivating design of the game itself. the paper concludes with open issues for future research, especially with the need to thoroughly evaluate the learning benefits.
482	214373	article	ieee journal on select areas in communications	\N	\N	\N	7	16	8	1998	aug	2005-05-30 16:24:07	\N	a simple transmit diversity technique for wireless communications	this paper presents a simple two-branch transmit diversity scheme. using two transmit antennas and one receive antenna the scheme provides the same diversity order as maximal-ratio receiver combining (mrrc) with one transmit antenna, and two receive antennas. it is also shown that the scheme may easily be generalized to two transmit antennas and m receive antennas to provide a diversity order of 2m. the new scheme does not require any bandwidth expansion or any feedback from the receiver to the transmitter and its computation complexity is similar to mrrc
483	214713	article	ieee/acm trans. netw.	\N	\N	ieee press	13	1	3	1993	jun	2005-05-31 01:10:05	piscataway, nj, usa	a generalized processor sharing approach to flow control in integrated services networks: the single-node case	an abstract is not available.
484	214716	article	intelligent systems, ieee [see also ieee intelligent systems and their applications]	\N	\N	\N	13	18	1	2003	\N	2005-05-31 01:35:26	\N	web services: been there, done that?	web services can be defined as loosely coupled, reusable software components that semantically encapsulate discrete functionality and are distributed and programmatically accessible over standard internet protocols. web services have received a lot of hype, the reasons for which are not easily determined. some of their benefits might even seem to waste away, once we touch on the nitty-gritty details, because web services per se do not offer a solution to underlying problems. the contributions included in this section delve into some of these issues, including: pitfalls of workflow issues; structuring procedural knowledge into problem-solving methods; discussing how a low initial entry barrier and simple technology are balanced against the long-term goal of easy integration; including semantics in a web service modeling framework; and building on new kinds of applications such as grid enterprises.
485	215770	article	the computer journal	\N	\N	\N	13	45	6	2002	\N	2005-06-01 16:32:49	\N	{rascal}: calculation of graph similarity using maximum common edge subgraphs	a new graph similarity calculation procedure is introduced for comparing labeled graphs. given a minimum similarity threshold, the procedure consists of an initial screening process to determine whether it is possible for the measure of similarity between the two graphs to exceed the minimum threshold, followed by a rigorous maximum common edge subgraph ({mces}) detection algorithm to compute the exact degree and composition of similarity. the proposed {mces} algorithm is based on a maximum clique formulation of the problem and is a significant improvement over other published algorithms. it presents new approaches to both lower and upper bounding as well as vertex selection.
486	215987	inproceedings	\N	iui	\N	acm press	7	\N	\N	2000	\N	2005-06-01 20:46:24	new york, ny, usa	user interactions with everyday applications as context for just-in-time information access	our central claim is that user interactions with everyday productivity applications (e.g., word processors, web browsers, etc.) provide rich contextual information that can be leveraged to support just-in-time access to task-relevant information. we discuss the requirements for such systems, and develop a general architecture for systems of this type. as evidence for our claim, we present watson, a system which gathers contextual information in the form of the text of the document the user is manipulating in order to proactively retrieve documents from distributed information repositories. we close by describing the results of several experiments with watson, which show it consistently provides useful information to its users.
487	215995	inproceedings	\N	chi	\N	acm press	7	\N	\N	2001	\N	2005-06-01 20:46:24	new york, ny, usa	optimizing search by showing results in context	we developed and evaluated seven interfaces for integrating semantic category information with web search results. list interfaces were based on the familiar ranked-listing of search results, sometimes augmented with a category name for each result. category interfaces also showed page titles and/or category names, but re-organized the search results so that items in the same category were grouped together visually. our user studies show that all category interfaces were more effective than list interfaces even when lists were augmented with category names for each result. the best category performance was obtained when both category names and individual page titles were presented. either alone is better than a list presentation, but both together provide the most effective means for allowing users to quickly examining search results. these results provide a better understanding of the perceptual and cognitive factors underlying the advantage of category groupings and provide some practical guidance to web search interface designers.
488	216962	article	nature reviews. microbiology	\N	\N	nature publishing group	8	3	6	2005	jun	2005-06-09 11:05:46	\N	the metagenomics of soil.	{phylogenetic surveys of soil ecosystems have shown that the number of prokaryotic species found in a single sample exceeds that of known cultured prokaryotes. soil metagenomics, which comprises isolation of soil dna and the production and screening of clone libraries, can provide a cultivation-independent assessment of the largely untapped genetic reservoir of soil microbial communities. this approach has already led to the identification of novel biomolecules. however, owing to the complexity and heterogeneity of the biotic and abiotic components of soil ecosystems, the construction and screening of soil-based libraries is difficult and challenging. this review describes how to construct complex libraries from soil samples, and how to use these libraries to unravel functions of soil microbial communities.}
489	218423	book	\N	\N	\N	morgan kaufmann	\N	\N	\N	1990	\N	2005-06-03 23:23:10	\N	computer architecture: a quantitative approach	an excellent successor to hennessy and patterson's _computer organization and design_, this book presents computer architecture and design as something quantitative that can be studied in the context of real running systems rather than in an abstract format. the concepts are again grounded in real machine architectures and many of the examples are contemporary architectures, such as powerpc chips and intel 80x86. _computer architecture_ follows the same outline as its predecessor, but covers information in more depth, moving rapidly from introductory discussions to issues just shy of computer design research. the format again includes an excellent mix of exercises and historical background. this book is recommended for people with some experience in digital design--or people who have read and understood the authors' first text.
490	220515	article	journal of the american society for information science	\N	\N	\N	14	47	1	1998	dec	2005-06-05 13:45:39	\N	stemming algorithms: a case study for detailed evaluation	the majority of information retrieval experiments are evaluated by measures such as average precision and average recall. fundamental decisions about the superiority of one retrieval technique over another are made solely on the basis of these measures. we claim that average performance figures need to be validated with a careful statistical analysis and that there is a great deal of additional information that can be uncovered by looking closely at the results of individual queries. this article is a case study of stemming algorithms which describes a number of novel approaches to evaluation and demonstrates their value. \\&copy; 1996 john wiley \\&amp; sons, inc.
491	220516	incollection	\N	\N	\N	morgan kaufmann publishers inc.	3	\N	\N	1997	\N	2005-06-05 14:05:25	san francisco, ca, usa	readings in information retrieval	an abstract is not available.
492	220600	book	\N	\N	\N	addison wesley	\N	\N	\N	1999	\N	2005-06-06 05:48:47	\N	{the unified software development process}	a software process defines the steps required to create software successfully. written by the same authors who brought you the unified modelling language (uml), _the unified software development process_ introduces a new standard for creating today's software that will certainly be useful for any software developer or manager who is acquainted with uml.  early sections introduce four basic principles of the unified process: that software should stress use cases (which show how it interacts with users), that the process is architecture-centric and that it is iterative and incremental. the authors then apply these principles to their software process, which involves everything from gathering system requirements to analysis, design, implementation and testing. the use-case examples are excellent and include concrete examples drawn from such areas as banking and inventory control.  the authors point out the connection between uml document types (like use- cases, class diagrams and state transition diagrams) with various models used throughout the software process. they provide very short, real-world examples that illustrate how their ideas have been successfully applied. the straightforward tour of the new unified software process gets extra elaboration--along with some advice--in later chapters that further describe the author's ideas on design. with the weight of these three expert authors behind it, readers can expect _the unified software development process_ to be an important book and one that will be valuable to any working designer or manager. --_richard dragan_
493	220695	article	journal of neuroscience	\N	\N	\N	-3289	16	\N	1996	\N	2005-06-06 09:26:17	\N	efficient coding of natural scenes in the lateral geniculate nucleus: experimental test of a computational theory	a recent computational theory suggests that visual processing in the retina and the lateral geniculate nucleus (lgn) serves to recode information into an efficient form (atick and redlich, 1990). information theoretic analysis showed that the representation of visual information at the level of the photoreceptors is inefficient, primarily attributable to a high degree of spatial and temporal correlation in natural scenes. it was predicted, therefore, that the retina and the lgn should recode this signal into a decorrelated form or, equivalently, into a signal with a "white" spatial and temporal power spectrum. in the present study, we tested directly the prediction that visual processing at the level of the lgn temporarily whitens the natural visual input. we recorded the responses of individual neurons in the lgn of the cat to natural, time-varying images (movies) and, as a control, to white-noise stimuli. although there is substantial temporal correlation in natural inputs (dong and atick, 1995b), we found that the power spectra of lgn responses were essentially white. between 3 and 15 hz, the power of the responses had an average variation of only +/-10.3%. thus, the signals that the lgn relays to visual cortex are temporarily decorrelated. furthermore, the responses of x-cells to natural inputs can be well predicted from their responses to white-noise inputs. we therefore conclude that whitening of natural inputs can be explained largely by the linear filtering properties (enroth-cugell and robson, 1966). our results suggest that the early visual pathway is well adapted for efficient coding of information in the natural visual environment, in agreement with the prediction of the computational theory.
494	220700	article	artificial intelligence	\N	\N	\N	18	17	\N	1981	\N	2005-06-06 09:26:17	\N	determining optical flow	optical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. a second constraint is needed. a method for finding the optical flow pattern is presented which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. an iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. the algorithm is robust in that it can handle image sequences that are quantified rather coarsely in space and time. it is also insensitive to quantization of brightness levels and additive noise. examples are included where the assumption of smoothness is violated at singular points or along lines in the image.
495	220966	inproceedings	\N	ubicomp	\N	springer-verlag	16	\N	\N	2001	\N	2005-06-06 20:49:15	london, uk	at home with ubiquitous computing: seven challenges	abstract. the smart home offers a new opportunity to augment people&#039;s lives with ubiquitous computing technology that provides increased communications, awareness, and functionality. recently, a number of trends have increased the likelihood that the aware home can soon become a reality. we examine a number of challenges from the technical, social, and pragmatic domains that we feel must be overcome before the vision of the smart home, posited by ubiquitous computing research, can become a reality. our hope in raising these issues is to create a conversation among researchers in the varied disciplines that make up ubiquitous computing. in particular, we hope to raise awareness of the existing literature on the adoption, use, and history of domestic technologies, as well as the use of situated studies, and the benefits that these can bring to bear on the design and evaluation of technologies for the home
496	220970	phdthesis	\N	\N	\N	\N	\N	\N	\N	2002	dec	2005-06-06 20:50:45	\N	system support for pervasive applications	pervasive computing provides an attractive vision for the future of computing. computational power will be available everywhere. mobile and stationary devices will dynamically connect and coordinate to seamlessly help people in accomplishing their tasks. for this vision to become a reality, developers must build applications that constantly adapt to a highly dynamic computing environment. to make the developers' task feasible, we present a system architecture for pervasive computing, called &#60;i>one.world&#60;/i>. our architecture provides an integrated and comprehensive framework for building pervasive applications. it includes services, such as discovery and migration, that help to build applications and directly simplify the task of coping with constant change. we describe our architecture and its programming model and reflect on our own and others' experiences with using it.
497	221092	article	multimedia, ieee	\N	\N	\N	4	12	2	2005	\N	2005-06-06 21:34:11	\N	video blogging: content to the max	the lure of video blogging combines the ubiquitous, grassroots, web-based journaling of blogging with the richness of expression available in multimedia. some claim that video blogging is an important force in a future world of video journalism and a powerful technical adjunct to our existing televised news sources. others point to the huge demands it imposes on networking resources, the lack of hard standards, and the poor usability of current video blogging systems as indicators that it's doomed to fail. like any nascent technology, video blogging has many unsolved problems. the field, however, is vibrant, the goals are fairly clear, and the challenges they pose to multimedia researchers are exciting indeed. developing the standards and technologies for video blogging requires a combination of approaches from various areas including media representation, information retrieval, multimedia content analysis, and video summarization. like the development of the web and text blogging before, video blogging only come about through open development and collaboration between engineers and researchers from diverse fields. most strikingly, it is fueled by the passion and enthusiasm of those creating content - those who go to the trouble of recording their lives and opinions within the fledgling medium, shaping it as a lively and useful resource for generations of internet users to come.
498	221156	article	nature biotechnology	\N	\N	nature publishing group	7	23	6	2005	jun	2005-06-17 18:25:01	\N	functional genome annotation through phylogenomic mapping.	accurate determination of functional interactions among proteins at the genome level remains a challenge for genomic research. here we introduce a genome-scale approach to functional protein annotation--phylogenomic mapping--that requires only sequence data, can be applied equally well to both finished and unfinished genomes, and can be extended beyond single genomes to annotate multiple genomes simultaneously. we have developed and applied it to more than 200 sequenced bacterial genomes. proteins with similar evolutionary histories were grouped together, placed on a three dimensional map and visualized as a topographical landscape. the resulting phylogenomic maps display thousands of proteins clustered in mountains on the basis of coinheritance, a strong indicator of shared function. in addition to systematic computational validation, we have experimentally confirmed the ability of phylogenomic maps to predict both mutant phenotype and gene function in the delta proteobacterium myxococcus xanthus.
499	221328	article	the journal of circuits, systems and computers	\N	\N	\N	45	8	1	1998	\N	2005-06-07 14:10:30	\N	the application of petri nets to workflow management	workflow management promises a new solution to an age-old problem: controlling, monitoring, optimizing and supporting business processes. what is new about workflow management is the explicit representation of the business process logic which allows for computerized support. this paper discusses the use of petri nets in the context of workflow management. petri nets are an established tool for modeling and analyzing processes. on the one hand, petri nets can be used as a design language for the specification of complex workflows. on the other hand, petri net theory provides for powerful analysis techniques which can be used to verify the correctness of workflow procedures. this paper introduces workflow management  as an application domain for petri nets, presents state-of-the-art results with respect to the verification of workflows, and highlights some petri-net-based workflow tools.  1 introduction  in former times, information systems were designed to support the execution of indiv...
500	221342	book	\N	\N	\N	{sage publications}	\N	\N	\N	1998	sep	2005-06-07 15:00:23	\N	basics of qualitative research : techniques and procedures for developing grounded theory	{<p>the second edition of this best-selling text continues to offer immensely practical advice and technical expertise to aid researchers in making sense of their collected data. basics of qualitative research, second edition presents methods that enable researchers to analyze and interpret their data, and ultimately build theory from it. highly accessible in their approach, authors anselm strauss (late of the university of san francisco and co-creator of grounded theory) and juliet corbin provide a step-by-step guide to the research act-- from the formation of the research question through several approaches to coding and analysis, to reporting on the research. full of definitions and illustrative examples, this highly accessible book concludes with chapters that present criteria for evaluating a study, as well as responses to common questions posed by students of qualitative research. significantly revised, basics of qualitative research remains a landmark volume in the study of qualitative methods. </p>}
501	221552	article	plos biology	\N	\N	\N	15	1	2	2003	nov	2005-06-08 04:33:57	\N	learning to control a {brain-machine} interface for reaching and grasping by primates	reaching and grasping in primates depend on the coordination of neural activity in large frontoparietal ensembles. here we demonstrate that primates can learn to reach and grasp virtual objects by controlling a robot arm through a closed-loop brain-machine interface (bmic) that uses multiple mathematical models to extract several motor parameters (i.e., hand position, velocity, gripping force, and the emgs of multiple arm muscles) from the electrical activity of frontoparietal neuronal ensembles. as single neurons typically contribute to the encoding of several motor parameters, we observed that high bmic accuracy required recording from large neuronal ensembles. continuous bmic operation by monkeys led to significant improvements in both model predictions and behavioral performance. using visual feedback, monkeys succeeded in producing robot reach-and-grasp movements even when their arms did not move. learning to operate the bmic was paralleled by functional reorganization in multiple cortical areas, suggesting that the dynamic properties of the bmic were incorporated into motor and sensory cortical representations.
502	222732	article	ieee intelligent systems	\N	\N	\N	7	19	4	2004	jul	2005-06-08 13:52:16	\N	ontology versioning in an ontology management framework	ontologies have become ubiquitous in information systems. they constitute the semantic web's backbone, facilitate e-commerce, and serve such diverse application fields as bioinformatics and medicine. as ontology development becomes increasingly widespread and collaborative, developers are creating ontologies using different tools and different languages. these ontologies cover unrelated or overlapping domains at different levels of detail and granularity. a uniform framework, which we present here, helps users manage multiple ontologies by leveraging data and algorithms developed for one tool in another. for example, by using an algorithm we developed for structural evaluation of ontology versions, this framework lets developers compare different ontologies and map similarities and differences among them. multiple-ontology management includes these tasks: maintain ontology libraries, import and reuse ontologies, translate ontologies from one formalism to another, support ontology versioning, specify transformation rules between different ontologies and version, merge ontologies, align and map between ontologies, extract an ontology's self-contained parts, support inference across multiple ontologies, support query across multiple ontologies.
503	222953	article	nature	\N	\N	nature publishing group	5	435	7043	2005	jun	2005-07-18 23:09:24	\N	a {microrna} polycistron as a potential human oncogene	to date, more than 200 {micrornas} have been described in humans; however, the precise functions of these regulatory, non-coding {rnas} remains largely obscure. one cluster of {micrornas}, the mir-17–92 polycistron, is located in a region of {dna} that is amplified in human b-cell lymphomas1. here we compared b-cell lymphoma samples and cell lines to normal tissues, and found that the levels of the primary or mature {micrornas} derived from the mir-17–92 locus are often substantially increased in these cancers. enforced expression of the mir-17–92 cluster acted with c-myc expression to accelerate tumour development in a mouse b-cell lymphoma model. tumours derived from haematopoietic stem cells expressing a subset of the mir-17–92 cluster and c-myc could be distinguished by an absence of apoptosis that was otherwise prevalent in c-myc-induced lymphomas. together, these studies indicate that non-coding {rnas}, specifically {micrornas}, can modulate tumour formation, and implicate the mir-17–92 cluster as a potential human oncogene.
504	223017	article	comptes rendus de l'acad\\'{e}mie des sciences. s\\'{e}rie iii, sciences de la vie	\N	\N	\N	20	324	9	2001	sep	2005-06-08 21:35:30	biologie cellulaire et mol\\'{e}culaire du neurone (inserm v261), institut pasteur, 25 rue docteur roux, 75724 paris, 15, france.	is there chaos in the brain? i. concepts of nonlinear dynamics and methods of investigation.	in the light of results obtained during the last two decades in a number of laboratories, it appears that some of the tools of nonlinear dynamics, first developed and improved for the physical sciences and engineering, are well-suited for studies of biological phenomena. in particular it has become clear that the different regimes of activities undergone by nerve cells, neural assemblies and behavioural patterns, the linkage between them, and their modifications over time, cannot be fully understood in the context of even integrative physiology, without using these new techniques. this report, which is the first of two related papers, is aimed at introducing the non expert to the fundamental aspects of nonlinear dynamics, the most spectacular aspect of which is chaos theory. after a general history and definition of chaos the principles of analysis of time series in phase space and the general properties of chaotic trajectories will be described as will be the classical measures which allow a process to be classified as chaotic in ideal systems and models. we will then proceed to show how these methods need to be adapted for handling experimental time series; the dangers and pitfalls faced when dealing with non stationary and often noisy data will be stressed, and specific criteria for suspecting determinism in neuronal cells and/or assemblies will be described. we will finally address two fundamental questions, namely i) whether and how can one distinguish, deterministic patterns from stochastic ones, and, ii) what is the advantage of chaos over randomness: we will explain why and how the former can be controlled whereas, notoriously, the latter cannot be tamed. in the second paper of the series, results obtained at the level of single cells and their membrane conductances in real neuronal networks and in the study of higher brain functions, will be critically reviewed. it will be shown that the tools of nonlinear dynamics can be irreplaceable for revealing hidden mechanisms subserving, for example, neuronal synchronization and periodic oscillations. the benefits for the brain of adopting chaotic regimes with their wide range of potential behaviours and their aptitude to quickly react to changing conditions will also be considered.
505	224537	article	evolutionary computation	\N	\N	\N	26	3	2	1995	\N	2005-06-09 20:05:32	\N	classifier fitness based on accuracy	in many classifier systems, the classifier strength parameter serves as a predictor of future payoff and as the classifier's fitness for the genetic algorithm. we investigate a classifier system, {xcs}, in which each classifier maintains a prediction of expected payoff, but the classifier's fitness is given by a measure of the prediction's accuracy. the system executes the genetic algorithm in niches defined by the match sets, instead of panmictically. these aspects of {xcs} result in its population tending to form a complete and accurate mapping x x a -> p from inputs and actions to payoff predictions. further, {xcs} tends to evolve classifiers that are maximally general, subject to an accuracy criterion. besides introducing a new direction for classifier system research, these properties of {xcs} make it suitable for a wide range of reinforcement learning situations where generalization over states is desirable.
506	225130	electronic	\N	\N	\N	\N	\N	\N	\N	1999	sep	2005-06-10 15:18:51	\N	modeling market mechanism with minority game	using the minority game model we study a broad spectrum of problems of market mechanism. we study the role of different types of agents: producers, speculators as well as noise traders. the central issue here is the information flow : producers feed in the information whereas speculators make it away. how well each agent fares in the common game depends on the market conditions, as well as their sophistication. sometimes there is much to gain with little effort, sometimes great effort virtually brings no more incremental gain. market impact is shown to play also an important role, a strategy should be judged when it is actually used in play for its quality. though the minority game is an extremely simplified market model, it allows to ask, analyze and answer many questions which arise in real markets.
507	225187	article	sigops oper. syst. rev.	\N	\N	acm press	15	36	SI	2002	\N	2005-06-10 20:19:18	new york, ny, usa	resource overbooking and application profiling in shared hosting platforms	in this paper, we present techniques for provisioning cpu and network resources in shared hosting platforms running potentially antagonistic third-party applications. the primary contribution of our work is to demonstrate the feasibility and benefits of overbooking resources in shared platforms, to maximize the platform yield: the revenue generated by the available resources. we do this by first deriving an accurate estimate of application resource needs by profiling applications on dedicated...
508	225734	article	briefings in bioinformatics	\N	\N	henry stewart publications	10	6	2	2005	jun	2006-02-08 03:51:07	\N	evolution of web services in bioinformatics	bioinformaticians have developed large collections of tools to make sense of the rapidly growing pool of molecular biological data. biological systems tend to be complex and in order to understand them, it is often necessary to link many data sets and use more than one tool. therefore, bioinformaticians have experimented with several strategies to try to integrate data sets and tools. owing to the lack of standards for data sets and the interfaces of the tools this is not a trivial task. over the past few years building services with web-based interfaces has become a popular way of sharing the data and tools that have resulted from many bioinformatics projects. this paper discusses the interoperability problem and how web services are being used to try to solve it, resulting in the evolution of tools with web interfaces from html/web form-based tools not suited for automatic workflow generation to a dynamic network of xml-based web services that can easily be used to create pipelines.
509	225743	article	bioinformatics	\N	\N	oxford university press	1	21	12	2005	jun	2006-03-15 21:52:00	\N	stochastic reaction-diffusion simulation with {mesord}	summary: {mesord} is a tool for stochastic simulation of chemical reactions and diffusion. in particular, it is an implementation of the next subvolume method, which is an exact method to simulate the markov process corresponding to the reaction-diffusion master equation.  availability: {mesord} is free software, written in c++ and licensed under the {gnu} general public license ({gpl}). {mesord} runs on linux, mac {os} x, {netbsd}, solaris and windows {xp}. it can be downloaded from http://mesord.sourceforge.net.  contact: johan.elf@icm.uu.se; johan.hattne@embl-hamburg.de  supplementary information:  {mesord} user's guide' and other documents are available at http://mesord.sourceforge.net. 10.1093/bioinformatics/bti431
510	226534	inproceedings	\N	cascon	\N	ibm press	\N	\N	\N	1997	\N	2005-06-12 11:20:20	\N	acme: an architecture description interchange language	numerous architectural description languages ({adls}) have been developed, each providing complementary capabilities for architectural development and analysis. unfortunately, each {adl} and supporting toolset operates in isolation, making it difficult to integrate those tools and share architectural descriptions. acme is being developed as a joint effort of the software architecture research community as a common interchange format for architecture design tools. acme provides a structural framework for characterizing architectures, together with annotation facilities for additional {adl}-specific information. this scheme permits subsets of {adl} tools to share architectural information that is jointly understood, while tolerating the presence of information that falls outside their common vocabulary. in this paper we describe acme's key features, rationale, and technical innovations.
511	226864	article	acm comput. surv.	\N	\N	acm	27	37	1	2005	mar	2005-06-13 03:13:27	new york, ny, usa	lineage retrieval for scientific data processing: a survey	scientific research relies as much on the dissemination and exchange of data sets as on the publication of conclusions. accurately tracking the lineage (origin and subsequent processing history) of scientific data sets is thus imperative for the complete documentation of scientific work. researchers are effectively prevented from determining, preserving, or providing the lineage of the computational data products they use and create, however, because of the lack of a definitive model for lineage retrieval and a poor fit between current data management tools and scientific software. based on a comprehensive survey of lineage research and previous prototypes, we present a metamodel to help identify and assess the basic components of systems that provide lineage retrieval for scientific data products.
512	226883	inproceedings	\N	proceedings of icml-00, 17th international conference on machine learning	\N	morgan kaufmann publishers, san francisco, us	7	\N	\N	2000	\N	2005-06-13 07:42:46	stanford, us	support vector machine active learning with applications to text classification	support vector machines have met with significant success in numerous real-world learning tasks. however, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. in many settings, we also have the option of using pool-based active learning. instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. we introduce an new...
513	227176	article	physical review letters	\N	\N	\N	\N	91	13	2003	sep	2005-06-14 03:33:32	compugen ltd., 72 pinchas rosen street, tel aviv 69512, israel.	preferential attachment in the protein network evolution.	the saccharomyces cerevisiae protein-protein interaction map, as well as many natural and man-made networks, shares the scale-free topology. the preferential attachment model was suggested as a generic network evolution model that yields this universal topology. however, it is not clear that the model assumptions hold for the protein interaction network. using a cross-genome comparison, we show that (a) the older a protein, the better connected it is, and (b) the number of interactions a protein gains during its evolution is proportional to its connectivity. therefore, preferential attachment governs the protein network evolution. evolutionary mechanisms leading to such preference and some implications are discussed.
514	227185	article	nature biotechnology	\N	\N	nature publishing group	7	22	1	2003	dec	2005-06-14 03:53:14	department of biomedical engineering, 201c clark hall, johns hopkins university, 3400 n. charles st., baltimore, maryland 21218, usa. joel.bader@jhu.edu	gaining confidence in high-throughput protein interaction networks	although genome-scale technologies have benefited from statistical measures of data quality, extracting biologically relevant pathways from high-throughput proteomics data remains a challenge. here we develop a quantitative method for evaluating proteomics data. we present a logistic regression approach that uses statistical and topological descriptors to predict the biological relevance of protein-protein interactions obtained from high-throughput screens for yeast. other sources of information, including {mrna} expression, genetic interactions and database annotations, are subsequently used to validate the model predictions without bias or cross-pollution. novel topological statistics show hierarchical organization of the network of high-confidence interactions: protein complex interactions extend one to two links, and genetic interactions represent an even finer scale of organization. knowledge of the maximum number of links that indicates a significant correlation between protein pairs (correlation distance) enables the integrated analysis of proteomics data with data from genetics and gene expression. the type of analysis presented will be essential for analyzing the growing amount of genomic and proteomics data in model organisms and humans.
515	227604	article	siam rev.	\N	\N	society for industrial and applied mathematics	26	47	1	2005	jan	2005-06-14 15:07:52	philadelphia, pa, usa	a survey of eigenvector methods for web information retrieval	web information retrieval is  significantly more challenging  than traditional well-controlled, small document collection information retrieval.  one main difference between traditional information retrieval and web information retrieval is the web's hyperlink structure.  this structure has been exploited by several of today's leading web search engines, particularly google and teoma.  in this survey paper, we focus on web information retrieval methods that use eigenvector computations, presenting the three popular methods of {hits}, {pagerank}, and {salsa}.
516	228031	article	internet computing, ieee	\N	\N	ieee	7	7	6	2003	nov	2005-06-14 22:19:42	\N	web services are not distributed objects	web services are frequently described as the latest incarnation of distributed object technology. this misconception, perpetuated by people from both industry and academia, seriously limits broader acceptance of the true web services architecture. although the architects of many distributed and internet systems have been vocal about the differences between web services and distributed objects, dispelling the myth that they are closely related appears difficult. many believe that web services is a distributed systems technology that relies on some form of distributed object technology. unfortunately, this is not the only common misconception about web services. we seek to clarify several widely held beliefs about the technology that are partially or completely wrong. within the distributed technology world, it is probably more appropriate to associate web services with messaging technologies because they share a common architectural view, although they address different application types. web services technology will have a dramatic enabling effect on worldwide interoperable distributed computing once everyone recognizes that web services are about interoperable document-centric computing, not distributed objects.
517	228353	article	proteins	\N	\N	wiley subscription services, inc., a wiley company	7	60	3	2005	aug	2005-06-15 11:55:26	cambridge crystallographic data centre, cambridge, united kingdom.	comparing protein–ligand docking programs is difficult	there is currently great interest in comparing protein–ligand docking programs. a review of recent comparisons shows that it is difficult to draw conclusions of general applicability. statistical hypothesis testing is required to ensure that differences in pose-prediction success rates and enrichment rates are significant. numerical measures such as root-mean-square deviation need careful interpretation and may profitably be supplemented by interaction-based measures and visual inspection of dockings. test sets must be of appropriate diversity and of good experimental reliability. the effects of crystal-packing interactions may be important. the method used for generating starting ligand geometries and positions may have an appreciable effect on docking results. for fair comparison, programs must be given search problems of equal complexity (e.g. binding-site regions of the same size) and approximately equal time in which to solve them. comparisons based on rescoring require local optimization of the ligand in the space of the new objective function. re-implementations of published scoring functions may give significantly different results from the originals. ostensibly minor details in methodology may have a profound influence on headline success rates. proteins 2005. {\\copyright} 2005 {wiley-liss}, inc.
518	228535	inproceedings	\N	proceedings of {ieee} {infocom}	\N	\N	\N	\N	\N	2000	mar	2005-06-15 16:38:18	\N	modeling {tcp} latency	several analytic models describe the steady-state throughput of bulk transfer tcp flows as a function of round trip time and packet loss rate. these models describe flows based on the assumption that they are long enough to sustain many packet losses. however, most tcp transfers across today's internet are short enough to see few, if any, losses and consequently their performance is dominated by startup effects such as connection establishment and slow start. this paper extends the steady-state model proposed in padhye et al. (1998), in order to capture these startup effects. the extended model characterizes the expected value and distribution of tcp connection establishment and data transfer latency as a function of transfer size, round trip time, and packet loss rate. using simulations, controlled measurements of tcp transfers, and live web measurements we show that, unlike earlier steady-state models for tcp performance, our extended model describes connection establishment and data transfer latency under a range of packet loss conditions, including no loss
519	228643	article	molecular biology and evolution	\N	\N	\N	8	18	3	2001	mar	2005-06-15 19:44:51	department of biology, mcmaster university, hamilton, ontario, canada.	codon bias and base composition are poor indicators of horizontally transferred genes	horizontal gene transfer is now recognized as an important mechanism of evolution. several methods to detect horizontally transferred genes have been suggested. these methods are based on either nucleotide composition or the failure to find a similar gene in closely related species. genes that evolve vertically between closely related species can be divided into those that retain homologous chromosomal positions (positional orthologs) and those that do not. by comparing open reading frames in the escherichia coli and salmonella typhi genomes, we identified 2,728 positional orthologs since these species split 100 {mya}. a group of 1,144 novel e. coli genes were unusually diverged from their s. typhi counterparts. these novel genes included those that had been horizontally transferred into e. coli, as well as members of gene pairs that had been rearranged or deleted. positional orthologs were used to investigate compositional methods of identifying horizontally transferred genes. a large number of e. coli genes with normal nucleotide composition have no apparent ortholog in s. typhi, and many genes of atypical composition do, in fact, have positional orthologs. a phylogenetic approach was employed to confirm selected examples of horizontal transmission among the novel groups of genes. our analysis of 80 e. coli genes determined that a number of genes previously classified as horizontally transferred based on base composition and codon bias were native, and genes previously classified as native appeared to be horizontally transferred. hence, atypical nucleotide composition alone is not a reliable indicator of horizontal transmission.
520	230052	article	proceedings of the national academy of sciences	\N	\N	national academy of sciences	4	100	26	2003	dec	2005-06-16 15:27:46	department of preventive medicine, keck school of medicine, university of southern california, los angeles, ca 90089, usa.	markov chain monte carlo without likelihoods	many stochastic simulation approaches for generating observations from a posterior distribution depend on knowing a likelihood function. however, for many complex probability models, such likelihoods are either impossible or computationally prohibitive to obtain. here we present a markov chain monte carlo method for generating observations from a posterior distribution without the use of likelihoods. it can also be used in frequentist applications, in particular for maximum-likelihood estimation. the approach is illustrated by an example of ancestral inference in population genetics. a number of open problems are highlighted in the discussion.
521	233773	book	\N	\N	\N	the mit press	\N	\N	\N	2001	dec	2005-06-21 17:12:33	\N	theoretical neuroscience: computational and mathematical modeling of neural systems	{theoretical neuroscience provides a quantitative basis for describing what nervous systems do, determining how they function, and uncovering the general principles by which they operate. this text introduces the basic mathematical and computational methods of theoretical neuroscience and presents applications in a variety of areas including vision, sensory-motor integration, development, learning, and memory.<br /> <br /> \	the book is divided into three parts. part i discusses the relationship between sensory stimuli and neural responses, focusing on the representation of information by the spiking activity of neurons. part ii discusses the modeling of neurons and neural circuits on the basis of cellular and synaptic biophysics. part iii analyzes the role of plasticity in development and learning. an appendix covers the mathematical methods used, and exercises are available on the book's web site.}
522	233775	book	\N	\N	\N	mcgraw-hill medical	\N	\N	\N	2000	jan	2005-06-21 17:13:50	\N	principles of neural science	now in resplendent color, this new edition continues to define the latest in the scientific understanding of the brain, the nervous system, and human behavior. each chapter is thoroughly revised and includes the impact of molecular biology in the mechanisms underlying developmental processes and in the pathogenesis of disease.
523	234370	proceedings	bioinformatics conference, 2002. proceedings. ieee computer society	\N	\N	\N	9	\N	\N	2002	\N	2007-02-16 01:14:17	\N	prediction of protein function using protein-protein interaction data	assigning functions to novel proteins is one of the most important problems in the post-genomic era. we develop a novel approach that applies the theory of markov random fields to infer a protein's functions using protein-protein interaction data and the functional annotations of its interaction protein partners. for each function of interest and a protein, we predict the probability that the protein has that function using bayesian approaches. unlike in other available approaches for protein annotation where a protein has or does not have a function of interest, we give a probability for having the function. this probability indicates how confident we are about the prediction. we apply our method to predict cellular functions (43 categories including a category "others") for yeast proteins defined in the yeast proteome database, using the protein-protein interaction data from the munich information center for protein sequences. we show that our approach outperforms other available methods for function prediction based on protein interaction data.
524	235238	article	trends in ecology \\& evolution	\N	\N	\N	6	20	5	2005	may	2005-06-23 06:14:38	\N	the invasion of language: emergence, change and death	research into the emergence and evolution of human language has received unprecedented attention during the past 15 years. efforts to better understand the processes of language emergence and evolution have proceeded in two main directions: from the top-down (linguists) and from the bottom-up (cognitive scientists). language can be viewed as an invading process that has had profound impact on the human phenotype at all levels, from the structure of the brain to modes of cultural interaction. in our view, the most effective way to form a connection between the two efforts (essential if theories for language evolution are to reflect the constraints imposed on language by the brain) lies in computational modelling, an approach that enables numerous hypotheses to be explored and tested against objective criteria and which suggest productive paths for empirical researchers to then follow. here, with the aim of promoting the cross-fertilization of ideas across disciplines, we review some of the recent research that has made use of computational methods in three principal areas of research into language evolution: language emergence, language change, and language death.
525	235618	article	physical review letters	\N	\N	\N	3	85	25	2000	dec	2005-06-23 16:32:55	\N	network robustness and fragility: percolation on random graphs	recent work on the internet, social networks, and the power grid has addressed the resilience of these networks to either random or targeted deletion of network nodes or links. such deletions include, for example, the failure of internet routers or power transmission lines. percolation models on random graphs provide a simple representation of this process but have typically been limited to graphs with poisson degree distribution at their vertices. such graphs are quite unlike real-world networks, which often possess power-law or other highly skewed degree distributions. in this paper we study percolation on graphs with completely general degree distribution, giving exact solutions for a variety of cases, including site percolation, bond percolation, and models in which occupation probabilities depend on vertex degree. we discuss the application of our theory to the understanding of network resilience.
526	235798	article	sci. comput. program.	\N	\N	elsevier north-holland, inc.	25	51	3	2004	jun	2005-06-23 22:44:06	amsterdam, the netherlands, the netherlands	lessons learned from real {dsl} experiments	over the years, our group, led by bob balzer, designed and implemented three domain-specific languages for use in real applications. each was invented to "showcase" {dsl} language design and implementation technology that was the focus of our then-current research. each of these was actually a prototype for what would have taken more time to engineer and polish before putting into practice. although each effort was essentially successful, none of the languages was ever followed up with the subsequent engineering efforts that we expected or at least hoped for. herein i elaborate where these language efforts succeeded and where they failed, gleaning lessons for others who take the somewhat risky step of committing to develop a {dsl} for a particular user community.
527	235839	book	\N	\N	zeuthen lecture book series	the mit press	\N	\N	\N	1998	\N	2005-06-23 23:19:43	cambridge, massachussets	modeling bounded rationality	{the notion of bounded rationality was initiated in the 1950s by herbert simon; only recently has it influenced mainstream economics. in this book, ariel rubinstein defines models of bounded rationality as those in which elements of the process of choice are explicitly embedded. the book focuses on the challenges of modeling bounded rationality, rather than on substantial economic implications.<br /> <br /> in the first part of the book, the author considers the modeling of choice. after discussing some psychological findings, he proceeds to the modeling of procedural rationality, knowledge, memory, the choice of what to know, and group decisions.<br /> <br /> in the second part, he discusses the fundamental difficulties of modeling bounded rationality in games. he begins with the modeling of a game with procedural rational players and then surveys repeated games with complexity considerations. he ends with a discussion of computability constraints in games. the final chapter includes a critique by herbert simon of the author's methodology and the author's response.<br /> <br /> the zeuthen lecture book series is sponsored by the institute of economics at the university of copenhagen.}
528	235888	article	ieee transactions on systems, man, and cybernetics	\N	\N	\N	14	21	3	1991	may	2005-06-23 23:19:43	\N	a survey of decision tree classifier methodology	decision tree classifiers (dtc's) are used successfully in many diverse areas such as radar signal classification, character recognition, remote sensing, medical diagnosis, expert systems, and speech recognition, to name only a few. perhaps, the most important feature of dtc's is their capability to break down a complex decision-making process into a collection of simpler decisions, thus providing a solution which is often easier to interpret. this paper presents a survey of current methods for dtc designs and the various existing issues. after considering potential advantages of dtc's over single stage classifiers, the subjects of tree structure design, feature selection at each internal node, and decision and search strategies are discussed. some remarks concerning the relation between decision trees and neural networks (nn) are also made.
529	235931	book	\N	\N	\N	w. w. norton and company	\N	\N	\N	2003	\N	2005-06-23 23:19:44	\N	six degrees: the science of a connected age	{you may be only six degrees away from kevin bacon, but would he let you  borrow his car? it depends on the structures within the network that  links you. when the power goes out, when we find that a stranger knows  someone we know, when dot-com stocks soar in price, networks are  evident. in <i>six degrees</i>, sociologist duncan watts examines  networks like these: what they are, how they're being studied, and what  we can use them for. to illustrate the often complicated mathematics  that describe such structures, watts uses plenty of examples from  life, without which this book would quickly move beyond a general  science readership. small chapters make each thought-provoking  conclusion easy to swallow, though some are hard to digest. for  instance, in a short bit on "coercive externalities," watts sums up  sociological research showing that: <p>  <blockquote>"conversations concerning politics displayed a consistent  pattern .... on election day, the strongest predictor of electoral  success was not which party an individual privately supported but which  party he or she expected would win."</blockquote> </p> <i>six degrees</i> attempts to help readers understand the new and  exciting field of networks and complexity. while considerably more  demanding than a general book like <i>the tipping point</i>,  it offers readers a snapshot of a riveting moment in science, when  understanding things like disease epidemics and the stock market seems  almost within our reach. <i>--therese littleton</i>} {the pioneering young scientist whose work on the structure of small worlds has triggered an avalanche of interest in networks.   in this remarkable book, duncan watts, one of the principal architects of network theory, sets out to explain the innovative research that he and other scientists are spearheading to create a blueprint of our connected planet. whether they bind computers, economies, or terrorist organizations, networks are everywhere in the real world, yet only recently have scientists attempted to explain their mysterious workings.  <p>from epidemics of disease to outbreaks of market madness, from people searching for information to firms surviving crisis and change, from the structure of personal relationships to the technological and social choices of entire societies, watts weaves together a network of discoveries across an array of disciplines to tell the story of an explosive new field of knowledge, the people who are building it, and his own peculiar path in forging this new science. 24 b/w illustrations.}
530	235962	book	\N	\N	\N	princeton university press	\N	\N	\N	2003	\N	2005-06-23 23:19:45	\N	small worlds : the dynamics of networks between order and randomness	everyone knows the small-world phenomenon: soon after meeting a stranger, we are surprised to discover that we have a mutual friend, or we are connected through a short chain of acquaintances. in his book, duncan watts uses this intriguing phenomenon{\\\\textendash}colloquially called 'six degrees of separation'{\\\\textendash}as a prelude to a more general exploration: under what conditions can a small world arise in any kind of network? the networks of this story are everywhere: the brain is a network of neurons; organisations are people networks; the global economy is a network of national economies, which are networks of markets, which are in turn networks of interacting producers and consumers. food webs, ecosystems, and the internet can all be represented as networks, as can strategies for solving a problem, topics in a conversation, and even words in a language. many of these networks, the author claims, will turn out to be small worlds. how do such networks matter? simply put, local actions can have global consequences, and the relationship between local and global dynamics depends critically on the network's structure. watts illustrates the subtleties of this relationship using a variety of simple models{\\\\textemdash}the spread of infectious disease through a structured population; the evolution of cooperation in game theory; the computational capacity of cellular automata; and the sychronisation of coupled phase-oscillators. watts's novel approach is relevant to many problems that deal with network connectivity and complex systems' behaviour in general: how do diseases (or rumours) spread through social networks? how does cooperation evolve in large groups? how do cascading failures propagate through large power grids, or financial systems? what is the most efficient architecture for an organisation, or for a communications network? this fascinating exploration will be fruitful in a remarkable variety of fields, including physics and mathematics, as well as sociology, economics, and biology.
531	235973	book	\N	\N	\N	perseus books group	\N	\N	\N	1999	\N	2005-06-23 23:19:45	\N	emergence: from chaos to order	{"emergence" is the notion that the whole is more than the sum of its parts. john holland, a macarthur fellow known as the "father of genetic algorithms," says this seemingly simple notion will be at the heart of the development of machines that can think for themselves. and while he claims that he'd rather do science than write about it, this is his second scientific philosophy book intended to increase public understanding of difficult concepts (his first was <i>hidden order: how adaptation builds complexity</i>). one of the questions that holland says emergence theory can help answer is: can we build systems from which more comes out than was put in? think of the food replicators in the imaginary future of <i>star trek</i>--with some basic chemical building blocks and simple rules, those machines can produce everything from klingon delicacies to earl grey tea. if scientists can understand and apply the knowledge they gather from studying emergent systems, we may soon witness the development of artificial intelligence, nanotech, biological machines, and other creations heretofore confined to science fiction. using games, molecules, maps, and scientific theories as examples, holland outlines how emergence works, emphasizing the interrelationships of simple rules and parts in generating a complex whole. because of the theoretical depth, this book probably won't appeal to the casual reader of popular science, but those interested in delving a little deeper into the future of science and engineering will be fascinated. holland's writing, while sometimes self-consciously precise, is clear, and he links his theoretical arguments to examples in the real world whenever possible. emergence offers insight not just to scientific advancement, but across many areas of human endeavor--business, the arts, even the evolution of society and the generation of new ideas. <i>--therese littleton</i> } {'he's the man who taught computers how to have sex. and now, for an encore, he's working on a theory to explain the complexity of life and its myriad manifestations on planet earth.'     new york times      in this book, one of today's most innovative thinkers, john h. holland, explains the theory of emergencedsa simple theory that the whole is greater than the sum of its parts. emergence demonstrates that a small number of rules or laws can generate incredibly complex systems. from the checkers-playing computer that learnt to beat its creator again and again, to a fertilized egg that can program the development of a trillion-cell organism, to the ant colonies that build bridges over chasms and navigate leaf-boats on streams, this fascinating and groundbreaking book contains wide-ranging implications for science, business, and the arts.     'john holland is an exceptionally imaginative person. often surprising, and always engaging, he takes the reader on a journey from simplicity to complexity'     sir robert may}
532	236029	inproceedings	\N	proc. sigmod	\N	\N	\N	\N	\N	2002	\N	2005-06-24 01:05:10	\N	holistic twig joins: optimal xml pattern matching	xml employs a tree-structured data model, and, naturally, xml queries specify patterns of selection predicates on multiple elements related by a tree structure. finding all occurrences of such a twig pattern in an xml database is a core operation for xml query processing. prior work has typically decomposed the twig pattern into binary structural (parent-child and ancestor-descendant) relationships, and twig matching is achieved by: (i) using structural join algorithms to match the binary relationships against the xml database, and (ii) stitching together these basic matches. a limitation of this approach for matching twig patterns is that intermediate result sizes can get large, even when the input and output sizes are more manageable.in this paper, we propose a novel holistic twig join algorithm, twigstack, for matching an xml query twig pattern. our technique uses a chain of linked stacks to compactly represent partial results to root-to-leaf query paths, which are then composed to obtain matches for the twig pattern. when the twig pattern uses only ancestor-descendant relationships between elements, twigstack is i/o and cpu optimal among all sequential algorithms that read the entire input: it is linear in the sum of sizes of the input lists and the final result list, but independent of the sizes of intermediate results. we then show how to use (a modification of) b-trees, along with twigstack, to match query twig patterns in sub-linear time. finally, we complement our analysis with experimental results on a range of real and synthetic data, and query twig patterns.
533	236100	article	genome research	\N	\N	\N	10	15	2	2005	feb	2005-06-24 13:35:59	department of computer science, stanford university, stanford, california 94305, usa.	{probcons}: probabilistic consistency-based multiple sequence alignment.	to study gene evolution across a wide range of organisms, biologists need accurate tools for multiple sequence alignment of protein families. obtaining accurate alignments, however, is a difficult computational problem because of not only the high computational cost but also the lack of proper objective functions for measuring alignment quality. in this paper, we introduce probabilistic consistency, a novel scoring function for multiple sequence comparisons. we present probcons, a practical tool for progressive protein multiple sequence alignment based on probabilistic consistency, and evaluate its performance on several standard alignment benchmark data sets. on the balibase, sabmark, and prefab benchmark alignment databases, probcons achieves statistically significant improvement over other leading methods while maintaining practical speed. probcons is publicly available as a web resource.
534	238820	article	brain and cognition	\N	\N	\N	18	55	1	2004	jun	2005-06-27 17:20:08	department of experimental psychology, university of oxford, south parks road, oxford ox1 3ud, england, uk. edmund.rolls@psy.ox.ac.uk	the functions of the orbitofrontal cortex.	the orbitofrontal cortex contains the secondary taste cortex, in which the reward value of taste is represented. it also contains the secondary and tertiary olfactory cortical areas, in which information about the identity and also about the reward value of odours is represented. the orbitofrontal cortex also receives information about the sight of objects from the temporal lobe cortical visual areas, and neurons in it learn and reverse the visual stimulus to which they respond when the association of the visual stimulus with a primary reinforcing stimulus (such as taste) is reversed. this is an example of stimulus-reinforcement association learning, and is a type of stimulus-stimulus association learning. more generally, the stimulus might be a visual or olfactory stimulus, and the primary (unlearned) positive or negative reinforcer a taste or touch. a somatosensory input is revealed by neurons that respond to the texture of food in the mouth, including a population that responds to the mouth feel of fat. in complementary neuroimaging studies in humans, it is being found that areas of the orbitofrontal cortex are activated by pleasant touch, by painful touch, by taste, by smell, and by more abstract reinforcers such as winning or losing money. damage to the orbitofrontal cortex can impair the learning and reversal of stimulus-reinforcement associations, and thus the correction of behavioural responses when there are no longer appropriate because previous reinforcement contingencies change. the information which reaches the orbitofrontal cortex for these functions includes information about faces, and damage to the orbitofrontal cortex can impair face (and voice) expression identification. this evidence thus shows that the orbitofrontal cortex is involved in decoding and representing some primary reinforcers such as taste and touch; in learning and reversing associations of visual and other stimuli to these primary reinforcers; and in controlling and correcting reward-related and punishment-related behavior, and thus in emotion. the approach described here is aimed at providing a fundamental understanding of how the orbitofrontal cortex actually functions, and thus in how it is involved in motivational behavior such as feeding and drinking, in emotional behavior, and in social behavior.
535	238866	article	journal of molecular biology	\N	\N	\N	10	346	1	2005	feb	2005-06-27 18:10:37	\N	the relationship between domain duplication and recombination	protein domains represent the basic evolutionary units that form proteins. domain duplication and shuffling by recombination are probably the most important forces driving protein evolution and hence the complexity of the proteome. while the duplication of whole genes as well as domain-encoding exons increases the abundance of domains in the proteome, domain shuffling increases versatility, i.e. the number of distinct contexts in which a domain can occur. here, we describe a comprehensive, genome-wide analysis of the relationship between these two processes. we observe a strong and robust correlation between domain versatility and abundance: domains that occur more often also have many different combination partners. this supports the view that domain recombination occurs in a random way. however, we do not observe all the different combinations that are expected from a simple random recombination scenario, and this is due to frequent duplication of specific domain combinations. when we simulate the evolution of the protein repertoire considering stochastic recombination of domains followed by extensive duplication of the combinations, we approximate the observed data well. our analyses are consistent with a stochastic process that governs domain recombination and thus protein divergence with respect to domains within a polypeptide chain. at the same time, they support a scenario in which domain combinations are formed only once during the evolution of the protein repertoire, and are then duplicated to various extents. the extent of duplication of different combinations varies widely and, in nature, will depend on selection for the domain combination based on its function. some of the pair-wise domain combinations that are highly duplicated also recur frequently with other partner domains, and thus represent evolutionary units larger than single protein domains, which we term  ” supra-domains”.
536	238884	article	embo reports	\N	\N	\N	4	5	3	2004	mar	2005-06-27 18:49:43	\N	the yeast coexpression network has a small-world, scale-free architecture and can be explained by a simple model	we investigated the gene coexpression network in saccharomyces cerevisiae, in which genes are linked when they are coregulated. this network is shown to have a scale-free, small-world architecture. such architecture is typical of biological networks in which the nodes are connected when they are involved in the same biological process. current models for the evolution of intracellular networks do not adequately reproduce the features that we observe in the network. we therefore derive a new model for its evolution based on the observation that there is a positive correlation between the sequence similarity of paralogues and their probability of coexpression or sharing of transcription factor binding sites ({tfbss}). the simple, neutralist's model consists of (1) coduplication of genes with their {tfbss}, (2) deletion and duplication of individual {tfbss} and (3) gene loss. a network is constructed by connecting genes that share multiple {tfbss}. our model reproduces the scale-free, small-world architecture of the coregulation network and the homology relations between coregulated genes without the need for selection either at the level of the network structure or at the level of gene regulation. [journal article; in english; england]
537	238896	article	nature genetics	\N	\N	nature publishing group	6	37	\N	2005	jun	2005-06-27 19:13:42	department of pathology, comprehensive cancer center, university of michigan medical school, ann arbor, michigan 48109, usa.	integrative analysis of the cancer transcriptome	{dna} microarrays have been widely applied to the study of human cancer, delineating myriad molecular subtypes of cancer, many of which are associated with distinct biological underpinnings, disease progression and treatment response. these primary analyses have begun to decipher the molecular heterogeneity of cancer, but integrative analyses that evaluate cancer transcriptome data in the context of other data sources are often capable of extracting deeper biological insight from the data. here we discuss several such integrative computational and analytical approaches, including meta-analysis, functional enrichment analysis, interactome analysis, transcriptional network analysis and integrative model system analysis.
538	238904	article	biophysical chemistry	\N	\N	\N	\N	\N	\N	-1	\N	2005-06-27 19:38:32	\N	thermodynamic calculations in biological systems	the ability to compute intra- and inter-molecular interactions provides the opportunity to gain a deeper understanding of previously intractable problems in biochemistry and biophysics. this review presents three examples in which molecular dynamics calculations were used to gain insight into the atomic detail underlying important experimental observations. the three examples are the following: (1) entropic contribution to rate acceleration that results from conformational constraints imposed on the reactants; (2) mechanism of force unfolding of a small protein molecule by the application of a force that separates its n- and c-terminals; and (3) loss of translational entropy experienced by small molecules when they bind to proteins.
539	239493	inproceedings	\N	proceedings of the eighth conference on european chapter of the association for computational linguistics	\N	association for computational linguistics	9	\N	\N	1997	\N	2005-06-28 15:17:47	morristown, nj, usa	{paradise}: a framework for evaluating spoken dialogue agents	this paper presents paradise (paradigm for dialogue system evaluation), a general framework for evaluating spoken dialogue agents. the framework decouples task requirements from an agent's dialogue behaviors, supports comparisons among dialogue strategies, enables the calculation of performance over subdialogues and whole dialogues, specifies the relative contribution of various factors to performance, and makes it possible to compare agents performing different tasks by normalizing for task complexity.
540	239628	article	nature	\N	\N	macmillan magazines ltd.	3	408	6810	2000	nov	2005-06-28 22:10:08	\N	surfing the p53 network	the p53 tumour-suppressor gene integrates numerous signals that control cell life and death. as when a highly connected node in the internet breaks down, the disruption of p53 has severe consequences. tumour-suppressor genes are needed to keep cells under control. just as a car's brakes regulate its speed, properly functioning tumour-suppressor genes act as brakes to the cycle of cell growth, {dna} replication and division into two new cells.
541	240229	article	nucl. acids. res.	\N	\N	\N	6	25	24	1997	\N	2005-06-29 13:15:48	\N	the {clustalx} windows interface: flexible strategies for multiple sequence alignment aided by quality analysis tools	clustal x is a new windows interface for the widely-used progressive multiple sequence alignment program clustal w, the new system is easy to use, providing an integrated system for performing multiple sequence and profile alignments and analysing the results, clustal x displays the sequence alignment in a window on the screen, a versatile sequence colouring scheme allows the user to highlight conserved features in the alignment, pull-down menus provide all the options required for traditional multiple sequence and profile alignment, new features include: the ability to cut-and-paste sequences to change the order of the alignment, selection of a subset of the sequences to be realigned, and selection of a sub-range of the alignment to be realigned and inserted back into the original alignment, alignment quality analysis can be performed and low-scoring segments or exceptional residues can be highlighted, quality analysis and realignment of selected residue ranges provide the user with a powerful tool to improve and refine difficult alignments and to trap errors in input sequences, clustal x has been compiled on sun solaris, irix5.3 on silicon graphics, digital unix on decstations, microsoft windows (32 bit) for pcs, linux elf for x86 pcs, and macintosh powermac.
542	240516	article	commun. acm	\N	\N	\N	7	\N	38	1995	mar	2005-06-30 04:32:22	\N	going digital: a look at assumptions underlying digital libraries	what are digital libraries, how should they be designed, how will they be used, and what relationship will they bear to what we now call â€œlibrariesâ€? although we cannot hope to answer all these crucial questions in this short article, we do hope to encourage, and in some small measure to shape, the dialog among computer scientists, librarians, and other interested parties out of which answers may arise. our contribution here is to make explicit, and to question, certain assumptions that underlie current digital library efforts. we will argue that current efforts are limited by a largely unexamined and unintended allegiance to an idealized view of what libraries have been, rather than what they actually are or could be. since these limits come from current ways of thinking about the problem, rather than being inherent in the technology or in social practice, expanding our conception of digital libraries should serve to expand the scope and the utility of development efforts.
543	240622	inproceedings	\N	proceedings of the seventh conference on world wide web	\N	\N	9	\N	\N	1998	apr	2005-06-30 11:33:43	brisbane, australia	automatic resource compilation by analyzing hyperlink structure and associated text	we describe the design, prototyping and evaluation of arc, a system for automatically compiling a list of authoritative web resources on any (sufficiently broad) topic. the goal of arc is to compile resource lists similar to those provided by yahoo! or infoseek. the fundamental difference is that these services construct lists either manually or through a combination of human and automated effort, while arc operates fully automatically. we describe the evaluation of arc, yahoo!, and infoseek resource lists by a panel of human users. this evaluation suggests that the resources found by arc frequently fare almost as well as, and sometimes better than, lists of resources that are manually compiled or classified into a topic. we also provide examples of arc resource lists for the reader to examine.
544	240673	book	\N	\N	\N	addison wesley	\N	\N	\N	1995	\N	2005-06-30 11:33:45	reading, ma	design patterns	{<i>design patterns</i> is a modern classic in the literature of object-oriented development, offering timeless and elegant solutions to common problems in software design. it describes patterns for managing object creation, composing objects into larger structures, and coordinating control flow between objects. the book provides numerous examples where using composition rather than inheritance can improve the reusability and flexibility of code. note, though, that it's not a tutorial but a catalog that you can use to find an object-oriented design pattern that's appropriate for the needs of your particular application--a selection for virtuoso programmers who appreciate (or require) consistent, well-engineered object-oriented designs.} {now on cd, this internationally acclaimed bestseller is more valuable than ever! <p> use the contents of the cd to create your own design documents and reusable components. the cd contains: 23 patterns you can cut and paste into your own design documents; sample code demonstrating pattern implementation; complete design patterns content in standard html format, with numerous hyperlinked cross-references; accessed through a standard web browser; java-based dynamic search mechanism, enhancing online seach capabilities; graphical user environment, allowing ease of navigation. <p> first published in 1995, this landmark work on object-oriented software design presents a catalog of simple and succinct solutions to common design problems. created by four experienced designers, the 23 patterns contained herein have become an essential resource for anyone developing reusable object-oriented software. in response to reader demand, the complete text and pattern catalog are now available on cd-rom. this electronic version of <i>design patterns</i> enables programmers to install the book directly onto a computer or network for use as an online reference for creating reusable object-oriented software. <p> the authors first describe what patterns are and how they can help you in the design process. they then systematically name, explain, evaluate, and catalog recurring designs in object-oriented systems. all patterns are compiled from real-world examples and include code that demonstrates how they may be implemented in object-oriented programming languages such as c++ and smalltalk. readers who already own the book will want the cd to take advantage of its dynamic search mechanism and ready-to-install patterns.}
545	240731	inproceedings	\N	proceedings of the seventh conference on world wide web	\N	elsevier science	\N	\N	\N	1998	\N	2005-06-30 11:33:47	brisbane, australia	efficient crawling through {url} ordering	in this paper we study in what order a crawler should visit the urls it has seen, in order to obtain more important pages first. obtaining important pages rapidly can be very useful when a crawler cannot visit the entire web in a reasonable amount of time. we define several importance metrics, ordering schemes, and performance evaluation measures for this problem. we also experimentally evaluate the ordering schemes on the stanford university web. our results show that a crawler with a good...
546	240813	article	science	\N	\N	\N	2	280	5360	1998	apr	2005-06-30 11:33:50	\N	strong regularities in world wide web surfing	one of the most common modes of accessing information in the world wide web is surfing from one document to another along hyperlinks. several large empirical studies have revealed common patterns of surfing behavior. a model that assumes that users make a sequence of decisions to proceed to another page, continuing as long as the value of the current page exceeds some threshold, yields the probability distribution for the number of pages that a user visits within a given web site. this model was verified by comparing its predictions with detailed measurements of surfing patterns. the model also explains the observed zipf-like distributions in page hits observed at web sites.
547	240878	inproceedings	\N	proceedings of the international joint conference on artificial intelligence ({ijcai})	\N	\N	7	\N	\N	2001	\N	2005-06-30 11:33:56	seattle, washington, usa	link analysis, eigenvectors and stability	the hits and the pagerank algorithms are eigenvector methods for identifying â€œauthoritativeâ€ or â€œinfluentialâ€ articles, given hyperlink or citation information. that such algorithms should give consistent answers is surely a desideratum, and in this paper, we address the question of when they can be expected to give stable rankings under small perturbations to the hyperlink patterns. using tools from matrix perturbation theory and markov chain theory, we provide conditions under which these methods are stable, and give specific examples of instability when these conditions are violated. we also briefly describe a modification to hits that improves its stability.
548	241030	article	proceedings of the national academy of sciences	\N	\N	national academy of sciences	5	100	16	2003	aug	2005-06-30 18:30:23	department of biostatistics, university of washington, seattle, wa 98195, usa. jstorey@u.washington.edu	statistical significance for genomewide studies	with the increase in genomewide experiments and the sequencing of multiple genomes, the analysis of large data sets has become commonplace in biology. it is often the case that thousands of features in a genomewide data set are tested against some null hypothesis, where a number of features are expected to be significant. here we propose an approach to measuring statistical significance in these genomewide studies based on the concept of the false discovery rate. this approach offers a sensible balance between the number of true and false positives that is automatically calibrated and easily interpreted. in doing so, a measure of statistical significance called the q value is associated with each tested feature. the q value is similar to the well known p value, except it is a measure of significance in terms of the false discovery rate rather than the false positive rate. our approach avoids a flood of false positive results, while offering a more liberal criterion than what has been used in genome scans for linkage.
549	241036	article	bioinformatics (oxford, england)	\N	\N	\N	12	18	3	2002	mar	2005-06-30 18:56:20	department of chemistry and biochemistry, university of california, los angeles, los angeles, ca 90095-1570, usa. leec@mbi.ucla.edu	multiple sequence alignment using partial order graphs.	progressive multiple sequence alignment ({msa}) methods depend on reducing an {msa} to a linear profile for each alignment step. however, this leads to loss of information needed for accurate alignment, and gap scoring artifacts. we present a graph representation of an {msa} that can itself be aligned directly by pairwise dynamic programming, eliminating the need to reduce the {msa} to a profile. this enables our algorithm (partial order alignment ({poa})) to guarantee that the optimal alignment of each new sequence versus each sequence in the {msa} will be considered. moreover, this algorithm introduces a new edit operator, homologous recombination, important for multidomain sequences. the algorithm has improved speed (linear time complexity) over existing {msa} algorithms, enabling construction of massive and complex alignments (e.g. an alignment of 5000 sequences in 4 h on a pentium {ii}). we demonstrate the utility of this algorithm on a family of multidomain {sh2} proteins, and on {est} assemblies containing alternative splicing and polymorphism. the partial order alignment program {poa} is available at http://www.bioinformatics.ucla.edu/poa.
550	241086	book	\N	\N	\N	{princeton university press}	\N	\N	\N	2004	nov	2005-06-30 23:54:37	\N	friction : an enthography of global connection	{<p>a wheel turns because of its encounter with the surface of the road; spinning in the air it goes nowhere. rubbing two sticks together produces heat and light; one stick alone is just a stick. in both cases, it is friction that produces movement, action, effect. challenging the widespread view that globalization invariably signifies a "clash" of cultures, anthropologist anna tsing here develops friction in its place as a metaphor for the diverse and conflicting social interactions that make up our contemporary world. </p><p>she focuses on one particular "zone of awkward engagement"--the rainforests of indonesia--where in the 1980s and the 1990s capitalist interests increasingly reshaped the landscape not so much through corporate design as through awkward chains of legal and illegal entrepreneurs that wrested the land from previous claimants, creating resources for distant markets. in response, environmental movements arose to defend the rainforests and the communities of people who live in them. not confined to a village, a province, or a nation, the social drama of the indonesian rainforest includes local and national environmentalists, international science, north american investors, advocates for brazilian rubber tappers, un funding agencies, mountaineers, village elders, and urban students, among others--all combining in unpredictable, messy misunderstandings, but misunderstandings that sometimes work out.</p><p>providing a portfolio of methods to study global interconnections, tsing shows how curious and creative cultural differences are in the grip of worldly encounter, and how much is overlooked in contemporary theories of the global.</p>}
551	241091	article	bmc bioinformatics	\N	\N	\N	\N	6	1	2005	apr	2005-07-01 00:10:01	center for genomics and bioinformatics, karolinska institutet, s-17177 stockholm, sweden. markus.wistrand@cgb.ki.se	improved profile {hmm} performance by assessment of critical algorithmic features in {sam} and {hmmer}.	{background}: profile hidden markov model ({hmm}) techniques are among the most powerful methods for protein homology detection. yet, the critical features for successful modelling are not fully known. in the present work we approached this by using two of the most popular {hmm} packages: {sam} and {hmmer}. the programs' abilities to build models and score sequences were compared on a {scop}/pfam based test set. the comparison was done separately for local and global {hmm} scoring. {results}: using default settings, {sam} was overall more sensitive. {sam}'s model estimation was superior, while {hmmer}'s model scoring was more accurate. critical features for model building were then analysed by comparing the two packages' algorithmic choices and parameters. the weighting between prior probabilities and multiple alignment counts held the primary explanation why {sam}'s model building was superior. our analysis suggests that {hmmer} gives too much weight to the sequence counts. {sam}'s emission prior probabilities were also shown to be more sensitive. the relative sequence weighting schemes are different in the two packages but performed equivalently. {conclusion}: {sam} model estimation was more sensitive, while {hmmer} model scoring was more accurate. by combining the best algorithmic features from both packages the accuracy was substantially improved compared to their default performance.
552	241096	misc	\N	\N	\N	\N	\N	\N	\N	2000	\N	2005-07-01 00:59:12	\N	on clusterings - good, bad and spectral	we motivate and develop a new bicriteria measure for assessing the quality of a clustering which avoids the drawbacks of existing measures. a simple recursive heuristic has poly-logarithmic worst-case guarantees under the new measure. the main result of the paper is the analysis of a popular spectral algorithm. one variant of spectral clustering turns out to have eective worst-case guarantees; another nds a \\good clustering if it exists.   supported in part by nsf grant ccr-9820850.
553	241241	article	annual review of anthropology	\N	\N	\N	18	31	\N	2002	oct	2005-07-01 12:40:13	\N	the anthropology of online communities	information and communication technologies based on the internet have enabled the emergence of new sorts of communities and communicative practicesphenomena worthy of the attention of anthropological researchers. despite early assessments of the revolutionary nature of the internet and the enormous transformations it would bring about, the changes have been less dramatic and more embedded in existing practices and power relations of everyday life. this review explores researchers' questions, approaches, and insights within anthropology and some relevant related fields, and it seeks to identify promising new directions for study. the general conclusion is that the technologies comprising the internet, and all the text and media that exist within it, are in themselves cultural products. anthropology is thus well suited to the further investigation of these new, and not so new, phenomena.
554	241630	article	nature genetics	nat genet	\N	nature publishing group	4	37	7	2005	jun	2006-03-13 07:15:17	\N	identification of hundreds of conserved and nonconserved human {micrornas}	{micrornas} are noncoding {rnas} of 22 nucleotides that suppress translation of target genes by binding to their {mrna} and thus have a central role in gene regulation in health and disease1, 2, 3, 4, 5. to date, 222 human {micrornas} have been identified6, 86 by random cloning and sequencing, 43 by computational approaches and the rest as putative {micrornas} homologous to {micrornas} in other species. to prove our hypothesis that the total number of {micrornas} may be much larger and that several have emerged only in primates, we developed an integrative approach combining bioinformatic predictions with microarray analysis and sequence-directed cloning. here we report the use of this approach to clone and sequence 89 new human {micrornas} (nearly doubling the current number of sequenced human {micrornas}), 53 of which are not conserved beyond primates. these findings suggest that the total number of human {micrornas} is at least 800.
555	241724	article	nat rev genet	\N	\N	nature publishing group	10	6	7	2005	jul	2005-08-27 08:00:50	\N	synthetic biology	synthetic biologists come in two broad classes. one uses unnatural molecules to reproduce emergent behaviours from natural biology, with the goal of creating artificial life. the other seeks interchangeable parts from natural biology to assemble into systems that function unnaturally. either way, a synthetic goal forces scientists to cross uncharted ground to encounter and solve problems that are not easily encountered through analysis. this drives the emergence of new paradigms in ways that analysis cannot easily do. synthetic biology has generated diagnostic tools that improve the care of patients with infectious diseases, as well as devices that oscillate, creep and play tic-tac-toe.
556	241904	book	\N	\N	\N	broadway	\N	\N	\N	2001	apr	2005-07-02 22:54:35	\N	irrational exuberance	{cnbc, day trading, the motley fool, silicon investor--not since the 1920s has there been such an intense fascination with the u.s. stock market. for an increasing number of americans, logging on to yahoo! finance is a habit more precious than that morning cup of joe (as thousands of sbux and yhoo shareholders know too well). yet while the market continues to go higher, many of us can't get alan greenspan's famous line out of our heads. in <i>irrational exuberance</i>, yale economics professor robert j. shiller examines this public fascination with stocks and sees a combination of factors that have driven stocks higher, including the rise of the internet, 401(k) plans, increased coverage by the popular media of financial news, overly optimistic cheerleading by analysts and other pundits, the decline of inflation, and the rise of the mutual fund industry. he writes: "perceived long-term risk is down.... emotions and heightened attention to the market create a desire to get into the game. such is irrational exuberance today in the united states."<p> by history's yardstick, shiller believes this market is grossly overvalued, and the factors that have conspired to create and amplify this event--the baby-boom effect, the public infatuation with the internet, and media interest--will most certainly abate. he fears that too many individuals and institutions have come to view stocks as their only investment vehicle, and that investors should consider looking beyond stocks as a way to diversify and hedge against the inevitable downturn. this is a serious and well-researched book that should read like a stephen king novel to anyone who has staked his or her future on the market's continued success. --<i>harry c. edwards</i> } {with a new afterword on the current state of the stock market, the ongoing debate over the \\&\\#8220;new economy,\\&\\#8221; and the larger implications of \\&\\#8220;irrational exuberance.\\&\\#8221;<br><br>in this controversial, hard-hitting account of today\\&\\#8217;s explosive market, robert j. shiller, a leading expert on market volatility, evokes alan greenspan\\&\\#8217;s infamous 1996 reference, \\&\\#8220;irrational exuberance,\\&\\#8221; to explain the alternately soaring and declining stock market.  shiller\\&\\#8217;s unconventional yet persuasive argument credits an unprecedented confluence of events with driving stocks to uncharted heights, and he analyzes the structural, cultural, and psychological factors behind these levels of growth not reflected in any other sector of the economy.  now more relevant than ever, this analysis is both chilling and convincing\\&\\#8212;a must-read for the individual investor, the policy maker, and the investment professional.} {in this bold and potentially urgent volume, robert j. shiller, a respected expert on market volatility, offers an unconventional interpretation of recent u.s. stock market highs and shows that alan greenspan's term "irrational exuberance" is a good description of the mood behind the market. he warns that poorer performance may be in the offing and tells us how we--as a country and individually--can respond.<p>shiller credits an unprecedented confluence of events with driving stocks to uncharted heights. he analyzes the structural and psychological factors that explain why the dow jones industrial average tripled between 1994 and 1999, a level of growth not reflected in any other sector of the economy. in contrast to many analysts, shiller stresses circumstances that alter investors' perceptions of the market.}
557	244746	article	plos computational biology	\N	\N	\N	7	1	1	2005	jun	2005-07-04 16:32:47	\N	predicting functional gene links from {phylogenetic-statistical} analyses of whole genomes	an important element of the developing field of proteomics is to understand protein-protein interactions and other functional links amongst genes. across-species correlation methods for detecting functional links work on the premise that functionally linked proteins will tend to show a common pattern of presence and absence across a range of genomes. we describe a maximum likelihood statistical model for predicting functional gene linkages. the method detects independent instances of the correlated gain or loss of pairs of proteins on phylogenetic trees, reducing the high rates of false positives observed in conventional across-species methods that do not explicitly incorporate a phylogeny. we show, in a dataset of 10,551 protein pairs, that the phylogenetic method improves by up to 35\\% on across-species analyses at identifying known functionally linked proteins. the method shows that protein pairs with at least two to three correlated events of gain or loss are almost certainly functionally linked. contingent evolution, in which one gene's presence or absence depends upon the presence of another, can also be detected phylogenetically, and may identify genes whose functional significance depends upon its interaction with other genes. incorporating phylogenetic information improves the prediction of functional linkages. the improvement derives from having a lower rate of false positives and from detecting trends that across-species analyses miss. phylogenetic methods can easily be incorporated into the screening of large-scale bioinformatics datasets to identify sets of protein links and to characterise gene networks.
558	244765	inproceedings	\N	siggraph 2001, computer graphics proceedings	\N	acm press / acm siggraph	5	\N	\N	2001	\N	2005-07-04 17:03:01	\N	image quilting for texture synthesis and transfer	we present a simple image-based method of generating novel visual appearance in which a new image is synthesized by stitching together small patches of existing images. we call this process  image quilting . first, we use quilting as a fast and very simple texture synthesis algorithm which produces surprisingly good results for a wide range of textures. second, we extend the algorithm to perform texture transfer &mdash; rendering an object with a texture taken from a different object. more generally, we demonstrate how an image can be re-rendered in the style of a different image. the method works directly on the images and does not require 3d information.
559	245758	article	notices of the american mathematical society	\N	\N	\N	7	50	5	2003	\N	2005-07-05 02:22:06	\N	the mathematics of learning: dealing with data	in statistical learning for regression problems, data is assumed to be an i.i.d. sample drawn from an unknown and arbitrary probability distribution $p$ over $\\bbb{r}^d\\times\\bbb{r}$.  given such a sample, the goal of the learner is to choose a low-risk function $h\\colon \\bbb{r}^d\\to\\bbb{r}$ from a fixed hypothesis space $\\scr{h}$, that is, a function $h\\in\\scr{h}$ such that the expected value of $ l(h(x),y)$, where $ l$ is a fixed real-valued loss function and the pair $(x,y)\\in\\bbb{r}^d\\times\\bbb{r}$ is drawn from $p$, is as close as possible to its minimum value in $\\scr{h}$. <p> in this paper, the authors provide an excellent survey of some recent results concerning the statistical performance of regularized learning algorithms.  in particular, the authors describe the regularized least-squares regressor with gaussian kernels.  the hypothesis space associated to this algorithm is the family of hyperplanes in the reproducing kernel hilbert space generated by the gaussian kernel with a given variance parameter. this parameter plays a crucial role in balancing the approximation error term (the $l_2$ distance to the bayes optimal regressor) and the risk term---recall that the sum of these two terms equals the expected square loss of the regressor. the analysis of the risk and the approximation error, reported in the paper, captures this tradeoff, opening the way to a principled choice of the kernel parameter. <p> the regularized learning algorithm can be naturally applied to problems other than regression. in particular, the authors show an application to classification problems describing the regularized least-squares algorithm, whose empirical performance is compared to that of support vector machines.
560	246028	article	bioinformatics	\N	\N	oxford university press	6	21	14	2005	jul	2008-04-17 13:50:47	\N	a bayesian regression approach to the inference of regulatory networks from gene expression data	motivation: there is currently much interest in reverse-engineering regulatory relationships between genes from microarray expression data. we propose a new algorithmic method for inferring such interactions between genes using data from gene knockout experiments. the algorithm we use is the sparse bayesian regression algorithm of tipping and faul. this method is highly suited to this problem as it does not require the data to be discretized, overcomes the need for an explicit topology search and, most importantly, requires no heuristic thresholding of the discovered connections.  results: using simulated expression data, we are able to show that this algorithm outperforms a recently published correlation-based approach. crucially, it does this without the need to set any ad hoc threshold on possible connections.  availability: matlab code which allows all experimental results to be reproduced is available at http://www.dcs.gla.ac.uk/\\~{}srogers/reg\\_nets.html  contact: srogers@dcs.gla.ac.uk  supplementary information: appendices and supplementary figures mentioned in the text can be found at http://www.dcs.gla.ac.uk/\\~{}srogers/reg\\_nets.html 10.1093/bioinformatics/bti487
561	248185	article	robotics and autonomous systems	\N	\N	\N	12	6	1\\&2	1990	jun	2005-07-06 23:24:01	\N	elephants don't play chess	this paper we argue that the symbol system hypothesis upon which classical {ai} is base is fundamentally flawed, and as such imposes severe limitations on the fitness of its progeny. further, we argue that the dogma of the symbol system hypothesis implicitly includes a number of largely unfounded great leaps of faith when called upon to provide a plausible path to the digital equivalent of human level intelligence. it is the chasms to be crossed by these leaps which now impede classical {ai}...
562	248994	article	language learning \\& technology	\N	\N	\N	4	7	2	2003	may	2005-07-07 19:04:47	\N	blogs and wikis: environments for on-line collaboration	language professionals have embraced the world of collaborative opportunities the internet hasintroduced. many tools -- e-mail, discussion forums, chat -- are by now familiar to many languageteachers. recent innovations -- blogs, wikis, and rss feeds -- may be less familiar but offer powerfulopportunities for online collaboration for both language professionals and learners. the underlyingtechnology of the new tools is xml ("extensible markup language") which separates content fromformatting, encourages use of meta-data, and enables machine processing of internet documents. thelatter is key in the ability to link automatically disparate documents of interest to individuals or groups.the new collaborative opportunities this enables have led some to consider the growing importance ofxml as the signal of the arrival of the second-generation web. (first paragraph)
563	249006	article	genome research	\N	\N	cold spring harbor laboratory press	11	13	9	2003	sep	2005-07-07 21:10:52	department of biology and genetics, center for bioinformatics, and genomics institute, university of pennsylvania, philadelphia, pennsylvania 19104, usa.	{orthomcl}: identification of ortholog groups for eukaryotic genomes	an international, peer-reviewed genome sciences journal featuring outstanding original research that offers novel insights into the biology of all organisms
564	249102	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2005-07-08 03:07:50	\N	inference in belief networks: a procedural guide	belief networks are popular tools for encoding uncertainty in expert systems. these networks rely on inference algorithms to compute beliefs in the context of observed evidence. one established method for exact inference on belief networks is the probability propagation in trees of clusters (pptc) algorithm, as developed by lauritzen and spiegelhalter and refined by jensen et al. pptc converts the belief network into a secondary structure, then computes probabilities by manipulating the secondary structure. in this document, we provide a self-contained, procedural guide to understanding and implementing pptc. we synthesize various optimizations to pptc that are scattered throughout the literature. we articulate undocumented, "open secrets" that are vital to that are vital to producing a robust and efficient implementation of pptc. we hope that this document makes probabilistic inference more accessible and affordable to those without extensive prior exposure.
565	251558	article	prog biophys mol biol	\N	\N	\N	35	86	1	2004	sep	2005-07-11 14:20:02	centre for mathematical biology, mathematical institute, 24-29 st. giles', oxford ox 1 3lb, uk. e.crampin@auckland.ac.nz	mathematical and computational techniques to deduce complex biochemical reaction mechanisms.	time series data can now be routinely collected for biochemical reaction pathways, and recently, several methods have been proposed to infer reaction mechanisms for metabolic pathways and networks. in this paper we provide a survey of mathematical techniques for determining reaction mechanisms for time series data on the concentration or abundance of different reacting components, with little prior information about the pathways involved.
566	252074	inbook	\N	human-computer interaction - interact	\N	ios	7	\N	\N	2001	\N	2005-07-12 12:41:42	amsterdam	augmenting museums and art galleries	this paper is concerned with the application of context-aware computing to museums and art galleries. the paper reports three studies addressing this issue. the first study involved a survey of visitors to an art gallery and focused on visitor activity and information requirements. this led to the conclusion that one could define visitor types and relate these to the amount, and level of detail, of information for the visitor types. the second study involves a comparative evaluation of...
567	254471	article	journal of computational physics	\N	\N	\N	8	118	\N	1995	\N	2005-07-13 16:34:53	\N	a fast level set method for propagating interfaces	a method is introduced to decrease the computational labor of the standard level set method for propagating interfaces. the fast approach uses only points close to the curve at every time step. we describe this new algorithm and compare its efficiency and accuracy with the standard level set approach. 1 a fast level set implementation the level set technique was introduced in [9] to track moving interfaces in a wide variety of problems. it relies on the relation between propagating interfaces...
568	254902	book	\N	\N	\N	brooks/cole	\N	\N	\N	1999	\N	2005-07-13 16:35:57	\N	image processing, analysis, and machine vision	{this robust text provides deep and wide coverage of the full range of topics encountered in the field of image processing and machine vision.  as a result, it can serve undergraduates, graduates, researchers, and professionals looking for a readable reference. the book's encyclopedic coverage of topics is wider than that found in any competing book, and it can be used in more than one course (both image processing and machine vision classes).  in addition, while advanced mathematics is not needed to understand basic concepts (making this a good choice for undergraduates), rigorous mathematical coverage is included for more advanced readers. this text is especially strong and up-to-date in its treatment of 3d vision, with many topics not covered at all in competing books. it is also distinguished by its easy-to-understand algorithm descriptions of difficult concepts, and a wealth of carefully selected problems and examples that can be worked with any general-purpose image processing software package or programming environment.}
569	255314	book	\N	\N	\N	oxford university press, usa	\N	\N	\N	2004	may	2005-07-13 21:38:25	\N	foundations of human sociality: economic experiments and ethnographic evidence from fifteen {small-scale} societies	what motives underlie the ways humans interact socially? are these the same for all societies? are these part of our nature, or influenced by our environments? over the last decade, research in experimental economics has emphatically falsified the textbook representation of homo economicus. hundreds of experiments suggest that people care not only about their own material payoffs, but also about such things as fairness, equity, and reciprocity. however, this research left fundamental questions unanswered: are such social preferences stable components of human nature, or are they modulated by economic, social, and cultural environments? until now, experimental research could not address this question because virtually all subjects had been university students. combining ethnographic and experimental approaches to fill this gap, this book breaks new ground in reporting the results of a large cross-cultural study aimed at determining the sources of social (non-selfish) preferences that underlie the diversity of human sociality. in this study, the same experiments carried out with university students were performed in fifteen small-scale societies exhibiting a wide variety of social, economic, and cultural conditions. the results show that the variation in behaviour is far greater than previously thought, and that the differences between societies in market integration and the importance of cooperation explain a substantial portion of this variation, which individual-level economic and demographic variables could not. the results also trace the extent to which experimental play mirrors patterns of interaction found in everyday life. the book includes a succinct but substantive introduction to the use of game theory as an analytical tool, and to its use in the social sciences for the rigorous testing of hypotheses about fundamental aspects of social behaviour outside artificially constructed laboratories. the editors also summarize the results of the fifteen case studies in a suggestive chapter about the scope of the project.
570	255551	article	international journal of computer vision	international journal of computer vision	\N	kluwer academic publishers	22	40	2	2000	nov	2005-07-14 05:03:42	hingham, ma, usa	the earth mover's distance as a metric for image retrieval	we investigate the properties of a metric between two distributions, the earth mover's distance ({emd}), for content-based image retrieval. the {emd} is based on the minimal cost that must be paid to transform one distribution into the other, in a precise sense, and was first proposed for certain vision problems by peleg, werman, and rom. for image retrieval, we combine this idea with a representation scheme for distributions that is based on vector quantization. this combination leads to an image comparison framework that often accounts for perceptual similarity better than other previously proposed methods. the {emd} is based on a solution to the transportation problem from linear optimization, for which efficient algorithms are available, and also allows naturally for partial matching. it is more robust than histogram matching techniques, in that it can operate on variable-length representations of the distributions that avoid quantization and other binning problems typical of histograms. when used to compare distributions with the same overall mass, the {emd} is a true metric. in this paper we focus on applications to color and texture, and we compare the retrieval performance of the {emd} with that of other distances.
571	256404	article	communications of the acm	\N	\N	\N	7	21	7	1978	jul	2005-07-15 05:33:05	\N	time, clocks, and the ordering of events in a distributed system	the concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. a distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. the use of the total ordering is illustrated with a method for solving synchorization problems. the algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.
572	256918	article	annual reviews in immunology	\N	\N	\N	19	20	\N	2002	\N	2005-07-15 13:44:28	\N	innate immune recognition	â–ª abstractâ€‚ the innate immune system is a universal and ancient form of host defense against infection. innate immune recognition relies on a limited number of germline-encoded receptors. these receptors evolved to recognize conserved products of microbial metabolism produced by microbial pathogens, but not by the host. recognition of these molecular structures allows the immune system to distinguish infectious nonself from noninfectious self. toll-like receptors play a major role in pathogen recognition and initiation of inflammatory and immune responses. stimulation of toll-like receptors by microbial products leads to the activation of signaling pathways that result in the induction of antimicrobial genes and inflammatory cytokines. in addition, stimulation of toll-like receptors triggers dendritic cell maturation and results in the induction of costimulatory molecules and increased antigen-presenting capacity. thus, microbial recognition by toll-like receptors helps to direct adaptive immune responses to antigens derived from microbial pathogens.
573	257982	book	\N	\N	\N	verso	\N	\N	\N	1991	jul	2005-07-16 21:27:39	\N	imagined communities: reflections on the origin and spread of nationalism	{what makes people love and die for nations, as well as hate and kill in their name? in this important work, benedict anderson focuses a much-needed clear eye on nationalism as cultural artefact, created and transformed through historical processes--a fated and thus pure attachment experienced every day through the connections language forges with a living and dead community.<p>in selecting the genealogy of "thinking" the nation, anderson chooses his trajectory well--thankfully reading not only from the social history of europe, but also from the experiences of its colonies and other states across the globe (the armed conflicts of 1978--79 indochina provided the immediate impetus for the original 1983 text). it is especially these states which anderson's later revisions address, with his wise realisation that so-called "official nationalism" in colonised asia and africa was not transplanted without intervention from that of the dynastic states of 19th-century europe. when dealing with such an emotive subject, anderson thankfully avoids favouring rhetoric over grounded analysis. he thoroughly explains the role of print language in imagining community, particularly with the development of the novel set in a society to which the reader may or may not belong, but can recognise, and the newspaper, which, perhaps replacing morning prayers, is read every day by people who have a sense of their fellow readers' existence.<p> the power of <i>imagined communities</i> ultimately lies in its applied resonances. the force of the argument of an "imagined community" is not only invaluable to sociologists or political economists, but it implicates each of us in compelling notions of identity and belonging whether our imagined community is with a nation or with others who buy, listen to and watch the same cultural products as ourselves. essential reading for anyone interested in a history of the present. --<i>fiona buckland</i>}
574	258855	inproceedings	\N	cikm	\N	acm press	7	\N	\N	2004	\N	2005-07-18 17:01:06	new york, ny, usa	swoogle: a search and metadata engine for the semantic web	swoogle is a crawler-based indexing and retrieval system for the semantic web. it extracts metadata for each discovered document, and computes relations between documents. discovered documents are also indexed by an information retrieval system which can use either character n-gram or urirefs as keywords to find relevant documents and to compute the similarity among a set of documents. one of the interesting properties we compute is <i>ontology rank</i>, a measure of the importance of a semantic web document.
575	259020	article	bioinformatics (oxford, england)	\N	\N	oxford university press	1	21	15	2005	aug	2007-04-23 22:23:40	\N	{provat}: a tool for {v}oronoi tessellation analysis of protein structures and complexes	10.1093/bioinformatics/bti523 summary: voronoi tessellation has proved to be a useful tool in protein structure analysis. we have developed provat, a versatile public domain software that enables computation and visualization of voronoi tessellations of proteins and protein complexes. it is a set of python scripts that integrate freely available specialized software (qhull, pymol etc.) into a pipeline. the calculation component of the tool computes voronoi tessellation of a given protein system in a way described by a user-supplied xml recipe and stores resulting neighbourhood information as text files with various styles. the python pickle file generated in the process is used by the visualization component, a pymol plug-in, that offers a gui to explore the tessellation visually.availability: provat source code can be downloaded from http://raven.bioc.cam.ac.uk/~swanand/provat1, which also provides a webserver for its calculation component, documentation and examples.contact: swanand@cryst.bioc.cam.ac.uk
576	259037	article	bioinformatics	\N	\N	oxford university press	11	21	15	2005	aug	2007-12-05 16:54:12	\N	computational cluster validation in post-genomic data analysis	motivation the discovery of novel biological knowledge from the ab initio analysis of post-genomic data relies upon the use of unsupervised processing methods, in particular clustering techniques. much recent research in bioinformatics has therefore been focused on the transfer of clustering methods introduced in other scientific fields and on the development of novel algorithms specifically designed to tackle the challenges posed by post-genomic data. the partitions returned by a clustering algorithm are commonly validated using visual inspection and concordance with prior biological knowledge—whether the clusters actually correspond to the real structure in the data is somewhat less frequently considered. suitable computational cluster validation techniques are available in the general data-mining literature, but have been given only a fraction of the same attention in {bioinformatics.results} this review paper aims to familiarize the reader with the battery of techniques available for the validation of clustering results, with a particular focus on their application to post-genomic data analysis. synthetic and real biological datasets are used to demonstrate the benefits, and also some of the perils, of analytical cluster {validation.availability}: the software used in the experiments is available at {http://dbkgroup.org/handl/clustervalidation/contact} {j.handl}@{postgrad.manchester.ac.uksupplementary} information: enlarged colour plots are provided in the supplementary material, which is available at http://dbkgroup.org/handl/clustervalidation/
577	260118	article	acm trans. inf. syst.	\N	\N	acm	39	22	3	2004	jul	2005-07-20 20:51:12	new york, ny, usa	{pocketlens}: toward a personal recommender system	recommender systems using collaborative filtering are a popular technique for reducing information overload and finding products to purchase. one limitation of current recommenders is that they are not portable. they can only run on large computers connected to the internet. a second limitation is that they require the user to trust the owner of the recommender with personal preference data. personal recommenders hold the promise of delivering high quality recommendations on palmtop computers, even when disconnected from the internet. further, they can protect the user's privacy by storing personal information locally, or by sharing it in encrypted form. in this article we present the new {pocketlens} collaborative filtering algorithm along with five peer-to-peer architectures for finding neighbors. we evaluate the architectures and algorithms in a series of offline experiments. these experiments show that pocketlens can run on connected servers, on usually connected workstations, or on occasionally connected portable devices, and produce recommendations that are as good as the best published algorithms to date.
578	260693	article	acm transactions on programming languages and systems	\N	\N	\N	32	7	1	1985	\N	2005-07-21 08:08:48	\N	generative communication in {l}inda	generative communication is the basis of a new distributed programming langauge that is intended for systems programming in distributed settings generally and on integrated network computers in particular. it differs from previous interprocess communication models in specifying that messages be added in tuple-structured form to the computation environment, where they exist as named, independent entities until some process chooses to receive them. generative communication results in a number of distinguishing properties in the new language, linda, that is built around it. linda is fully distributed in space and distributed in time; it allows distributed sharing, continuation passing, and structured naming. we discuss these properties and their implications, then give a series of examples. linda presents novel implementation problems that we discuss in part ii. we are particularly concerned with implementation of the dynamic global name space that the generative communication model requires.
579	260715	inproceedings	\N	{sosp}	\N	\N	\N	\N	\N	2001	oct	2005-07-21 08:08:48	chateau lake louise, banff, canada	wide-area cooperative storage with {cfs}	the cooperative file system (cfs) is a new peer-to-peer readonly storage system that provides provable guarantees for the efficiency, robustness, and load-balance of file storage and retrieval. cfs does this with a completely decentralized architecture that can scale to large systems. cfs servers provide a distributed hash table (dhash) for block storage. cfs clients interpret dhash blocks as a file system. dhash distributes and caches blocks at a fine granularity to achieve load balance, uses replication for robustness, and decreases latency with server selection. dhash finds blocks using the chord location protocol, which operates in time logarithmic in the number of servers.  cfs is implemented using the sfs file system toolkit and runs on linux, openbsd, and freebsd. experience on a globally deployed prototype shows that cfs delivers data to clients as fast as ftp. controlled tests show that cfs is scalable: with 4,096 servers, looking up a block of data involves contacting only seven servers. the tests also demonstrate nearly perfect robustness and unimpaired performance even when as many as half the servers fail.  1.
580	260716	inproceedings	\N	5th symposium on operating systems design and implementation	\N	\N	\N	\N	\N	2002	dec	2005-07-21 08:08:48	\N	{farsite}: federated, available, and reliable storage for an incompletely trusted environment	farsite is a serverless, distributed file system that does not assume mutual trust among the client computers on which it runs. logically, the system functions as a central file server, but physically, there is no central server machine. instead, a group of desktop client computers collaboratively establish a virtual file server that can be accessed by any of the clients. the system provides a global name space for files, location-transparent access to both private files and shared public files, and improved reliability relative to storing files on a desktop workstation. it does this by distributing multiple encrypted replicas of each file among a set of client machines. files are referenced through a hierarchical directory structure that is maintained by a distributed directory service. our broad objective is to figure out how to build highly available, reliable, and secure systems on a substrate of cooperating but mutually distrusting hosts. in the context of our distributed file system, we have identified three specific goals: * to provide high availability and reliability for file storage. * to provide security and resistance to byzantine threats. * to have the system automatically configure and tune itself adaptively.
581	260912	inproceedings	\N	seventh annual international conference on mobile computing and networks (mobicom)	\N	\N	10	\N	\N	2001	\N	2005-07-21 08:08:53	\N	{spins}: security protocols for sensor networks	as sensor networks edge closer towards wide-spread deployment, security issues become a central concern. so far, much research has focused on making sensor networks feasible and useful, and has not concentrated on security. we present a suite of security building blocks optimized for resource-constrained environments and wireless communication. spins has two secure building blocks: snep and mtesla. snep provides the following important baseline security primitives: data confidentiality, two-party data authentication, and data freshness. a particularly hard problem is to provide efficient broadcast authentication, which is an important mechanism for sensor networks. mtesla is a new protocol which provides authenticated broadcast for severely resource-constrained environments. we implemented the above protocols, and show that they are practical even on minimal hardware: the performance of the protocol suite easily matches the data rate of our network.additionally, we demonstrate that the suite can be used for building higher level protocols.
582	260973	inproceedings	\N	proceedings of context modeling and reasoning workshop at percom 2004	\N	\N	\N	\N	\N	2004	\N	2005-07-21 08:08:56	\N	ontology based context modeling and reasoning using {owl}	here we propose an owl encoded context ontology (conon) for modeling context in pervasive computing environments, and for supporting logic-based context reasoning. conon provides an upper context ontology that captures general concepts about basic context, and also provides extensibility for adding domain-specific ontology in a hierarchical manner. based on this context ontology, we have studied the use of logic reasoning to check the consistency of context information, and to reason over low-level, explicit context to derive high-level, implicit context. by giving a performance study for our prototype, we quantitatively evaluate the feasibility of logic based context reasoning for nontime-critical applications in pervasive computing environments, where we always have to deal carefully with the limitation of computational resources.
583	261218	article	{ieee} journal on selected areas in communications ({jsac})	\N	\N	\N	\N	20	8	2002	oct	2005-07-21 08:08:57	\N	{scribe}: a large-scale and decentralized application-level multicast infrastructure	this paper presents scribe, a scalable application-level multicast infrastructure. scribe supports large numbers of groups, with a potentially large number of members per group. scribe is built on top of pastry, a generic peer-to-peer object location and routing substrate overlayed on the internet, and leverages pastry's reliability, self-organization, and locality properties. pastry is used to create and manage groups and to build efficient multicast trees for the dissemination of messages to each group. scribe provides best-effort reliability guarantees, and we outline how an application can extend scribe to provide stronger reliability. simulation results, based on a realistic network topology model, show that scribe scales across a wide range of groups and group sizes. also, it balances the load on the nodes while achieving acceptable delay and link stress when compared with internet protocol multicast.
584	261219	techreport	\N	\N	\N	\N	\N	\N	MSR-TR-2004-80	2004	jul	2005-07-21 08:08:57	\N	network coding for large scale content distribution	we propose a new scheme for content distribution of large files that is based on network coding. with network coding, each node of the distribution network is able to generate and transmit encoded blocks of information. the randomization introduced by the coding process eases the scheduling of block propagation, and, thus, makes the distribution more efficient. this is particularly important in large unstructured overlay networks, where the nodes need to make block forwarding decisions based on local information only. we compare network coding to other schemes that transmit unencoded information (i.e. blocks of the original file) and, also, to schemes in which only the source is allowed to generate and transmit encoded packets. we study the performance of network coding in heterogeneous networks with dynamic node arrival and departure patterns, clustered topologies, and when incentive mechanisms to discourage free-riding are in place. we demonstrate through simulations of scenarios of practical interest that the expected file download time improves by more than 20-30\\\\ with network coding compared to coding at the server only and, by more than 2-3 times compared to sending unencoded information. moreover, we show that network coding improves the robustness of the system and is able to smoothly handle extreme situations where the server and nodes leave the system.
585	261290	article	science	\N	\N	american association for the advancement of science	9	277	5331	1997	sep	2005-07-21 09:30:33	laboratory of genetics, university of wisconsin-madison, 445 henry mall, madison, wi 53706, usa. ecoli@genetics.wisc.edu	the complete genome sequence of escherichia coli k-12	the 4,639,221–base pair sequence of escherichia {colik}-12 is presented. of 4288 protein-coding genes annotated, 38 percent have no attributed function. comparison with five other sequenced microbes reveals ubiquitous as well as narrowly distributed gene families; many families of similar genes within e. coli are also evident. the largest family of paralogous proteins contains 80 {abc} transporters. the genome as a whole is strikingly organized with respect to the local direction of replication; guanines, oligonucleotides possibly related to replication and recombination, and most genes are so oriented. the genome also contains insertion sequence ({is}) elements, phage remnants, and many other patches of unusual composition indicating genome plasticity through horizontal transfer.
586	262715	article	reviews of modern physics	\N	\N	\N	261	65	\N	1993	\N	2005-07-22 16:36:16	\N	pattern formation outside of equilibrium	not available
587	263024	inproceedings	\N	proceedings of the 25th international conference on software engineering	icse	ieee computer society	10	\N	\N	2003	\N	2005-07-22 23:50:44	washington, dc, usa	new directions on agile methods: a comparative analysis	agile software development methods have caught the attention of software engineers and researchers worldwide. scientific research is yet scarce. this paper reports results from a study, which aims to organize, analyze and make sense out of the dispersed field of agile software development methods. the comparative analysis is performed using the method's life-cycle coverage, project management support, type of practical guidance, fitness-for-use and empirical evidence as the analytical lenses. the results show that agile software development methods, without rationalization, cover certain/different phases of the software development life-cycle and most of the them do not offer adequate support for project management. yet, many methods still attempt to strive for universal solutions (as opposed to situation appropriate) and the empirical evidence is still very limited based on the results, new directions are suggested in principal it is suggested to place emphasis on methodological quality -- not method quantity.
588	264058	book	\N	\N	\N	prentice hall	\N	\N	\N	1993	sep	2005-07-29 16:30:53	\N	on {lisp}: advanced techniques for common {lisp}	{perhaps the author gives the best description of this book: "<i>on lisp</i> deals mostly  with the kinds of programs you could only write in lisp." the book provides extensive information  on the advanced features of lisp, which are not found in other popular programming languages. after  showing how flexibly functions can be manipulated, <i>on lisp</i> moves on to the best discussion of  macros available, which includes details of the possible pitfalls (various referential bugs, for example). the book concludes with a demonstration of various advanced constructs that can be implemented in lisp using  the tools developed in the earlier part of the book. as with his other book, <i>ansi  common lisp</i>, graham writes in a fluid style that is a pleasure to read. }
589	264330	book	\N	\N	\N	the mit press	\N	\N	\N	2002	may	2005-07-25 18:50:40	\N	distributed work	{technological advances and changes in the global economy are increasing the geographic distribution of work in industries as diverse as banking, wine production, and clothing design. many workers communicate regularly with distant coworkers; some monitor and manipulate tools and objects at a distance. work teams are spread across different cities or countries. joint ventures and multiorganizational projects entail work in many locations. two famous examples--the hudson bay companys seventeenth-century fur trading empire and the electronic community that created the original linux computer operating system--suggest that distributed work arrangements can be flexible, innovative, and highly successful. at the same time, distributed work complicates workers professional and personal lives. distributed work alters how people communicate and how they organize themselves and their work, and it changes the nature of employee-employer relationships.<br /> <br /> this book takes a multidisciplinary approach to the study of distributed work groups and organizations, the challenges inherent in distributed work, and ways to make distributed work more effective. specific topics include division of labor, incentives, managing group members, facilitating interaction among distant workers, and monitoring performance. the final chapters focus on distributed work in one domain, collaborative scientific research. the contributors include psychologists, cognitive scientists, sociologists, anthropologists, historians, economists, and computer scientists.}
590	264340	article	nature neuroscience	\N	\N	nature publishing group	5	7	5	2004	may	2005-07-25 19:24:04	neuroscience statistics research laboratory, department of anesthesia and critical care, massachusetts general hospital, and division of health sciences and technology, harvard medical school, boston, 02114, usa. brown@neurostat.mgh.harvard.edu	multiple neural spike train data analysis: state-of-the-art and future challenges.	multiple electrodes are now a standard tool in neuroscience research that make it possible to study the simultaneous activity of several neurons in a given brain region or across different regions. the data from multi-electrode studies present important analysis challenges that must be resolved for optimal use of these neurophysiological measurements to answer questions about how the brain works. here we review statistical methods for the analysis of multiple neural spike-train data and discuss future challenges for methodology research.
591	265615	article	cognition	\N	\N	\N	26	70	2	1999	mar	2005-07-26 18:19:42	university of arizona, tucson 85721, usa. rgomez@u.arizona.edu	artificial grammar learning by 1-year-olds leads to specific and abstract knowledge.	four experiments used the head-turn preference procedure to assess whether infants could extract and remember information from auditory strings produced by a miniature artificial grammar. in all four experiments, infants generalized to new structure by discriminating new grammatical strings from ungrammatical ones after less than 2 min exposure to the grammar. infants acquired specific information about the grammar as demonstrated by the ability to discriminate new grammatical strings from those with illegal endpoints (experiment 1). infants also discriminated new grammatical strings from those with string-internal pairwise violations (experiments 2 and 3). infants in experiment 4 abstracted beyond specific word order as demonstrated by the ability to discriminate new strings produced by their training grammar from strings produced by another grammar despite a change in vocabulary between training and test. we discuss the implications of these findings for the study of language acquisition.
592	266089	book	\N	\N	\N	{new riders press}	\N	\N	\N	2002	oct	2005-07-27 12:40:09	\N	information architecture: blueprints for the web	{<p>all web sites have an architecture, whether you design one or not-just as every building has an architecture, from the lowly shanty by the railroad track to chicago's tallest skyscraper. unfortunately, most web sites are shanties, not skyscrapers. companies that hastily threw up a web site in the dot-com boom days were visited by building inspector jakob neilsen, who told them their site should be condemned. but now we are entering a time of rebuilding, and we've got a chance to get it right.</p> <p><i>information architecture: blueprints for the web</i> introduces the core concepts of information architecture: organizing web site content so that it can be found, designing web site interaction so that it's pleasant to use, and creating an interface that is easy to understand. this book will help designers, project managers, programmers, and other information architecture practitioners avoid the costly mistakes of the past by teaching the skills of information architecture swiftly and clearly. use this book and you will pass the usability inspection with flying colors!</p>}
593	266137	electronic	\N	\N	\N	\N	\N	\N	\N	2002	apr	2005-07-27 17:30:52	\N	a tutorial on principal components analysis	this tutorial is designed to give the reader an understanding of principal components analysis ({pca}). {pca} is a useful statistical technique that has found application in fields such as face recognition and image compression, and is a common technique for finding patterns in data of high dimension. before getting to a description of {pca}, this tutorial first introduces mathematical concepts that will be used in {pca}. it covers standard deviation, covariance, eigenvectors and eigenvalues. this background knowledge is meant to make the {pca} section very straightforward, but can be skipped if the concepts are already familiar. there are examples all the way through this tutorial that are meant to illustrate the concepts being discussed. if further information is required, the mathematics textbook  ” elementary linear algebra 5e” by howard anton, publisher john wiley \\& sons inc, {isbn} 0-471-85223-6 is a good source of information regarding the mathematical background.
594	266206	article	sigmis database	\N	\N	acm	15	35	1	2004	\N	2005-07-27 22:45:51	new york, ny, usa	the experienced "sense" of a virtual community: characteristics and processes	e-commerce strategists advise companies to create virtual communities for their customers. but what does this involve? research on face-to-face communities identifies the concept of "sense of community:" a characteristic of successful communities distinguished by members' helping behaviors and members' emotional attachment to the community and other members. does a sense of virtual community exist in online settings, and what does it consist of? answering these questions is key, if we are to provide guidance to businesses attempting to create virtual {communities.the} paper explores the concept of sense of virtual community in a newsgroup we call multiple sports newsgroup ({msn}). we first demonstrate that {msn} does indeed have a sense of virtual community, but that the dimensions of the sense of community in {msn} differ somewhat from those reported for physical communities. the nature of these differences is plausibly related to the differences between electronic and face-to-face communication. we next describe the behavioral processes that contribute to the sense of virtual community at {msn}-exchanging support, creating identities and making identifications, and the production of trust. again, these processes are similar to those found in non-virtual communities, but they are related to the challenges of electronic communication. lastly, we consider the question of how sense of community may come about and discuss the implications for electronic business.
595	270356	article	trends in biochemical sciences	\N	\N	\N	6	30	1	2005	jan	2005-07-31 18:52:40	university of cambridge, department of chemistry, lensfield road, cambridge, uk.	protein folding and the organization of the protein topology universe.	the mechanism by which proteins fold to their native states has been the focus of intense research in recent years. the rate-limiting event in the folding reaction is the formation of a conformation in a set known as the transition-state ensemble. the structural features present within such ensembles have now been analysed for a series of proteins using data from a combination of biochemical and biophysical experiments together with computer-simulation methods. these studies show that the topology of the transition state is determined by a set of interactions involving a small number of key residues and, in addition, that the topology of the transition state is closer to that of the native state than to that of any other fold in the protein universe. here, we review the evidence for these conclusions and suggest a molecular mechanism that rationalizes these findings by presenting a view of protein folds that is based on the topological features of the polypeptide backbone, rather than the conventional view that depends on the arrangement of different types of secondary-structure elements. by linking the folding process to the organization of the protein structure universe, we propose an explanation for the overwhelming importance of topology in the transition states for protein folding.
596	270721	article	the learning organization: an international journal	\N	\N	emerald group publishing limited	8	12	5	2005	may	2006-02-07 12:52:13	\N	the semantic learning organization	purpose --- the aim of this paper is introducing the concept of a ``semantic learning organization'' (slo) as an extension of the concept of ``learning organization'' in the technological domain. design/methodology/approach --- the paper takes existing definitions and conceptualizations of both learning organizations and semantic web technology to develop the new concept. findings --- the main points in which semantic web technology can be applied to learning in organizations are identified, and ontological accounts of organizational earning behaviour are pointed out as the main open question to develop the concept of a slo. originality/value --- the paper provides a new conceptual framework for semantic web applications in organizational learning, which can be used as a roadmap for further research.
597	270819	book	\N	\N	\N	sage publications ltd	\N	\N	\N	2001	feb	2005-08-01 18:02:01	\N	handbook of action research: participative inquiry and practice	{<p>"a remarkable reframing of action research that engages the spirit as well as the mind, in inquiry that matters, shared among inquirers who matter. 'validity as we once knew it will never be the same after these improvements. wonderfully provocative!"<br>  --karl weick, university of michigan </p> <p>"this is truly a significant work. not only has action research reached maturity, but in the context of the postmodern constructionist debates its scope has been dramatically expanded, its conceptual underpinnings deepened, and its forms of practice enormously enriched. the present confluence of humanism and pragmatism has inspired lively conversations between us; the work has the potential to transform the very idea of social science."<br>  --kenneth j. gergen, author of <b>an invitation to social construction</b> and mary gergen, author of <b>feminist reconstructions in psychology</b> </p> <p>"a wholly new kind of human inquiry is emerging. it is to do with taking our own, previously ignored, spontaneously responsive, living involvements with our surroundings seriously. rather than with views and perspectives, rather than with a one-way manipulative understanding, gained by merely observing movements from a distance, it is concerned with quite a different kind of participatory, experiential understanding - the kind of understanding we have when playing a part in an activity which also to an extent 'plays' us. workers are beginning to bring to light the many different knowledges present to us in the different practical ways in which we can be relationally involved with the others and othernesses around us. everything changes when we get up close and personal. all that is solid melts into air! in this exciting <b>handbook</b>, reason and bradbury have collected together a large number of the central workers in this new and developing sphere of participative inquiry. overall, in the detailed explorations they conduct, just as in becoming familiar with a new and strange landscape, they help us get to know our `way about' in its rich and intricate 'landscape'. literally, this is a landmark volume." <br> --john shotter, university of new hampshire </p> <p>"the <b>handbook of action research</b> is truly a remarkable book. we are greatly indebted to the editors peter reason and hilary bradbury, who managed to avoid the usual tower of babel, and succeeded to forge and orchestrate the somewhat incoherent mosaic of action research, with its many voices, into an intelligent comprehensive and logical whole. this handbook provides a much needed clarification of a critical transition in the social sciences." <br> --hans van beinum, international journal of action research and organizational renewal </p> <p>the publication of the <b>handbook of action research</b> is a publishing milestone, drawing together the different strands of action research, demonstrating their diverse applications and showing their interrelations. far-reaching in scale and scope, the handbook informs readers about the latest approaches, both quantitative and qualitative, in social inquiry, and moves the field forward with fresh insights and applications. throughout, the contributing authors grapple with questions of how to integrate knowledge with action, how to collaborate with co-researchers in the field, and how to present the necessarily 'messy' components in a coherent fashion. the organization of the volume reflects the many different issues and levels of analysis represented. this volume is an essential resource for scholars and professionals engaged in social and political inquiry, organizational research and education. </p>}
598	270986	book	\N	\N	\N	addison-wesley professional	\N	\N	\N	2004	aug	2005-08-01 21:05:40	\N	refactoring to patterns	{what is this book about? <br>this book is about the marriage of refactoring\\&\\#151;the process of improving the design of existing code\\&\\#151;with patterns, the classic solutions to recurring design problems. refactoring to patterns suggests that using patterns to improve an existing design is better than using patterns early in a new design. this is true whether code is years old or minutes old. we improve designs with patterns by applying sequences of low-level design transformations, known as refactorings.    <p>what are the goals of this book? <br>this book was written to help you:   <p>understand how to combine refactoring and patterns  <br>improve the design of existing code with pattern-directed refactorings  <br>identify areas of code in need of pattern-directed refactorings  <br>learn why using patterns to improve existing code is better than using patterns early in a new design  <br>to achieve these goals, this book includes the following features:   <p>a catalog of 27 refactorings  <br>examples based on real-world code, not the toy stuff  <br>pattern descriptions, including real-world pattern examples  <br>a collection of smells (i.e., problems) that indicate the need for pattern-directed refactorings  <br>examples of different ways to implement the same pattern  <br>advice for when to refactor to, towards, or away from patterns  <br>to help individuals or groups learn the 27 refactorings in the book, you\\&\\#146;ll find a suggested study sequence on the inside back cover of the book.   <p>who should read this book?  <p>this book is for object-oriented programmers engaged in or interested in improving the design of existing code. many of these programmers use patterns and/or practice refactoring but have never implemented patterns by refactoring; others know little about refactoring and patterns and would like to learn more.   <p>this book is useful for both greenfield development, in which you are writing a new system or feature from scratch, and legacy development, in which you are mostly maintaining a legacy system.   <p>what background do you need? <br>this book assumes you are familiar with design concepts like tight coupling and loose coupling as well as object-oriented concepts like inheritance, polymorphism, encapsulation, composition, interfaces, abstract and concrete classes, abstract and static methods, and so forth.   <p>i use java examples in this book. i find that java tends to be easy for most object-oriented programmers to read. i\\&\\#146;ve gone out of my way to not use fancy java features, so whether you code in c++, c\\#, visual basic .net, python, ruby, smalltalk, or some other object-oriented language, you ought to be able to understand the java code in this book.   <p>this book is closely tied to martin fowler\\&\\#146;s classic book refactoring f. it contains references to low-level refactorings, such as:   <p>extract method  <br>extract interface  <br>extract superclass  <br>extract subclass  <br>pull up method  <br>move method  <br>rename method   <p>refactoring also contains references to more sophisticated refactorings, such as:   <p>replace inheritance with delegation  <br>replace conditional with polymorphism  <br>replace type code with subclasses   <p>to understand the pattern-directed refactorings in this book, you don\\&\\#146;t need to know every refactoring listed above. instead, you can follow the example code that illustrates how the listed refactorings are implemented. however, if you want to get the most out of this book, i do recommend that you have refactoring close by your side. it\\&\\#146;s an invaluable refactoring resource, as well as a useful aid for understanding this book.   <p>the patterns i write about come from the classic book design patterns dp, as well as from authors such as kent beck, bobby woolf, and myself. these are patterns that my colleagues and i have refactored to, towards, or away from on real-world projects. by learning the art of pattern-directed refactorings, you\\&\\#146;ll understand how to refactor to, towards, or away from patterns not mentioned in this book.   <p>you don\\&\\#146;t need expert knowledge of these patterns to read this book, though some knowledge of patterns is useful. to help you understand the patterns i\\&\\#146;ve written about, this book includes brief pattern summaries, uml sketches of patterns, and many example implementations of patterns. to get a more detailed understanding of the patterns, i recommend that you study this book in conjunction with the patterns literature i reference.   <p>this book uses uml 2.0 diagrams. if you don\\&\\#146;t know uml very well, you\\&\\#146;re in good company. i know the basics. while writing this book, i kept the third edition of fowler\\&\\#146;s uml distilled fowler, ud close by my side and referred to it often.}
599	271442	inproceedings	\N	sigmetrics	\N	acm press	9	24	1	1996	may	2005-08-02 09:27:47	new york, ny, usa	self-similarity in world wide web traffic: evidence and possible causes	the notion of self-similarity has been shown to apply to wide-area and local-area network traffic. we show evidence that the subset of network traffic that is due to world wide web (www) transfers can show characteristics that are consistent with self-similarity, and we present a hypothesized explanation for that self-similarity. using a set of traces of actual user executions of ncsa mosaic, we examine the dependence structure of www traffic. first, we show evidence that www traffic exhibits behavior that is consistent with self-similar traffic models. then we show that the self-similarity in such traffic can be explained based on the underlying distributions of www document sizes, the effects of caching and user preference in file transfer, the effect of user &ldquo;think time&rdquo;, and the superimposition of many such transfers in a local-area network. to do this, we rely on empirically measured distributions both from client traces and from data independently collected at www servers
600	272317	inproceedings	\N	\N	\N	\N	7	\N	\N	1996	\N	2005-08-03 17:47:16	\N	a fast quantum mechanical algorithm for database search	imagine a phone directory containing n names arranged in completely random order. in order to find someone's phone number with a 50% probability, any classical algorithm (whether deterministic or probabilistic) will need to look at a minimum of n/2 names. quantum mechanical systems can be in a superposition of states and simultaneously examine multiple names. by properly adjusting the phases of various operations, successful computations reinforce each other while others interfere randomly. as a result, the desired phone number can be obtained in only o(sqrt(n)) steps. the algorithm is within a small constant factor of the fastest possible quantum mechanical algorithm.
601	272363	article	briefings in bioinformatics	\N	\N	oxford university press	14	6	1	2005	mar	2005-08-03 23:18:37	department of medical informatics and clinical epidemiology, school of medicine, oregon health \\& science university, 3181 s.w. sam jackson park road, portland, or 97239-309, usa. cohenaa@ohsu.edu	a survey of current work in biomedical text mining	the volume of published biomedical research, and therefore the underlying biomedical knowledge base, is expanding at an increasing rate. among the tools that can aid researchers in coping with this information overload are text mining and knowledge extraction. significant progress has been made in applying text mining to named entity recognition, text classification, terminology extraction, relationship extraction and hypothesis generation. several research groups are constructing integrated flexible text-mining systems intended for multiple uses. the major challenge of biomedical text mining over the next 5–10 years is to make these systems useful to biomedical researchers. this will require enhanced access to full text, better understanding of the feature space of biomedical literature, better methods for measuring the usefulness of systems to users, and continued cooperation with the biomedical research community to ensure that their needs are addressed.
602	273082	article	journal of economic growth	\N	\N	\N	39	1	\N	1996	\N	2005-08-04 11:27:22	\N	growth, income distribution and democracy: what the data say	this paper investigates the relationship between income distribution, democratic institutions, and growth. it does so by addressing three main issues: the properties and reliability of the income distribution data, the robustness of the reduced form relationships between income distribution and growth estimated so far, and the specific channels through which income distribution affects growth. the main conclusion in this regard is that there is strong empirical support for two types of explanations, linking income distribution to sociopolitical instability and to the education/fertility decision. a third channel, based on the interplay of borrowing constraints and investment in human capital, also seems to receive some support by the data, although it is probably the hardest to test with the existing data. by contrast, there appears to be less empirical support for explanations based on the effects of income distribution on fiscal policy.
603	273865	book	\N	\N	\N	center of spoken language understanding, carnegie melon university	\N	\N	\N	1996	\N	2005-08-04 17:42:47	pittsburgh, usa	survey of the state of the art in human language technology	this book surveys the state of the art of human language technology. the goal of the survey is to provide an interested reader with an overview of the field---the main areas of work, the capabilities and limitations of current technology, and the technical challenges that must be overcome to realize the vision of graceful human computer interaction using natural communication skills. the book consists of thirteen chapters written by 97 different authors.
604	274045	inproceedings	\N	proceedings of the 9th international conference on intelligent user interfaces	iui	acm	7	\N	\N	2004	\N	2005-08-04 20:12:46	new york, ny, usa	what role can adaptive support play in an adaptable system?	as computer applications become larger with every new version, there is a growing need to provide some way for users to manage the interface complexity. there are three different potential solutions to this problem: 1) an adaptable interface that allows users to customize the application to suit their needs; 2) an adaptive interface that performs the adaptation for the users; or 3) a combination of the adaptive and adaptable solutions, an approach that would be suitable in situations where users are not customizing effectively on their own. in this paper we examine what it means for users to engage in effective customization of a menu-based graphical user interface. we examine one aspect of effective customization, which is how characteristics of the users' tasks and customization behaviour affect their performance on those tasks. we do so by using a process model simulation based on cognitive modelling that generates quantitative predictions of user performance. our results show that users can engage in customization behaviours that vary in efficiency. we use these results to suggest how adaptive support could be added to an adaptable interface to improve the effectiveness of the users' customization.
605	274117	inproceedings	\N	proceedings of the second acm international conference on digital libraries	dl	acm	9	\N	\N	1997	\N	2005-08-04 22:27:59	new york, ny, usa	annotation: from paper books to the digital library	an abstract is not available.
606	275186	article	bioinformatics	\N	\N	\N	\N	\N	\N	-1	\N	2005-08-05 18:34:47	\N	wavelets in bioinformatics and computational biology: state of art and perspectives	motivation: at a recent meeting{dagger}, the wavelet transform was depicted as a small child kicking back at its father, the fourier transform. wavelets are more efficient and faster than fourier methods in capturing the essence of data. nowadays there is a growing interest in using wavelets in the analysis of biological sequences and molecular biology-related signals.  results: this review is intended to summarize the potential of state of the art wavelets, and in particular wavelet statistical methodology, in different areas of molecular biology: genome sequence, protein structure and microarray data analysis. i conclude by discussing the use of wavelets in modeling biological structures.  contact: plio@hgmp.mrc.ac.uk 10.1093/bioinformatics/19.1.2
607	275498	article	science	\N	\N	\N	2	309	5736	2005	aug	2005-08-05 19:55:58	\N	rewiring of the yeast transcriptional network through the evolution of motif usage	recent experiments revealed large-scale differences in the transcription programs of related species, yet little is known about the genetic basis underlying the evolution of gene expression and its contribution to phenotypic diversity. here we describe a large-scale modulation of the yeast transcription program that is connected to the emergence of the capacity for rapid anaerobic growth. genes coding for mitochondrial and cytoplasmic ribosomal proteins display a strongly correlated expression pattern in candida albicans, but this correlation is lost in the fermentative yeast saccharomyces cerevisiae. we provide evidence that this change in gene expression is connected to the loss of a specific cis-regulatory element from dozens of genes following the apparent whole-genome duplication event. our results shed new light on the genetic mechanisms underlying the large-scale evolution of transcriptional networks.
608	276211	book	\N	\N	\N	{princeton university press}	\N	\N	\N	2005	aug	2005-08-07 03:28:10	\N	digital formations : {it} and new architectures in the global realm	{<p>computer-centered networks and technologies are reshaping social relations and constituting new social domains on a global scale, from virtually borderless electronic markets and internet-based large-scale conversations to worldwide open source software development communities, transnational corporate production systems, and the global knowledge-arenas associated with ngo networks. this book explores how such "digital formations" emerge from the ever-changing intersection of computer-centered technologies and the broad range of social contexts that underlie much of what happens in cyberspace.</p><p> while viewing technologies fundamentally in social rather than technical terms, <i>digital formations</i> nonetheless emphasizes the importance of recognizing the specific technical capacities of digital technologies. importantly, it identifies digital formations as a new area of study in the social sciences and in thinking about globalization. the ten chapters, by leading scholars, examine key social, political, and economic developments associated with these new configurations of organization, space, and interaction. they address the operation of digital formations and their implications for the development of longstanding institutions and for their wider contexts and fields, and they consider the political, economic, and other forces shaping those formations and how the formations, in turn, are shaping such forces.</p><p> following a conceptual introduction by the editors are chapters by hayward alker, jonathan bach and david stark, lars-erik cederman and peter a. kraus, dieter ernst, d. linda garcia, doug guthrie, robert latham, warren sack, saskia sassen, and steven weber.</p>}
609	277054	book	\N	\N	\N	the mit press	\N	\N	\N	2005	sep	2005-08-09 02:02:14	\N	making things public: atmospheres of democracy	{in this groundbreaking editorial and curatorial project, more than 100 writers, artists, and philosophers rethink what politics is about. in a time of political turmoil and anticlimax, this book redefines politics as operating in the realm of <i>things</i>. politics is not just an arena, a profession, or a system, but a concern for things brought to the attention of the fluid and expansive constituency of the public. but how are things made public? what, we might ask, is a republic, a <i>res publica</i>, a public thing, if we do not know how to make things public? there are many other kinds of assemblies, which are not political in the usual sense, that gather a public around things -- scientific laboratories, supermarkets, churches, and disputes involving natural resources like rivers, landscapes, and air. the authors of <i>making things public</i> -- and the zkm show that the book accompanies -- ask what would happen if politics revolved around disputed things. instead of looking for democracy only in the official sphere of professional politics, they examine the new atmospheric conditions -- technologies, interfaces, platforms, networks, and mediations that allow things to be made public. they show us that the old definition of politics is too narrow; there are many techniques of representation -- in politics, science, and art -- of which parliaments and congresses are only a part.<br /> <br /> the authors include such prominent thinkers as richard rorty, simon schaffer, peter galison, richard powers, lorraine daston, richard aczel, and donna haraway; their writings are accompanied by excerpts from john dewey, shakespeare, swift, la fontaine, and melville. more than 500 color images document the new idea of what bruno latour and peter weibel call an "object-oriented democracy."}
610	277591	inproceedings	\N	chi	chi ea	acm	1	\N	\N	2004	\N	2005-08-10 00:04:20	new york, ny, usa	personal information management	personal information management ({pim}) is the management of information (e.g. files, emails, and bookmarks) by an individual in support of his/her roles and tasks. although {pim} is practiced daily by millions of people, a research community has never been established. this {sig} aims to provide an opportunity for researchers, students and designers who share an interest in the field to meet and discuss key issues. we hope the {sig} will lay the foundation for an ongoing {pim} research community.
611	277677	article	genetics	\N	\N	\N	-388	131	2	1992	\N	2005-08-10 01:20:30	\N	analysis of molecular variance inferred from metric distances among {dna} haplotypes: application to human mitochondrial {dna} restriction data	we present here a framework for the study of molecular variation within a single species. information on dna haplotype divergence is incorporated into an analysis of variance format, derived from a matrix of squared-distances among all pairs of haplotypes. this analysis of molecular variance (amova) produces estimates of variance components and f-statistic analogs, designated here as phi-statistics, reflecting the correlation of haplotypic diversity at different levels of hierarchical subdivision. the method is flexible enough to accommodate several alternative input matrices, corresponding to different types of molecular data, as well as different types of evolutionary assumptions, without modifying the basic structure of the analysis. the significance of the variance components and phi-statistics is tested using a permutational approach, eliminating the normality assumption that is conventional for analysis of variance but inappropriate for molecular data. application of amova to human mitochondrial dna haplotype data shows that population subdivisions are better resolved when some measure of molecular differences among haplotypes is introduced into the analysis. at the intraspecific level, however, the additional information provided by knowing the exact phylogenetic relations among haplotypes or by a nonlinear translation of restriction-site change into nucleotide diversity does not significantly modify the inferred population genetic structure. monte carlo studies show that site sampling does not fundamentally affect the significance of the molecular variance components. the amova treatment is easily extended in several different directions and it constitutes a coherent and flexible framework for the statistical analysis of molecular data.
612	277764	article	proceedings of the national academy of sciences of the united states of america	\N	\N	\N	4	81	3	1984	\N	2005-08-10 01:20:53	\N	lengths of chromosomal segments conserved since divergence of man and mouse	linkage relationships of homologous loci in man and mouse were used to estimate the mean length of autosomal segments conserved during evolution. comparison of the locations of >83 homologous loci revealed 13 conserved segments. map distances between the outermost markers of these 13 segments are known for the mouse and range from 1 to 24 centimorgans. methods were developed for using this sample of conserved segments to estimate the mean length of all conserved autosomal segments in the genome. this mean length was estimated to be \\$8.1\\pm 1.6\\$ centimorgans. evidence is presented suggesting that chromosomal rearrangements that determine the lengths of these segments are randomly distributed within the genome. the estimated mean length of conserved segments was used to predict the probability that certain loci, such as peptidase-3 and renin, are linked in man given that homologous loci are x centimorgans apart in the mouse. the mean length of conserved segments was also used to estimate the number of chromosomal rearrangements that have disrupted linkage since divergence of man and mouse. this estimate was shown to be \\$178\\pm 39\\$ rearrangements.
613	277967	article	behav res ther	\N	\N	\N	9	31	6	1993	jul	2005-08-10 11:46:54	department of psychology, southern methodist university, dallas, tx 75275.	putting stress into words: health, linguistic, and therapeutic implications.	when individuals are asked to write or talk about personally upsetting experiences, significant improvements in physical health are found. analyses of subjects' writing about traumas indicate that those whose health improves most tend to use a higher proportion of negative emotion words than positive emotion words. independent of verbal emotion expression, the increasing use of insight, causal, and associated cognitive words over several days of writing is linked to health improvement. that is, the construction of a coherent story together with the expression of negative emotions work together in therapeutic writing. evidence of these processes are also seen in specific links between word production and immediate autonomic nervous system activity. implications for therapy and for considering the mind and body as fluid, dynamic systems are discussed.
614	278151	article	proceedings of the ieee	\N	\N	\N	18	67	\N	1979	\N	2005-08-10 20:25:11	\N	{statistical and structural approaches to texture}	in this survey we review the image processing literature on the various approaches and models investigators have used for texture. these include statistical approaches of autocorrelation function, optical transforms, digital transforms, textural edgeness, structural element, gray tone cooccurrence, run lengths, and autoregressive models. we discuss and generalize some structural approaches to texture based on more complex primitives than gray tone. we conclude with some structural-statistical generalizations which apply the statistical techniques to the structural primitives.
615	279432	inproceedings	\N	icdcs	\N	\N	9	\N	\N	2002	jul	2005-08-11 18:56:16	vienna	improving search in {peer-to-peer} networks	peer-to-peer systems have emerged as a popular way to share huge volumes of data. the usability of these systems depends on effective techniques to find and retrieve data; however, current techniques used in existing p2p systems are often very inefficient. in this paper, we present three techniques for efficient search in p2p systems. we present the design of these techniques, and then evaluate them using a combination of analysis and experiments over gnutella, the largest open p2p system in operation. we show that while our techniques maintain the same quality of results as currently used techniques, they use up to 5 times fewerresources. in addition, we designed our techniques to be simple, so that they can be easily incorporated into existing systems for immediate impact.
616	280464	article	nature	\N	\N	\N	1	401	\N	1999	sep	2005-08-12 22:29:48	\N	the diameter of the world wide web	despite its increasing role in communication, the world wide web remains the least controlled medium: any individual or institution can create websites with unrestricted number of documents and links. while great efforts are made to map and characterize the internet's infrastructure, little is known about the topology of the web. here we take a first step to fill this gap: we use local connectivity measurements to construct a topological model of the world wide web, allowing us to explore and characterize its large scale properties.
617	280770	article	proceedings of the national academy of sciences of the united states of america	\N	\N	national academy of sciences	5	102	15	2005	apr	2005-08-13 10:59:27	department of plant systems biology, flanders interuniversity institute for biotechnology, ghent university, technologiepark 927, b-9052 ghent, belgium.	modeling gene and genome duplications in eukaryotes	recent analysis of complete eukaryotic genome sequences has revealed that gene duplication has been rampant. moreover, next to a continuous mode of gene duplication, in many eukaryotic organisms the complete genome has been duplicated in their evolutionary past. such large-scale gene duplication events have been associated with important evolutionary transitions or major leaps in development and adaptive radiations of species. here, we present an evolutionary model that simulates the duplication dynamics of genes, considering genome-wide duplication events and a continuous mode of gene duplication. modeling the evolution of the different functional categories of genes assesses the importance of different duplication events for gene families involved in specific functions or processes. by applying our model to the arabidopsis genome, for which there is compelling evidence for three whole-genome duplications, we show that gene loss is strikingly different for large-scale and small-scale duplication events and highly biased toward certain functional classes. we provide evidence that some categories of genes were almost exclusively expanded through large-scale gene duplication events. in particular, we show that the three whole-genome duplications in arabidopsis have been directly responsible for >90\\% of the increase in transcription factors, signal transducers, and developmental genes in the last 350 million years. our evolutionary model is widely applicable and can be used to evaluate different assumptions regarding small- or large-scale gene duplication events in eukaryotic genomes.
618	281947	book	\N	\N	\N	scribner	\N	\N	\N	2005	may	2005-08-15 04:32:38	\N	mind wide open : your brain and the neuroscience of everyday life	{given the opportunity to watch the inner workings of his own brain, steven johnson jumps at the chance. he reveals the results in <i>mind wide open</i>, an engaging and personal account of his foray into edgy brain science. in the 21st century, johnson observes, we have become used to ideas such as "adrenaline rushes" and "serotonin levels," without really recognizing that complex neurobiology has become a commonplace thing to talk about. he sees recent laboratory revelations about the brain as crucial for understanding ourselves and our psyches in new, post-freudian ways. readers shy about slapping electrodes on their own temples can get a vicarious scientific thrill as johnson tries out empathy tests, neurofeedback, and fmri scans. the results paint a distinct picture of the author, and uncover general brain secrets at the same time. memory, fear, love, alertness--all the multitude of states housed in our brains are shown to be the results of chemical and electrical interactions constantly fed and changed by input from our senses. <i>mind wide open</i> both satisfies curiosity and provokes more questions, leaving readers wondering about their own gray matter. <i>--therese littleton</i>} {<p>  in this nationally bestselling, compulsively readable account of what makes brain science a vital component of people's quest to know themselves, acclaimed science writer steven johnson subjects his own brain to a battery of tests to find out what's really going on inside. he asks:  <p>  <ul>  \	<li>how do we "read" other people?  \	<li><p>  \	<li>what is the neurochemistry behind love and sex?  \	<li><p>  \	<li>what does it mean that the brain is teeming with powerful chemicals closely related to recreational drugs?  \	<li><p>  \	<li>why does music move us to tears?  \	<li><p>  \	<li>where do breakthrough ideas come from?  </ul>  <p>  johnson answers these and many more questions arising from the events of our everyday lives. you do not have to be a neuroscientist to wonder, for example, why do you smile? and why do you sometimes smile inappropriately, even if you don't want to? how do others read your inappropriate smile? how does such interplay occur neurochemically, and what, if anything, can you do about it?  <p>  fascinating and rewarding, <i>mind wide open</i> speaks to brain buffs, self-obsessed neurotics, barstool psychologists, mystified parents, grumpy spouses, exasperated managers, and anyone who enjoys speculating and gossiping about the motivations and behaviors of other human beings. steven johnson shows us the transformative power of understanding brain science and offers new modes of introspection and tools for better parenting, better relationships, and better living.} {"brilliantly exploring today's cutting-edge brain research, mind wide open is an unprecedented journey into the essence of human personality, allowing readers to understand themselves and the people in their lives as never before.    using a mix of experiential reportage, personal storytelling, and fresh scientific discovery, steven johnson describes how the brain works -- its chemicals, structures, and subroutines -- and how these systems connect to the day-to-day realities of individual lives. for a hundred years, he says, many of us have assumed that the most powerful route to self-knowledge took the form of lying on a couch, talking about our childhoods. the possibility entertained in this book is that you can follow another path, in which learning about the brain's mechanics can widen one's self-awareness as powerfully as any therapy or meditation or drug.   in mind wide open, johnson embarks on this path as his own test subject, participating in a battery of attention tests, learning to control video games by altering his brain waves, scanning his own brain with a \\$2 million fmri machine, all in search of a modern answer to the oldest of questions: who am i?}
619	282032	inproceedings	\N	oopsla	\N	acm	19	37	11	2002	nov	2005-08-15 09:11:30	new york, ny, usa	ownership types for safe programming: preventing data races and deadlocks	this paper presents a new static type system for multithreaded programs; well-typed programs in our system are guaranteed to be free of data races and deadlocks. our type system allows programmers to partition the locks into a fixed number of equivalence classes and specify a partial order among the equivalence classes. the type checker then statically verifies that whenever a thread holds more than one lock, the thread acquires the locks in the descending {order.our} system also allows programmers to use recursive tree-based data structures to describe the partial order. for example, programmers can specify that nodes in a tree must be locked in the  tree order . our system allows mutations to the data structure that change the partial order at runtime. the type checker statically verifies that the mutations do not introduce cycles in the partial order, and that the changing of the partial order does not lead to deadlocks. we do not know of any other sound static system for preventing deadlocks that allows changes to the partial order at {runtime.our} system uses a variant of ownership types to prevent data races and deadlocks. ownership types provide a statically enforceable way of specifying object encapsulation. ownership types are useful for preventing data races and deadlocks because the lock that protects an object can also protect its encapsulated objects. this paper describes how to use our type system to statically enforce object encapsulation as well as prevent data races and deadlocks. the paper also contains a detailed discussion of different ownership type systems and the encapsulation guarantees they provide.
620	282193	inproceedings	\N	proceedings of the 14th conference on computational linguistics - volume 2	coling	association for computational linguistics	6	\N	\N	1992	\N	2005-08-15 11:53:10	stroudsburg, pa, usa	automatic acquisition of hyponyms from large text corpora	we describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. we identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. we describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. a subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. extensions and applications to areas such as information retrieval are suggested.
621	291875	article	phys rev e stat nonlin soft matter phys	\N	\N	\N	\N	71	5 Pt 2	2005	may	2005-08-16 01:11:39	complex systems research group, x-rays unit, riaidt, edificio cactus, university of santiago de compostela, 15706 santiago de compostela, spain. estrada66@yahoo.com	subgraph centrality in complex networks.	we introduce a new centrality measure that characterizes the participation of each node in all subgraphs in a network. smaller subgraphs are given more weight than larger ones, which makes this measure appropriate for characterizing network motifs. we show that the subgraph centrality [{c(s})(i)] can be obtained mathematically from the spectra of the adjacency matrix of the network. this measure is better able to discriminate the nodes of a network than alternate measures such as degree, closeness, betweenness, and eigenvector centralities. we study eight real-world networks for which {c(s})(i) displays useful and desirable properties, such as clear ranking of nodes and scale-free characteristics. compared with the number of links per node, the ranking introduced by {c(s})(i) (for the nodes in the protein interaction network of s. cereviciae) is more highly correlated with the lethality of individual proteins removed from the proteome.
622	296232	article	computer	\N	\N	ieee computer society press	2	34	4	2001	\N	2005-08-17 11:41:18	\N	software architecture: introducing {ieee} {standard} 1471	{ieee} standard 1471 identifies sound practices to establish a framework and vocabulary for software architecture concepts.
623	296261	inproceedings	\N	ewsa	\N	\N	5	\N	\N	2004	\N	2005-08-17 11:41:19	\N	software architecture: the next step	this position paper makes the following claims that, in our opinion, are worthwhile to discuss at the workshop. 1) the first phase of software architecture research, where the key concepts are components and connectors, has matured the technology to a level where industry adoption is wide-spread and few fundamental issues remain. 2) the traditional view on software architecture suffers from a number of key problems that cannot be solved without changing our perspective on the notion of software architecture. these problems include the lack of first-class representation of design decisions, the fact that these design decisions are cross-cutting and intertwined, that these problems lead to high maintenance cost, because of which design rules and constraints are easily violated and obsolete design decisions are not removed. 3) as a community, we need to take the next step and adopt the perspective that a software architecture is, fundamentally, a composition of architectural design decisions. these design decisions should be represented as first-class entities in the software architecture and it should, at least before system deployment, be possible to add, remove and change architectural design decisions against limited effort.
624	297071	article	j. am. chem. soc.	\N	\N	\N	18	117	19	1995	\N	2005-08-18 10:02:44	\N	a second generation force field for the simulation of proteins, nucleic acids, and organic molecules	we present the derivation of a new molecular mechanical force field for simulating the structures, conformational energies, and interaction energies of proteins, nucleic acids, and many related organic molecules in condensed phases. this effective two-body force field is the successor to the weiner et al, force field and was developed with some of the same philosophies, such as the use of a simple diagonal potential function and electrostatic potential fit atom centered charges. the need for a 10-12 function for representing hydrogen bonds is no longer necessary due to the improved performance of the new charge model and new van der waals parameters. these new charges are determined using a 6-31g basis set and restrained electrostatic potential (resp) fitting and have been shown to reproduce interaction energies, free energies of solvation, and conformational energies of simple small molecules to a good degree of accuracy. furthermore, the new resp charges exhibit less variability as a function of the molecular conformation used in the charge determination. the new van der waals parameters have been derived from liquid simulations and include hydrogen parameters which take into account the effects of any geminal electronegative atoms. the bonded parameters developed by weiner et al. were modified as necessary to reproduce experimental vibrational frequencies and structures. most of the simple dihedral parameters have been retained from weiner et. al., but a complex set of phi and psi parameters which do a good job of reproducing the energies of the low-energy conformations of glycyl and alanyl dipeptides has been developed for the peptide backbone.
625	298192	book	\N	\N	\N	{lawrence erlbaum associates}	\N	\N	\N	2005	may	2005-08-19 07:25:35	\N	interdisciplinary collaboration:  an emerging cognitive science	{interdisciplinary collaboration calls attention to a serious need to study the problems and processes of interdisciplinary inquiry, to reflect on the current state of scientific knowledge regarding interdisciplinary collaboration, and to encourage research that studies interdisciplinary cognition in relation to the ecological contexts in which it occurs. it contains reflections and research on interdisciplinarity found in a number of different contexts by practitioners and scientists from a number of disciplines and several chapters represent attempts by cognitive scientists to look critically at the cognitive science enterprise itself. representing all of the seven disciplines listed in the official logo of the cognitive science society and its journal--anthropology, artificial intelligence, education, linguistics, neuroscience, philosophy, and psychology--this book is divided into three parts: *part i sets the stage by providing three broad overviews of literature and theory on interdisciplinary research and education. *part ii examines varied forms of interdisciplinarity in situ rather than the more traditional macrolevel interview or survey approaches to studying group work. *part iii consists of noted cognitive scientists who reflect on their experiences and turn the analytical lenses of their own discipline to the critical examination of cognitive science itself as a case study in interdisciplinary collaboration. interdisciplinary collaboration is intended for scholars at the graduate level and beyond in cognitive science and education.}
626	298217	article	science	\N	\N	american association for the advancement of science	15	289	5481	2000	aug	2005-11-25 16:41:15	department of molecular biophysics \\& biochemistry and howard hughes medical institute, new haven, ct 06520-8114, usa.	the complete atomic structure of the large ribosomal subunit at 2.4 a resolution	the large ribosomal subunit catalyzes peptide bond formation and binds initiation, termination, and elongation factors. we have determined the crystal structure of the large ribosomal subunit {fromhaloarcula} marismortui at 2.4 angstrom resolution, and it includes 2833 of the subunit's 3045 nucleotides and 27 of its 31 proteins. the domains of its {rnas} all have irregular shapes and fit together in the ribosome like the pieces of a three-dimensional jigsaw puzzle to form a large, monolithic structure. proteins are abundant everywhere on its surface except in the active site where peptide bond formation occurs and where it contacts the small subunit. most of the proteins stabilize the structure by interacting with several {rna} domains, often using idiosyncratically folded extensions that reach into the subunit's interior.
627	299199	book	\N	\N	\N	springer	\N	\N	\N	2005	nov	2005-08-20 08:06:24	\N	wiki : web collaboration	wikis are web-based applications that allow all users not only to view pages but also to change them. the recent success of the internet encyclopedia wikipedia has drawn increasing attention from private users, small organizations and enterprises to the various possible uses of wikis. their simple structure and straightforward operation make them a serious alternative to expensive content management systems and also provide a basis for many applications in the area of collaborative work. we show the practical use of wikis in carrying out projects for users as well as for maintainers. this includes a step-by-step introduction to wiki philosophy, social effects and functions, a survey of their controls and components, and the installation and configuration of the wiki clones {mediawiki} and {twiki}. in order to exemplify the possibilities of the software, we use it as a project tool for planning a conference.
628	299529	article	bioinformatics (oxford, england)	\N	\N	oxford university press	1	21	16	2005	aug	2007-11-28 08:36:10	\N	{biomart} and bioconductor: a powerful link between biological databases and microarray data analysis.	{biomart} is a new bioconductor package that integrates {biomart} data resources with data analysis software in bioconductor. it can annotate a wide range of gene or gene product identifiers (e.g. {entrez-gene} and affymetrix probe identifiers) with information such as gene symbol, chromosomal coordinates, gene ontology and {omim} annotation. furthermore {biomart} enables retrieval of genomic sequences and single nucleotide polymorphism information, which can be used in data analysis. fast and up-to-date data retrieval is possible as the package executes direct {sql} queries to the {biomart} databases (e.g. ensembl). the {biomart} package provides a tight integration of large, public or locally installed {biomart} databases with data analysis in bioconductor creating a powerful environment for biological data mining.
629	300210	misc	\N	\N	\N	\N	\N	\N	\N	2003	\N	2005-08-22 03:56:45	\N	designing {dccp}: congestion control without reliability	{dccp
630	300569	electronic	\N	\N	\N	\N	\N	\N	\N	1989	\N	2005-08-23 03:00:19	\N	the design of browsing and berrypicking techniques	first, a new model of searching in online and other information systems, called "berrypicking," is discussed.  this model, it is argued, is much closer to the real behavior of information searchers than the traditional model of information retrieval is, and, consequently, will guide our thinking better in the design of effective interfaces.  second, the research literature of manual information seeking behavior is drawn on for suggestions of capabilities that users might like to have in online systems.  third, based on the new model and the research on information seeking, suggestions are made for how new search capabilities could be incorporated into the design of search interfaces.  particular attention is given to the nature and types of browsing that can be facilitated.
631	300647	article	journal of the american society for information science	\N	\N	\N	14	50	10	1999	jul	2005-08-23 10:23:53	\N	information seeking behavior of scientists in the electronic information age: astronomers, chemists, mathematicians, and physicists	the information seeking behavior of astronomers, chemists, mathematicians, and physicists at the university of oklahoma was assessed using an electronically distributed questionnaire. all of the scientists surveyed relied greatly on the journal literature to support their research and creative activities. the mathematicians surveyed indicated an additional reliance on monographs, preprints, and attendance at conferences and personal communication to support their research activities. similarly, all scientists responding scanned the latest issues of journals to keep abreast of current developments in their fields, with the mathematicians again reporting attendance at conferences and personal communication. despite an expression by the scientists for more electronic services, the majority preferred access to journal articles in a print, rather than an electronic, form. the primary deficit in library services appeared to be in access to electronic bibliographic databases. the data suggest that a primary goal of science libraries is to obtain access to as many appropriate electronic bibliographic finding aids and databases possible. although the results imply the ultimate demise of the printed bibliographic reference tool, they underscore the continued importance to scientists of the printed peer-reviewed journal article.
632	302103	article	journal of management studies	\N	\N	\N	15	37	6	2000	sep	2005-08-24 09:19:25	\N	communities of practice, foucault and {actor-network} therory	the paper discusses some of the main contributions to the theory of communities of practice ({cop} theory), especially as it relates to organizational learning. the paper does not attempt a full overview but concentrates on the notion of power relations. early {cop} theory was formulated as part of situated learning theory, and promised to work on issues of social context and unequal power relations. foucault\\&\\#146;s work and actor-network theory ({ant}) is introduced and forms the basis of a constructive critique of {cop} theory. the paper argues that {cop} theory and {ant} can enrich each other and together make a stronger contribution to our understanding of organizational learning. specifically, these perspectives question the value in viewing organizations as formal, canonical entities as far as learning and change are concerned.
633	303461	article	rev. geophys.	\N	\N	\N	40	40	1	2002	\N	2005-08-25 13:10:54	\N	advanced spectral methods for climatic time series	the analysis of univariate or multivariate time series provides crucial information to describe, understand, and predict climatic variability. the discovery and implementation of a number of novel methods for extracting useful information from time series has recently revitalized this classical field of study. considerable progress has also been made in interpreting the information so obtained in terms of dynamical systems theory. in this review we describe the connections between time series analysis and nonlinear dynamics, discuss signal-to-noise enhancement, and present some of the novel methods for spectral analysis. the various steps, as well as the advantages and disadvantages of these methods, are illustrated by their application to an important climatic time series, the southern oscillation index. this index captures major features of interannual climate variability and is used extensively in its prediction. regional and global sea surface temperature data sets are used to illustrate multivariate spectral methods. open questions and further prospects conclude the review.
634	304987	article	network: computation in neural systems	\N	\N	\N	38	3	2	1992	\N	2005-08-26 19:36:47	\N	could information theory provide an ecological theory of sensory processing?	the sensory pathways of animals are well adapted to processing a special class of signals, namely stimuli from the animal's environment. an important fact about natural stimuli is that they are typically very redundant and hence the sampled representation of these signals formed by the array of sensory cells is inefficient. one could argue for some animals and pathways, as the author does in this review, that efficiency of information representation in the nervous system has several evolutionary advantages. consequently, one might expect that much of the processing in the early levels of these sensory pathways could be dedicated towards recording incoming signals into a more efficient form. the author explores the principle of efficiency of information representation as a design principle for sensory processing. he gives a preliminary discussion on how this principle could be applied in general to predict neural processing and then discuss concretely some neural systems where it recently has been shown to be successful. in particular, he examines the fly's lmc coding strategy and the mammalian retinal coding in the spatial, temporal and chromatic domains.
635	305371	unpublished	\N	\N	\N	\N	\N	\N	\N	2005	\N	2005-08-27 05:05:22	\N	fast and loose reasoning is morally correct	functional programmers often reason about programs as if they were written in a total language, expecting the results to carry over to non-total (partial) languages. we justify such reasoning.two languages are defined, one total and one partial, with identical syntax. the semantics of the partial language includes partial and infinite values, and all types are lifted, including the function spaces. a partial equivalence relation (per) is then defined, the domain of which is the total subset of the partial language. for types not containing function spaces the per relates equal values, and functions are related if they map related values to related values.it is proved that if two closed terms have the same semantics in the total language, then they have related semantics in the partial language. it is also shown that the per gives rise to a bicartesian closed category which can be used to reason about values in the domain of the relation.
636	305936	book	\N	\N	\N	basic books	\N	\N	\N	1999	jan	2005-12-13 15:28:27	\N	g\\"{o}del, escher, bach: an eternal golden braid	twenty years after it topped the bestseller charts, douglas r. hofstadter's <{i>g\\"{o}del}, escher, bach: an eternal golden {braid</i}> is still something of a marvel. besides being a profound and entertaining meditation on human thought and creativity, this book looks at the surprising points of contact between the music of bach, the artwork of escher, and the mathematics of g\\"{o}del. it also looks at the prospects for computers and artificial intelligence ({ai}) for mimicking human thought. for the general reader and the computer techie alike, this book still sets a standard for thinking about the future of computers and their relation to the way we think.<p> hofstadter's great achievement in <{i>g\\"{o}del}, escher, {bach</i}> was making abstruse mathematical topics (like undecidability, recursion, and 'strange loops') accessible and remarkably entertaining. borrowing a page from lewis carroll (who might well have been a fan of this book), each chapter presents dialogue between the tortoise and achilles, as well as other characters who dramatize concepts discussed later in more detail. allusions to bach's music (centering on his <{i>musical} {offering</i}>) and escher's continually paradoxical artwork are plentiful here. this more approachable material lets the author delve into serious number theory (concentrating on the ramifications of g\\"{o}del's theorem of incompleteness) while stopping along the way to ponder the work of a host of other mathematicians, artists, and thinkers.<p> the world has moved on since 1979, of course. the book predicted that computers probably won't ever beat humans in chess, though deep blue beat garry kasparov in 1997. and the vinyl record, which serves for some of hofstadter's best analogies, is now left to collectors. sections on recursion and the graphs of certain functions from physics look tantalizing, like the fractals of recent chaos theory. and {ai} has moved on, of course, with mixed results. yet <{i>g\\"{o}del}, escher, {bach</i}> remains a remarkable achievement. its intellectual range and ability to let us visualize difficult mathematical concepts help make it one of this century's best for anyone who's interested in computers and their potential for <{i>real</i}> intelligence. <{i>--richard} {dragan</i}><p> <{b>topics} {covered</b}>: {j.s}. bach, {m.c}. escher, kurt g\\"{o}del: biographical information and work, artificial intelligence ({ai}) history and theories, strange loops and tangled hierarchies, formal and informal systems, number theory, form in mathematics, figure and ground, consistency, completeness, euclidean and {non-euclidean} geometry, recursive structures, theories of meaning, propositional calculus, typographical number theory, zen and mathematics, levels of description and computers; theory of mind: neurons, minds and thoughts; undecidability; self-reference and self-representation; turing test for machine intelligence. winner of the pulitzer prize, this book applies godel's seminal contribution to modern mathematics to the study of the human mind and the development of artificial intelligence.
637	306080	article	neuroimage	\N	\N	\N	29	19	4	2003	aug	2006-08-14 14:40:03	the wellcome department of imaging neuroscience, institute of neurology, queen square, london wc1n 3bg, uk. k.friston@fil.ion.ucl.ac.uk	dynamic causal modelling.	in this paper we present an approach to the identification of nonlinear inputâ€“stateâ€“output systems. by using a bilinear approximation to the dynamics of interactions among states, the parameters of the implicit causal model reduce to three sets. these comprise (1) parameters that mediate the influence of extrinsic inputs on the states, (2) parameters that mediate intrinsic coupling among the states, and (3) [bilinear] parameters that allow the inputs to modulate that coupling. identification proceeds in a bayesian framework given known, deterministic inputs and the observed responses of the system. we developed this approach for the analysis of effective connectivity using experimentally designed inputs and fmri responses. in this context, the coupling parameters correspond to effective connectivity and the bilinear parameters reflect the changes in connectivity induced by inputs. the ensuing framework allows one to characterise fmri experiments, conceptually, as an experimental manipulation of integration among brain regions (by contextual or trial-free inputs, like time or attentional set) that is revealed using evoked responses (to perturbations or trial-bound inputs, like stimuli). as with previous analyses of effective connectivity, the focus is on experimentally induced changes in coupling (cf., psychophysiologic interactions). however, unlike previous approaches in neuroimaging, the causal model ascribes responses to designed deterministic inputs, as opposed to treating inputs as unknown and stochastic.
638	307311	misc	\N	\N	\N	\N	\N	\N	\N	2003	\N	2005-08-30 13:36:25	\N	a delay tolerant network architecture for challenged internets	the highly successful architecture and protocols of today's internet may operate poorly in environments characterized by very long delay paths and frequent network partitions. these problems are exacerbated by end nodes with limited power or memory resources. often deployed in mobile and extreme environments lacking continuous connectivity, many such networks have their own specialized protocols, and do not utilize ip. to achieve interoperability between them, we propose a network architecture...
639	307623	article	\N	\N	\N	kluwer, b.v.	21	\N	\N	2003	\N	2005-08-31 03:05:20	deventer, the netherlands, the netherlands	how practice matters: a relational view of knowledge sharing	this paper addresses the issue of knowledge sharing practices in complex organizations. the authors propose that a refined understanding of the relational thinking underpinning practice theories is required if we want to further our comprehension of knowledge sharing and distinguish existing approaches. knowledge sharing, we argue, is defined by the specific differences and dependencies in practices existing within or across communities. changes in those differences and dependencies leads to the formation of new knowledge. specifying the differences, dependencies and changes provides the first analytical step in understanding knowledge sharing as it takes shape in and across communities of practice. the authors apply this relational perspective to probe the discrepancies and complementarities among three seminal approaches to knowing within and across communities of practice.
640	307877	article	acm transactions on graphics	\N	\N	\N	30	15	4	1996	\N	2005-08-31 07:22:10	\N	drawing graphs nicely using simulated annealing	the paradigm of simulated annealing is applied to the problem of drawing graphs &ldquo;nicely.&rdquo; our algorithm deals with general undirected graphs with straight-line edges, and employs several simple criteria for the aesthetic quality of the result. the algorithm is flexible, in that the relative weights of the criteria can be changed. for graphs of modest size it produces good results, competitive with those produced by other methods, notably, the &ldquo;spring method&rdquo; and its variants.
641	308885	article	protein science	\N	\N	cold spring harbor laboratory press	12	13	4	2004	apr	2005-08-31 15:18:43	nci-frederick, building 469, room 151, frederick, md 21702, usa.	a new, structurally nonredundant, diverse data set of protein–protein interfaces and its implications	here, we present a diverse, structurally nonredundant data set of two-chain protein–protein interfaces derived from the {pdb}. using a sequence order-independent structural comparison algorithm and hierarchical clustering, 3799 interface clusters are obtained. these yield 103 clusters with at least five nonhomologous members. we divide the clusters into three types. in type i clusters, the global structures of the chains from which the interfaces are derived are also similar. this cluster type is expected because, in general, related proteins associate in similar ways. in type {ii}, the interfaces are similar; however, remarkably, the overall structures and functions of the chains are different. the functional spectrum is broad, from enzymes/inhibitors to immunoglobulins and toxins. the fact that structurally different monomers associate in similar ways, suggests  ” good” binding architectures. this observation extends a paradigm in protein science: it has been well known that proteins with similar structures may have different functions. here, we show that it extends to interfaces. in type {iii} clusters, only one side of the interface is similar across the cluster. this structurally nonredundant data set provides rich data for studies of protein–protein interactions and recognition, cellular networks and drug design. in particular, it may be useful in addressing the difficult question of what are the favorable ways for proteins to interact. (the data set is available at http://protein3d.ncifcrf.gov/?keskino/ and {http://home.ku.edu.tr/?okeskin/interface}/{interfaces}.html.)
642	309243	article	nature	\N	\N	\N	2	405	6790	2000	jun	2005-08-31 18:42:19	\N	ecosystem size determines food-chain length in lakes	food-chain length is an important characteristic of ecological communities(1) : it influences community structure(2), ecosystem functions(1-4) and contaminant concentrations in top predators(5,6). since elton(7) first noted that food-chain length was variable among natural systems, ecologists have considered many explanatory hypotheses(1,4,8,9), but few are supported by empirical evidence(4,10,11). here we test three hypotheses that predict food-chain length to be determined by productivity alone (productivity hypothesis)(4,10,12,13), ecosystem size alone (ecosystem-size hypothesis)(14,15) or a combination of productivity and ecosystem size (productive-space hypothesis)(7,16-18). the productivity and productive-space hypotheses propose that food-chain length should increase with increasing resource availability; however, the productivity hypothesis does not include ecosystem size as a determinant of resource availability. the ecosystem-size hypothesis is based on the relationship between ecosystem size and species diversity, habitat availability and habitat heterogeneity(14,15). we find that food-chain length increases with ecosystem size, but that the length of the food chain is not related to productivity. our results support the hypothesis that ecosystem size, and not resource availability, determines food-chain length in these natural ecosystems.
643	309283	article	proceedings of the national academy of sciences of the united states of america	\N	\N	\N	5	95	12	1998	jun	2005-08-31 18:42:23	\N	prokaryotes: the unseen majority	the number of prokaryotes and the total amount of their cellular carbon on earth are estimated to be 4-6 x 10(30) cells and 350-550 pg of c (1 pg = 10(15) g), respectively. thus, the total amount of prokaryotic carbon is 60-100\\% of the estimated total carbon in plants, and inclusion of prokaryotic carbon in global models will almost double estimates of the amount of carbon stored in living organisms. in addition, the earth's prokaryotes contain 85-130 pg of n and 9-14 pg of p, or about 10-fold more of these nutrients than do plants, and represent the largest pool of these nutrients: in living organisms. most of the earth's prokaryotes occur in the open ocean, in soil, and in oceanic and terrestrial subsurface, where the numbers of cells are 1.2 x 10(29), 2.6 x 10(29), 3.5 x 10(30), and 0.25-2.5 x 10(30), respectively. the numbers of heterotrophic prokaryotes in the upper 200 m of the open ocean, the ocean below 200 m, and soil are consistent with average turnover times of 6-25 days, 0.8 yr, and 2.5 yr, respectively. although subject to a great deal of uncertainty, the estimate for the average turnover time of prokaryotes in the subsurface is on the order of 1-2 x 10(3) yr. the cellular production-rate for all prokaryotes on earth is estimated at 1.7 x 10(30) cells/yr and is highest in the open ocean. the large population size and rapid growth of prokaryotes provides an enormous capacity for genetic diversity.
644	309498	article	machine learning	\N	\N	\N	\N	\N	\N	2000	\N	2005-09-01 01:13:34	\N	{boostexter}: a boosting-based system for text categorization	this work focuses on algorithms which learn from examples to perform multiclass text and speech categorization tasks. our approach is based on a new and improved family of boosting algorithms. we describe in detail an implementation, called boostexter, of the new boosting algorithms for text categorization tasks. we present results comparing the performance of boostexter and a number of other text-categorization algorithms on a variety of tasks. we conclude by describing the application of our system to automatic call-type identification from unconstrained spoken customer responses.
645	310608	article	proceedings of the national academy of sciences of the united states of america	\N	\N	\N	5	101	24	2004	jun	2005-09-01 18:48:41	department of molecular and cell biology, university of california, berkeley, ca 94720, usa. hunter@ocf.berkeley.edu	coevolution of gene expression among interacting proteins	physically interacting proteins or parts of proteins are expected to evolve in a coordinated manner that preserves proper interactions. such coevolution at the amino acid-sequence level is well documented and has been used to predict interacting proteins, domains, and amino acids. interacting proteins are also often precisely coexpressed with one another, presumably to maintain proper stoichiometry among interacting components. here, we show that the expression levels of physically interacting proteins coevolve. we estimate average expression levels of genes from four closely related fungi of the genus saccharomyces using the codon adaptation index and show that expression levels of interacting proteins exhibit coordinated changes in these different species. we find that this coevolution of expression is a more powerful predictor of physical interaction than is coevolution of amino acid sequence. these results demonstrate that gene expression levels can coevolve, adding another dimension to the study of the coevolution of interacting proteins and underscoring the importance of maintaining coexpression of interacting proteins over evolutionary time. our results also suggest that expression coevolution can be used for computational prediction of protein-protein interactions.
646	310706	article	journal of economic theory	\N	\N	\N	30	106	\N	2002	\N	2005-09-02 06:20:20	\N	the evolution of social and economic networks	we examine the dynamic formation and stochastic evolution of networks connecting individuals. the payoff to an individual from an economic or social activity depends on the network of connections among individuals. over time individuals form and sever links connecting themselves to other individuals based on the improvement that the resulting network offers them relative to the current network. we call such sequences of networks, `improving paths,â€™ and show that such sequences can include cycles and study conditions on underlying allocation rules that characterize cycles. building on the concept of improving paths, we consider a stochastic evolutionary process where in addition to intended changes in the network there is a small probability of unintended changes or errors. predictions can be made regarding the relative likelihood that the stochastic process will lead to any given network at some time, and the evolutionary process selects from among the statically stable networks and cycles. we apply these results to a series of models including the gale-shapley marriage market. in some cases, the evolutionary process selects only inefficient networks even though there are efficient networks that are statically stable, and moreover even if interventions that reallocate value are admissible.
647	311031	article	ieee trans biomed eng	\N	\N	\N	6	51	6	2004	jun	2005-09-02 23:33:50	department of computer science, brown university, providence, ri 02912, usa. fwood@cs.brown.edu	on the variability of manual spike sorting.	the analysis of action potentials, or "spikes," is central to systems neuroscience research. spikes are typically identified from raw waveforms manually for off-line analysis or automatically by human-configured algorithms for on-line applications. the variability of manual spike "sorting" is studied and its implications for neural prostheses discussed. waveforms were recorded using a micro-electrode array and were used to construct a statistically similar synthetic dataset. results showed wide variability in the number of neurons and spikes detected in real data. additionally, average error rates of 23\\% false positive and 30\\% false negative were found for synthetic data.
648	311226	article	nature	\N	\N	nature publishing group	5	412	6849	2001	aug	2005-09-03 18:04:43	nec research institute, 4 independence way, new jersey 08540, usa. adrienne@research.nj.nec.com	efficiency and ambiguity in an adaptive neural code	we examine the dynamics of a neural code in the context of stimuli whose statistical properties are themselves evolving dynamically. adaptation to these statistics occurs over a wide range of timescales—from tens of milliseconds to minutes. rapid components of adaptation serve to optimize the information that action potentials carry about rapid stimulus variations within the local statistical ensemble, while changes in the rate and statistics of action-potential firing encode information about the ensemble itself, thus resolving potential ambiguities. the speed with which information is optimized and ambiguities are resolved approaches the physical limit imposed by statistical sampling and noise.
649	311564	electronic	\N	\N	\N	\N	\N	\N	\N	2003	may	2006-05-28 08:30:24	\N	information flow in social groups	we present a study of information flow that takes into account the observation that an item relevant to one person is more likely to be of interest to individuals in the same social circle than those outside of it. this is due to the fact that the similarity of node attributes in social networks decreases as a function of the graph distance. an epidemic model on a scale-free network with this property has a finite threshold, implying that the spread of information is limited. we tested our predictions by measuring the spread of messages in an organization and also by numerical experiments that take into consideration the organizational distance among individuals.
650	312040	inproceedings	\N	caise	\N	lncs	\N	\N	\N	2005	jun	2005-09-06 15:08:19	\N	automated reasoning on feature models	software product line (spl) engineering has proved to be an effective method for software production. however, in the spl community it is well recognized that variability in spls is increasing by the thousands. hence, an automatic support is needed to deal with variability in spl. most of the current proposals for automatic reasoning on spl are not devised to cope with extraâ€“functional features. in this paper we introduce a proposal to model and reason on an spl using constraint programming. we take into account functional and extraâ€“functional features, improve current proposals and present a running, yet feasible implementation.
651	312061	article	software process: improvement and practice	\N	\N	\N	22	10	1	2005	\N	2005-09-06 15:08:21	\N	formalizing cardinality-based feature models and their specialization.	feature modeling is an important approach to capture the commonalities and variabilities in system families and product lines. cardinality-based feature modeling integrates a number of existing extensions of the original feature-modeling notation from feature-oriented domain analysis. staged configuration is a process that allows the incremental configuration of cardinality-based feature models. it can be achieved by performing a step-wise specialization of the feature model. in this article, we argue that cardinality-based feature models can be interpreted as a special class of context-free grammars. we make this precise by specifying a translation from a feature model into a context-free grammar. consequently, we provide a semantic interpretation for cardinality-based feature models by assigning an appropriate semantics to the language recognized by the corresponding grammar. finally, we give an account on how feature model specialization can be formalized as transformations on the grammar equivalent of feature models. copyright Â© 2005 john wiley & sons, ltd.
652	312063	article	journal of computing and information technology	\N	\N	\N	16	10	1	2002	\N	2005-09-06 15:08:21	\N	{domain--specific} language design requires feature descriptions	a domain-specific language (dsl) provides a notation tailored towards an application domain and is based on the relevant concepts and features of that domain. as such, a dsl is a means to describe and generate members of a family of programs in the domain. a prerequisite for the design of a dsl is a detailed analysis and structuring of the application domain. graphical feature diagrams have been proposed to organize the dependencies between such features, and to indicate which ones are common to all family members and which ones vary. in this paper, we study feature diagrams in more details, as well as their relationship to domain-specific languages. we propose the feature description language (fdl), a textual language to describe features. we explore automated manipulation of feature descriptions such as normalization, expansion to disjunctive normal form, variability computation and constraint satisfaction. feature descriptions can be directly mapped to uml diagrams which in their turn can be used for java code generation. the value of fdl is assessed via a case study in the use and expressiveness of feature descriptions for the area of documentation generators.
653	312073	inproceedings	\N	proceedings of {thefifthinternational} conference on software reuse	\N	\N	9	\N	\N	1998	\N	2005-09-06 15:08:21	canada	integrating feature modeling with the {rseb}	we have integrated the feature modeling of feature-oriented domain analysis (foda) into the processes and work products of the reuse-driven software engineering business (rseb). the rseb is a use case driven systematic reuse process: architecture and reusable subsystems are first described by use cases and then transformed into object models that are traceable to these use cases. variability in the rseb is captured by structuring use case and object models using explicit variation points and variants. traditional domain engineering steps have been distributed into the steps of the architectural and component system development methods of the rseb. but the rseb prescribes no explicit models of the essential features that characterize the different versions. building on our experience in applying foda and rseb to the telecom domain, we have added explicit domain engineering steps and an explicit feature model to the rseb to support domain engineering and component reuse. these additions provide an effective reuse oriented model as a `catalog' capability to link use cases, variation points, reusable components and configured applications
654	312090	book	\N	\N	\N	the {mit} press	\N	\N	\N	1998	\N	2005-09-06 15:08:21	\N	programming with constraints: an introduction	the job of the constraint programmer is to use mathematical constraints to model real world constraints and objects. in this book, kim marriott and peter stuckey provide the first comprehensive introduction to the discipline of constraint programming and, in particular, constraint logic programming. the book covers the necessary background material from artificial intelligence, logic programming, operations research, and mathematical programming. topics discussed range from constraint-solving techniques to programming methodologies for constraint programming languages. because there is not yet a universally used syntax for constraint logic programming languages, the authors present the programs in a way that is independent of any existing programming language. practical exercises cover how to use the book with a number of existing constraint languages.
655	312115	book	\N	\N	\N	academic press	\N	\N	\N	1995	\N	2005-09-06 15:08:22	\N	foundations of constraint satisfaction	constraint satisfaction is a general problem in which the goal is to find values for a set of variables that will satisfy a given set of constraints. it is the core of many applications in artificial intelligence, and has found its application in many areas, such as planning and scheduling. because of its generality, most ai researchers should be able to benefit from having good knowledge of techniques in this field. this book is the most comprehensive book on the field of constraint satisfaction so far. it covers both the theoretical and the implemenatation aspects of the subject. it provides a framework for studying this field, relates different research, and resolves ambiguity in a number of concepts and algorithms in the literature. this book provides a solid foundation for researchers in this field. it is also an invaluable text for graduate and research level students in cognitive science and artificial intelligence.
656	312817	inproceedings	\N	www	\N	acm press	10	\N	\N	2005	\N	2005-09-07 23:05:47	new york, ny, usa	improving recommendation lists through topic diversification	in this work we present topic diversi&#64257;cation, a novel method designed to balance and diversify personalized recommenda- tion lists in order to re&#64258;ect the user’s complete spectrum of interests. though being detrimental to average accuracy, we show that our method improves user satisfaction with rec- ommendation lists, in particular for lists generated using the common item-based collaborative &#64257;ltering algorithm. our work builds upon prior research on recommender sys- tems, looking at properties of recommendation lists as en- tities in their own right rather than speci&#64257;cally focusing on the accuracy of individual recommendations. we introduce the intra-list similarity metric to assess the topical diver- sity of recommendation lists and the topic diversi&#64257;cation approach for decreasing the intra-list similarity. we evalu- ate our method using book recommendation data, including o&#64260;ine analysis on 361, 349 ratings and an online study in- volving more than 2, 100 subjects.
657	315482	article	journal of computationnal biology	\N	\N	\N	45	7	1-2	2000	\N	2005-09-10 19:11:50	\N	probabilistic and statistical properties of words: an overview	in the following, an overview is given on statistical and probabilistic properties of words, as occurring in the analysis of biological sequences. counts of occurrence, counts of clumps, and renewal counts are distinguished, and exact distributions as well as normal approximations, poisson process approximations, and compound poisson approximations are derived. here, a sequence is modelled as a stationary ergodic markov chain; a test for determining the appropriate order of the markov chain is described. the convergence results take the error made by estimating the markovian transition probabilities into account. the main tools involved are moment generating functions, martingales, stein's method, and the chen-stein method. similar results are given for occurrences of multiple patterns, and, as an example, the problem of unique recoverability of a sequence from sbh chip data is discussed. special emphasis lies on disentangling the complicated dependence structure between word occurrences, due to self-overlap as well as due to overlap between words. the results can be used to derive approximate, and conservative, confidence intervals for tests.
658	315520	article	j mol biol j	\N	\N	\N	27	157	\N	1982	\N	2005-09-10 19:11:52	\N	{a simple method for displaying the hydropathic character of a protein.}	a computer program that progressively evaluates the hydrophilicity and hydrophobicity of a protein along its amino acid sequence has been devised. for this purpose, a hydropathy scale has been composed wherein the hydrophilic and hydrophobic properties of each of the 20 amino acid side-chains is taken into consideration. the scale is based on an amalgam of experimental observations derived from the literature. the program uses a moving-segment approach that continuously determines the average hydropathy within a segment of predetermined length as it advances through the sequence. the consecutive scores are plotted from the amino to the carboxy terminus. at the same time, a midpoint line is printed that corresponds to the grand average of the hydropathy of the amino acid compositions found in most of the sequenced proteins. in the case of soluble, globular proteins there is a remarkable correspondence between the interior portions of their sequence and the regions appearing on the hydrophobic side of the midpoint line, as well as the exterior portions and the regions on the hydrophilic side. the correlation was demonstrated by comparisons between the plotted values and known structures determined by crystallography. in the case of membrane-bound proteins, the portions of their sequences that are located within the lipid bilayer are also clearly delineated by large uninterrupted areas on the hydrophobic side of the midpoint line. as such, the membrane-spanning segments of these proteins can be identified by this procedure. although the method is not unique and embodies principles that have long been appreciated, its simplicity and its graphic nature make it a very useful tool for the evaluation of protein structures.
659	315523	article	proc. natl. acad. sci. usa	\N	\N	\N	4	87	6	1990	\N	2005-09-10 19:11:52	\N	methods for assessing the statistical significance of molecular sequence features by using general scoring schemes.	an unusual pattern in a nucleic acid or protein sequence or a region of strong similarity shared by two or more sequences may have biological significance. it is therefore desirable to know whether such a pattern can have arisen simply by chance. to identify interesting sequence patterns, appropriate scoring values can be assigned to the individual residues of a single sequence or to sets of residues when several sequences are compared. for single sequences, such scores can reflect biophysical properties such as charge, volume, hydrophobicity, or secondary structure potential; for multiple sequences, they can reflect nucleotide or amino acid similarity measured in a wide variety of ways. using an appropriate random model, we present a theory that provides precise numerical formulas for assessing the statistical significance of any region with high aggregate score. a second class of results describes the composition of high-scoring segments. in certain contexts, these permit the choice of scoring systems which are "optimal" for distinguishing biologically relevant patterns. examples are given of applications of the theory to a variety of protein sequences, highlighting segments with unusual biological features. these include distinctive charge regions in transcription factors and protooncogene products, pronounced hydrophobic segments in various receptor and transport proteins, and statistically significant subalignments involving the recently characterized cystic fibrosis gene.
660	315559	article	genome biol j	\N	\N	\N	\N	5	\N	2004	\N	2005-09-10 19:11:52	\N	{versatile and open software for comparing large genomes.}	the newest version of mummer easily handles comparisons of large eukaryotic genomes at varying evolutionary distances, as demonstrated by applications to multiple genomes. two new graphical viewing tools provide alternative ways to analyze genome alignments. the new system is the first version of mummer to be released as open-source software. this allows other developers to contribute to the code base and freely redistribute the code. the mummer sources are available at http://www.tigr.org/software/mummer webcite.
661	317978	inproceedings	\N	web clustering workshop at edbt 2004	\N	lncs springer	\N	\N	\N	2004	\N	2005-09-13 16:19:02	crete, greece	query recommendation using query logs in search engines	in this paper we propose a method that, given a query submitted to a search engine, suggests a list of related queries. the related queries are based in previously issued queries, and can be issued by the user to the search engine to tune or redirect the search process. the method proposed is based on a query clustering process in which groups of semantically similar queries are identified. the clustering process uses the content of historical preferences of users registered in the query log of the search engine. the method not only discovers the related queries, but also ranks them according to a relevance criterion. finally, we show with experiments over the query log of a search engine the effectiveness of the method.
662	319962	article	neural computation	\N	\N	\N	14	11	7	1999	\N	2005-09-14 16:21:26	\N	correlations without synchrony	peaks in spike train correlograms are usually taken as indicative of spike timing synchronization between neurons. strictly speaking, however, a peak merely indicates that the two spike trains were not independent. two biologically plausible ways of departing from independence that are capable of generating peaks very similar to spike timing peaks are described here: covariations over trials in response latency and covariations over trials in neuronal excitability. since peaks due to these interactions can be similar to spike timing peaks, interpreting a correlogram may be a problem with ambiguous solutions. what peak shapes do latency or excitability interactions generate? when are they similar to spike timing peaks? when can they be ruled out from having caused an observed correlogram peak? these are the questions addressed here. the previous article in this issue proposes quantitative methods to tell cases apart when latency or excitability covariations cannot be ruled out.
663	320258	article	acm trans. comput.-hum. interact.	\N	\N	acm	31	12	2	2005	jun	2005-11-12 00:28:51	new york, ny, usa	a multilevel analysis of sociability, usability, and community dynamics in an online health community	the aim of this research is to develop an in-depth understanding of the dynamics of online group interaction and the relationship between the participation in an online community and an individual's off-line life. the 2\\&half;-year study of a thriving online health support community (bob's {acl} {wwwboard}) used a broad fieldwork approach, guided by the ethnographic research techniques of observation, interviewing, and archival research in combination with analysis of the group's dynamics during a one-week period. research tools from the social sciences were used to develop a thick, rich description of the group. the significant findings of this study include: dependable and reliable technology is more important than state-of-the-art technology in this community; strong community development exists despite little differentiation of the community space provided by the software; members reported that participation in the community positively influenced their offline lives; strong group norms of support and reciprocity made externally-driven governance unnecessary; tools used to assess group dynamics in face-to-face groups provide meaningful information about online group dynamics; and, membership patterns in the community and strong subgroups actively contributed to the community's stability and vitality.
664	321328	article	journal of molecular biology	\N	\N	\N	24	213	4	1990	jun	2005-09-15 16:02:42	department of biochemistry, university of salzburg, austria.	calculation of conformational ensembles from potentials of mean force. an approach to the knowledge-based prediction of local structures in globular proteins.	we present a prototype of a new approach to the folding problem of polypeptide chains. this approach is based on the analysis of known protein structures. it derives the energy potentials for the atomic interactions of all amino acid residue pairs as a function of the distance between the involved atoms. these potentials are then used to calculate the energies of all conformations that exist in the data base with respect to a given sequence. then, by using only the most stable conformations, clusters of the most probable conformations for the given sequence are obtained. to discuss the results properly we introduce a new classification of segments based on their conformational stability. special care is taken to allow for sparse data sets. the use of the method is demonstrated in the discussion of the identical oligopeptide sequences found in different conformations in unrelated proteins. {vntfv}, for example, adopts a beta-strand in ribonuclease but it is found in an alpha-helical conformation in erythrocruorin. in the case of {vntfv} the ensemble obtained consists of a single cluster of beta-strand conformations, indicating that this may be the preferred conformation for the pentapeptide. when the flanking residues are included in the calculation the hepapeptide {p-vntfv}-h (ribonuclease) again yields an ensemble of beta-strands. however, in the ensemble of {d-vntfv}-a (erythrocruorin) the major cluster is of alpha-helical type. in the present study we concentrate on the local aspects of protein conformations. however, the theory presented is quite general and not restricted to oligopeptides. we indicate extensions of the approach to the calculation of global conformations of proteins as well as conceivable applications to a number of molecular systems.
665	323050	book	\N	\N	\N	penguin books	\N	\N	\N	1995	oct	2005-09-17 00:58:20	\N	how buildings learn: what happens after they're built	{all buildings are forced to adapt over time because of physical deterioration, changing surroundings and the life within--yet very few buildings adapt gracefully, according to brand. houses, he notes, respond to families' tastes, ideas, annoyance and growth; and institutional buildings change with expensive reluctance and delay; while commercial structures have to adapt quickly because of intense competitive pressures. creator of the whole earth catalog and founder of coevolution quarterly (now whole earth review ), brand splices a conversational text with hundreds of extensively captioned photographs and drawings juxtaposing buildings that age well with those that age poorly. he buttresses his critique with insights gleaned from facilities managers, planners, preservationists, building historians and futurists. this informative, innovative handbook sets forth a strategy for constructing adaptive buildings that incorporates a conservationist approach to design, use of traditional materials, attention to local vernacular styles and budgeting to allow for continuous adjustment and maintenance.}
666	325139	article	trends genet	\N	\N	\N	10	19	11	2003	nov	2005-09-18 07:15:33	bauer center for genomics research, harvard university, 7 divinity avenue, cambridge, ma 02138, usa. yfleung@cgr.harvard.edu	fundamentals of {cdna} microarray data analysis.	microarray technology is a powerful approach for genomics research. the multi-step, data-intensive nature of this technology has created an unprecedented informatics and analytical challenge. it is important to understand the crucial steps that can affect the outcome of the analysis. in this review, we provide an overview of the contemporary trend on various main analysis steps in the microarray data analysis process, which includes experimental design, data standardization, image acquisition and analysis, normalization, statistical significance inference, exploratory data analysis, class prediction and pathway analysis, as well as various considerations relevant to their implementation.
667	325817	inproceedings	\N	hypertext	\N	acm press	9	\N	\N	2005	\N	2005-09-19 05:18:01	new york, ny, usa	distributed, real-time computation of community preferences	we describe the integration of smart digital objects with hebbian learning to create a distributed, real-time, scalable approach to adapting to a community's preferences. we designed an experiment using popular music as the subject matter. each digital object corresponded to a music album and contained links to other music albums. by dynamically generating links among digital objects according to user traversal patterns, then hierarchically organizing these links according to shared metadata values, we created a network of digital objects that self-organized in real-time according to the preferences of the user community. furthermore, the similarity between user preferences and generated link structure was more pronounced between collections of objects aggregated by shared metadata values.
668	326502	article	bioinformatics	\N	\N	oxford university press	7	21	19	2005	jan	2005-09-19 20:42:05	harvard-partners center for genetics and genomics, 77 avenue louis pasteur, boston, ma, 02115.	comparative analysis of algorithms for identifying amplifications and deletions in array {cgh} data	motivation: array comparative genomic hybridization ({cgh}) can reveal chromosomal aberrations in the genomic {dna}. these amplifications and deletions at the {dna} level are important in the pathogenesis of cancer and other diseases. while a large number of approaches have been proposed for analyzing the large array {cgh} datasets, the relative merits of these methods in practice are not clear.
669	326547	article	pnas	\N	\N	\N	5	96	25	1999	\N	2005-09-19 23:15:32	\N	{a physical basis for protein secondary structure}	a physical theory of protein secondary structure is proposed and tested by performing exceedingly simple monte carlo simulations. in essence, secondary structure propensities are predominantly a consequence of two competing local effects, one favoring hydrogen bond formation in helices and turns; the other opposing the attendant reduction in sidechain conformational entropy on helix and turn formation. these sequence specific biases are densely dispersed throughout the unfolded polypeptide chain, where they serve to preorganize the folding process and largely, but imperfectly, anticipate the native secondary structure. [journal article; 55 refs; in english; summary in english]
670	326870	article	natural computing	natural computing	\N	kluwer academic publishers	49	1	1	2002	may	2005-09-20 13:42:57	hingham, ma, usa	evolution strategies - a comprehensive introduction	this article gives a comprehensive introduction into one of the main branches of evolutionary computation – the evolution strategies ({es}) the history of which dates back to the 1960s in germany. starting from a survey of history the philosophical background is explained in order to make understandable why {es} are realized in the way they are. basic {es} algorithms and design principles for variation and selection operators as well as theoretical issues are presented, and future branches of {es} research are discussed.
671	327100	article	annual review of fluid mechanics	\N	\N	\N	35	30	\N	1998	jan	2005-09-20 16:19:22	\N	lattice boltzmann method for fluid flows	we present an overview of the lattice boltzmann method ({lbm}), a parallel and efficient algorithm for simulating single-phase and multiphase fluid flows and for incorporating additional physical complexities. the {lbm} is especially useful for modeling complicated boundary conditions and multiphase interfaces. recent extensions of this method are described, including simulations of fluid turbulence, suspension flows, and reaction diffusion systems.
672	327306	article	annals of software engineering	\N	\N	\N	37	1	\N	1995	\N	2005-09-21 02:10:55	\N	cost models for future software life cycle processes: {cocomo} 2.0	current software cost estimation models, such as the 1981 constructive cost model (cocomo) for software cost estimation and its 1987 ada cocomo update, have been experiencing increasing difficulties in estimating the costs of software developed to new life cycle processes and capabilities. these include non-sequential and rapid-development process models; reuse-driven approaches involving commercial off-the-shelf (cots) packages, re-engineering, applications composition, and applications generation capabilities; object-oriented approaches supported by distributed middleware; and software process maturity initiatives. this paper summarizes research in deriving a baseline cocomo 2.0 model tailored to these new forms of software development, including rationale for the model decisions. the major new modeling capabilities of cocomo 2.0 are a tailorable family of software sizing models, involving object points, function points, and source lines of code; nonlinear models for software reuse and re-engineering; an exponentdriver approach for modeling relative software diseconomies of scale; and several additions, deletions and updates to previous cocomo effort-multiplier cost drivers. this model is serving as a framework for an extensive current data collection and analysis effort to further refine and calibrate the model's estimation capabilities.
673	327800	misc	\N	\N	\N	\N	\N	\N	\N	2004	may	2005-09-21 06:42:22	\N	{swrl}: a semantic web rule language combining {owl} and {ruleml}	this document contains a proposal for a semantic web rule language (swrl) based on a combination of the owl dl and owl lite sublanguages of the owl web ontology language with the unary/binary datalog ruleml sublanguages of the rule markup language. swrl includes a high-level abstract syntax for horn-like rules in both the owl dl and owl lite sublanguages of owl. a model-theoretic semantics is given to provide the formal meaning for owl ontologies including rules written in this abstract syntax. an xml syntax based on ruleml and the owl xml presentation syntax as well as an rdf concrete syntax based on the owl rdf/xml exchange syntax are also given, along with several examples.
674	327974	article	proc natl acad sci u s a	\N	\N	\N	\N	\N	\N	2005	sep	2005-09-21 11:03:19	department of mathematics, stanford university, stanford, ca 94305-2125.	statistical signals in bioinformatics.	the arthur m. sackler colloquium of the national academy of sciences, "frontiers in bioinformatics: unsolved problems and challenges," organized by david eisenberg, russ altman, and myself, was held october 15-17, 2004, to provide a forum for discussing concepts and methods in bioinformatics serving the biological and medical sciences. the deluge of genomic and proteomic data in the last two decades has driven the creation of tools that search and analyze biomolecular sequences and structures. bioinformatics is highly interdisciplinary, using knowledge from mathematics, statistics, computer science, biology, medicine, physics, chemistry, and engineering.
675	328154	inproceedings	\N	proceedings of the 16th international conference on supercomputing	\N	\N	11	\N	\N	2005	jun	2005-09-21 13:36:06	new york, new york, united states	search and replication in unstructured {peer-to-peer} networks	decentralized and unstructured peer-to-peer networks such as gnutella are attractive for certain applications because they require no centralized directories and no precise control over network topology or data placement. however, the flooding-based query algorithm used in gnutella does not scale; each query generates a large amount of traffic and large systems quickly become overwhelmed by the query-induced load. this paper explores, through simulation, various alternatives to gnutella's query algorithm, data replication strategy, and network topology. we propose a query algorithm based on multiple random walks that resolves queries almost as quickly as gnutella's flooding method while reducing the network traffic by two orders of magnitude in many cases. we also present simulation results on a distributed replication strategy proposed in [8]. finally, we find that among the various network topologies we consider, uniform random graphs yield the best performance.
676	328165	inproceedings	\N	proceedings of the twenty-third annual joint conference of the ieee computer and communications societies, infocom 2004	\N	\N	10	1	\N	2004	mar	2005-09-21 13:36:06	hong kong	random walks in {peer-to-peer} networks	we quantify the effectiveness of random walks for searching and construction of unstructured peer-to-peer (p2p) networks. we have identified two cases where the use of random walks for searching achieves better results than flooding: a) when the overlay topology is clustered, and h) when a client re-issues the same query while its horizon does not change much. for construction, we argue that an expander can he maintained dynamically with constant operations per addition. the key technical ingredient of our approach is a deep result of stochastic processes indicating that samples taken from consecutive steps of a random walk can achieve statistical properties similar to independent sampling (if the second eigenvalue of the transition matrix is hounded away from 1, which translates to good expansion of the network; such connectivity is desired, and believed to hold, in every reasonable network and network model). this property has been previously used in complexity theory for construction of pseudorandom number generators. we reveal another facet of this theory and translate savings in random bits to savings in processing overhead.
677	331510	article	science (new york, n.y.)	\N	\N	american association for the advancement of science	3	309	5743	2005	sep	2005-09-23 22:32:30	laboratory of living matter and center for studies in physics and biology, the rockefeller university, 1230 york avenue, box 34, new york, ny 10021-6399, usa.	phenotypic diversity, population growth, and information in fluctuating environments.	organisms in fluctuating environments must constantly adapt their behavior to survive. in clonal populations, this may be achieved through sensing followed by response or through the generation of diversity by stochastic phenotype switching. here we show that stochastic switching can be favored over sensing when the environment changes infrequently. the optimal switching rates then mimic the statistics of environmental changes. we derive a relation between the long-term growth rate of the organism and the information available about its fluctuating environment.
678	332173	article	science	\N	\N	american association for the advancement of science	4	286	5438	1999	oct	2005-09-25 13:48:37	\N	evolutionarily conserved pathways of energetic connectivity in protein families	for mapping energetic interactions in proteins, a technique was developed that uses evolutionary data for a protein family to measure statistical interactions between amino acid positions. for the {pdz} domain family, this analysis predicted a set of energetically coupled positions for a binding site residue that includes unexpected long-range interactions. mutational studies confirm these predictions, demonstrating that the statistical energy function is a good indicator of thermodynamic coupling in proteins. sets of interacting residues form connected pathways through the protein fold that may be the basis for efficient energy conduction within proteins.
679	333232	article	bioinformatics	\N	\N	oxford university press	4	21	18	2005	sep	2005-11-08 17:02:55	\N	{microrna} identification based on sequence and structure alignment	motivation: {micrornas} ({mirna}) are [\\~{}]22 nt long non-coding {rnas} that are derived from larger hairpin {rna} precursors and play important regulatory roles in both animals and plants. the short length of the {mirna} sequences and relatively low conservation of {pre-mirna} sequences restrict the conventional sequence-alignment-based methods to finding only relatively close homologs. on the other hand, it has been reported that {mirna} genes are more conserved in the secondary structure rather than in primary sequences. therefore, secondary structural features should be more fully exploited in the homologue search for new {mirna} genes.  results: in this paper, we present a novel genome-wide computational approach to detect {mirnas} in animals based on both sequence and structure alignment. experiments show this approach has higher sensitivity and comparable specificity than other reported homologue searching methods. we applied this method on anopheles gambiae and detected 59 new {mirna} genes.  availability: this program is available at http://bioinfo.au.tsinghua.edu.cn/miralign  contact: daulyd@tsinghua.edu.cn  supplementary information: supplementary information is available at http://bioinfo.au.tsinghua.edu.cn/miralign/supplementary.htm 10.1093/bioinformatics/bti562
680	333373	article	sigkdd explorations	\N	\N	\N	\N	\N	\N	2003	\N	2005-09-27 20:53:57	\N	link mining: a new data mining challenge	a key challenge for data mining is tackling the problem of mining richly structured datasets, where the objects are linked in some way. links among the objects may demonstrate certain patterns, which can be helpful for many data mining tasks and are usually hard to capture with traditional statistical models. recently there has been a surge of interest in this area, fueled largely by interest in web and hypertext mining, but also by interest in mining social networks, security and law enforcement data, bibliographic citations and epidemiological records.
681	333377	inproceedings	\N	woss	\N	acm press	5	\N	\N	2002	\N	2005-09-27 21:27:13	new york, ny, usa	model-based adaptation for self-healing systems	traditional mechanisms that allow a system to detect and recover from errors are typically wired into applications at the level of code where they are hard to change, reuse, or analyze. an alternative approach is to use externalized adaptation: one or more models of a system are maintained at run time and external to the application as a basis for identifying problems and resolving them. in this paper we provide an overview of recent research in which we use architectural models as the basis for such problem diagnosis and repair. these models can be specialized to the particular style of the system, the quality of interest, and the dimensions of run time adaptation that are permitted by the running system.
682	333928	article	acm transactions on computer systems	\N	\N	\N	\N	8	2	1990	\N	2005-09-28 17:28:12	\N	multicast routing in datagram internetworks and extended {lans}	multicasting, the transmission of a packet to a  group  of hosts, is an important service for improving the efficiency and robustness of distributed systems and applications. although multicast capability is available and widely used in local area networks, when those lans are interconnected by store-and-forward routers, the multicast service is usually not offered across the resulting  internetwork . to address this limitation, we specify extensions to two common internetwork routing algorithms distance-vector routing and link-state routing to support low-delay datagram multicasting beyond a single lan. we also describe modifications to the single-spanning-tree routing algorithm commonly used by link-layer bridges, to reduce the costs of multicasting in large extended lans. finally, we discuss how the use of multicast scope control and hierarchical multicast routing allows the multicast service to scale up to large internetworks.
683	334010	inproceedings	\N	proc. of the 16th acm symposium on operating system principles (sosp)	\N	\N	\N	\N	\N	1997	\N	2005-09-28 17:28:14	\N	agile {application-aware} adaptation for mobility	in this paper we show that application-aware adaptation, a collaborative partnership between the operating system and applications, offers the most general and effective approach to mobile information access. we describe the design of odyssey, a prototype implementing this approach, and show how it supports concurrent execution of diverse mobile applications. we identify agility as a key attribute of adaptive systems, and describe how to quantify and measure it. we present the results of our...
684	334077	inproceedings	\N	proc. of infocom	\N	\N	\N	\N	\N	2004	\N	2005-09-28 17:28:16	\N	service capacity of peer to peer networks	we study the 'service capacity' of peer to peer (p2p) file sharing applications. we begin by considering a transient regime which is key to capturing the ability of such systems to handle bursty traffic, e.g., flash crowds. in this context our models, based on age dependent branching processes, exhibit exponential growth in service capacity, and permit the study of sensitivity of this growth to system policies and parameters. then we consider a model for such systems in steady state and show how the average delay seen by peers would scale in the offered load and rate at which peers exit the system. we find that the average delays scale well in the offered load. in particular the delays are upper bounded by some constant given any offered load and even decrease in the offered load if peers exit the system slowly. we validate many of our findings by analyzing traces obtained from a second generation p2p application called bittorrent.
685	334264	article	nature	\N	\N	nature publishing group	5	437	7062	2005	sep	2005-10-06 02:53:14	\N	towards a proteome-scale map of the human protein-protein interaction network	systematic mapping of protein–protein interactions, or 'interactome' mapping, was initiated in model organisms, starting with defined biological processes1, 2 and then expanding to the scale of the proteome3, 4, 5, 6, 7. although far from complete, such maps have revealed global topological and dynamic features of interactome networks that relate to known biological properties8, 9, suggesting that a human interactome map will provide insight into development and disease mechanisms at a systems level. here we describe an initial version of a proteome-scale map of human binary protein–protein interactions. using a stringent, high-throughput yeast two-hybrid system, we tested pairwise interactions among the products of 8,100 currently available gateway-cloned open reading frames and detected 2,800 interactions. this data set, called {ccsb}-{hi1}, has a verification rate of 78\\% as revealed by an independent co-affinity purification assay, and correlates significantly with other biological attributes. the {ccsb}-{hi1} data set increases by 70\\% the set of available binary interactions within the tested space and reveals more than 300 new connections to over 100 disease-associated proteins. this work represents an important step towards a systematic and comprehensive human interactome project.
686	334436	article	ieee comput. graph. appl.	\N	\N	ieee computer society press	11	19	6	1999	nov	2005-09-28 22:27:25	los alamitos, ca, usa	what's real about virtual reality?	as is usual with infant technologies, the realization of the early dreams for {vr} and harnessing it to real work has taken longer than the wild hype predicted, but it is now happening. i assess the current state of the art, addressing the perennial questions of technology and applications. by 1994, one could honestly say that {vr} "almost works." many workers at many centers could doe quite exciting {demos.nevertheless}, the enabling technologies had limitations that seriously impeded building {vr} systems for any real work except entertainment and vehicle simulators. some of the worst problems were end-to-end system latencies, low-resolution head-mounted displays, limited tracker range and accuracy, and costs. the technologies have made great strides. today one can get satisfying {vr} experiences with commercial off-the-shelf equipment. moreover, technical advances have been accompanied by dropping costs, so it is both technically and economically feasible to do significant application. {vr} really works. that is not to say that all the technological problems and limitations have been solved. {vr} technology today "barely works." nevertheless, coming over the mountain pass from "almost works" to "barely works" is a major transition for the {discipline.i} have sought out applications that are now in daily productive use, in order to find out exactly what is real. separating these from prototype systems and feasibility demos is not always easy. people doing daily production applications have been forthcoming about lessons learned and surprises encountered. as one would expect, the initial production applications are those offering high value over alternate approaches. these applications fall into a few classes. i estimate that there are about a hundred installations in daily productive use worldwide.
687	335441	article	nature	\N	\N	nature publishing group	7	412	6843	2001	jul	2005-09-29 21:11:09	max planck institute for biological cybernetics, tuebingen, germany. nikos.logothetis@tuebingen.mpg.de	neurophysiological investigation of the basis of the {fmri} signal	functional magnetic resonance imaging ({fmri}) is widely used to study the operational organization of the human brain, but the exact relationship between the measured {fmri} signal and the underlying neural activity is unclear. here we present simultaneous intracortical recordings of neural signals and {fmri} responses. we compared local field potentials ({lfps}), single- and multi-unit spiking activity with highly spatio-temporally resolved blood-oxygen-level-dependent ({bold}) {fmri} responses from the visual cortex of monkeys. the largest magnitude changes were observed in {lfps}, which at recording sites characterized by transient responses were the only signal that significantly correlated with the haemodynamic response. linear systems analysis on a trial-by-trial basis showed that the impulse response of the neurovascular system is both animal- and site-specific, and that {lfps} yield a better estimate of {bold} responses than the multi-unit responses. these findings suggest that the {bold} contrast mechanism reflects the input and intracortical processing of a given area rather than its spiking output.
688	335719	proceedings	autonomic computing, 2005. icac 2005. proceedings. second international conference on	\N	\N	\N	11	\N	\N	2005	\N	2005-09-29 23:45:45	\N	design and evaluation of an autonomic workflow engine	in this paper we present the design and evaluate the performance of an autonomic workflow execution engine. although there exist many distributed workflow engines, in practice, it remains a difficult problem to deploy such systems in an optimal configuration. furthermore, when facing an unpredictable workload with high variability, manual reconfiguration is not an option. thanks to its autonomic controller, the engine features self-configuration, self-tuning and self-healing properties. the engine runs on a cluster of computers using a tuple space to coordinate its various components. its autonomic controller monitors its performance and responds to workload variations by altering the configuration. in case failures occur, the controller can recover the workflow execution state from persistent storage and migrate it to a different node of the cluster. such interventions are carried out without any human supervision. as part of the results of our performance evaluation, we compare different autonomic control strategies and discuss how they can automatically tune the system.
689	335723	proceedings	autonomic computing, 2004. proceedings. international conference on	autonomic computing, 2004. proceedings. international conference on	\N	\N	7	\N	\N	2004	jun	2005-09-29 23:57:19	\N	an architectural approach to autonomic computing	we describe an architectural approach to achieving the goals of autonomic computing. the architecture that we outline describes interfaces and behavioral requirements for individual system components, describes how interactions among components are established, and recommends design patterns that engender the desired system-level properties of self-configuration, self-optimization, self-healing and self-protection. we have validated many of these ideas in two prototype autonomic computing systems.
690	336740	article	journal of chemical information and computer sciences	\N	\N	\N	7	41	5	2001	\N	2005-09-30 12:53:40	\N	is there a difference between leads and drugs? {a} historical perspective	to be considered for further development, lead structures should display the following properties: (1) simple chemical features, amenable for chemistry optimization; (2) membership to an established sar series; (3) favorable patent situation; and (4) good absorption, distribution, metabolism, and excretion (adme) properties. there are two distinct categories of leads: those that lack any therapeutic use (i.e., "pure" leads), and those that are marketed drugs themselves but have been altered to yield novel drugs. we have previously analyzed the design of leadlike combinatorial libraries starting from 18 lead and drug pairs of structures (s. j. teague et al. angew. chem., int. ed. engl. 1999, 38, 3743-3748). here, we report results based on an extended dataset of 96 lead-drug pairs, of which 62 are lead structures that are not marketed as drugs, and 75 are drugs that are not presumably used as leads. we examined the following properties: mw (molecular weight), cmr (the calculated molecular refractivity), rng (the number of rings), rtb (the number of rotatable bonds), the number of hydrogen bond donors (hdo) and acceptors (hac), the calculated logarithm of the n-octanol/water partition (clogp), the calculated logarithm of the distribution coefficient at ph 7.4 (logd(74)), the daylight-fingerprint druglike score (dfps), and the property and pharmacophore features score (ppfs). the following differences were observed between the medians of drugs and leads: deltamw = 69; deltacmr = 1.8; deltarng = deltahac =1; deltartb = 2; deltaclogp = 0.43; deltalogd(74) = 0.97; deltahdo = 0; deltadfps = 0.15; deltappfs = 0.12. lead structures exhibit, on the average, less molecular complexity (less mw, less number of rings and rotatable bonds), are less hydrophobic (lower clogp and logd(74)), and less druglike (lower druglike scores). these findings indicate that the process of optimizing a lead into a drug results in more complex structures. this information should be used in the design of novel combinatorial libraries that are aimed at lead discovery.
691	336990	article	american psychologist	\N	\N	\N	11	60	6	2005	sep	2005-09-30 15:50:31	\N	the gender similarities hypothesis	the differences model, which argues that males and females are vastly different psychologically, dominates the popular media. here, the author advances a very different view, the gender similarities hypothesis, which holds that males and females are similar on most, but not all, psychological variables. results from a review of 46 meta-analyses support the gender similarities hypothesis. gender differences can vary substantially in magnitude at different ages and depend on the context in which measurement occurs. overinflated claims of gender differences carry substantial costs in areas such as the workplace and relationships.
692	337170	article	sigkdd explorations	\N	\N	\N	10	6	2	2004	dec	2005-09-30 16:42:29	\N	learning by googling	the goal of giving a well-defined meaning to information is currently shared by endeavors such as the semantic web as well as by current trends within knowledge management. they all depend on the large-scale formalization of knowledge and on the availability of formal metadata about information resources. however, the question how to provide the necessary formal metadata in an effective and efficient way is still not solved to a satisfactory extent. certainly, the most effective way to provide such metadata as well as formalized knowledge is to let humans encode them directly into the system, but this is neither efficient nor feasible. furthermore, as current social studies show, individual knowledge is often less powerful than the collective knowledge of a certain community.as a potential way out of the  knowledge acquisition bottleneck , we present a novel methodology that acquires collective knowledge from the world wide web using the google tm  api. in particular, we present pankow, a concrete instantiation of this methodology which is evaluated in two experiments: one with the aim of classifying novel instances with regard to an existing ontology and one with the aim of learning sub-/superconcept relations.
693	337765	article	iee proceedings - computers and digital techniques	\N	\N	\N	14	152	2	2005	\N	2005-09-30 22:41:10	\N	reconfigurable computing: architectures and design methods	reconfigurable computing is becoming increasingly attractive for many applications. this survey covers two aspects of reconfigurable computing: architectures and design methods. the paper includes recent advances in reconfigurable architectures, such as the alters stratix {ii} and xilinx virtex 4 {fpga} devices. the authors identify major trends in general-purpose and special-purpose design methods. it is shown that reconfigurable computing designs are capable of achieving up to 500 times speedup and 70\\% energy savings over microprocessor implementations for specific applications.
694	338247	article	ieee transactions on medical imaging	\N	\N	ieee	9	18	8	1999	aug	2005-10-01 14:00:09	division of radiological sciences and medical engineering, guy's, king's, and st. thomas' school of medicine, king's college london, guy's hospital, london, uk. d.rueckert@umds.ac.uk	nonrigid registration using free-form deformations: application to breast {mr} images	in this paper the authors present a new approach for the nonrigid registration of contrast-enhanced breast {mri}. a hierarchical transformation model of the motion of the breast has been developed. the global motion of the breast is modeled by an affine transformation while the local breast motion is described by a free-form deformation ({ffd}) based on b-splines. normalized mutual information is used as a voxel-based similarity measure which is insensitive to intensity changes as a result of the contrast enhancement. registration is achieved by minimizing a cost function, which represents a combination of the cost associated with the smoothness of the transformation and the cost associated with the image similarity. the algorithm has been applied to the fully automated registration of three-dimensional ({3-d}) breast {mri} in volunteers and patients. in particular, the authors have compared the results of the proposed nonrigid registration algorithm to those obtained using rigid and affine registration techniques. the results clearly indicate that the nonrigid registration algorithm is much better able to recover the motion and deformation of the breast than rigid or affine registration algorithms.
695	339286	article	computer magazine of the computer group news of the ieee computer group society, ; acm cr 8905-0314	\N	\N	\N	\N	21	2	1988	\N	2005-10-03 04:32:45	\N	the sprite network operating system	sprite is a new operating system for networked uniprocessor and multiprocessor workstations with large physical memories. it implements a set of kernel calls much like those of 4.3 {bsd} {unix}, with extensions to allow processes on the same workstation to share memory and to allow processes to migrate between workstations. the implementation of the sprite kernel contains several interesting features, including a remote procedure call facility for communication between kernels, the use of prefix...
696	339334	techreport	\N	\N	\N	\N	\N	\N	49372	2002	\N	2005-10-03 11:38:32	\N	pagerank, {hits} and a unified framework for link analysis	two popular webpage ranking algorithms are {hits} and {pagerank}. {hits} emphasizes mutual reinforcement between authority and hub webpages, while {pagerank} emphasizes hyperlink weight normalization and web surfing based on random walk models. we systematically generalize /combine these concepts into a unified framework. the ranking framework contains a large algorithm space
697	339382	article	nature	\N	\N	nature publishing group	3	405	6782	2000	may	2005-10-03 14:49:42	department of biochemistry, university of washington, seattle 98195, usa.	a surprising simplicity to protein folding	the polypeptide chains that make up proteins have thousands of atoms and hence millions of possible inter-atomic interactions. it might be supposed that the resulting complexity would make prediction of protein structure and protein-folding mechanisms nearly impossible. but the fundamental physics underlying folding may be much simpler than this complexity would lead us to expect: folding rates and mechanisms appear to be largely determined by the topology of the native (folded) state, and new methods have shown great promise in predicting protein-folding mechanisms and the three-dimensional structures of proteins.
698	339630	article	medical image analysis	\N	\N	\N	15	6	\N	2002	\N	2005-10-03 18:19:50	\N	processing and visualization for diffusion tensor {mri}	this paper presents processing and visualization techniques for diffusion tensor magnetic resonance imaging (dt-mri). in dt-mri, each voxel is assigned a tensor that describes local water diffusion. the geometric nature of diffusion tensors enables us to quantitatively characterize the local structure in tissues such as bone, muscle, and white matter of the brain. this makes dt-mri an interesting modality for image analysis. in this paper we present a novel analytical solution to the stejskal-tanner diffusion equation system whereby a dual tensor basis, derived from the diffusion sensitizing gradient configuration, eliminates the need to solve this equation for each voxel. we further describe decomposition of the diffusion tensor based on its symmetrical properties, which in turn describe the geometry of the diffusion ellipsoid. a simple anisotropy measure follows naturally from this analysis. we describe how the geometry or shape of the tensor can be visualized using a coloring scheme based on the derived shape measures. in addition, we demonstrate that human brain tensor data when filtered can effectively describe macrostructural diffusion, which is important in the assessment of fiber-tract organization. we also describe how white matter pathways can be monitored with the methods introduced in this paper. dt-mri tractography is useful for demonstrating neural connectivity (in vivo) in healthy and diseased brain tissue. (c) 2002 elsevier science b.v. all rights reserved.
699	339850	article	nature reviews. neuroscience	\N	\N	nature publishing group	10	6	10	2005	oct	2005-10-22 14:51:14	\N	what makes us tick? functional and neural mechanisms of interval timing.	time is a fundamental dimension of life. it is crucial for decisions about quantity, speed of movement and rate of return, as well as for motor control in walking, speech, playing or appreciating music, and participating in sports. traditionally, the way in which time is perceived, represented and estimated has been explained using a pacemaker-accumulator model that is not only straightforward, but also surprisingly powerful in explaining behavioural and biological data. however, recent advances have challenged this traditional view. it is now proposed that the brain represents time in a distributed manner and tells the time by detecting the coincidental activation of different neural populations.
700	339999	article	trends in cognitive sciences	\N	\N	\N	8	4	5	2000	may	2005-10-04 03:01:27	\N	contextual cueing of visual attention	visual context information constrains what to expect and where to look, facilitating search for and recognition of objects embedded in complex displays. this article reviews a new paradigm called contextual cueing, which presents well-defined, novel visual contexts and aims to understand how contextual information is learned and how it guides the deployment of visual attention. in addition, the contextual cueing task is well suited to the study of the neural substrate of contextual learning. for example, amnesic patients with hippocampal damage are impaired in their learning of novel contextual information, even though learning in the contextual cueing task does not appear to rely on conscious retrieval of contextual memory traces. we argue that contextual information is important because it embodies invariant properties of the visual environment such as stable spatial layout information as well as object covariation information. sensitivity to these statistical regularities allows us to interact more effectively with the visual world.
701	340705	electronic	\N	\N	\N	\N	\N	\N	\N	2005	sep	2005-10-04 15:14:40	\N	requirements for digital preservation systems: a {bottom-up} approach	the field of digital preservation is being defined by a set of standards developed top-down, starting with an abstract reference model (oais) and gradually adding more specific detail. systems claiming conformance to these standards are entering production use. work is underway to certify that systems conform to requirements derived from oais.   we complement these requirements derived top-down by presenting an alternate, bottom-up view of the field. the fundamental goal of these systems is to ensure that the information they contain remains accessible for the long term. we develop a parallel set of requirements based on observations of how existing systems handle this task, and on an analysis of the threats to achieving the goal. on this basis we suggest disclosures that systems should provide as to how they satisfy their goals.
702	341384	article	ieee-ec	\N	\N	\N	\N	3	2	1999	jul	2005-10-05 13:53:28	\N	parameter control in {e}volutionary {a}lgorithms	the issue of controlling values of various parameters of an evolutionary algorithm is one of the most important and promising areas of research in evolutionary computation: it has a potential of adjusting the algorithm to the problem while solving the problem. in the paper we: 1) revise the terminology, which is unclear and confusing, thereby providing a classification of such control mechanisms, and 2) survey various forms of control which have been studied by the evolutionary computation community in recent years. our classification covers the major forms of parameter control in evolutionary computation and suggests some directions for further research
703	342965	article	nature biotechnology	\N	\N	nature publishing group	7	23	10	2005	oct	2005-11-01 21:21:01	\N	discovery of regulatory elements in vertebrates through comparative genomics	we have analyzed issues of reliability in studies in which comparative genomic approaches have been applied to the discovery of regulatory elements at a genome-wide level in vertebrates. we point out some potential problems with such studies, including difficulties in accurately identifying orthologous promoter regions. many of these subtle analytical problems have become apparent only when studying the more complex vertebrate genomes. by determining motif reliability, we compared existing tools when applied to the discovery of vertebrate regulatory elements. we then used a statistical clustering method to produce a computational catalog of high quality putative regulatory elements from vertebrates, some of which are widely conserved among vertebrates and many of which are novel regulatory elements. the results provide a glimpse into the wealth of information that comparative genomics can yield and suggest the need for further improvement of genome-wide comparative computational techniques.
704	343691	article	science	\N	\N	\N	4	295	5560	2002	mar	2005-10-07 11:34:53	\N	modeling the heart-from genes to cells to the whole organ	successful physiological analysis requires an understanding of the functional interactions between the key components of cells, organs, and systems, as well as how these interactions change in disease states. this information resides neither in the genome nor even in the individual proteins that genes code for. it lies at the level of protein interactions within the context of subcellular, cellular, tissue, organ, and system structures. there is therefore no alternative to copying nature and computing these interactions to determine the logic of healthy and diseased states. the rapid growth in biological databases; models of cells, tissues, and organs; and the development of powerful computing hardware and algorithms have made it possible to explore functionality in a quantitative manner all the way from the level of genes to the physiological function of whole organs and regulatory systems. this review illustrates this development in the case of the heart. systems physiology of the 21st century is set to become highly quantitative and, therefore, one of the most computer-intensive disciplines.
705	343769	article	bioinformatics	\N	\N	\N	12	15	1	1999	jan	2005-10-07 11:34:55	\N	e-{cell}: software environment for whole-cell simulation	motivation: genome sequencing projects and further systematic functional analyses of complete gene sets are producing an unprecedented mass of molecular information for a wide range of model organisms. this provides us with a detailed account of the cell with which we may begin to build models for simulating intracellular molecular processes to predict the dynamic behavior of living cells. previous work in biochemical and genetic simulation has isolated well-characterized pathways for detailed analysis, but methods for building integrative models of the cell that incorporate gene regulation, metabolism and signaling have not been established. we, therefore, were motivated to develop a software environment for building such integrative models based on gene sets, and running simulations to conduct experiments in silico. results: e-cell, a modeling and simulation environment for biochemical and genetic processes, has been developed. the e-cell system allows a user to define functions of proteins, protein-protein interactions, protein-dna interactions, regulation of gene expression and other features of cellular metabolism, as a set of reaction rules. e-cell simulates cell behavior by numerically integrating the differential equations described implicitly in these reaction rules. the user can observe, through a computer display, dynamic changes in concentrations of proteins, protein complexes and other chemical compounds in the cell. using this software, we constructed a model of a hypothetical cell with only 127 genes sufficient for transcription, translation, energy production and phospholipid synthesis. most of the genes are taken from mycoplasma genitalium, the organism having the smallest known chromosome, whose complete 580 kb genome sequence was determined at tigr in 1995. we discuss future applications of the e-cell system with special respect to genome engineering. availability: the e-cell software is available upon request. supplementary information: the complete list of rules of the developed cell model with kinetic parameters can be obtained via our web site at: http://e-cell.org/.
706	343795	article	comput. applic. biosci.	\N	\N	\N	8	9	5	1993	oct	2005-10-07 11:34:55	\N	{gepasi}: a software package for modelling the dynamics, steady states and control of biochemical and other systems	gepasi is a software system for modelling chemical and biochemical reaction networks on computers running microsoft windows. for any system of up to 45 metabolites and 45 reactions, each with any user-defined or one of 35 predefined rate equations, one can produce trajectories of the metabolite concentrations and obtain a steady state (if it does exist). when steady-state solutions are produced, elasticity and control coefficients, as defined in metabolic control analysis, are calculated. gepasi also allows the automatic generation of a sequence of simulations with different combinations of parameter values, effectively scanning a hyper-solid in parameter space. together with the ability to produce user-defined columnar data files, these features allow for both very quick and systematic study of biochemical pathway models. the source code (in c) is available on request from the author, and while the user interface is dependent on having ms-windows as the operating system, the numerical part is portable to other operating systems. gepasi is suitable both for research and educational purposes. although gepasi was written with biochemical pathways in mind, it can equally be used to simulate other dynamical systems. 10.1093/bioinformatics/9.5.563
707	343798	article	bioinformatics	\N	\N	\N	8	20	7	2004	\N	2005-10-07 11:34:55	\N	{compucell}, a multi-model framework for simulation of morphogenesis	motivation: compucell is a multi-model software framework for simulation of the development of multicellular organisms known as morphogenesis. it models the interaction of the gene regulatory network with generic cellular mechanisms, such as cell adhesion, division, haptotaxis and chemotaxis. a combination of a state automaton with stochastic local rules and a set of differential equations, including subcellular ordinary differential equations and extracellular reaction{\\^a}Â€Â“diffusion partial differential equations, model gene regulation. this automaton in turn controls the differentiation of the cells, and cell{\\^a}Â€Â“cell and cell{\\^a}Â€Â“extracellular matrix interactions that give rise to cell rearrangements and pattern formation, e.g. mesenchymal condensation. the cellular potts model, a stochastic model that accurately reproduces cell movement and rearrangement, models cell dynamics. all these models couple in a controllable way, resulting in a powerful and flexible computational environment for morphogenesis, which allows for simultaneous incorporation of growth and spatial patterning. results: we use compucell to simulate the formation of the skeletal architecture in the avian limb bud. availability: binaries and source code for microsoft windows, linux and solaris are available for download from http://sourceforge.net/projects/compucell/ contact: compucell@cse.nd.edu
708	343921	book	\N	\N	\N	mit press	\N	\N	\N	1997	\N	2005-10-07 11:55:48	cambridge, mass.	being there : {p}utting {b}rain, {b}ody, and {w}orld {t}ogether {a}gain	{brain, body, and world are united in a complex dance of circular causation and extended computational activity. in <i>being there</i>, andy clark weaves these several threads into a pleasing whole and goes on to address foundational questions concerning the new tools and techniques needed to make sense of the emerging sciences of the embodied mind. clark brings together ideas and techniques from robotics, neuroscience, infant psychology, and artificial intelligence. he addresses a broad range of adaptive behaviors, from cockroach locomotion to the role of linguistic artifacts in higher-level thought.}
709	344216	article	acm trans. inf. syst.	\N	\N	acm	26	22	1	2004	jan	2005-10-07 13:31:58	new york, ny, usa	latent semantic models for collaborative filtering	collaborative filtering aims at learning predictive models of user preferences, interests or behavior from community data, that is, a database of available user preferences. in this article, we describe a new family of model-based algorithms designed for this task. these algorithms rely on a statistical modelling technique that introduces latent class variables in a mixture model setting to discover user communities and prototypical interest profiles. we investigate several variations to deal with discrete and continuous response variables as well as with different objective functions. the main advantages of this technique over standard memory-based methods are higher accuracy, constant time prediction, and an explicit and compact model representation. the latter can also be used to mine for user communitites. the experimental evaluation shows that substantial improvements in accucracy over existing methods and published results can be obtained.
710	344278	inproceedings	\N	{chi}	\N	\N	7	\N	\N	2000	\N	2005-10-07 14:00:19	\N	{developing a context-aware electronic tourist guide: some issues and experiences}	in this paper, we describe our experiences of developing and evaluating guide, an intelligent electronic tourist guide. the guide system has been built to overcome many of the limitations of the traditional information and navigation tools available to city visitors. for example, group-based tours are inherently inflexible with fixed starting times and fixed durations and (like most guidebooks) are constrained by the need to satisfy the interests of the majority rather than the specific interests of individuals. following a period of requirements capture, involving experts in the field of tourism, we developed and installed a system for use by visitors to lancaster. the system combines mobile computing technologies with a wireless infrastructure to present city visitors with information tailored to both their personal and environmental contexts. in this paper we present an evaluation of guide, focusing on the quality of the visitor's experience when using the system.
711	344318	inproceedings	\N	ubicomp 2001 proceedings	lecture notes in computer science	springer	18	2201	\N	2001	\N	2005-10-07 14:00:20	\N	{privacy by design -- principles of privacy-aware ubiquitous systems}	this paper tries to serve as an introductory reading to privacy issues in the field of ubiquitous computing. it develops six principles for guiding system design, based on a set of fair information practices common in most privacy legislation in use today: notice, choice and consent, proximity and locality, anonymity and pseudonymity, security, and access and recourse. a brief look at the history of privacy protection, its legal status, and its expected utility is provided as a background.
712	344542	article	acm transactions on programming languages and systems	\N	\N	\N	34	12	1	1990	jan	2005-10-07 15:24:20	\N	interprocedural slicing using dependence graphs	the notion of a program slice, originally introduced by mark weiser, is useful in program debugging, automatic parallelization, and program integration. a slice of a program is taken with respect to a program point p and a variable x; the slice consists of all statements of the program that might affect the value of x at point p. this paper concerns the problem of interprocedural slicing generating a slice of an entire program, where the slice crosses the boundaries of procedure calls. to solve this problem, we introduce a new kind of graph to represent programs, called a system dependence graph, which extends previous dependence representations to incorporate collections of procedures (with procedure calls) rather than just monolithic programs. our main result is an algorithm for interprocedural slicing that uses the new representation. (it should be noted that our work concerns a somewhat restricted kind of slice: rather than permitting a program to b e sliced with respect to program point p and an arbitrary variable, a slice must be taken with respect to a variable that is defined or used at p.) the chief difficulty in interprocedural slicing is correctly accounting for the calling context of a called procedure. to handle this problem, system dependence graphs include some data dependence edges that represent transitive dependences due to the effects of procedure calls, in addition to the conventional direct-dependence edges. these edges are constructed with the aid of an auxiliary structure that represents calling and parameter-linkage relationships. this structure takes the form of an attribute grammar. the step of computing the required transitive-dependence edges is reduced to the construction of the subordinate characteristic graphs for the grammar's nonterminals.
713	344613	book	\N	\N	\N	addison wesley	\N	\N	\N	1996	\N	2005-10-07 15:24:23	\N	{object-oriented} design heuristics	object-oriented design heuristics offers insight into object-oriented design improvement. the more than sixty guidelines presented in this book are language-independent and allow you to rate the integrity of a software design. the heuristics are not written as hard and fast rules; they are meant to serve as warning mechanisms which allow the flexibility of ignoring the heuristic as necessary. this tutorial-based approach, born out of the author's extensive experience developing software, teaching thousands of students, and critiquing designs in a variety of domains, allows you to apply the guidelines in a personalized manner. the heuristics cover important topics ranging from classes and objects (with emphasis on their relationships including association, uses, containment, and both single and multiple inheritance) to physical object-oriented design. you will gain an understanding of the synergy that exists between design heuristics and the popular concept of design patterns; heuristics can highlight a problem in one facet of a design while patterns can provide the solution. programmers of all levels will find value in this book. the newcomer will discover a fast track to understanding the concepts of object-oriented programming. at the same time, experienced programmers seeking to strengthen their object-oriented development efforts will appreciate the insightful analysis. in short, with object-oriented design heuristics as your guide, you have the tools to become a better software developer.
714	344617	inproceedings	\N	proceedings of the 24th international conference on software engineering	\N	\N	10	\N	\N	2002	may	2005-10-07 15:24:23	\N	concern graphs: finding and describing concerns using structural program dependencies	many maintenance tasks address concerns, or features, that are not well modularized in the source code comprising a system. existing approaches available to help software developers locate and manage scattered concerns use a representation based on lines of source code, complicating the analysis of the concerns. in this paper, we introduce the concern graph representation that abstracts the implementation details of a concern and makes explicit the relationships between different parts of the concern. the abstraction used in a concern graph has been designed to allow an obvious and inexpensive mapping back to the corresponding source code. to investigate the practical tradeoffs related to this approach, we have built the feature exploration and analysis tool (feat) that allows a developer to manipulate a concern representation extracted from a java system, and to analyze the relationships of that concern to the code base. we have used this tool to find and describe concerns related to software change tasks. we have performed case studies to evaluate the feasibility, usability, and scalability of the approach. our results indicate that concern graphs can be used to document a concern for change, that developers unfamiliar with concern graphs can use them effectively, and that the underlying technology scales to industrial-sized programs.
715	345172	misc	\N	\N	\N	\N	\N	\N	\N	2002	\N	2005-10-07 19:26:47	\N	an integrated experimental environment for distributed systems and networks	three experimental environments traditionally support network and distributed systems research: network emulators, network simulators, and live networks. the continued use of multiple approaches highlights both the value and inadequacy of each. netbed, a descendant of emulab, provides an experimentation facility that integrates these approaches, allowing researchers to configure and access networks composed of emulated, simulated, and wide-area nodes and links. netbed's primary goals are  ease of use, control , and  realism , achieved through consistent use of virtualization and abstraction.by providing operating system-like services, such as resource allocation and scheduling, and by virtualizing heterogeneous resources, netbed acts as a virtual machine for network experimentation. this paper presents netbed's overall design and implementation and demonstrates its ability to improve experimental automation and efficiency. these, in turn, lead to new methods of experimentation, including automated parameter-space studies within emulation and straightforward comparisons of simulated, emulated, and wide-area scenarios.
716	345209	article	plos biol	\N	\N	public library of science	\N	3	11	2005	oct	2005-11-12 15:00:48	department of biomedical engineering, johns hopkins university, baltimore, maryland, united states of america.	dynamic properties of network motifs contribute to biological network organization	biological networks, such as those describing gene regulation, signal transduction, and neural synapses, are representations of large-scale dynamic systems. discovery of organizing principles of biological networks can be enhanced by embracing the notion that there is a deep interplay between network structure and system dynamics. recently, many structural characteristics of these non-random networks have been identified, but dynamical implications of the features have not been explored comprehensively. we demonstrate by exhaustive computational analysis that a dynamical property—stability or robustness to small perturbations—is highly correlated with the relative abundance of small subnetworks (network motifs) in several previously determined biological networks. we propose that robust dynamical stability is an influential property that can determine the non-random structure of biological networks.
717	347146	misc	\N	\N	\N	\N	\N	\N	\N	1998	\N	2005-10-10 20:16:32	\N	condensation -- conditional density propagation for visual tracking	the problem of tracking curves in dense visual clutter is challenging. kalman filtering is inadequate because it is based on gaussian densities which, being unimodal, cannot represent simultaneous alternative hypotheses. the condensation algorithm uses "factored sampling", previously applied to the interpretation of static images, in which the probability distribution of possible interpretations is represented by a randomly generated set. condensation uses learned dynamical models, together...
718	347359	article	sigcomm comput. commun. rev.	\N	\N	acm press	16	26	3	1996	jul	2005-10-11 04:02:24	new york, ny, usa	simulation-based comparisons of tahoe, reno and {sack} {tcp}	this paper uses simulations to explore the benefits of adding selective acknowledgments (sack) and selective repeat to tcp. we compare tahoe and reno tcp, the two most common reference implementations for tcp, with two modified versions of reno tcp. the first version is new-reno tcp, a modified version of tcp without sack that avoids some of reno tcp's performance problems when multiple packets are dropped from a window of data. the second version is sack tcp, a conservative extension of reno tcp modified to use the sack option being proposed in the internet engineering task force (ietf). we describe the congestion control algorithms in our simulated implementation of sack tcp and show that while selective acknowledgments are not required to solve reno tcp's performance problems when multiple packets are dropped, the absence of selective acknowledgments does impose limits to tcp's ultimate performance. in particular, we show that without selective acknowledgments, tcp implementations are constrained to either retransmit at most one dropped packet per round-trip time, or to retransmit packets that might have already been successfully delivered.
719	347869	article	systematic biology	\N	\N	\N	8	52	5	2003	oct	2005-10-11 15:01:07	lirmm, cnrs, 161 rue ada, 34392, montpellier cedex 5, france.	a simple, fast, and accurate algorithm to estimate large phylogenies by maximum likelihood.	the increase in the number of large data sets and the complexity of current probabilistic sequence evolution models necessitates fast and reliable phylogeny reconstruction methods. we describe a new approach, based on the maximum- likelihood principle, which clearly satisfies these requirements. the core of this method is a simple hill-climbing algorithm that adjusts tree topology and branch lengths simultaneously. this algorithm starts from an initial tree built by a fast distance-based method and modifies this tree to improve its likelihood at each iteration. due to this simultaneous adjustment of the topology and branch lengths, only a few iterations are sufficient to reach an optimum. we used extensive and realistic computer simulations to show that the topological accuracy of this new method is at least as high as that of the existing maximum-likelihood programs and much higher than the performance of distance-based and parsimony approaches. the reduction of computing time is dramatic in comparison with other maximum-likelihood packages, while the likelihood maximization ability tends to be higher. for example, only 12 min were required on a standard personal computer to analyze a data set consisting of 500 {rbcl} sequences with 1,428 base pairs from plant plastids, thus reaching a speed of the same order as some popular distance-based and parsimony algorithms. this new method is implemented in the {phyml} program, which is freely available on our web page: {http://www.lirmm.fr/w3ifa/maas}/.
720	348334	article	educational psychologist	\N	\N	lawrence erlbaum associates, inc.	31	28	2	1993	\N	2005-10-11 22:54:56	\N	perceived {self-efficacy} in cognitive development and functioning	in this article, i review the diverse ways in which perceived self-efficacy contributes to cognitive development and functioning. perceived self-efficacy exerts its influence through four major processes. they include cognitive, motivational, affective, and selection processes. there are three different levels at which perceived self-efficacy operates as an important contributor to academic development. students' beliefs in their efficacy to regulate their own learning and to master academic activities determine their aspirations, level of motivation, and academic accomplishments. teachers' beliefs in their personal efficacy to motivate and promote learning affect the types of learning environments they create and the level of academic progress their students achieve. faculties' beliefs in their collective instructional efficacy contribute significantly to their schools' level of academic achievement. student body characteristics influence school-level achievement more strongly by altering faculties' beliefs in their collective efficacy than through direct affects on school achievement.
721	348659	book	\N	\N	\N	wiley	\N	\N	\N	2003	oct	2005-10-12 08:33:33	\N	remote sensing and image interpretation	from recent developments in digital image processing to the next generation of satellite systems, this book provides a comprehensive introduction to the field of remote sensing and image interpretation. this book is discipline neutral, so readers in any field of study can gain a clear understanding of these systems and their virtually unlimited applications.<br> * the authors underscore close interactions among the related areas of remote sensing, {gis}, {gps}, digital image processing, and environmental modeling.<br> * appendices include material on sources of remote sensing data and information, remote sensing periodicals, online glossaries, and online tutorials.
722	349172	article	nature reviews neuroscience	\N	\N	nature publishing group	11	5	6	2004	jun	2005-10-12 17:19:10	\N	dopamine, learning and motivation	the hypothesis that dopamine is important for reward has been proposed in a number of forms, each of which has been challenged. normally, rewarding stimuli such as food, water, lateral hypothalamic brain stimulation and several drugs of abuse become ineffective as rewards in animals given performance-sparing doses of dopamine antagonists. dopamine release in the nucleus accumbens has been linked to the efficacy of these unconditioned rewards, but dopamine release in a broader range of structures is implicated in the 'stamping-in' of memory that attaches motivational importance to otherwise neutral environmental stimuli.
723	349634	article	jama : the journal of the american medical association	\N	\N	\N	7	280	15	1998	oct	2005-10-13 01:42:41	department of clinical epidemiology and biostatistics, mcmaster university faculty of health sciences, hamilton, ontario, canada.	effects of computer-based clinical decision support systems on physician performance and patient outcomes: a systematic review.	{context}: many computer software developers and vendors claim that their systems can directly improve clinical decisions. as for other health care interventions, such claims should be based on careful trials that assess their effects on clinical performance and, preferably, patient outcomes. {objective}: to systematically review controlled clinical trials assessing the effects of computer-based clinical decision support systems ({cdsss}) on physician performance and patient outcomes. {data} {sources}: we updated earlier reviews covering 1974 to 1992 by searching the {medline}, {embase}, {inspec}, {scisearch}, and the cochrane library bibliographic databases from 1992 to march 1998. reference lists and conference proceedings were reviewed and evaluators of {cdsss} were contacted. {study} {selection}: studies were included if they involved the use of a {cdss} in a clinical setting by a health care practitioner and assessed the effects of the system prospectively with a concurrent control. {data} {extraction}: the validity of each relevant study (scored from 0-10) was evaluated in duplicate. data on setting, subjects, computer systems, and outcomes were abstracted and a power analysis was done on studies with negative findings. {data} {synthesis}: a total of 68 controlled trials met our criteria, 40 of which were published since 1992. quality scores ranged from 2 to 10, with more recent trials rating higher (mean, 7.7) than earlier studies (mean, 6.4) (p<.001). effects on physician performance were assessed in 65 studies and 43 found a benefit (66\\%). these included 9 of 15 studies on drug dosing systems, 1 of 5 studies on diagnostic aids, 14 of 19 preventive care systems, and 19 of 26 studies evaluating {cdsss} for other medical care. six of 14 studies assessing patient outcomes found a benefit. of the remaining 8 studies, only 3 had a power of greater than 80\\% to detect a clinically important effect. {conclusions}: published studies of {cdsss} are increasing rapidly, and their quality is improving. the {cdsss} can enhance clinical performance for drug dosing, preventive care, and other aspects of medical care, but not convincingly for diagnosis. the effects of {cdsss} on patient outcomes have been insufficiently studied.
724	349688	article	cognitive, affective, \\& behavioral neuroscience	\N	\N	\N	23	1	2	2001	\N	2005-10-13 05:44:09	\N	interactions between frontal cortex and basal ganglia in working memory: a computational model	the frontal cortex and the basal ganglia interact via a relatively well understood and elaborate system of interconnections. in the context of motor function, these interconnections can be understood as disinhibiting, or releasing the brakes, on frontal motor action plans: the basal ganglia detect appropriate contexts for performing motor actions and enable the frontal cortex to execute such actions at the appropriate time. we build on this idea in the domain of working memory through the use of computational neural network models of this circuit. in our model, the frontal cortex exhibits robust active maintenance, whereas the basal ganglia contribute a selective, dynamic gating function that enables frontal memory representations to be rapidly updated in a task-relevant manner. we apply the model to a novel version of the continuous performance task that requires subroutine-like selective working memory updating and compare and contrast our model with other existing models and theories of frontal-cortex basal-ganglia interactions.
725	349730	article	nature biotechnology	\N	\N	nature publishing group	4	22	10	2004	oct	2005-10-13 10:37:11	\N	ten thousand interactions for the molecular biologist	previous studies have suggested that nature is restricted to about 1,000 protein folds to perform a great diversity of functions. here, we use protein interaction data from different sources and three-dimensional structures to suggest that the total number of interaction types is also limited, and estimate that most interactions in nature will conform to one of about 10,000 types. we currently know fewer than 2,000, and at the present rate of structure determination, it will be more than 20 years before we know a full representative set.
726	349734	article	plos computational biology	\N	\N	public library of science	88	1	2	2005	jul	2005-10-13 10:43:11	\N	bioinformatics for {whole-genome} shotgun sequencing of microbial communities	the application of whole-genome shotgun sequencing to microbial communities represents a major development in metagenomics, the study of uncultured microbes via the tools of modern genomic analysis. in the past year, whole-genome shotgun sequencing projects of prokaryotic communities from an acid mine biofilm, the sargasso sea, minnesota farm soil, three deep-sea whale falls, and deep-sea sediments have been reported, adding to previously published work on viral communities from marine and fecal samples. the interpretation of this new kind of data poses a wide variety of exciting and difficult bioinformatics problems. the aim of this review is to introduce the bioinformatics community to this emerging field by surveying existing techniques and promising new approaches for several of the most interesting of these computational problems.
727	350137	article	acm computing surveys	\N	\N	\N	17	28	4	1996	\N	2005-10-14 03:01:08	\N	formal methods: state of the art and future directions	we survey recent progress in the development of mathematical techniques for specifying and verifying complex hardware and software systems. many of these techniques are capable of handling industrial-sized examples; in fact, in some cases these techniques are already being used on a regular basis in industry. success in formal specification can be attributed to notations that are accessible to system designers and to new methodologies for applying these notations effectively. success in...
728	350303	article	cell	\N	\N	\N	9	122	6	2005	sep	2005-10-14 05:42:52	whitehead institute for biomedical research, 9 cambridge center, cambridge, massachusetts 02142, usa.	core transcriptional regulatory circuitry in human embryonic stem cells.	the transcription factors {oct4}, {sox2}, and {nanog} have essential roles in early development and are required for the propagation of undifferentiated embryonic stem ({es}) cells in culture. to gain insights into transcriptional regulation of human {es} cells, we have identified {oct4}, {sox2}, and {nanog} target genes using genome-scale location analysis. we found, surprisingly, that {oct4}, {sox2}, and {nanog} co-occupy a substantial portion of their target genes. these target genes frequently encode transcription factors, many of which are developmentally important homeodomain proteins. our data also indicate that {oct4}, {sox2}, and {nanog} collaborate to form regulatory circuitry consisting of autoregulatory and feedforward loops. these results provide new insights into the transcriptional regulation of stem cells and reveal how {oct4}, {sox2}, and {nanog} contribute to pluripotency and self-renewal.
729	350522	inproceedings	\N	usenix summer	\N	\N	9	\N	\N	1990	\N	2005-10-14 10:22:18	\N	why aren't operating systems getting faster as fast as hardware?	this note evaluates several hardware platforms and operating systems using a set of benchmarks that test memory bandwidth and various operating system features such as kernel entry/exit and file systems. the overall conclusion is that operating system performance does not seem to be improving at the same rate as the base speed of the underlying hardware. copyright \\'{o} 1989 digital equipment corporation d i g i t a l western research laboratory 100 hamilton avenue palo alto, california 94301 {usa}...
730	352058	article	human brain mapping	\N	\N	\N	5	8	2-3	1999	\N	2005-10-16 16:14:37	nuclear magnetic resonance center, massachusetts general hospital, charlestown 02129, usa. dale@nmr.mgh.harvard.edu	optimal experimental design for event-related {fmri}.	an important challenge in the design and analysis of event-related or single-trial functional magnetic resonance imaging (fmri) experiments is to optimize statistical efficiency, i.e., the accuracy with which the event-related hemodynamic response to different stimuli can be estimated for a given amount of imaging time. several studies have suggested that using a fixed inter-stimulus-interval (isi) of at least 15 sec results in optimal statistical efficiency or power and that using shorter isis results in a severe loss of power. in contrast, recent studies have demonstrated the feasibility of using isis as short as 500 ms while still maintaining considerable efficiency or power. here, we attempt to resolve this apparent contradiction by a quantitative analysis of the relative efficiency afforded by different event-related experimental designs. this analysis shows that statistical efficiency falls off dramatically as the isi gets sufficiently short, if the isi is kept fixed for all trials. however, if the isi is properly jittered or randomized from trial to trial, the efficiency improves monotonically with decreasing mean isi. importantly, the efficiency afforded by such variable isi designs can be more than 10 times greater than that which can be achieved by fixed isi designs. these results further demonstrate the feasibility of using identical experimental designs with fmri and electro-/magnetoencephalography (eeg/meg) without sacrificing statistical power or efficiency of either technique, thereby facilitating comparison and integration across imaging modalities. hum. brain mapping 8:109-114, 1999. Â© 1999 wiley-liss, inc.
731	352260	inproceedings	\N	proceedings of ecml-98, 10th european conference on machine learning	\N	springer verlag, heidelberg, de	11	\N	1398	1998	\N	2005-10-16 21:35:16	chemnitz, de	naive (bayes) at forty: the independence assumption in information retrieval.	. the naive bayes classifier, currently experiencing a renaissance in machine learning, has long been a core technique in information retrieval. we review some of the variations of naive bayes models used for text retrieval and classification, focusing on the distributional assumptions made about word occurrences in documents. 1 introduction the naive bayes classifier, long a favorite punching bag of new classification techniques, has recently emerged as a focus of research itself in machine...
732	352535	book	\N	\N	\N	springer	\N	\N	\N	2005	jul	2005-10-17 04:24:29	\N	an invitation to {3-d} vision: from images to geometric models (interdisciplinary applied mathematics)	{this book gives senior undergraduate and beginning graduate students and researchers in computer vision, applied mathematics, computer graphics, and robotics a self-contained introduction to the geometry of 3d vision; that is the reconstruction of 3d models of objects from a collection of 2d images. following a brief introduction, part i provides background materials for the rest of the book. the two fundamental transformations, namely rigid body motion and perspective projection are introduced and image formation and feature extraction discussed. part ii covers the classic theory of two view geometry based on the so-called epipolar constraint. part iii shows that a more proper tool for studying the geometry of multiple views is the so- called rank considtion on the multiple view matrix. part iv develops practical reconstruction algorithms step by step as well as discusses possible extensions of the theory. exercises are provided at the end of each chapter. software for examples and algorithms are available on the author's website.}
733	352713	misc	ieee communications magazine	\N	\N	\N	12	\N	\N	2002	aug	2007-05-30 11:47:53	\N	a survey on sensor networks	recent advancement in wireless communica- tions and electronics has enabled the develop- ment of low-cost sensor networks. the sensor networks can be used for various application areas (e.g., health, military, home). for different application areas, there are different technical issues that researchers are currently resolving. the current state of the art of sensor networks is captured in this article, where solutions are discussed under their related protocol stack layer sections. this article ...
734	352715	article	proceedings of the national academy of sciences of the united states of america	\N	\N	\N	5	102	20	2005	may	2005-10-17 13:11:18	school of computer science, tel aviv university, tel aviv 69978, israel.	conservation and evolvability in regulatory networks: the evolution of ribosomal regulation in yeast.	transcriptional modules of coregulated genes play a key role in regulatory networks. comparative studies show that modules of coexpressed genes are conserved across taxa. however, little is known about the mechanisms underlying the evolution of module regulation. here, we explore the evolution of cis-regulatory programs associated with conserved modules by integrating expression profiles for two yeast species and sequence data for a total of 17 fungal genomes. we show that although the cis-elements accompanying certain conserved modules are strictly conserved, those of other conserved modules are remarkably diverged. in particular, we infer the evolutionary history of the regulatory program governing ribosomal modules. we show how a cis-element emerged concurrently in dozens of promoters of ribosomal protein genes, followed by the loss of a more ancient cis-element. we suggest that this formation of an intermediate redundant regulatory program allows conserved transcriptional modules to gradually switch from one regulatory mechanism to another while maintaining their functionality. our work provides a general framework for the study of the dynamics of promoter evolution at the level of transcriptional modules and may help in understanding the evolvability and increased redundancy of transcriptional regulation in higher organisms.
735	353833	book	\N	\N	\N	wiley	\N	\N	\N	2002	oct	2005-10-18 15:15:38	\N	statistical pattern recognition	statistical pattern recognition is a very active area of study and research, which has seen many advances in recent years. new and emerging applications - such as data mining, web searching, multimedia data retrieval, face recognition, and cursive handwriting recognition - require robust and efficient pattern recognition techniques. statistical decision making and estimation are regarded as fundamental to the study of pattern recognition. statistical pattern recognition, second edition has been fully updated with new methods, applications and references. it provides a comprehensive introduction to this vibrant area - with material drawn from engineering, statistics, computer science and the social sciences - and covers many application areas, such as database design, artificial neural networks, and decision support systems. * provides a self-contained introduction to statistical pattern recognition. * each technique described is illustrated by real examples. * covers bayesian methods, neural networks, support vector machines, and unsupervised classification. * each section concludes with a description of the applications that have been addressed and with further developments of the theory. * includes background material on dissimilarity, parameter estimation, data, linear algebra and probability. * features a variety of exercises, from 'open-book' questions to more lengthy projects. the book is aimed primarily at senior undergraduate and graduate students studying statistical pattern recognition, pattern processing, neural networks, and data mining, in both statistics and engineering departments. it is also an excellent source of reference for technical professionals working in advanced information development environments.
736	354229	article	journal of universal computer science	\N	\N	\N	\N	\N	7	-1	\N	2005-10-18 22:27:27	\N	total functional programming	the driving idea of functional programming is to make programming more closely related to mathematics. a program in a functional language such as haskell or miranda consists of equations which are both computation rules and a basis for simple algebraic reasoning about the functions and data structures they define. the existing model of functional programming, although elegant and powerful, is compromised to a greater extent than is commonly recognised by the presence of partial functions. we consider a simple discipline of total functional programming designed to exclude the possibility of non-termination. among other things this requires a type distinction between data, which is finite, and codata, which is potentially infinite.
737	354778	book	\N	\N	\N	prentice hall	\N	\N	\N	1992	\N	2005-10-19 11:28:45	englewood cliffs	applied multivariate statistical analysis	most of the observable phenomena in the empirical sciences are of a multivariate nature.in financial studies, assets in stock markets are observed simultaneously and their joint development is analyzed to better understand general tendencies and to track indices. in medicine recorded observations of subjects in different locations are the basis of reliable diagnoses and medication. in quantitative marketing consumer preferences are collected in order to construct models of consumer behavior. the underlying theoretical structure of these and many other quantitative studies of applied sciences is multivariate. focussing on applications this book presents the tools and concepts of multivariate data analysis in a way that is understandable for non-mathematicians and practitioners who face statistical data analysis.  in this second edition a wider scope of methods and applications of multivariate statistical analysis is introduced. all quantlets have been translated into the r and matlab language and are made available online.
738	354781	article	journal of the royal statistical soc., series b	\N	\N	\N	21	58	\N	1996	\N	2005-10-19 11:28:45	\N	discriminant analysis by gaussian mixtures	fisher-rao linear discriminant analysis (lda) is a valuable tool for multigroup classification. lda is equivalent to maximum likelihood classification assuming gaussian distributions for each class. in this paper, we fit gaussian mixtures to each class to facilitate effective classification in non-normal settings, especially when the classes are clustered. low dimensional views are an important by-product of lda--our new techniques inherit this feature. we can control the within-class spread of the subclass centres relative to the between-class spread. our technique for fitting these models permits a natural blend with nonparametric versions of lda.
739	354787	book	\N	\N	\N	addison-wesley	\N	\N	\N	1979	\N	2005-10-19 11:28:45	reading, ma.	introduction to automata theory, languages, and computation	this book is a rigorous exposition of formal languages and models of computation, with an introduction to computational complexity. the authors present the theory in a concise and straightforward manner, with an eye out for the practical applications. exercises at the end of each chapter, including some that have been solved, help readers confirm and enhance their understanding of the material. this book is appropriate for upper-level computer science undergraduates who are comfortable with mathematical arguments.
740	355384	article	\N	\N	\N	\N	\N	\N	\N	2005	oct	2005-10-19 19:43:49	\N	towards a theory of {scale-free} graphs: definition, properties, and implications (extended version)	although the â€œscale-freeâ€ literature is large and growing, it gives neither a precise definition of scale-free graphs nor rigorous proofs of many of their claimed properties. in fact, it is easily shown that the existing theory has many inherent contradictions and verifiably false claims. in this paper, we propose a new, mathematically precise, and structural definition of the extent to which a graph is scale-free, and prove a series of results that recover many of the claimed properties while suggesting the potential for a rich and interesting theory. with this definition, scale-free (or its opposite, scale-rich) is closely related to other structural graph properties such as various notions of self-similarity (or respectively, self-dissimilarity). scale-free graphs are also shown to be the likely outcome of random construction processes, consistent with the heuristic definitions implicit in existing random graph approaches. our approach clarifies much of the confusion surrounding the sensational qualitative claims in the scale-free literature, and offers rigorous and quantitative alternatives.
741	355644	article	reviews of modern physics	\N	\N	\N	57	51	3	1979	jul	2005-10-20 04:46:25	\N	the topological theory of defects in ordered media	aspects of the theory of homotopy groups are described in a mathematical style closer to that of condensed matter physics than that of topology. the aim is to make more readily accessible to physicists the recent applications of homotopy theory to the study of defects in ordered media. although many physical examples are woven into the development of the subject, the focus is on mathematical pedagogy rather than on a systematic review of applications.
742	356505	article	psychological review	\N	\N	american psychological association	\N	84	3	1977	may	2005-10-20 18:26:17	\N	telling more than we can know: verbal reports on mental processes	evidence is reviewed which suggests that there may be little or no direct introspective access to higher order cognitive processes. subjects are sometimes (a) unaware of the existence of a stimulus that importantly influenced a response, (b) unaware of the existence of the response, and (c) unaware that the stimulus has affected the response. it is proposed that when people attempt to report on their cognitive processes, that is, on the processes mediating the effects of a stimulus on a response, the do not do so on the basis of any true introspection. instead, their reports are based on a priori, implicit casual theories or judgments about the extent to which a particular stimulus is a plausible cause of a given response. this suggests that though people may not be able to observe directly their cognitive processes, the will sometimes be able to report accurately about them. accurate reports will occur when influential stimuli are salient and are plausible causes of the responses they produce, and will not occur the stimuli are not salient or are not plausible causes.
743	359106	incollection	\N	designing interaction: psychology at the human-computer interface	\N	cambridge university press	26	\N	\N	1991	\N	2005-10-21 09:42:15	\N	beyond the interface: encountering artifacts in use	in this paper, we provide a brief overview and critique of the descriptions and concepts that are currently used in the hci area coming primarily from the cognitive science tradition, as they seem to embed within them certain assumptions which are overly limiting. then we look at some recent arguments for re-organizing our conception of the field, or extending the field, coming primarily from within the field itself (as presently constituted). section 4 then presents a more elaborated "activity-theoretical" framework as one possible alternative, or perhaps complementary, framework that may give a richer depiction of the hci field. in section 5 we return to look more specifically at the different theoretical viewpoints, especially in regard to their re-framing of issues in the field, relating the different emphases to another field, software engineering. this section tries to summarize some of the main form that can serve as a basis for future discussion.
744	359369	inproceedings	\N	proceedings of the \\$3^{rd}\\$ conference on designing interactive systems	\N	acm	9	\N	\N	2000	\N	2005-10-21 09:42:17	\N	on the move with a magic thing: role playing in concept design of mobile services and devices.	designing concepts for new mobile services and devices, poses several challenges to the design. we consider user participation as a way to address part of the challenges. we show how our effort relates to current and past research. in particular, pd (participatory design) has inspired us in developing two participatory techniques. the two techniques are organized around situations either staged or real where users and designers can envision and enact future scenarios: a role-playing game with toys, and spes (situated and participative enactment of scenarios). they were developed in an industry-funded project that investigates services for the nomadic internet user of the future. we then discuss how the techniques help in facing the design challenges.
745	359525	inproceedings	\N	proceedings of dare 2000 on designing augmented reality environments	\N	acm press	9	\N	\N	2000	\N	2005-10-21 09:42:18	\N	{cybercode}: designing augmented reality environments with visual tags	the cybercode is a visual tagging system based on a 2d-barcode technology and provides several features not provided by other tagging systems. cybercode tags can be recognized by the low-cost cmos or ccd cameras found in more and more mobile devices, and it can also be used to determine the 3d position of the tagged object as well as its id number. this paper describes examples of augmented reality applications based on cybercode, and discusses some key characteristics of tagging technologies that must be taken into account when designing augmented reality environments.
746	361471	article	proteins	\N	\N	\N	\N	\N	\N	2005	sep	2005-10-22 08:58:50	center of excellence in bioinformatics, university at buffalo, 901 washington st., buffalo, ny 14203.	{tasser}: an automated method for the prediction of protein tertiary structures in {casp6}.	the recently developed {tasser} ({threading/assembly}/refinement) method is applied to predict the tertiary structures of all {casp6} targets. {tasser} is a hierarchical approach that consists of template identification by the threading program {prospector\\_3}, followed by tertiary structure assembly via rearranging continuous template fragments. assembly occurs using parallel hyperbolic monte carlo sampling under the guide of an optimized, reduced force field that includes knowledge-based statistical potentials and spatial restraints extracted from threading alignments. models are automatically selected from the monte carlo trajectories in the low temperature replicas using the clustering program {spicker}. for all 90 {casp} targets/domains, {prospector\\_3} generates initial alignments with an average root-mean-square deviation to native ({rmsd}) of 8.4 a with 79\\% coverage. after {tasser} reassembly, the average {rmsd} decreases to 5.4 a over the same aligned residues; the overall cumulative {tm}-score increases from 39.44 to 52.53. despite significant improvements over the {prospector\\_3} template alignment observed in all target categories, the overall quality of the final models is essentially dictated by the quality of threading templates: the average {tm}-score of {tasser} models in the three categories are respectively 0.79 ({cm}, 43 targets/domains), 0.47 ({fr}, 37 targets/domains), and 0.30 ({nf}, 10 targets/domains). this highlights the need to develop novel (or improved) approaches to identify very distant targets as well as better {nf} algorithms. proteins 2005. (c) 2005 {wiley-liss}, inc.
747	361809	article	open grid service infrastructure wg, global grid forum	\N	\N	\N	\N	\N	\N	2002	\N	2005-10-22 22:41:02	\N	the physiology of the grid: an open grid services architecture for distributed systems integration	in both e-business and e-science, we often need to integrate services across distributed, heterogeneous, dynamic &#034;virtual organizations&#034; formed from the disparate resources within a single enterprise and/or from external resource sharing and service provider relationships. this integration can be technically challenging because of the need to achieve various qualities of service when running on top of different native platforms. we present an open grid services architecture that addresses these challenges. building on concepts and technologies from the grid and web services communities, this architecture defines a uniform exposed service semantics (the grid service); defines standard mechanisms for creating, naming, and discovering transient grid service instances; provides location transparency and multiple protocol bindings for service instances; and supports integration with underlying native platform facilities. the open grid services architecture also defines, in terms of web services description language (wsdl) interfaces and associated conventions, mechanisms required for creating and composing sophisticated distributed systems, including lifetime management, change management, and notification. service bindings can support reliable invocation, authentication, authorization, and delegation, if required. our presentation complements an earlier foundational article, &#034;the anatomy of the grid,&#034; by describing how grid mechanisms can implement a service-oriented architecture, explaining how grid functionality can be incorporated into a web services framework, and illustrating how our architecture can be applied within commercial computing as a basis for distributed system integration--within and across organizational domains.
748	361881	inproceedings	\N	csmr	\N	\N	8	\N	\N	2001	\N	2005-10-23 03:06:37	\N	metrics based refactoring	refactoring is one key issue to increase internal software quality during the whole software lifecycle. since identifying structures where refactorings should be applied often is explained with subjective perceptions like &ldquo;bad taste&rdquo; or &ldquo;bad smell&rdquo;, an automatic refactoring location finder seems difficult. we show that a special kind of metrics can support these subjective perceptions and thus can be used as an effective and efficient way to get support for the decision of where to apply refactoring. due to the fact that the software developer is the last authority, we provide powerful and metrics based software visualisation to support the developers in judging their products. the authors demonstrate this approach for four typical refactorings and present both a tool supporting the identification and case studies of its application
749	363536	article	j mol biol	\N	\N	\N	-1070	307	\N	2001	\N	2005-10-24 10:52:49	\N	{evolution of function in protein superfamilies, from a structural perspective.}	the recent growth in protein databases has revealed the functional diversity of many protein superfamilies. we have assessed the functional variation of homologous enzyme superfamilies containing two or more enzymes, as defined by the {cath} protein structure classification, by way of the enzyme commission {(ec)} scheme. combining sequence and structure information to identify relatives, the majority of superfamilies display variation in enzyme function, with 25 % of superfamilies in the {pdb} having members of different enzyme types. we determined the extent of functional similarity at different levels of sequence identity for 486,000 homologous pairs (enzyme/enzyme and enzyme/non-enzyme), with structural and sequence relatives included. for single and multi-domain proteins, variation in {ec} number is rare above 40 % sequence identity, and above 30 %, the first three digits may be predicted with an accuracy of at least 90 %. for more distantly related proteins sharing less than 30 % sequence identity, functional variation is significant, and below this threshold, structural data are essential for understanding the molecular basis of observed functional differences. to explore the mechanisms for generating functional diversity during evolution, we have studied in detail 31 diverse structural enzyme superfamilies for which structural data are available. a large number of variations and peculiarities are observed, at the atomic level through to gross structural rearrangements. almost all superfamilies exhibit functional diversity generated by local sequence variation and domain shuffling. commonly, substrate specificity is diverse across a superfamily, whilst the reaction chemistry is maintained. in many superfamilies, the position of catalytic residues may vary despite playing equivalent functional roles in related proteins. the implications of functional diversity within supefamilies for the structural genomics projects are discussed. more detailed information on these superfamilies is available at {http://www.biochem.ucl.ac.uk/bsm/fam-ec/.}
750	364210	inproceedings	\N	usenix workshop on electronic commerce	\N	\N	\N	\N	\N	1995	\N	2005-10-25 08:53:01	\N	economic mechanism design for computerized agents	the field of economic mechanism design has been an active area of research in economics for at least 20 years. this field uses the tools of economics and game theory to design â€˜â€˜rules of interactionâ€™ â€™ for economic transactions that will, in principle, yield some desired outcome. in this paper i provide an overview of this subject for an audience interested in applications to electronic commerce and discuss some special problems that arise in this context. 1 mechanism design as an example of mechanism design in action, let us consider the case of designing an auction to award an item to one of n individuals. each individual i has a â€˜â€˜maximum willingness to payâ€™ â€™ or â€˜â€˜valueâ€™ â€™ for the item that we denote by vi. we assume that this value is private information known only by person i. our goal is to design an auction that will award the item to the person with the highest value.
751	366148	article	the journal of neuroscience	\N	\N	society for neuroscience	20	12	12	1992	dec	2005-10-27 00:17:13	department of neurobiology, stanford university school of medicine, california 94305.	the analysis of visual motion: a comparison of neuronal and psychophysical performance	we compared the ability of psychophysical observers and single cortical neurons to discriminate weak motion signals in a stochastic visual display. all data were obtained from rhesus monkeys trained to perform a direction discrimination task near psychophysical threshold. the conditions for such a comparison were ideal in that both psychophysical and physiological data were obtained in the same animals, on the same sets of trials, and using the same visual display. in addition, the psychophysical task was tailored in each experiment to the physiological properties of the neuron under study; the visual display was matched to each neuron's preference for size, speed, and direction of motion. under these conditions, the sensitivity of most {mt} neurons was very similar to the psychophysical sensitivity of the animal observers. in fact, the responses of single neurons typically provided a satisfactory account of both absolute psychophysical threshold and the shape of the psychometric function relating performance to the strength of the motion signal. thus, psychophysical decisions in our task are likely to be based upon a relatively small number of neural signals. these signals could be carried by a small number of neurons if the responses of the pooled neurons are statistically independent. alternatively, the signals may be carried by a much larger pool of neurons if their responses are partially intercorrelated.
752	366475	article	nature	\N	\N	nature publishing group	4	\N	7063	-1	\N	2006-05-18 08:27:39	\N	mapping determinants of human gene expression by regional and genome-wide association	to study the genetic basis of natural variation in gene expression, we previously carried out genome-wide linkage analysis and mapped the determinants of similar to 1,000 expression phenotypes(1). in the present study, we carried out association analysis with dense sets of single-nucleotide polymorphism ( snp) markers from the international hapmap project(2). for 374 phenotypes, the association study was performed with markers only from regions with strong linkage evidence; these regions all mapped close to the expressed gene. for a subset of 27 phenotypes, analysis of genome-wide association was performed with > 770,000 markers. the association analysis with markers under the linkage peaks confirmed the linkage results and narrowed the candidate regulatory regions for many phenotypes with strong linkage evidence. the genome-wide association analysis yielded highly significant results that point to the same locations as the genome scans for about 50% of the phenotypes. for one candidate determinant, we carried out functional analyses and confirmed the variation in cis-acting regulatory activity. our findings suggest that association studies with dense snp maps will identify susceptibility loci or other determinants for some complex traits or diseases.
753	366673	inproceedings	\N	sosp	\N	acm press	14	39	5	2005	dec	2005-10-27 12:40:05	new york, ny, usa	scalability, fidelity, and containment in the potemkin virtual honeyfarm	the rapid evolution of large-scale worms, viruses and bot-nets have made internet malware a pressing concern. such infections are at the root of modern scourges including ddos extortion, on-line identity theft, spam, phishing, and piracy. however, the most widely used tools for gathering intelligence on new malware -- network honeypots -- have forced investigators to choose between monitoring activity at a large scale or capturing behavior with high fidelity. in this paper, we describe an approach to minimize this tension and improve honeypot scalability by up to six orders of magnitude while still closely emulating the execution behavior of individual internet hosts. we have built a prototype honeyfarm system, called  potemkin , that exploits virtual machines, aggressive memory sharing, and late binding of resources to achieve this goal. while still an immature implementation, potemkin has emulated over 64,000 internet honeypots in live test runs, using only a handful of physical servers.
754	366727	proceedings	security and privacy, 2002. proceedings. 2002 ieee symposium on	proceedings of ieee symposium on security and privacy	\N	\N	8	\N	\N	2002	\N	2005-10-27 14:06:53	\N	stateful intrusion detection for high-speed networks	as networks become faster there is an emerging need for security, analysis techniques that can keep up with the increased network throughput. existing network-based intrusion detection sensors can barely, keep up with bandwidths of a few hundred mbps. analysis tools that can deal with higher throughput are unable to maintain state between different steps of an attack or they are limited to the analysis of packet headers. we propose a partitioning approach to network security, analysis that supports in-depth, stateful intrusion detection on high-speed links. the approach is centered around a slicing mechanism that divides the overall network traffic into subsets of manageable size. the traffic partitioning is done so that a single slice contains all the evidence necessary to detect a specific attack, making sensor-to-sensor interactions unnecessary. this paper describes the approach and presents a first experimental evaluation of its effectiveness.
755	368224	inbook	\N	designing virtual communities in the service of learning	\N	cambridge university press	\N	\N	\N	2004	\N	2005-10-28 01:11:01	\N	teacher professional development, technology, and communities of practice: are we putting the cart before the horse?	over the past decade, education reform and teacher training projects have spent a great deal of effort to create and support sustainable, scalable online communities of education professionals. for the most part, those communities have been created in isolation from the existing local professional communities within which the teachers practice. we argue that focusing on online technology solely as a mechanism to deliver training and/or create online networks places the cart before the horse by ignoring the internet's even greater potential to help support and strengthen local communities of practice within which teachers work. in this article we seek guideposts to help education technologists understand the nature of local k-12 education communities of practice--specifically their reciprocal relationship with teacher professional development and instructional improvement interventions--as a prerequisite to designing online sociotechnical infrastructure that supports the professional growth of education professionals.
756	368247	article	trends in cognitive sciences	\N	\N	\N	6	4	11	2000	nov	2005-10-28 02:12:21	\N	the episodic buffer: a new component of working memory?	in 1974, baddeley and hitch proposed a three-component model of working memory. over the years, this has been successful in giving an integrated account not only of data from normal adults, but also neuropsychological, developmental and neuroimaging data. there are, however, a number of phenomena that are not readily captured by the original model. these are outlined here and a fourth component to the model, the episodic buffer, is proposed. it comprises a limited capacity system that provides temporary storage of information held in a multimodal code, which is capable of binding information from the subsidiary systems, and from long-term memory, into a unitary episodic representation. conscious awareness is assumed to be the principal mode of retrieval from the buffer. the revised model differs from the old principally in focussing attention on the processes of integrating information, rather than on the isolation of the subsystems. in doing so, it provides a better basis for tackling the more complex aspects of executive control in working memory.
757	369239	article	new left review	\N	\N	\N	\N	26	\N	2004	mar	2005-10-28 17:02:33	\N	planet of slums	future history of the third world'spost-industrial megacities. a billion-strong global proletariat ejected from the formal economy, with islam and pentecostalism as songs of the dispossessed.
758	373277	article	neuroimage	\N	\N	\N	12	26	3	2005	jul	2005-10-31 13:06:59	\N	unified segmentation	a probabilistic framework is presented that enables image registration, tissue classification, and bias correction to be combined within the same generative model. a derivation of a log-likelihood objective function for the unified model is provided. the model is based on a mixture of gaussians and is extended to incorporate a smooth intensity variation and nonlinear registration with tissue probability maps. a strategy for optimising the model parameters is described, along with the requisite partial derivatives of the objective function.
759	373611	article	molecular biology and evolution	\N	\N	oxford university press	10	23	2	2006	feb	2005-10-31 13:38:11	program in computation and neural systems, california institute of technology, pasadena, ca 91125-4100, usa.	a single determinant dominates the rate of yeast protein evolution	a gene's rate of sequence evolution is among the most fundamental evolutionary quantities in common use, but what determines evolutionary rates has remained unclear. here, we carry out the first combined analysis of seven predictors (gene expression level, dispensability, protein abundance, codon adaptation index, gene length, number of protein-protein interactions, and the gene's centrality in the interaction network) previously reported to have independent influences on protein evolutionary rates. strikingly, our analysis reveals a single dominant variable linked to the number of translation events which explains 40-fold more variation in evolutionary rate than any other, suggesting that protein evolutionary rate has a single major determinant among the seven predictors. the dominant variable explains nearly half the variation in the rate of synonymous and protein evolution. we show that the two most commonly used methods to disentangle the determinants of evolutionary rate, partial correlation analysis and ordinary multivariate regression, produce misleading or spurious results when applied to noisy biological data. we overcome these difficulties by employing principal component regression, a multivariate regression of evolutionary rate against the principal components of the predictor variables. our results support the hypothesis that translational selection governs the rate of synonymous and protein sequence evolution in yeast.
760	373647	inproceedings	\N	\N	\N	\N	7	\N	\N	-1	\N	2005-10-31 15:29:29	\N	estimating continuous distributions in bayesian classifiers	when modeling a probability distribution with a bayesian network, we are faced with the problem of how to handle continuous variables. most previous work has either solved the problem by discretizing, or assumed that the data are generated by a single gaussian. in this paper we abandon the normality assumption and instead use statistical methods for nonparametric density estimation. for a naive bayesian classifier, we present experimental results on a variety of natural and artificial domains,...
761	373702	article	jcdl '02	\N	\N	\N	8	\N	\N	2002	\N	2005-10-31 19:14:11	\N	a graph-based recommender system for digital library	research shows that recommendations comprise a valuable service for users of a digital library [11]. while most existing recommender systems rely either on a content-based approach or a collaborative approach to make recommendations, there is potential to improve recommendation quality by using a combination of both approaches (a hybrid approach). in this paper, we report how we tested the idea of using a graph-based recommender system that naturally combines the content-based and collaborative approaches. due to the similarity between our problem and a concept retrieval task, a hopfield net algorithm was used to exploit high-degree book-book, useruser and book-user associations. sample hold-out testing and preliminary subject testing were conducted to evaluate the system, by which it was found that the system gained improvement with respect to both precision and recall by combining content-based and collaborative approaches. however, no significant improvement was observed by exploiting high-degree associations.
762	373720	misc	\N	\N	\N	\N	\N	\N	\N	1994	\N	2005-10-31 19:46:51	\N	formalising trust as a computational concept	trust is a judgement of unquestionable utility â€” as humans we use it every day of our lives. however, trust has suffered from an imperfect understanding, a plethora of definitions, and informal use in the literature and in everyday life. it is common to say â€œi trust you, â€ but what does that mean? this thesis provides a clarification of trust. we present a formalism for trust which provides us with a tool for precise discussion. the formalism is implementable: it can be embedded in an artificial agent, enabling the agent to make trust-based decisions. its applicability in the domain of distributed artificial intelligence (dai) is raised. the thesis presents a testbed populated by simple trusting agents which substantiates the utility of the formalism. the formalism provides a step in the direction of a proper understanding and definition of human trust. a contribution of the thesis is its detailed exploration of the possibilities of future work in the area. summary 1. overview this thesis presents an overview of trust as a social phenomenon and discusses it formally. it argues that trust is: â€¢ a means for understanding and adapting to the complexity of the environment. â€¢ a means of providing added robustness to independent agents. â€¢ a useful judgement in the light of experience of the behaviour of others. â€¢ applicable to inanimate others. the thesis argues these points from the point of view of artificial agents. trust in an artificial agent is a means of providing an additional tool for the consideration of other agents and the environment in which it exists. moreover, a formalisation of trust enables the embedding of the concept into an artificial agent. this has been done, and is documented in the thesis. 2. exposition there are places in the thesis where it is necessary to give a broad outline before going deeper. in consequence it may seem that the subject is not receiving a thorough treatment, or that too much is being discussed at one time! (this is particularly apparent in the first and second chapters.) to present a thorough understanding of trust, we have proceeded breadth first in the introductory chapters. chapter 3 expands, depth first, presenting critical views of established researchers.
763	376684	article	acm transactions on computer systems	\N	\N	\N	26	18	2	2000	\N	2005-11-02 03:20:28	\N	soft updates: a solution to the metadata update problem in file systems	metadata updates, such as file creation and block allocation, have consistently been identified as a source of performance, integrity, security, and availability problems for file systems.  soft updates  is an implementation technique for low-cost sequencing of fine-grained updates to write-back cache blocks. using soft updates to track and enforce metadata update dependencies, a file system can safely use delayed writes for almost all file operations. this article describes soft updates, their incorporation into the 4.4bsd fast file system, and the resulting effects on the sytem. we show that a disk-based file system using soft updates achieves memory-based file system performance while providing stronger integrity and security guarantees than most disk-based file  systems. for workloads that frequently perform updates on metadata (such as creating and deleting files), this improves performance by more than a factor of two and up to a factor of 20 when compared to the conventional synchronous write approach and by 4-19% when compared to an aggressive write-ahead logging approach. in addition, soft updates can improve file system availablity by relegating crash-recovery assistance (e.g., the  fsck  utility) to an optional and background role, reducing file system recovery time to less than one second.
764	377043	article	nature	\N	\N	\N	3	430	\N	2004	\N	2005-11-02 10:11:17	\N	fine-scale phylogenetic architecture of a complex bacterial community	although molecular data have revealed the vast scope of microbial diversity, two fundamental questions remain unanswered even for well-defined natural microbial communities: how many bacterial types co-exist, and are such types naturally organized into phylogenetically discrete units of potential ecological significance? it has been argued that without such information, the environmental function, population biology and biogeography of microorganisms cannot be rigorously explored. here we address these questions by comprehensive sampling of two large 16s ribosomal rna clone libraries from a coastal bacterioplankton community. we show that compensation for artefacts generated by common library construction techniques reveals fine-scale patterns of community composition. at least 516 ribotypes (unique rrna sequences) were detected in the sample and, by statistical extrapolation, at least 1,633 co-existing ribotypes in the sampled population. more than 50% of the ribotypes fall into discrete clusters containing less than 1% sequence divergence. this pattern cannot be accounted for by interoperon variation, indicating a large predominance of closely related taxa in this community. we propose that such microdiverse clusters arise by selective sweeps and persist because competitive mechanisms are too weak to purge diversity from within them.
765	377816	article	journal for research in mathematics education	\N	\N	\N	31	23	1	1992	\N	2005-11-02 12:35:07	\N	a constructivist alternative to the representational view of mind in mathematics education	the representational view of mind in mathematics education is evidenced by theories that characterize learning as a process in which students modify their internal mental representations to construct mathematical relationships or structures that mirror those embodied in external instructional representations. it is argued that, psychologically, this view falls prey to the learning paradox, that, anthropologically, it fails to consider the social and cultural nature of mathematical activity and that, pedagogically, it leads to recommendations that are at odds with the espoused goal of encouraging learning with understanding. these difficulties are seen to arise from the dualism created between mathematics in students' heads and mathematics in their environment. an alternative view is then outlined and illustrated that attempts to transcend this dualism by treating mathematics as both an individual, constructive activity and as a communal, social practice. it is suggested that such an approach might make it possible to explain how students construct mathematical meanings and practices that, historically, took several thousand years to evolve without attributing to students the ability to peek around their internal representations and glimpse a mathematically prestructured environment. in addition, it is argued that this approach might offer a way to go beyond the traditional tripartite scheme of the teacher, the student, and mathematics that has traditionally guided reform efforts in mathematics education.
766	378422	article	journal of eukaryotic microbiology	\N	\N	blackwell publishing inc	52	52	5	2005	oct	2005-11-02 21:45:03	department of biology, dalhousie university, halifax, ns b3h 4j1, canada.	the new higher level classification of eukaryotes with emphasis on the taxonomy of protists	abstract.  this revision of the classification of unicellular eukaryotes updates that of levine et al. (1980) for the protozoa and expands it to include other protists. whereas the previous revision was primarily to incorporate the results of ultrastructural studies, this revision incorporates results from both ultrastructural research since 1980 and molecular phylogenetic studies. we propose a scheme that is based on nameless ranked systematics. the vocabulary of the taxonomy is updated, particularly to clarify the naming of groups that have been repositioned. we recognize six clusters of eukaryotes that may represent the basic groupings similar to traditional  ” kingdoms.” the multicellular lineages emerged from within monophyletic protist lineages: animals and fungi from opisthokonta, plants from archaeplastida, and brown algae from stramenopiles.
767	381128	article	nature biotechnology	\N	\N	nature publishing group	9	23	11	2005	oct	2005-11-10 21:19:48	\N	computational design and experimental validation of oligonucleotide-sensing allosteric ribozymes	allosteric rnas operate as molecular switches that alter folding and function in response to ligand binding. a common type of natural allosteric rnas is the riboswitch; designer rnas with similar properties can be created by rna engineering. we describe a computational approach for designing allosteric ribozymes triggered by binding oligonucleotides. four universal types of rna switches possessing and, or, yes and not boolean logic functions were created in modular form, which allows ligand specificity to be changed without altering the catalytic core of the ribozyme. all computationally designed allosteric ribozymes were synthesized and experimentally tested in vitro. engineered ribozymes exhibit >1,000-fold activation, demonstrate precise ligand specificity and function in molecular circuits in which the self-cleavage product of one rna triggers the action of a second. this engineering approach provides a rapid and inexpensive way to create allosteric rnas for constructing complex molecular circuits, nucleic acid detection systems and gene control elements.
768	381880	inproceedings	\N	cscw	\N	acm	9	\N	\N	2004	\N	2005-11-05 17:31:00	new york, ny, usa	collaborating around collections: informing the continued development of photoware	this paper explores the embodied interactional ways in which people naturally collaborate around and share collections of photographs. we employ ethnographic studies of paper-based photograph use to consider requirements for distributed collaboration around digital photographs. distributed sharing is currently limited to the 'passing on' of photographs to others, by email, webpages, or mobile phones. to move beyond this, a fundamental challenge for photoware consists of developing support for the practical achievement of sharing 'at a distance'. specifically, this entails augmenting the natural production of accounts or 'photo-talk' to support the distributed achievement of sharing.
769	381881	book	\N	\N	\N	{the mit press}	\N	\N	\N	2004	sep	2005-11-05 17:34:17	\N	technology as experience	{in <i>technology as experience</i>, john mccarthy and peter wright argue that any account of what is often called the user experience must take into consideration the emotional, intellectual, and sensual aspects of our interactions with technology. we don't just use technology, they point out; we live with it. they offer a new approach to understanding human-computer interaction through examining the felt experience of technology. drawing on the pragmatism of such philosophers as john dewey and mikhail bakhtin, they provide a framework for a clearer analysis of technology as experience.<br /> <br /> just as dewey, in <i>art as experience</i>, argued that art is part of everyday lived experience and not isolated in a museum, mccarthy and wright show how technology is deeply embedded in everyday life. the "zestful integration" or transcendent nature of the aesthetic experience, they say, is a model of what human experience with technology might become.<br /> <br /> mccarthy and wright illustrate their theoretical framework with real-world examples that range from online shopping to ambulance dispatch. their approach to understanding human computer interaction -- seeing it as creative, open, and relational, part of felt experience -- is a measure of the fullness of technology's potential to be more than merely functional.}
770	381884	inproceedings	\N	isese	\N	ieee computer society	9	\N	\N	2004	\N	2005-11-05 17:43:42	washington, dc, usa	an ethnographic study of copy and paste programming practices in {oopl}	although programmers frequently copy and paste code when they develop software, implications of common copy and paste ({c\\&p};) usage patterns have not been studied previously. we have conducted an ethnographic study in order to understand programmers' {c\\&p}; programming practices and discover opportunities to assist common {c\\&p}; usage patterns. we observed programmers using an instrumented eclipse {ide} and then analyzed why and how they use {c\\&p}; operations. based on our analysis, we constructed a taxonomy of {c\\&p}; usage patterns. this paper presents our taxonomy of {c\\&p}; usage patterns and discusses our insights with examples drawn from our observations. from our insights, we propose a set of tools that both can reduce software maintenance problems incurred by {c\\&p}; and can better support the intents of commonly used {c\\&p}; scenarios.
771	382687	inproceedings	\N	oopsla	\N	acm press	8	33	10	1998	oct	2005-11-07 11:46:11	new york, ny, usa	dynamic class loading in the java virtual machine	class loaders are a powerful mechanism for dynamically loading software components on the java platform. they are unusual in supporting all of the following features: laziness, type-safe linkage, user-defined extensibility, and multiple communicating namespaces.  we present the notion of class loaders and demonstrate some of their interesting uses. in addition, we discuss how to maintain type safety in the presence of user-defined dynamic class loading.  1 introduction  in this paper, we...
772	382953	electronic	\N	\N	\N	\N	\N	\N	\N	2005	jul	2005-11-07 18:26:21	\N	self-similar scale-free networks and disassortativity	self-similar networks with scale-free degree distribution have recently attracted much attention, since these apparently incompatible properties were reconciled in a paper by song et al. by an appropriate box-counting method that enters the measurement of the fractal dimension. we study two genetic regulatory networks ({\\it saccharomyces cerevisiae} and {\\it escherichai coli} and show their self-similar and scale-free features, in extension to the datasets studied by song et al. moreover, by a number of numerical results we support the conjecture that self-similar scale-free networks are not assortative. from our simulations so far these networks seem to be disassortative instead. we also find that the qualitative feature of disassortativity is scale-invariant under renormalization, but it appears as an intrinsic feature of the renormalization prescription, as even assortative networks become disassortative after a sufficient number of renormalization steps.
773	384511	inproceedings	\N	proceedings of icml-99, 16th international conference on machine learning	\N	morgan kaufmann publishers, san francisco, us	9	\N	\N	1999	\N	2005-11-09 11:12:25	bled, sl	transductive inference for text classification using support vector machines	this paper introduces transductive support vector machines ({tsvms}) for text classification. while regular support vector machines ({svms}) try to induce a general decision function for a learning task, transductive support vector machines take into account a particular test set and try to minimize misclassifications of just those particular examples. the paper presents an analysis of why {tsvms} are well suited for text classification. these theoretical findings are supported by experiments on...
774	386078	article	social studies of science	\N	\N	\N	42	14	3	1984	\N	2005-11-09 21:22:26	\N	the social construction of facts and artefacts: or how the sociology of science and the sociology of technology might benefit each other	the need for an integrated social constructivist approach towards the study of science and technology is outlined. within such a programme both scientific facts and technological artefacts are to be understood as social constructs. literature on the sociology of science, the science-technology relationship, and technology studies is reviewed. the empirical programme of relativism within the sociology of scientific knowledge and a recent study of the social construction of technological artefacts are combined to produce the new approach. the concepts of 'interpretative flexibility' and 'closure mechanism', and the notion of 'social group' are developed and illustrated by reference to a study of solar physics and a study of the development of the bicycle. the paper concludes by setting out some of the terrain to be explored in future studies.
775	386121	inproceedings	\N	advanced functional programming	\N	\N	39	\N	\N	1995	\N	2005-11-10 00:31:18	\N	functional programming with overloading and {higher-order} polymorphism	. the hindley/milner type system has been widely adopted as  a basis for statically typed functional languages. one of the main reasons  for this is that it provides an elegant compromise between flexibility, allowing  a single value to be used in different ways, and practicality, freeing  the programmer from the need to supply explicit type information.  focusing on practical applications rather than implementation or theoretical  details, these notes examine a range of extensions that ...
776	386174	article	physical biology	\N	\N	institute of physics publishing	\N	2	4	2005	nov	2005-12-06 19:13:16	\N	constrained geometric simulation of diffusive motion in proteins	we describe a new computational method,  {froda}  (framework rigidity optimized dynamic algorithm), for exploring the internal mobility of proteins. the rigid regions in the protein are first determined, and then replaced by ghost templates which are used to guide the movements of the atoms in the protein. using random moves, the available conformational phase space of a 100 residue protein can be well explored in approximately 10–100 min of computer time using a single processor. all of the covalent, hydrophobic and hydrogen bond constraints are maintained, and van der waals overlaps are avoided, throughout the simulation. we illustrate the results of a  {froda}  simulation on barnase, and show that good agreement is obtained with nuclear magnetic resonance experiments. we additionally show how  {froda}  can be used to find a pathway from one conformation to another. this directed dynamics is illustrated with the protein dihydrofolate reductase.
777	388727	article	bioinformatics	\N	\N	\N	\N	17 Suppl 1	\N	2001	\N	2005-11-11 16:23:14	mrc laboratory of molecular biology, hills road, cambridge, cb2 2qh, uk. apic@mrc-lmb.cam.ac.uk	an insight into domain combinations.	domains are the building blocks of all globular proteins, and are units of compact three-dimensional structure as well as evolutionary units. there is a limited repertoire of domain families, so that these domain families are duplicated and combined in different ways to form the set of proteins in a genome. proteins are gene products. the processes that produce new genes are duplication and recombination as well as gene fusion and fission. we attempt to gain an overview of these processes by studying the structural domains in the proteins of seven genomes from the three kingdoms of life: eubacteria, archaea and eukaryota. we use here the domain and superfamily definitions in structural classification of proteins database ({scop}) in order to map pairs of adjacent domains in genome sequences in terms of their superfamily combinations. we find 624 out of the 764 superfamilies in {scop} in these genomes, and the 624 families occur in 585 pa...
778	391336	inproceedings	tools with artificial intelligence, 2000. ictai 2000. proceedings. 12th ieee international conference on	tools with artificial intelligence, 2000. ictai 2000. proceedings. 12th ieee international conference on	\N	ieee	7	\N	\N	2000	\N	2005-11-13 00:04:23	\N	using latent semantic analysis to identify similarities in source code to support program understanding	the paper describes the results of applying latent semantic analysis (lsa), an advanced information retrieval method, to program source code and associated documentation. latent semantic analysis is a corpus based statistical method for inducing and representing aspects of the meanings of words and passages (of natural language) reflective in their usage. this methodology is assessed for application to the domain of software components (i.e., source code and its accompanying documentation). here lsa is used as the basis to cluster software components. this clustering is used to assist in the understanding of a nontrivial software system, namely a version of mosaic. applying latent semantic analysis to the domain of source code and internal documentation for the support of program understanding is a new application of this method and a departure from the normal application domain of natural language
779	392247	article	science	\N	\N	\N	-796	268	5212	1995	may	2005-11-14 09:41:09	\N	borders of multiple visual areas in humans revealed by functional magnetic resonance imaging.	the borders of human visual areas v1, v2, {vp}, v3, and v4 were precisely and noninvasively determined. functional magnetic resonance images were recorded during phase-encoded retinal stimulation. this volume data set was then sampled with a cortical surface reconstruction, making it possible to calculate the local visual field sign (mirror image versus non-mirror image representation). this method automatically and objectively outlines area borders because adjacent areas often have the opposite field sign. cortical magnification factor curves for striate and extrastriate cortical areas were determined, which showed that human visual areas have a greater emphasis on the center-of-gaze than their counterparts in monkeys. retinotopically organized visual areas in humans extend anteriorly to overlap several areas previously shown to be activated by written words.
780	392495	article	plos biol	\N	\N	public library of science	\N	3	12	2005	nov	2005-11-14 19:51:17	department of biology, duke university, durham, north carolina, united states of america.	ancient and recent positive selection transformed opioid {cis-regulation} in humans	strong positive selection has resulted in changes to the regulation of the human prodynorphin gene, with evidence for increased expression as compared with chimp. additionally, recent selection has led to different alleles in different parts of the world.
781	392815	book	\N	\N	\N	\N	\N	\N	\N	-1	\N	2005-11-14 23:53:41	\N	educating the net generation	the net generation has grown up with information technology. the aptitudes, attitudes, expectations, and learning styles of net gen students reflect the environment in which they were raised -- one that is decidedly different from that which existed when faculty and administrators were growing up.  this collection explores the net gen and the implications for institutions in areas such as teaching, service, learning space design, faculty development, and curriculum. contributions by educators and students are included.
782	393074	article	proc natl acad sci u s a	\N	\N	\N	5	101	52	2004	dec	2005-11-15 05:41:31	department of physics and center for complex network research, university of notre dame, notre dame, in 46556, usa.	the topological relationship between the large-scale attributes and local interaction patterns of complex networks.	recent evidence indicates that the abundance of recurring elementary interaction patterns in complex networks, often called subgraphs or motifs, carry significant information about their function and overall organization. yet, the underlying reasons for the variable quantity of different subgraph types, their propensity to form clusters, and their relationship with the networks' global organization remain poorly understood. here we show that a network's large-scale topological organization and its local subgraph structure mutually define and predict each other, as confirmed by direct measurements in five well studied cellular networks. we also demonstrate the inherent existence of two distinct classes of subgraphs, and show that, in contrast to the low-density type {ii} subgraphs, the highly abundant type i subgraphs cannot exist in isolation but must naturally aggregate into subgraph clusters. the identified topological framework may have important implications for our understanding of the origin and function of subgraphs in all complex networks.
783	393136	article	science	\N	\N	\N	3	298	\N	2002	nov	2005-11-15 06:57:43	\N	colloidosomes: selectively permeable capsules composed of colloidal particles	we present an approach to fabricate solid capsules with precise control of size, permeability, mechanical strength, and compatibility. the capsules are fabricated by the self-assembly of colloidal particles onto the interface of emulsion droplets. after the particles are locked together to form elastic shells, the emulsion droplets are transferred to a fresh continuous-phase fluid that is the same as that inside the droplets. the resultant structures, which we call "colloidosomes," are hollow, elastic shells whose permeability and elasticity can be precisely controlled. the generality and robustness of these structures and their potential for cellular immunoisolation are demonstrated by the use of a variety of solvents, particles, and contents.
784	393513	inproceedings	\N	kdd workshop on link discovery: issues, approaches and applications (linkkdd)	\N	\N	\N	\N	\N	2005	\N	2005-11-15 13:37:34	\N	group and topic discovery from relations and text	we present a probabilistic generative model of entity relationships and textual attributes that simultaneously discover groups among the entities and topics among the corresponding text. block-models of relationship data have been studied in social network analysis for some time. here we simultaneously cluster in several modalities at once, incorporating the words associated with certain relationships. significantly, joint inference allows the discovery of groups to be guided by the emerging topics, and vice-versa. we present experimental results on two large data sets: sixteen years of bills put before the u.s. senate, comprising their corresponding text and voting records, and 43 years of similar data from the united nations. we show that in comparison with traditional, separate latent-variable models for words or blockstructures for votes, the group-topic model's joint inference improves both the groups and the topics discovered.
785	398140	article	free radic biol med	\N	\N	\N	18	38	3	2005	feb	2005-11-17 06:57:43	department of environmental health sciences, johns hopkins university bloomberg school of public health, baltimore, md 21205, usa. cho2@niehs.nih.gov	gene expression profiling of {nrf2}-mediated protection against oxidative injury.	nuclear factor e2 p45-related factor 2 ({nrf2}) contributes to cellular protection against oxidative insults and chemical carcinogens via transcriptional activation of antioxidant/detoxifying enzymes. to understand the molecular basis of {nrf2}-mediated protection against oxidative lung injury, pulmonary gene expression profiles were characterized in nrf2-disrupted (nrf2(-/-)) and wild-type (nrf2(+/+)) mice exposed to hyperoxia or air. genes expressed constitutively higher in nrf2(+/+) mice than in nrf2(-/-) mice included antioxidant defense enzyme and immune cell receptor genes. higher basal expression of heat shock protein and structural genes was detected in nrf2(-/-) mice relative to nrf2(+/+) mice. hyperoxia enhanced expression of 175 genes (> or = twofold) and decreased expression of 100 genes (> or =50\\%) in wild-type mice. hyperoxia-induced upregulation of many well-known/new antioxidant/defense genes (e.g., txnrd1, ex, cp-2) and other novel genes (e.g., pkc-alpha, tcf-3, ppar-gamma) was markedly greater in nrf2(+/+) mice than in nrf2(-/-) mice. in contrast, induced expression of genes encoding extracellular matrix and cytoskeletal proteins was higher in nrf2(-/-) mice than in nrf2(+/+) mice. these {nrf2}-dependent gene products might have key roles in protection against hyperoxic lung injury. results from our global gene expression profiles provide putative downstream molecular mechanisms of oxygen tissue toxicity.
786	398285	inproceedings	\N	www	\N	acm	11	\N	\N	2005	\N	2005-11-17 10:20:38	new york, ny, usa	sampling search-engine results	we consider the problem of efficiently sampling web search engine query results. in turn, using a small random sample instead of the full set of results leads to efficient approximate algorithms for several applications, such as:  determining the set of categories in a given taxonomy spanned by the search results; finding the range of metadata values associated to the result set in order to enable "multi-faceted search;" estimating the size of the result set; data mining associations to the query terms. we present and analyze an efficient algorithm for obtaining uniform random samples applicable to any search engine based on posting lists and document-at-a-time evaluation. (to our knowledge, all popular web search engines, e.g. google, inktomi, {altavista}, {alltheweb}, belong to this {class.)furthermore}, our algorithm can be modified to follow the modern object-oriented approach whereby posting lists are viewed as streams equipped with a  next  method, and the  next  method for boolean and other complex queries is built from the  next  method for primitive terms. in our case we show how to construct a basic  next(p)  method that samples term posting lists with probability p, and show how to construct  next(p)  methods for boolean operators ({and}, {or}, {wand}) from primitive {methods.finally}, we test the efficiency and quality of our approach on both synthetic and real-world data.
787	398542	article	cell	\N	\N	\N	11	115	7	2003	dec	2005-11-17 12:04:29	department of biology, massachusetts institute of technology, cambridge, ma 02139, usa.	prediction of mammalian {microrna} targets.	{micrornas} ({mirnas}) can play important gene regulatory roles in nematodes, insects, and plants by basepairing to {mrnas} to specify posttranscriptional repression of these messages. however, the {mrnas} regulated by vertebrate {mirnas} are all unknown. here we predict more than 400 regulatory target genes for the conserved vertebrate {mirnas} by identifying {mrnas} with conserved pairing to the 5' region of the {mirna} and evaluating the number and quality of these complementary sites. rigorous tests using shuffled {mirna} controls supported a majority of these predictions, with the fraction of false positives estimated at 31\\% for targets identified in human, mouse, and rat and 22\\% for targets identified in pufferfish as well as mammals. eleven predicted targets (out of 15 tested) were supported experimentally using a {hela} cell reporter system. the predicted regulatory targets of mammalian {mirnas} were enriched for genes involved in transcriptional regulation but also encompassed an unexpectedly broad range of other functions.
788	400050	article	nat rev genet	\N	\N	\N	8	4	3	2003	mar	2005-11-18 15:43:41	\N	the modern molecular clock	the discovery of the molecular clock â€” a relatively constant rate of molecular evolution â€” provided an insight into the mechanisms of molecular evolution, and created one of the most useful new tools in biology. the unexpected constancy of rate was explained by assuming that most changes to genes are effectively neutral. theory predicts several sources of variation in the rate of molecular evolution. however, even an approximate clock allows time estimates of events in evolutionary history, which provides a method for testing a wide range of biological hypotheses ranging from the origins of the animal kingdom to the emergence of new viral epidemics.
789	400549	article	information theory, ieee transactions on	\N	\N	ieee	73	44	6	1998	oct	2005-11-19 07:34:19	\N	fading channels: information-theoretic and communications aspects	in this paper we review the most peculiar and interesting information-theoretic and communications features of fading channels. we first describe the statistical models of fading channels which are frequently used in the analysis and design of communication systems. next, we focus on the information theory of fading channels, by emphasizing capacity as the most important performance measure. both single-user and multiuser transmission are examined. further, we describe how the structure of fading channels impacts code design, and finally overview equalization of fading multipath channels
790	400626	article	bioinformatics	\N	\N	\N	7	15	3	1999	\N	2005-11-19 10:45:08	\N	{dialign 2: improvement of the segment-to-segment approach to multiple sequence alignment}	motivation: the performance and time complexity of an improved version of the segment-to-segment approach to multiple sequence alignment is discussed. in this approach, alignments are composed from gap-free segment pairs, and the score of an alignment is defined as the sum of so-called weights of these segment pairs. results: a modification of the weight function used in the original version of the alignment program dialign has two important advantages: it can be applied to both globally and locally related sequence sets, and the running time of the program is considerably improved. the time complexity of the algorithm is discussed theoretically, and the program running time is reported for various test examples. availability: the program is available on-line at the bielefeld university bioinformatics server (bibiserv) http://bibiserv.techfak.uni-bielefeld.de/dial ign/
791	400642	article	nat biotech	\N	\N	\N	1	22	\N	2004	\N	2005-11-19 10:45:08	\N	what is dynamic programming?	sequence alignment methods often use something called a 'dynamic programming' algorithm. what is dynamic programming and how does it work? dynamic programming algorithms are a good place to start understanding what's really going on inside computational biology software. the heart of many well-known programs is a dynamic programming algorithm, or a fast approximation of one, including sequence database search programs like blast and fasta, multiple sequence alignment programs like clustalw, profile search programs like hmmer, gene finding programs like genscan and even rna-folding programs like mfold and phylogenetic inference programs like phylip.
792	400676	article	bioinformatics	\N	\N	\N	12	16	8	2000	\N	2005-11-19 10:45:08	\N	a systematic approach to dynamic programming in bioinformatics	motivation: dynamic programming is probably the most popular programming method in bioinformatics. sequence comparison, gene recognition, rna structure prediction and hundreds of other problems are solved by ever new variants of dynamic programming. currently, the development of a successful dynamic programming algorithm is a matter of experience, talent and luck. the typical matrix recurrence relations that make up a dynamic programming algorithm are intricate to construct, and difficult to implement reliably. no general problem independent guidance is available. results: this article introduces a systematic method for constructing dynamic programming solutions to problems in biosequence analysis. by a conceptual splitting of the algorithm into a recognition and an evaluation phase, algorithm development is simplified considerably, and correct recurrences can be derived systematically. without additional effort, the method produces an early, executable prototype expressed in a functional programming language. the method is quite generally applicable, and, while programming effort decreases, no overhead in terms of ultimate program efficiency is incurred. contact: robert@techfak.uni-bielefeld.de
793	400687	article	j. mol. biol.	\N	\N	\N	15	264	\N	1996	\N	2005-11-19 10:45:08	\N	{significant improvement in accuracy of multiple protein sequence alignments by iterative refinement as assessed by reference to structural alignments}	the relative performances of four strategies for aligning a large number of protein sequences were assessed by referring to corresponding structural alignments of 54 independent families. multiple sequence alignment of a family was constructed by a given method from the sequences of known structures and their homologues, and the subset consisting of the sequences of known structures was extracted from the whole alignment and compared with the structural counterpart in a residue-to-residue fashion. gap-opening and -extension penalties were optimized for each family and method. each of the four multiple alignment methods gave significantly more accurate alignments than the conventional pairwise method. in addition, a clear difference in performance was detected among three of the four multiple alignment methods examined. the currently most popular progressive method ranked worst among the four, and the randomized iterative strategy that optimizes the sum-of-pairs score ranked next worst. the two best-performing strategies, one of which was newly developed, both pursue an optimal weighted sum-of-pairs score, where the pair weights were introduced to correct for uneven representations of subgroups in a family. the new method uses doubly nested iterations to make alignment, phylogenetic tree and pair weights mutually consistent. most importantly, the improvement in accuracy of alignments obtained by these iterative methods over pairwise or progressive method tends to increase with decreasing average sequence identity, implying that iterative refinement is more effective for the generally difficult alignment of remotely related sequences. four well-known amino acid substitution matrices were also tested in combination with the various methods. however, the effects of substitution matrices were found to be minor in the framework of multiple alignment, and the same order of relative performance of the alignment methods was observed with any of the matrices.
794	400695	article	curr. opin. struct. biol.	\N	\N	\N	9	12	\N	2002	\N	2005-11-19 10:45:08	\N	the accuracy of ribosomal {rna} comparative structure models	the determination of the 16s and 23s rrna secondary structure models was initiated shortly after the first complete 16s and 23s rrna sequences were determined in the late 1970s. the structures that are common to all 16s rrnas and all 23s rrnas were determined using comparative methods from the analysis of thousands of rrna sequences. twenty-plus years later, the 16s and 23s rrna comparative structure models have been evaluated against the recently determined high-resolution crystal structures of the 30s and 50s ribosomal subunits. nearly all of the predicted covariation-based base pairs, including the regular base pairs and helices, and the irregular base pairs and tertiary interactions, were present in the 30s and 50s crystal structures.
795	400698	article	bioinformatics	\N	\N	\N	17	17	9	2001	\N	2005-11-19 10:45:08	\N	{evolutionary hmms: a bayesian approach to multiple alignment}	motivation: we review proposed syntheses of probabilistic sequence alignment, profiling and phylogeny. we develop a multiple alignment algorithm for bayesian inference in the links model proposed by thorne et al. (1991, j. mol. evol. , 33, 114Ã¢Â€Â“124). the algorithm, described in detail in section 3, samples from and/or maximizes the posterior distribution over multiple alignments for any number of dna or protein sequences, conditioned on a phylogenetic tree. the individual sampling and maximization steps of the algorithm require no more computational resources than pairwise alignment.methods: we present a software implementation (handel) of our algorithm and report test results on (i) simulated data sets and (ii) the structurally informed protein alignments of balibase (thompson et al. , 1999, nucleic acids res. , 27, 2682Ã¢Â€Â“2690).results: we find that the mean sum-of-pairs score (a measure of residue-pair correspondence) for the balibase alignments is only 13% lower for handelthan for clustalw(thompson et al. , 1994, nucleic acids res. , 22, 4673Ã¢Â€Â“4680), despite the relative simplicity of the links model (clustalw uses affine gap scores and increased penalties for indels in hydrophobic regions). with reference to these benchmarks, we discuss potential improvements to the links model and implications for bayesian multiple alignment and phylogenetic profiling.availability: the source code to handelis freely distributed on the internet at http://www.biowiki.org/handel under the terms of the gnu public license (gpl, 2000, http://www.fsf.org./copyleft/gpl.html).contact: ihh@fruitfly.org
796	400712	article	bioinformatics	\N	\N	\N	5	20	14	2004	\N	2005-11-19 10:45:08	\N	{alignment of rna base pairing probability matrices}	motivation: many classes of functional rna molecules are characterized by highly conserved secondary structures but little detectable sequence similarity. reliable multiple alignments can therefore be constructed only when the shared structural features are taken into account. since multiple alignments are used as input for many subsequent methods of data analysis, structure-based alignments are an indispensable necessity in rna bioinformatics.  results: we present here a method to compute pairwise and progressive multiple alignments from the direct comparison of base pairing probability matrices. instead of attempting to solve the folding and the alignment problem simultaneously as in the classical sankoff's algorithm, we use mccaskill's approach to compute base pairing probability matrices which effectively incorporate the information on the energetics of each sequences. a novel, simplified variant of sankoff's algorithms can then be employed to extract the maximum-weight common secondary structure and an associated alignment.  availability: the programs pmcomp and pmmulti described in this contribution are implemented in perl and can be downloaded together with the example datasets from http://www.tbi.univie.ac.at/rna/pmcomp/. a web server is available at http://rna.tbi.univie.ac.at/cgi-bin/pmcgi.pl 10.1093/bioinformatics/bth229
797	400727	article	bmc bioinformatics	\N	\N	\N	\N	6	1	2005	\N	2005-11-19 10:45:08	\N	{accelerated probabilistic inference of rna structure evolution}	background:pairwise stochastic context-free grammars (pair scfgs) are powerful tools for evolutionary analysis of rna, including simultaneous rna sequence alignment and secondary structure prediction, but the associated algorithms are intensive in both cpu and memory usage. the same problem is faced by other rna alignment-and-folding algorithms based on sankoff's 1985 algorithm. it is therefore desirable to constrain such algorithms, by pre-processing the sequences and using this first pass to limit the range of structures and/or alignments that can be considered.results:we demonstrate how flexible classes of constraint can be imposed, greatly reducing the computational costs while maintaining a high quality of structural homology prediction. any score-attributed context-free grammar (e.g. energy-based scoring schemes, or conditionally normalized pair scfgs) is amenable to this treatment. it is now possible to combine independent structural and alignment constraints of unprecedented general flexibility in pair scfg alignment algorithms. we outline several applications to the bioinformatics of rna sequence and structure, including waterman-eggert n-best alignments and progressive multiple alignment. we evaluate the performance of the algorithm on test examples from the rfam database.conclusion:a program, stemloc, that implements these algorithms for efficient rna sequence alignment and structure prediction is available under the gnu general public license.
798	400750	article	bmc bioinformatics	\N	\N	\N	\N	4	1	2003	\N	2005-11-19 10:45:08	\N	{rsearch: finding homologs of single structured rna sequences}	background: for many rna molecules, secondary structure rather than primary sequence is the evolutionarily conserved feature. no programs have yet been published that allow searching a sequence database for homologs of a single rna molecule on the basis of secondary structure. results: we have developed a program, rsearch, that takes a single rna sequence with its secondary structure and utilizes a local alignment algorithm to search a database for homologous rnas. for this purpose, we have developed a series of base pair and single nucleotide substitution matrices for rna sequences called ribosum matrices. rsearch reports the statistical confidence for each hit as well as the structural alignment of the hit. we show several examples in which rsearch outperforms the primary sequence search programs blast and ssearch. the primary drawback of the program is that it is slow. the c code for rsearch is freely available from our lab's website. conclusion: rsearch outperforms primary sequence programs in finding homologs of structured rna sequences.
799	400777	article	nucl. acids res.	\N	\N	\N	7	30	14	2002	\N	2005-11-19 10:45:09	\N	{mafft: a novel method for rapid multiple sequence alignment based on fast fourier transform}	a multiple sequence alignment program, mafft, has been developed. the cpu time is drastically reduced as compared with existing methods. mafft includes two novel techniques. (i) homo logous regions are rapidly identified by the fast fourier transform (fft), in which an amino acid sequence is converted to a sequence composed of volume and polarity values of each amino acid residue. (ii) we propose a simplified scoring system that performs well for reducing cpu time and increasing the accuracy of alignments even for sequences having large insertions or extensions as well as distantly related sequences of similar length. two different heuristics, the progressive method (fft-ns-2) and the iterative refinement method (fft-ns-i), are implemented in mafft. the performances of fft-ns-2 and fft-ns-i were compared with other methods by computer simulations and benchmark tests; the cpu time of fft-ns-2 is drastically reduced as compared with clustalw with comparable accuracy. fft-ns-i is over 100 times faster than t-coffee, when the number of input sequences exceeds 60, without sacrificing the accuracy.
800	400784	article	proc. natl. acad. sci. usa	\N	\N	\N	5	101	19	2004	\N	2005-11-19 10:45:09	\N	incorporating chemical modification constraints into a dynamic programming algorithm for prediction of {rna} secondary structure	a dynamic programming algorithm for prediction of rna secondary structure has been revised to accommodate folding constraints determined by chemical modification and to include free energy increments for coaxial stacking of helices when they are either adjacent or separated by a single mismatch. furthermore, free energy parameters are revised to account for recent experimental results for terminal mismatches and hairpin, bulge, internal, and multibranch loops. to demonstrate the applicability of this method, in vivo modification was performed on 5s rrna in both escherichia coli and candida albicans with 1-cyclohexyl-3-(2-morpholinoethyl) carbodiimide metho-p-toluene sulfonate, dimethyl sulfate, and kethoxal. the percentage of known base pairs in the predicted structure increased from 26.3% to 86.8% for the e. coli sequence by using modification constraints. for c. albicans, the accuracy remained 87.5% both with and without modification data. on average, for these sequences and a set of 14 sequences with known secondary structure and chemical modification data taken from the literature, accuracy improves from 67% to 76%. this enhancement primarily reflects improvement for three sequences that are predicted with <40% accuracy on the basis of energetics alone. for these sequences, inclusion of chemical modification constraints improves the average accuracy from 28% to 78%. for the 11 sequences with <6% pseudoknotted base pairs, structures predicted with constraints from chemical modification contain on average 84% of known canonical base pairs.
801	400850	article	nucl. acids res.	\N	\N	\N	3	18	\N	1990	\N	2005-11-19 10:45:09	\N	sequence logos: a new way to display consensus sequences	a graphical method is presented for displaying the patterns in a set of aligned sequences. the characters representing the sequence are stacked on top of each other for each position in the aligned sequences. the height of each letter is made proportional to its frequency, and the letters are sorted so the most common one is on top. the height of the entire stack is then adjusted to signify the information content of the sequences at that position. from these  sequence logos', one can determine not only the consensus sequence but also the relative frequency of bases and the information content (measured in bits) at every position in a site or sequence. the logo displays both significant residues and subtle sequence patterns. 10.1093/nar/18.20.6097
802	400861	book	\N	\N	\N	w.h. freeman and company limited	\N	\N	\N	1995	\N	2005-11-19 10:45:09	\N	the major transitions in evolution	{over the history of life there have been several major changes in the way genetic information is organized and transmitted from one generation to the next. these transitions include the origin of life itself, the first eukaryotic cells, reproduction by sexual means, the appearance of<br>multicellular plants and animals, the emergence of cooperation and of animal societies, and the unique language ability of humans. this ambitious book provides the first unified discussion of the full range of these transitions. the authors highlight the similarities between different<br>transitions--between the union of replicating molecules to form chromosomes and of cells to form multicellular organisms, for example--and show how understanding one transition sheds light on others. they trace a common theme throughout the history of evolution: after a major transition some<br>entities lose the ability to replicate independently, becoming able to reproduce only as part of a larger whole. the authors investigate this pattern and why selection between entities at a lower level does not disrupt selection at more complex levels. their explanation encompasses a compelling<br>theory of the evolution of cooperation at all levels of complexity. engagingly written and filled with numerous illustrations, this book can be read with enjoyment by anyone with an undergraduate training in biology. it is ideal for advanced discussion groups on evolution and includes accessible<br>discussions of a wide range of topics, from molecular biology and linguistics to insect societies.}
803	400878	article	nucleic acids res	\N	\N	\N	\N	27	13	1999	\N	2005-11-19 10:45:09	\N	{a comprehensive comparison of multiple sequence alignment programs}	{in recent years improvements to existing programs and the introduction of new iterative algorithms have changed the state-of-the-art in protein sequence alignment. this paper presents the first systematic study of the most commonly used alignment programs using balibase benchmark alignments as test cases. even below the 'twilight zone' at 10-20\\% residue identity, the best programs were capable of correctly aligning on average 47\\% of the residues. we show that iterative algorithms often offer improved alignment accuracy though at the expense of computation time. a notable exception was the effect of introducing a single divergent sequence into a set of closely related sequences, causing the iteration to diverge away from the best alignment. global alignment programs generally performed better than local methods, except in the presence of large n/c-terminal extensions and internal insertions. in these cases, a local algorithm was more successful in identifying the most conserved motifs. this study enables us to propose appropriate alignment strategies, depending on the nature of a particular set of sequences. the employment of more than one program based on different alignment techniques should significantly improve the quality of automatic protein sequence alignment methods. the results also indicate guidelines for improvement of alignment algorithms.}
804	400885	article	science	\N	\N	\N	5	249	\N	1990	\N	2005-11-19 10:45:09	\N	systematic evolution of ligands by exponential enrichment: {rna} ligands to bacteriophage {t4 dna} polymerase	high-affinity nucleic acid ligands for a protein were isolated by a procedure that depends on alternate cycles of ligand selection from pools of variant sequences and amplification of the bound species. multiple rounds exponentially enrich the population for the highest affinity species that can be clonally isolated and characterized. in particular one eight-base region of an rna that interacts with the t4 dna polymerase was chosen and randomized. two different sequences were selected by this procedure from the calculated pool of 65,536 species. one is the wild-type sequence found in the bacteriophage mrna; one is varied from wild type at four positions. the binding constants of these two rna's to t4 dna polymerase are equivalent. these protocols with minimal modification can yield high-affinity ligands for any protein that binds nucleic acids as part of its function; high-affinity ligands could conceivably be developed for any target molecule. 10.1126/science.2200121
805	400913	article	siam journal of computing	\N	\N	\N	17	18	6	1989	\N	2005-11-19 10:45:09	\N	simple fast algorithms for the editing distance between trees and related problems	ordered labeled trees are trees in which the left-to-right order among siblings is significant. the distance between two ordered trees is considered to be the weighted number of edit operations (insert, delete, and modify) to transform one tree to another. the problem of approximate tree matching is also considered. specifically, algorithms are designed to answer the following kinds of questions:1. what is the distance between two trees? 2. what is the minimum distance between \\\\$t_1 \\\\$ and \\\\$t_2 \\\\$ when zero or more subtrees can be removed from \\\\$t_2 \\\\$? 3. let the pruning of a tree at node \\\\$n\\\\$ mean removing all the descendants of node \\\\$n\\\\$. the analogous question for prunings as for subtrees is answered.a dynamic programming algorithm is presented to solve the three questions in sequential time \\\\$o( t_1   \\\\times  t_2   \\\\times \\\\min ({\\\\textit{depth}}(t_1 ),{\\\\textit{leaves}}(t_1 )) \\\\times \\\\min ({\\\\textit{depth}}(t_2 ),{\\\\textit{leaves}}(t_2 )))\\\\$ and space \\\\$o( t_1   \\\\times  t_2  )\\\\$ compared with \\\\$o( t_1   \\\\times  t_2   \\\\times ({\\\\textit{depth}}(t_1 ))^2 \\\\times ({\\\\textit{depth}}(t_2 ))^2 )\\\\$ for the best previous published algorithm due to tai [j. assoc. comput. mach., 26 (1979), pp, 422-433]. further, the algorithm presented here can be parallelized to give time \\\\$o( t_1   \\\\times  t_2  )\\\\$.
806	400914	article	current opinion in structural biology	\N	\N	\N	7	10	3	2000	\N	2005-11-19 10:45:09	\N	calculating nucleic acid secondary structure	new results for calculating nucleic acid secondary structure by free energy minimization and phylogenetic comparisons have recently been reported. a complete set of dna energy parameters is now available and the rna parameters have been improved. although databases of rna secondary structures are still derived and expanded using computer-assisted, ad hoc comparative analysis, a number of new computer algorithms combine covariation analysis with energy methods.
807	400921	article	science	\N	\N	\N	9	297	5585	2002	\N	2005-11-19 10:45:09	\N	{whole-genome shotgun assembly and analysis of the genome of fugu rubripes}	the compact genome of fugu rubripes has been sequenced to over 95% coverage, and more than 80% of the assembly is in multigene-sized scaffolds. in this 365-megabase vertebrate genome, repetitive dna accounts for less than one-sixth of the sequence, and gene loci occupy about one-third of the genome. as with the human genome, gene loci are not evenly distributed, but are clustered into sparse and dense regions. some "giant" genes were observed that had average coding sequence sizes but were spread over genomic lengths significantly larger than those of their human orthologs. although three-quarters of predicted human proteins have a strong match to fugu, approximately a quarter of the human proteins had highly diverged from or had no pufferfish homologs, highlighting the extent of protein evolution in the 450 million years since teleosts and mammals diverged. conserved linkages between fugu and human genes indicate the preservation of chromosomal segments from the common vertebrate ancestor, but with considerable scrambling of gene order.
808	400952	article	nature	\N	\N	\N	28	428	6982	2004	\N	2005-11-19 10:45:10	\N	{genome sequence of the brown norway rat yields insights into mammalian evolution}	{the laboratory rat (rattus norvegicus) is an indispensable tool in experimental medicine and drug development, having made inestimable contributions to human health. we report here the genome sequence of the brown norway (bn) rat strain. the sequence represents a high-quality 'draft' covering over 90\\% of the genome. the bn rat sequence is the third complete mammalian genome to be deciphered, and three-way comparisons with the human and mouse genomes resolve details of mammalian evolution. this first comprehensive analysis includes genes and proteins and their relation to human disease, repeated sequences, comparative genome-wide studies of mammalian orthologous chromosomal regions and rearrangement breakpoints, reconstruction of ancestral karyotypes and the events leading to existing species, rates of variation, and lineage-specific and lineage-independent evolutionary events such as expansion of gene families, orthology relations and protein evolution.}
809	400963	article	genome res	\N	\N	\N	10	14	4	2004	\N	2005-11-19 10:45:10	\N	{comparative recombination rates in the rat, mouse, and human genomes}	{levels of recombination vary among species, among chromosomes within species, and among regions within chromosomes in mammals. this heterogeneity may affect levels of diversity, efficiency of selection, and genome composition, as well as have practical consequences for the genetic mapping of traits. we compared the genetic maps to the genome sequence assemblies of rat, mouse, and human to estimate local recombination rates across these genomes. humans have greater overall levels of recombination, as well as greater variance. in rat and mouse, the size of the chromosome and proximity to telomere have less effect on local recombination rate than in human. at the chromosome level, rat and mouse x chromosomes have the lowest recombination rates, whereas human chromosome x does not show the same pattern. in all species, local recombination rate is significantly correlated with several sequence variables, including gc\\%, cpg density, repetitive elements, and the neutral mutation rate, with some pronounced differences between species. recombination rate in one species is not strongly correlated with the rate in another, when comparing homologous syntenic blocks of the genome. this comparative approach provides additional insight into the causes and consequences of genomic heterogeneity in recombination.}
810	400971	article	proc natl acad sci u s a	\N	\N	\N	5	100	20	2003	\N	2005-11-19 10:45:10	\N	{evolution's cauldron: duplication, deletion, and rearrangement in the mouse and human genomes}	this study examines genomic duplications, deletions, and rearrangements that have happened at scales ranging from a single base to complete chromosomes by comparing the mouse and human genomes. from whole-genome sequence alignments, 344 large (>100-kb) blocks of conserved synteny are evident, but these are further fragmented by smaller-scale evolutionary events. excluding transposon insertions, on average in each megabase of genomic alignment we observe two inversions, 17 duplications (five tandem or nearly tandem), seven transpositions, and 200 deletions of 100 bases or more. this includes 160 inversions and 75 duplications or transpositions of length >100 kb. the frequencies of these smaller events are not substantially higher in finished portions in the assembly. many of the smaller transpositions are processed pseudogenes; we define a Ã¢Â€ÂœsyntenicÃ¢Â€Â subset of the alignments that excludes these and other small-scale transpositions. these alignments provide evidence that Ã¢Â‰Âˆ2% of the genes in the human/mouse common ancestor have been deleted or partially deleted in the mouse. there also appears to be slightly less nontransposon-induced genome duplication in the mouse than in the human lineage. although some of the events we detect are possibly due to misassemblies or missing data in the current genome sequence or to the limitations of our methods, most are likely to represent genuine evolutionary events. to make these observations, we developed new alignment techniques that can handle large gaps in a robust fashion and discriminate between orthologous and paralogous alignments.
811	400984	article	bmc bioinformatics	\N	\N	\N	\N	5	1	2004	\N	2005-11-19 10:45:10	\N	{benchmarking tools for the alignment of functional noncoding dna}	{background:} numerous tools have been developed to align genomic sequences. however, their relative performance in specific applications remains poorly characterized. alignments of protein-coding sequences typically have been benchmarked against correct alignments inferred from structural data. for noncoding sequences, where such independent validation is lacking, simulation provides an effective means to generate correct alignments with which to benchmark alignment tools. {results:} using rates of noncoding sequence evolution estimated from the genus drosophila, we simulated alignments over a range of divergence times under varying models incorporating point substitution, insertion/deletion events, and short blocks of constrained sequences such as those found in cis-regulatory regions. we then compared correct alignments generated by a modified version of the {rose} simulation platform to alignments of the simulated derived sequences produced by eight pairwise alignment tools {(avid,} {blastz,} chaos, {clustalw,} {dialign,} lagan, needle, and {waba)} to determine the off-the-shelf performance of each tool. as expected, the ability to align noncoding sequences accurately decreases with increasing divergence for all tools, and declines faster in the presence of insertion/deletion evolution. global alignment tools {(avid,} {clustalw,} lagan, and needle) typically have higher sensitivity over entire noncoding sequences as well as in constrained sequences. local tools {(blastz,} chaos, and {waba)} have lower overall sensitivity as a consequence of incomplete coverage, but have high specificity to detect constrained sequences as well as high sensitivity within the subset of sequences they align. tools such as {dialign,} which generate both local and global outputs, produce alignments of constrained sequences with both high sensitivity and specificity for divergence distances in the range of 1.25-3.0 substitutions per site. {conclusion:} for species with genomic properties similar to drosophila, we conclude that a single pair of optimally diverged species analyzed with a high performance alignment tool can yield accurate and specific alignments of functionally constrained noncoding sequences. further algorithm development, optimization of alignment parameters, and benchmarking studies will be necessary to extract the maximal biological information from alignments of functional noncoding {dna.}
812	401001	article	nucleic acids res	\N	\N	\N	2	26	1	1998	\N	2005-11-19 10:45:10	\N	{pfam: multiple sequence alignments and hmm-profiles of protein domains}	{pfam contains multiple alignments and hidden markov model based profiles (hmm-profiles) of complete protein domains. the definition of domain boundaries, family members and alignment is done semi-automatically based on expert knowledge, sequence similarity, other protein family databases and the ability of hmm-profiles to correctly identify and align the members. release 2.0 of pfam contains 527 manually verified families which are available for browsing and on-line searching via the world wide web in the uk at http://www.sanger.ac.uk/pfam/ and in the us at http://genome.wustl. edu/pfam/ pfam 2.0 matches one or more domains in 50\\% of swissprot-34 sequences, and 25\\% of a large sample of predicted proteins from the caenorhabditis elegans genome.}
813	402517	book	\N	\N	\N	cambridge university press	\N	\N	\N	2003	apr	2005-11-21 10:24:14	\N	{haskell\\~{}98} language and libraries, the revised report	haskell is the worldâ€™s leading lazy functional programming language, widely used for teaching, research, and applications. the language continues to develop rapidly, but in 1998 the community decided to capture a stable snapshot of the language: haskell 98. all haskell compilers support haskell 98, so practitioners and educators alike have a stable base for their work. this book constitutes the agreed definition of the haskell 98, both the language itself and its supporting libraries. it has been considerably revised and refined since the original definition, and appears in print for the first time. it should be a standard reference work for anyone involved in research, teaching, or application of haskell.
814	403648	book	\N	\N	\N	thomson computer press	\N	\N	\N	1996	\N	2005-11-21 20:24:24	\N	software metrics: {a} rigorous and practical approach	from the publisher: the second edition of  software metrics  provides an up-to-date, coherent, and rigorous framework for controlling, managing, and predicting software development processes. with an emphasis on real-world applications, fenton and pfleeger apply basic ideas in measurement theory to quantify software development resources, processes, and products.  the book offers an accessible and comprehensive introduction to software metrics, now an essential component of software engineering for both classroom and industry.  software metrics  features extensive case studies from hewlett packard, ibm, the u.s. department of defense, motorola, and others, in addition to worked examples and exercises. the second edition includes up-to-date material on process maturity and measurement, goal-question-metric, planning a metrics program, measurement in practice, experimentation, empirical studies, iso9216, and metric tools.
815	403658	article	ieee software	\N	\N	\N	8	7	5	1990	sep	2005-11-21 20:24:24	\N	seven myths of formal methods	seven myths on formal methods (difficult, expensive, not widely used). with their own experience
816	404740	article	educational technology research and development	\N	\N	\N	\N	49	\N	2002	\N	2005-11-22 17:15:54	\N	a history of instructional design and technology: part {ii}: a history of instructional design	this second of a two-part article focuses on the history of instructional design in the united states. starting with its origins during world war ii, major events in the development of instructional design are described. factors affecting the field over the last two decades, including increasing interest in cognitive psychology, microcomputers, performance technology, and constructivism, are also described. (contains 105 references.) (author/aef)
817	405773	book	\N	\N	\N	w.w. norton \\& co.	\N	\N	\N	2003	may	2005-11-23 12:53:20	\N	nexus: small worlds and the groundbreaking theory of networks	as _chaos_ explained the science of disorder, _nexus_ reveals the new science of connection and the odd logic of six degrees of separation.  "if you ever wanted to know how many links connect you and the pope, or why when the u.s. federal reserve bank sneezes the global economy catches cold, read this book," writes john l. casti (santa fe institute). this "cogent and engaging" (_nature_) work presents the fundamental principles of the emerging field of "small-worlds" theoryâ€”the idea that a hidden pattern is the key to how networks interact and exchange information, whether that network is the information highway or the firing of neurons in the brain. mathematicians, physicists, computer scientists, and social scientists are working to decipher this complex organizational system, for it may yield a blueprint of dynamic interactions within our physical as well as social worlds.  highlighting groundbreaking research behind network theory, "mark buchanan's graceful, lucid, nontechnical and entertaining prose" (mark granovetter) documents the mounting support among various disciplines for the small-worlds idea and demonstrates its practical applications to diverse problemsâ€”from the volatile global economy or the human genome project to the spread of infectious disease or ecological damage. _nexus_ is an exciting introduction to the hidden geometry that weaves our lives so inextricably together.
818	405843	article	briefings in bioinformatics	\N	\N	oxford university press	10	6	3	2005	sep	2005-11-23 13:26:37	genomatix software gmbh, landsberger strasse 6, munich, d-80339, germany. scherf@genomatix.de	the next generation of literature analysis: integration of genomic analysis into text mining	text-mining systems are indispensable tools to reduce the increasing flux of information in scientific literature to topics pertinent to a particular interest in focus. most of the scientific literature is published as unstructured free text, complicating the development of data processing tools, which rely on structured information. to overcome the problems of free text analysis, structured, hand-curated information derived from literature is integrated in text-mining systems to improve precision and recall. in this paper several text-mining approaches are reviewed and the next step in development of text-mining systems, which is based on a concept of multiple lines of evidence, is described: results from literature analysis are combined with evidence from experiments and genome analysis to improve the accuracy of results and to generate additional knowledge beyond what is known solely from literature.
819	406295	article	journal of financial economics	\N	\N	\N	34	7	3	1979	sep	2005-11-23 15:40:49	\N	option pricing: a simplified approach	this paper presents a simple discrete-time model for valuing options. the fundamental economic principles of option pricing by arbitrage methods are particularly clear in this setting. its development requires only elementary mathematics, yet it contains as a special limiting case the celebrated {black-scholes} model, which has previously been derived only by much more difficult methods. the basic model readily lends itself to generalization in many ways. moreover, by its very construction, it gives rise to a simple and efficient numerical procedure for valuing options for which premature exercise may be optimal.
820	406529	article	nature	\N	\N	nature publishing group	4	438	7067	2005	nov	2005-11-25 16:29:52	\N	foundations for engineering biology	engineered biological systems have been used to manipulate information, construct materials, process chemicals, produce energy, provide food, and help maintain or enhance human health and our environment. unfortunately, our ability to quickly and reliably engineer biological systems that behave as expected remains quite limited. foundational technologies that make routine the engineering of biology are needed. vibrant, open research communities and strategic leadership are necessary to ensure that the development and application of biological technologies remains overwhelmingly constructive.
821	406673	article	journal of memory and language	\N	\N	\N	18	49	1	2003	jul	2005-11-23 20:22:22	\N	the effects of common ground and perspective on domains of referential interpretation	addressees' eye movements were tracked as they followed instructions given by a confederate speaker hidden from view. experiment 1 used objects in common ground (known to both participants) or privileged ground (known to the addressee). although privileged objects interfered with reference to an identical object in common ground, addressees were always more likely to look at an object in common ground than privileged ground. experiment 2 used definite and indefinite referring expressions with early or late points of disambiguation, depending on the uniqueness of the display objects. the speaker's and addressee's perspectives matched when the speaker was accurately informed about the display, and mismatched when the speaker was misinformed. when perspectives matched, addressees identified the target faster with early than with late disambiguation displays. when perspectives mismatched, addressees still identified the target quickly, showing an ability to use the speaker's perspective. these experiments demonstrate that although addressees cannot completely ignore information in privileged ground, common ground and perspective each have immediate effects on reference resolution.
822	406703	inproceedings	\N	www 2005	\N	\N	\N	\N	\N	-1	\N	2005-11-23 21:36:13	\N	{trustguard}: countering vulnerabilities in reputation management for decentralized overlay networks	reputation systems have been popular in estimating the trustworthiness and predicting the future behavior of nodes in a large-scale distributed system where nodes may transact with one another without prior knowledge or experience. one of the fundamental challenges in distributed reputation management is to understand vulnerabilities and develop mechanisms that can minimize the potential damages to a system by malicious nodes. in this paper, we identify three vulnerabilities that are...
823	407408	article	nature (london)	\N	\N	\N	9	422	\N	2003	\N	2005-11-24 16:29:51	\N	mass spectrometry-based proteomics	recent successes illustrate the role of mass spectrometry-based proteomics as an indispensable tool for molecular and cellular biology and for the emerging field of systems biology. these include the study of proteinâˆ’protein interactions via affinity-based isolations on a small and proteome-wide scale, the mapping of numerous organelles, the concurrent description of the malaria parasite genome and proteome, and the generation of quantitative protein profiles from diverse species. the ability of mass spectrometry to identify and, increasingly, to precisely quantify thousands of proteins from complex samples can be expected to impact broadly on biology and medicine.
824	407474	article	nature (london)	\N	\N	\N	9	422	\N	2003	\N	2005-11-24 16:29:51	\N	from words to literature in structural proteomics	technical advances on several frontiers have expanded the applicability of existing methods in structural biology and helped close the resolution gaps between them. as a result, we are now poised to integrate structural information gathered at multiple levels of the biological hierarchy â€” from atoms to cells â€” into a common framework. the goal is a comprehensive description of the multitude of interactions between molecular entities, which in turn is a prerequisite for the discovery of general structural principles that underlie all cellular processes.
825	407633	inproceedings	\N	\N	\N	\N	\N	\N	\N	-1	\N	2005-11-24 18:16:09	\N	{self-organized} stigmergic document maps: environment as mechanism for context learning	social insect societies and more specifically ant colonies, are distributed systems that, in spite of the simplicity of their individuals, present a highly structured social organization. as a result of this organization, ant colonies can accomplish complex tasks that in some cases exceed the individual capabilities of a single ant. the study of ant colonies behavior and of their self-organizing capabilities is of interest to knowledge retrieval/ management and decision support systems...
826	407635	misc	\N	\N	\N	\N	\N	\N	\N	1998	\N	2005-11-24 18:19:24	\N	less is more - genetic optimisation of nearest neighbour classifiers	the present paper deals with optimisation of nearest neighbour rule classifiers via genetic algorithms. the methodology consists on implement a genetic algorithm capable of search the input feature space used by the nnr classifier. results show that is adequate to perform feature reduction and simultaneous improve the recognition rate. some practical examples prove that is possible to recognise portuguese granites in 100%, with only 3 morphological features (from an original set of 117...
827	407636	article	lecture notes in computer science	\N	\N	\N	\N	1923	\N	2000	\N	2005-11-24 18:20:07	\N	map segmentation by colour cube genetic {k-mean} clustering	segmentation of a colour image composed of different kinds of texture regions can be a hard problem, namely to compute for an exact texture fields and a decision of the optimum number of segmentation areas in an image when it contains similar and/or unstationary texture fields. in this work, a method is described for evolving adaptive procedures for these problems. in many real world applications data clustering constitutes a fundamental issue whenever behavioural or feature domains can be ...
828	407638	inproceedings	\N	applications of evolutionary computing. evoworkshops2001: evocop, evoflight, evoiasp, evolearn, and evostim. proceedings	\N	springer-verlag	9	2037	\N	2001	JanAug-JanSep	2005-11-24 18:21:38	como, italy	the biological concept of neoteny in evolutionary colour image segmentation: simple experiments in simple {non-memetic} genetic algorithms	neoteny, also spelled paedomorphosis, can be defined in biological terms as the retention by an organism of juvenile or even larval traits into later life. in some species, all morphological development is retarded; the organism is juvenilized but sexually mature. such shifts of reproductive capability would appear to have adaptive significance to organisms that exhibit it. in terms of evolutionary theory, the process of paedomorphosis suggests that larval stages and developmental phases of...
829	407639	misc	\N	\N	\N	\N	\N	\N	\N	2000	\N	2005-11-24 18:22:28	\N	artificial ant colonies in digital image habitats - a mass behaviour effect study on pattern recognition	some recent studies have pointed that , the self-organization of neurons into brain-like structures, and the self-organization of ants into a swarm are similar in many respects. if possible to implement, these features could lead to important developments in pattern recognition systems, where perceptive capabilities can emerge and evolve from the interaction of many simple local rules. the principle of the method is inspired by the work of chialvo and millonas who developed the first numerical...
830	407641	misc	\N	\N	\N	\N	\N	\N	\N	-1	\N	2005-11-24 18:26:39	\N	swarms on continuous data	while being it extremely important, many exploratory data analysis ({eda} [21]) systems have the inhability to perform classification and visualization in a continuous basis or to self-organize new data-items into the older ones (evenmore into new labels if necessary), which can be crucial in {kdd} - knowledge discovery [10,1], retrieval and data mining systems [15,10] (interactive and online forms of web applications are just one example). this disadvantge is also present in more recent approaches ...
831	407642	misc	\N	\N	\N	\N	\N	\N	\N	2000	\N	2005-11-24 18:27:17	\N	image colour segmentation by genetic algorithms	segmentation of a colour image composed of different kinds of texture regions can be a hard problem, namely to compute for an exact texture fields and a decision of the optimum number of segmentation areas in an image when it contains similar and/or unstationary texture fields. in this work, a method is described for evolving adaptive procedures for these problems. in many real world applications data clustering constitutes a fundamental issue whenever behavioural or feature domains can be...
832	407643	misc	\N	\N	\N	\N	\N	\N	\N	1999	\N	2005-11-24 18:27:52	\N	from feature extraction to classification: a multidisciplinary approach applied to portuguese granites	the purpose of this paper is to present a complete methodology based on a multidisciplinary approach, that goes from the extraction of features till the classification of a set of different portuguese granites. the set of tools to extract the features that characterise polished surfaces of the granites is mainly based on mathematical morphology. the classification methodology is based on a genetic algorithm capable of search the input feature space used by the nearest neighbour rule classifier. ...
833	407644	misc	\N	\N	\N	\N	\N	\N	\N	-1	\N	2005-11-24 18:28:34	\N	on image filtering, noise and morphological size intensity diagrams	in the absence of a pure noise-free image it is hard to define what noise is, in any original noisy image, and as a consequence also where it is, and in what amount. in fact, the definition of noise depends largely on our own aim in the whole image analysis process, and (perhaps more important) in our self-perception of noise. for instance, when we perceive noise as disconnected and small it is normal to use mmaf filters to treat it. there is two evidences of this. first, in many instances...
834	407645	misc	\N	\N	\N	\N	\N	\N	\N	-1	\N	2005-11-24 18:29:02	\N	artificial neoteny in evolutionary image segmentation	neoteny, also spelled paedomorphosis, can be defined in biological terms as the retention by an organism of juvenile or even larval traits into later life. in some species, all morphological development is retarded; the organism is juvenilized but sexually mature. such shifts of reproductive capability would appear to have adaptive significance to organisms that exhibit it. in terms of evolutionary theory, the process of paedomorphosis suggests that larval stages and developmental phases of...
835	407678	book	\N	\N	\N	\N	\N	\N	\N	-1	\N	2005-11-24 18:42:15	\N	intrusion detection systems using adaptive regression splines	past few years have witnessed a growing recognition of soft computing technologies for the construction of intelligent and reliable intrusion detection systems. due to increasing incidents of cyber attacks, building effective intrusion detection systems ({idss}) are essential for protecting information systems security, and yet it remains an elusive goal and a great challenge. in this paper, we report a performance analysis between multivariate adaptive regression splines ({mars}), neural networks and support vector machines. the {mars} procedure builds flexible regression models by fitting separate splines to distinct intervals of the predictor variables. a brief comparison of different neural network learning algorithms is also given.
836	407679	inproceedings	\N	\N	\N	\N	\N	\N	\N	-1	\N	2005-11-24 18:44:23	\N	swarming around shellfish larvae images	the collection of wild larvae seed as a source of raw material is a major sub industry of shellfish aquaculture. to predict when, where and in what quantities wild seed will be available, it is necessary to track the appearance and growth of planktonic larvae. one of the most difficult groups to identify, particularly at the species level are the bivalvia. this difficulty arises from the fact that fundamentally all bivalve larvae have a similar shape and colour. identification based on gross morphological appearance is limited by the time-consuming nature of the microscopic examination and by the limited availability of expertise in this field. molecular and immunological methods are also being studied. we describe the application of computational pattern recognition methods to the automated identification and size analysis of scallop larvae. for identification, the shape features used are binary invariant moments; that is, the features are invariant to shift (position within the image), scale (induced either by growth or differential image magnification) and rotation. images of a sample of scallop and non-scallop larvae covering a range of maturities have been analysed. in order to overcome the automatic identification, as well as to allow the system to receive new unknown samples at any moment, a self-organized and unsupervised ant-like clustering algorithm based on swarm intelligence is proposed, followed by simple {k-nnr} nearest neighbour classification on the final map. results achieve a full recognition rate of 100\\% under several situations (k =1 or 3).
837	407681	inbook	\N	\N	\N	\N	\N	\N	\N	-1	\N	2005-11-24 18:46:39	\N	{antids}: {self-organized} ant-based clustering model for intrusion detection system	security of computers and the networks that connect them is increasingly becoming of great significance. computer security is defined as the protection of computing systems against threats to confidentiality, integrity, and availability. there are two types of intruders: the external intruders who are unauthorized users of the machines they attack, and internal intruders, who have permission to access the system with some restrictions. due to the fact that it is more and more improbable to a system administrator to recognize and manually intervene to stop an attack, there is an increasing recognition that {id} systems should have a lot to earn on following its basic principles on the behavior of complex natural systems, namely in what refers to self-organization, allowing for a real distributed and collective perception of this phenomena. with that aim in mind, the present work presents a self-organized ant colony based intrusion detection system ({antids}) to detect intrusions in a network infrastructure. the performance is compared among conventional soft computing paradigms like decision trees, support vector machines and linear genetic programming to model fast, online and efficient intrusion detection systems.
838	407688	inbook	\N	\N	\N	\N	\N	\N	\N	-1	\N	2005-11-24 18:48:43	\N	exploiting and evolving rn mathematical morphology feature spaces	a multidisciplinary methodology that goes from the extraction of features till the classification of a set of different portuguese granites is presented in this paper. the set of tools to extract the features that characterise the polished surfaces of granites is mainly based on mathematical morphology. the classification methodology is based on a genetic algorithm capable of search for the input feature space used by the nearest neighbour rule classifier. results show that is adequate to perform feature reduction and simultaneous improve the recognition rate. moreover, the present methodology represents a robust strategy to understand the proper nature of the textures studied and their discriminant features.
839	407689	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2005-11-24 18:50:03	\N	social cognitive maps, swarm collective perception and distributed search on dynamic landscapes	swarm intelligence ({si}) is the property of a system whereby the collective behaviors of (unsophisticated) entities interacting locally with their environment cause coherent functional global patterns to emerge. {si} provides a basis with wich it is possible to explore collective (or distributed) problem solving without centralized control or the provision of a global model. to tackle the formation of a coherent social collective intelligence from individual behaviors, we discuss several concepts related to {self-organization}, stigmergy and social foraging in animals. then, in a more abstract level we suggest and stress the role played not only by the environmental media as a driving force for societal learning, as well as by positive and negative feedbacks produced by the many interactions among agents. finally, presenting a simple model based on the above features, we will adress the collective adaptation of a social community to a cultural (environmenatl, contextual) or media informational dynamical landscape, represented here - for the purpose of different experiments - by several three-dimensional mathematical functions that suddenly change over time. results indicate that the collective intelligence is able to cope and quickly adapt to unforseen situations even when over the same cooperative foraging period, the community is requested to deal with two different and contradictory purposes.
840	407690	inbook	\N	\N	\N	\N	\N	\N	\N	-1	\N	2005-11-24 18:51:57	\N	varying the population size of artificial foraging swarms on time varying landscapes	swarm intelligence ({si}) is the property of a system whereby the collective behaviors of (unsophisticated) entities interacting locally with their environment cause coherent functional global patterns to emerge. {si} provides a basis with wich it is possible to explore collective (or distributed) problem solving without centralized control or the provision of a global model. in this paper we present a swarm search algorithm with varying population of agents. the swarm is based on a previous model with fixed population which proved its effectiveness on several computation problems. we will show that the variation of the population size provides the swarm with mechanisms that improves its self-adaptability and causes the emergence of a more robust self-organized behavior, resulting in a higher efficiency on searching peaks and valleys over dynamic search landscapes represented here - for the purpose of different experiments - by several three-dimensional mathematical functions that suddenly change over time. we will also show that the present swarm, for each function, self-adapts towards an optimal population size, thus self-regulating.
841	407710	inbook	\N	\N	\N	\N	\N	\N	\N	-1	\N	2005-11-24 18:54:52	\N	stock market prediction using multi expression programming	the use of intelligent systems for stock market predictions has been widely established. in this paper we introduce a genetic programming technique (called {multi-expression} programming) for the prediction of two stock indices. the performance is then compared with an artifcial neural network trained using {levenberg-marquardt} algorithm, support vector machine, {takagi-sugeno} neuro-fuzzy model, a difference boosting neural network. we considered nasdaq-100 index of nasdaq stock {marketsm} and the {s\\&p} {cnx} {nifty} stock index as test data.
842	407711	inbook	\N	\N	\N	\N	\N	\N	\N	-1	\N	2005-11-24 18:56:07	\N	on ants, bacteria and dynamic environments	wasps, bees, ants and termites all make effective use of their environment and resources by displaying collective  ” swarm” intelligence. termite colonies - for instance - build nests with a complexity far beyond the comprehension of the individual termite, while ant colonies dynamically allocate labor to various vital tasks such as foraging or defense without any central decision-making ability. recent research suggests that microbial life can be even richer: highly social, intricately networked, and teeming with interactions, as found in bacteria. what strikes from these observations is that both ant colonies and bacteria have similar natural mechanisms based on stigmergy and {self-organization} in order to emerge coherent and sophisticated patterns of global behaviour. keeping in mind the above characteristics we will present a simple model to tackle the collective adaptation of a social swarm based on real ant colony behaviors ({ssa} algorithm) for tracking extrema in dynamic environments and highly multimodal complex functions described in the well-know de jong test suite. then, for the purpose of comparison, a recent model of artificial bacterial foraging ({bfoa} algorithm) based on similar stigmergic features is described and analyzed. final results indicate that the {ssa} collective intelligence is able to cope and quickly adapt to unforeseen situations even when over the same cooperative foraging period, the community is requested to deal with two different and contradictory purposes, while outperforming {bfoa} in adaptive speed.
843	407712	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2005-11-24 18:57:31	\N	{self-regulated} artificial ant colonies on digital image habitats	artificial life models, swarm intelligent and evolutionary computation algorithms are usually built on fixed size populations. some studies indicate however that varying the population size can increase the adaptability of these systems and their capability to react to changing environments. in this paper we present an extended model of an artificial ant colony system designed to evolve on digital image habitats. we will show that the present swarm can adapt the size of the population according to the type of image on which it is evolving and reacting faster to changing images, thus converging more rapidly to the new desired regions, regulating the number of his image foraging agents. finally, we will show evidences that the model can be associated with the mathematical morphology watershed algorithm to improve the segmentation of digital grey-scale images.
844	407741	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2005-11-24 18:59:33	\N	societal implicit memory and his speed on tracking extrema over dynamic environments using {self-regulatory} swarms	in order to overcome difficult dynamic optimization and environment extrema tracking problems, we propose a {self-regulated} swarm ({srs}) algorithm which hybridizes the advantageous characteristics of swarm intelligence as the emergence of a societal environmental memory or cognitive map via collective pheromone laying in the landscape (properly balancing the exploration/exploitation nature of the search strategy), with a simple evolutionary mechanism that trough a direct reproduction procedure linked to local environmental features is able to self-regulate the above exploratory swarm population, speeding it up globally. in order to test his adaptive response and robustness, we have recurred to different dynamic multimodal complex functions as well as to dynamic optimization control ({doc}) problems. measures were made for different dynamic settings and parameters such as, environmental upgrade frequencies, landscape changing speed severity, type of dynamic (linear or circular), and to dramatic changes on the algorithmic search purpose over each test environment (e.g. shifting the extrema). finally, comparisons were made with traditional genetic algorithms ({ga}), bacterial foraging optimization algorithms ({bfoa}), as well as with more recently proposed {co-evolutionary} approaches. {srs}, were able to demonstrate quick adaptive responses, while outperforming the results obtained by the other approaches. additionally, some successful behaviors were found: {srs} was able not only to achieve quick adaptive responses, as to maintaining a number of different solutions, while adapting to new unforeseen extrema; the possibility to spontaneously create and maintain different subpopulations on different peaks, emerging different exploratory corridors with intelligent path planning capabilities; the ability to request for new agents over dramatic changing periods, and economizing those foraging resources over periods of stabilization. finally, results prove that the present {srs} collective swarm of bio-inspired agents is able to track about 65\\% of moving peaks traveling up to ten times faster than the velocity of a single individual composing that precise swarm tracking system. this emerged behavior is probably one of the most interesting ones achieved by the present work.
845	408095	article	proceedings of the national academy of sciences	\N	\N	national academy of sciences	5	96	8	1999	apr	2005-11-25 12:02:03	laboratoire de biom\\'{e}trie, g\\'{e}n\\'{e}tique et biologie des populations, unit\\'{e} mixte de recherche centre national de la recherche scientifique 5558, villeurbanne cedex, france. duret@biomserv.univ-lyon1.fr	expression pattern and, surprisingly, gene length shape codon usage in caenorhabditis, drosophila, and arabidopsis	we measured the expression pattern and analyzed codon usage in 8,133, 1,550, and 2,917 genes, respectively, from caenorhabditis elegans, drosophila melanogaster, and arabidopsis thaliana. in those three species, we observed a clear correlation between codon usage and gene expression levels and showed that this correlation is not due to a mutational bias. this provides direct evidence for selection on silent sites in those three distantly related multicellular eukaryotes. surprisingly, there is a strong negative correlation between codon usage and protein length. this effect is not due to a smaller size of highly expressed proteins. thus, for a same-expression pattern, the selective pressure on codon usage appears to be lower in genes encoding long rather than short proteins. this puzzling observation is not predicted by any of the current models of selection on codon usage and thus raises the question of how translation efficiency affects fitness in multicellular organisms.
846	409394	misc	\N	\N	\N	\N	\N	\N	\N	2006	oct	2005-11-26 20:00:25	\N	the foundations of statistical mechanics from entanglement: individual states vs. averages	statistical mechanics is one of the most successful areas of physics. yet, almost 150 years since its inception, its foundations and basic postulates are still the subject of debate. here we suggest that the main postulate of statistical mechanics, the equal a priori probability postulate, should be abandoned as misleading and unnecessary. we argue that it should be replaced by a general canonical principle, whose physical content is fundamentally different from the postulate it replaces: it refers to individual states, rather than to ensemble or time averages. furthermore, whereas the original postulate is an unprovable assumption, the principle we propose is mathematically proven. the key element in this proof is the quantum entanglement between the system and its environment. our approach separates the issue of finding the canonical state from finding out how close a system is to it, allowing us to go even beyond the usual boltzmannianÂ situation.
847	411236	article	science	\N	\N	\N	4	308	5726	2005	may	2005-11-29 14:48:06	department of ecology and evolutionary biology, university of california, irvine, ca 92697, usa.	the effects of artificial selection on the maize genome	domestication promotes rapid phenotypic evolution through artificial selection. we investigated the genetic history by which the wild grass teosinte (zea mays ssp. parviglumis) was domesticated into modern maize (z. mays ssp. mays). analysis of single-nucleotide polymorphisms in 774 genes indicates that 2 to 4\\% of these genes experienced artificial selection. the remaining genes retain evidence of a population bottleneck associated with domestication. candidate selected genes with putative function in plant growth are clustered near quantitative trait loci that contribute to phenotypic differences between maize and teosinte. if we assume that our sample of genes is representative, \\^{a}?¼1200 genes throughout the maize genome have been affected by artificial selection.
848	411662	article	bull med libr assoc	\N	\N	\N	\N	78	1	1990	jan	2005-11-30 10:26:51	\N	medical literature as a potential source of new knowledge	specialized biomedical literatures have been found that are implicitly linked by arguments that they respectively contain, but which nonetheless do not cite or refer to each other. the combined arguments lead to new inferences and conclusions that cannot be drawn from the separate literatures. one such analysis identified one set of articles showing that dietary fish oils lead to certain blood and vascular changes, and a second set containing evidence that similar changes might benefit patients with raynaud's syndrome. yet these two literatures had no articles in common and had never before been cited together; neither literature mentioned the other or suggested that dietary fish oil might benefit raynaud patients. two years after publication of that analysis, the first clinical trial demonstrating such a beneficial effect was reported independently by others. a second example of literature synthesis, based on eleven indirect connections, led to an inference that magnesium deficiency might be a causal factor in migraine headache. a third example calls attention to implicit connections between arginine intake and blood levels of somatomedins, a potentially fruitful but neglected area of research with implications for the decline with age of thymic function and protein synthesis. a model and an online search strategy to aid in identifying other logically related noninteractive literatures is described. such structures are probably not rare and may provide the foundation for a literature-based approach to scientific discovery.
849	415431	article	bioinformatics	\N	\N	oxford university press	1	16	10	2000	oct	2005-11-30 16:37:28	the sanger centre, wellcome trust genome campus, hinxton, cambridge, cb10 1sa, uk. kmr@sanger.ac.uk	artemis: sequence visualization and annotation	summary: artemis is a {dna} sequence visualization and annotation tool that allows the results of any analysis or sets of analyses to be viewed in the context of the sequence and its six-frame translation. artemis is especially useful in analysing the compact genomes of bacteria, archaea and lower eukaryotes, and will cope with sequences of any size from small genes to whole genomes. it is implemented in java, and can be run on any suitable platform. sequences and annotation can be read and written directly in {embl}, {genbank} and {gff} format.
850	415502	article	genome research	\N	\N	\N	7	12	10	2002	oct	2005-11-30 16:54:04	\N	the bioperl toolkit: perl modules for the life sciences.	the bioperl project is an international open-source collaboration of biologists, bioinformaticians, and computer scientists that has evolved over the past 7 yr into the most comprehensive library of perl modules available for managing and manipulating life-science information. bioperl provides an easy-to-use, stable, and consistent programming interface for bioinformatics application programmers. the bioperl modules have been successfully and repeatedly used to reduce otherwise complex tasks to only a few lines of code. the bioperl object model has been proven to be flexible enough to support enterprise-level applications such as {ensembl}, while maintaining an easy learning curve for novice perl programmers. bioperl is capable of executing analyses and processing results from programs such as {blast}, {clustalw}, or the {emboss} suite. interoperation with modules written in python and java is supported through the evolving {biocorba} bridge. bioperl provides access to data stores such as {genbank} and {swissprot} via a flexible series of sequence input/output modules, and to the emerging common sequence data storage format of the open bioinformatics database access project. this study describes the overall architecture of the toolkit, the problem domains that it addresses, and gives specific examples of how the toolkit can be used to solve common life-sciences problems. we conclude with a discussion of how the open-source nature of the project has contributed to the development effort.
851	416066	article	ann. statist.	\N	\N	\N	92	32	MR2060166	2004	\N	2005-11-30 18:24:46	\N	least angle regression	with discussion, and a rejoinder by the authors
852	416216	electronic	\N	\N	\N	\N	\N	\N	\N	1997	jul	2005-11-30 18:57:09	\N	quantum algorithms and the fourier transform	the quantum algorithms of deutsch, simon and shor are described in a way which highlights their dependence on the fourier transform. the general construction of the fourier transform on an abelian group is outlined and this provides a unified way of understanding the efficacy of these algorithms. finally we describe an efficient quantum factoring algorithm based on a general formalism of kitaev and contrast its structure to the ingredients of shor's algorithm.
853	418523	inproceedings	\N	ec	\N	acm press	3	\N	\N	2001	\N	2005-12-01 17:01:34	new york, ny, usa	incentives for sharing in peer-to-peer networks	we consider the free-rider problem in peer-to-peer file sharing networks such as napster: that individual users are provided with no incentive for adding value to the network. we examine the design implications of the assumption that users will selfishly act to maximize their own rewards, by constructing a formal game theoretic model of the system and analyzing equilibria of user strategies under several novel payment mechanisms. we support and extend this work with results from experiments...
854	419283	article	science (new york, n.y.)	\N	\N	\N	4	310	5753	2005	dec	2005-12-02 14:10:47	\N	the ins and outs of {dna} transfer in bacteria.	transformation and conjugation permit the passage of {dna} through the bacterial membranes and represent dominant modes for the transfer of genetic information between bacterial cells or between bacterial and eukaryotic cells. as such, they are responsible for the spread of fitness-enhancing traits, including antibiotic resistance. both processes usually involve the recognition of double-stranded {dna}, followed by the transfer of single strands. elaborate molecular machines are responsible for negotiating the passage of macromolecular {dna} through the layers of the cell surface. all or nearly all the machine components involved in transformation and conjugation have been identified, and here we present models for their roles in {dna} transport.
855	420474	inproceedings	sensor network protocols and applications, 2003. proceedings of the first ieee. 2003 ieee international workshop on	\N	\N	\N	11	\N	\N	2003	\N	2005-12-21 20:37:03	\N	data {mules}: modeling a three-tier architecture for sparse sensor networks	this paper presents and analyzes an architecture to collect sensor data in sparse sensor networks. our approach exploits the presence of mobile entities (called {mules}) present in the environment. {mules} pick up data from the sensors when in close range, buffer it, and drop off the data to wired access points. this can lead to substantial power savings at the sensors as they only have to transmit over a short range. this paper focuses on a simple analytical model for understanding performance as system parameters are scaled. our model assumes two-dimensional random walk for mobility and incorporates key system variables such as number of {mules}, sensors and access points. the performance metrics observed are the data success rate (the fraction of generated data that reaches the access points) and the required buffer capacities on the sensors and the {mules}. the modeling along with simulation results can be used for further analysis and provide certain guidelines for deployment of such systems.
856	421527	article	pervasive 2004	pervasive 2004, pervasive computing: second international conference, linz/vienna, austria	\N	springer-verlag gmbh	16	\N	\N	2004	apr	2005-12-04 02:49:28	\N	activity recognition from {user-annotated} acceleration data	abstract. in this work, algorithms are developed and evaluated to detect physical activities from data acquired using five small biaxial accelerometers worn simultaneously on different parts of the body. acceleration data was collected from 20 subjects without researcher supervision or observation. subjects were asked to perform a sequence of everyday tasks but not told specifically where or how to do them. mean, energy, frequency-domain entropy, and correlation of acceleration data was calculated and several classifiers using these features were tested. decision tree classifiers showed the best performance recognizing everyday activities with an overall accuracy rate of 84%. the results show that although some activities are recognized well with subject-independent training data, others appear to require subject-specific training data. the results suggest that multiple accelerometers aid in recognition because conjunctions in acceleration feature values can effectively discriminate many activities. with just two biaxial accelerometers â€“ thigh and wrist â€“ the recognition performance dropped only slightly. this is the first work to investigate performance of recognition algorithms with multiple, wire-free accelerometers on 20 activities using datasets annotated by the subjects themselves. 1
857	421931	article	nature	\N	\N	nature publishing group	4	439	7074	2005	dec	2005-12-25 19:08:58	\N	quasispecies diversity determines pathogenesis through cooperative interactions in a viral population	an {rna} virus population does not consist of a single genotype; rather, it is an ensemble of related sequences, termed quasispecies1, 2, 3, 4. quasispecies arise from rapid genomic evolution powered by the high mutation rate of {rna} viral replication5, 6, 7, 8. although a high mutation rate is dangerous for a virus because it results in nonviable individuals, it has been hypothesized that high mutation rates create a 'cloud' of potentially beneficial mutations at the population level, which afford the viral quasispecies a greater probability to evolve and adapt to new environments and challenges during infection4, 9, 10, 11. mathematical models predict that viral quasispecies are not simply a collection of diverse mutants but a group of interactive variants, which together contribute to the characteristics of the population4, 12. according to this view, viral populations, rather than individual variants, are the target of evolutionary selection4, 12. here we test this hypothesis by examining the consequences of limiting genomic diversity on viral populations. we find that poliovirus carrying a high-fidelity polymerase replicates at wild-type levels but generates less genomic diversity and is unable to adapt to adverse growth conditions. in infected animals, the reduced viral diversity leads to loss of neurotropism and an attenuated pathogenic phenotype. notably, using chemical mutagenesis to expand quasispecies diversity of the high-fidelity virus before infection restores neurotropism and pathogenesis. analysis of viruses isolated from brain provides direct evidence for complementation between members in the quasispecies, indicating that selection indeed occurs at the population level rather than on individual variants. our study provides direct evidence for a fundamental prediction of the quasispecies theory and establishes a link between mutation rate, population dynamics and pathogenesis.
858	421963	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2005-12-05 01:39:39	\N	ontology of folksonomy	ontologies are enabling technology for the semantic web.  they are a means for people to state what they mean by formal terms used in data that they might generate or consume.  folksonomies are an emergent phenomenon of the social web.  they are created as people associate terms with content that they generate or consume.  recently the two ideas have been put into opposition, as if they were right and left poles of a political spectrum.  this piece is an attempt to shed some cool light on the subject, and to preview some new work that applies the two ideas together to enable an internet ecology for folksonomies.
859	422273	inproceedings	\N	iptps	\N	springer-verlag	10	\N	\N	2002	\N	2005-12-05 17:29:05	london, uk	{peer-to-peer} caching schemes to address flash crowds	flash crowds can cripple a web site's performance. since they  are infrequent and unpredictable, these floods do not justify the cost of  traditional commercial solutions. we describe backslash, a collaborative  web mirroring system run by a collective of web sites that wish to protect  themselves from flash crowds. backslash is built on a distributed hash  table overlay and uses the structure of the overlay to cache aggressively  a resource that experiences an uncharacteristically high request ...
860	422282	inproceedings	\N	osdi	\N	\N	\N	\N	\N	2002	dec	2005-12-05 17:49:48	boston, massachusetts	{s}ecure routing for structured peer-to-peer overlay networks	structured peer-to-peer overlay networks provide a sub-strate for the construction of large-scale, decentralized applications, including distributed storage, group com-munication, and content distribution. these overlays are highly resilient; they can route messages correctly even when a large fraction of the nodes crash or the network partitions. but current overlays are not secure; even a small fraction of malicious nodes can prevent correct message delivery throughout the overlay. this prob-lem is particularly serious in open peer-to-peer systems, where many diverse, autonomous parties without pre-existing trust relationships wish to pool their resources. this paper studies attacks aimed at preventing correct message delivery in structured peer-to-peer overlays and presents defenses to these attacks. we describe and eval-uate techniques that allow nodes to join the overlay, to maintain routing state, and to forward messages securely in the presence of malicious nodes. 1
861	422598	article	science	\N	\N	\N	\N	260	5110	1993	may	2005-12-05 18:05:07	\N	tissue engineering	the loss or failure of an organ or tissue is one of the most frequent, devastating, and costly problems in human health care. a new field, tissue engineering, applies the principles of biology and engineering to the development of functional substitutes for damaged tissue. this article discusses the foundations and challenges of this interdisciplinary field and its attempts to provide solutions to tissue creation and repair. the loss or failure of an organ or tissue is one of the most frequent, devastating, and costly problems in human health care. a new field, tissue engineering, applies the principles of biology and engineering to the development of functional substitutes for damaged tissue. this article discusses the foundations and challenges of this interdisciplinary field and its attempts to provide solutions to tissue creation and repair.
862	423151	incollection	\N	advances in neural information processing systems 17	\N	mit press	7	\N	\N	2005	\N	2005-12-06 02:49:19	cambridge, ma	hierarchical bayesian inference in networks of spiking neurons	[bayesian inference, hidden markov model, spiking neurons, noisy integrate-and-fire model, single leve, hierarchical inference] there is growing evidence from psychophysical and neurophysiological studies that the brain utilizes bayesian principles for inference and decision making. an important open question is how bayesian inference for arbitrary graphical models can be implemented in networks of spiking neurons. in this paper, they show that recurrent networks of noisy integrate-and-fire neurons can perform approximate bayesian inference for dynamic and hierarchical graphical models. the membrane potential dynamics of neurons is used to implement belief propagation in the log domain. the spiking probability of a neuron is shown to approximate the posterior probability of the preferred state encoded by the neuron, given past inputs. they illustrate the model using two examples: (1) a motion detection network in which the spiking probability of a direction-selective neuron becomes proportional to the posterior probability of motion in a preferred direction, and (2) a two-level hierarchical network that produces attentional effects similar to those observed in visual cortical areas v2 and v4. the hierarchical model offers a new bayesian interpretation of attentional modulation in v2 and v4.
863	423826	book	\N	\N	\N	mit press	\N	\N	\N	2005	jun	2005-12-07 00:25:42	\N	handbook of computer game studies	{new media students, teachers, and professionals have long needed a comprehensive scholarly treatment of digital games that deals with the history, design, reception, and aesthetics of games along with their social and cultural context. <i>the handbook of computer game studies</i> fills this need with a definitive look at the subject from a broad range of perspectives. contributors come from cognitive science and artificial intelligence, developmental, social, and clinical psychology, history, film, theater, and literary studies, cultural studies, and philosophy as well as game design and development. the text includes both scholarly articles and journalism from such well-known voices as douglas rushkoff, sherry turkle, henry jenkins, katie salen, eric zimmerman, and others.<br /> <br /> part i considers the "prehistory" of computer games (including slot machines and pinball machines), the development of computer games themselves, and the future of mobile gaming. the chapters in part ii describe game development from the designer's point of view, including the design of play elements, an analysis of screenwriting, and game-based learning. part iii reviews empirical research on the psychological effects of computer games, and includes a discussion of the use of computer games in clinical and educational settings. part iv considers the aesthetics of games in comparison to film and literature, and part v discusses the effect of computer games on cultural identity, including gender and ethnicity. finally, part vi looks at the relation of computer games to social behavior, considering, among other matters, the inadequacy of laboratory experiments linking games and aggression and the different modes of participation in computer game culture.}
864	423871	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2005-12-07 07:09:33	\N	improved prediction of signal peptides: {signalp} 3.0.	we describe improvements of the currently most popular method for prediction of classically secreted proteins, signalp. signalp consists of two different predictors based on neural network and hidden markov model algorithms, where both components have been updated. motivated by the idea that the cleavage site position and the amino acid composition of the signal peptide are correlated, new features have been included as input to the neural network. this addition, combined with a thorough error-correction of a new data set, have improved the performance of the predictor significantly over signalp version 2. in version 3, correctness of the cleavage site predictions has increased notably for all three organism groups, eukaryotes, gram-negative and gram-positive bacteria. the accuracy of cleavage site prediction has increased in the range 6â€“17\\\\ over the previous version, whereas the signal peptide discrimination improvement is mainly due to the elimination of false-positive predictions, as well as the introduction of a new discrimination score for the neural network. the new method has been benchmarked against other available methods. predictions can be made at the publicly available web server http://www.cbs.dtu.dk/services/signalp/
865	427434	article	theoretical computer science	\N	\N	\N	152	138	\N	1994	\N	2005-12-07 10:41:56	\N	a theory of timed automata	we propose timed (finite) automata to model the behavior of real-time systems over time. our definition provides a simple, and yet powerful, way to annotate state-transition graphs with timing constraints using finitely many real-valued clocks. a timed automaton accepts timed wordsâ€“infinite sequences in which a real-valued time of occurrence is associated with each symbol. we study timed automata from the perspective of formal language theory: we consider closure properties, decision problems, and subclasses. we consider both nondeterministic and deterministic transition structures, and both bÃ¼chi and muller acceptance conditions. we show that nondeterministic timed automata are closed under union and intersection, but not under complementation, whereas deterministic timed muller automata are closed under all boolean operations. the main construction of the paper is an (pspace) algorithm for checking the emptiness of the language of a (nondeterministic) timed automaton. we also prove that the universality problem and the language inclusion problem are solvable only for the deterministic automata: both problems are undecidable (Î 11-hard) in the nondeterministic case and pspace-complete in the deterministic case. finally, we discuss the application of this theory to automatic verification of real-time requirements of finite-state systems.
866	429981	article	futures	\N	\N	\N	18	38	1	2006	feb	2005-12-07 12:07:44	\N	navigating towards sustainable development: a system dynamics approach	traditional fragmented and mechanistic science is unable to cope with issues about sustainability, as these are often related to complex, self-organizing systems. in the paper, sustainable development is seen as an unending process defined neither by fixed goals nor by specific means of achieving them. it is argued that, in order to understand the sources of and the solutions to modern problems, linear and mechanistic thinking must give way to non-linear and organic thinking, more commonly referred to as systems thinking. system dynamics, which operates in a whole-system fashion, is put forward as a powerful methodology to deal with issues of sustainability. examples of successful applications are given. any system in which humans are involved is characterized by the following essential system properties: bounded rationality, limited certainty, limited predictability, indeterminate causality, and evolutionary change. we need to resort to an adaptive approach, where we go through a learning process and modify our decision rules and our mental models of the real world as we go along. this will enable us to improve system performance by setting dynamic improvement goals (moving targets) for it. finally, it is demonstrated how causal loop diagrams can be used to find the leverage points of a system.
867	430074	book	\N	\N	\N	sage publications	\N	\N	\N	1999	dec	2005-12-07 17:27:28	\N	designing qualitative research.	{<p>"i chose catherine marshall and gretchen rossman because of its accessibility and writing style. it is very reader friendly and mixes examples with interpretive commentary. i like this approach and my students seem to appreciate this style."</p><p>---walter heinecke, university of virginia</p><p>the complexities and conundrums that are a part of the qualitative research enterprise demand a solid, well-planned research design. in this new edition of a best-seller, authors catherine marshall and gretchen b. rossman continue to provide clear and direct guidance for writing successful proposals that fit into the framework of qualitative research. with new material, including expanded coverage of focus groups, action research, and interviewing, <b>designing qualitative research, third editio</b>n is full of vignettes that illustrate the methodological challenges facing the researcher. this text will continue to be an invaluable resource to teachers and students of research methods across the disciplines and a must for the library of those using qualitative approaches. </p> }
868	430293	inproceedings	\N	proceedings of acm intelligent user interfaces conference	\N	\N	7	\N	\N	2001	\N	2005-12-08 03:40:26	santa fe, new mexico, usa	implicit interest indicators	recommender systems provide personalized suggestions about items that users will find interesting. typically, recommender systems require a user interface that can ``intelligently'' determine the interest of a user and use this information to make suggestions. the common solution, ``explicit ratings'', where users tell the system what they think about a piece of information, is well-understood and fairly precise. however, having to stop to enter explicit ratings can alter normal patterns of browsing and reading. a more ``intelligent'' method is to use implicit ratings, where a rating is obtained by a method other than obtaining it directly from the user. these implicit interest indicators have obvious advantages, including removing the cost of the user rating, and that every user interaction with the system can contribute to an implicit rating. current recommender systems mostly do not use implicit ratings, nor is the ability of implicit ratings to predict actual user interest well-understood. this research studies the correlation between various implicit ratings and the explicit rating for a single web page. a web browser was developed to record the user's actions (implicit ratings) and the explicit rating of a page. actions included mouse clicks, mouse movement, scrolling and elapsed time. this browser was used by over 80 people that browsed more than 2500 web pages. using the data collected by the browser, the individual implicit ratings and some combinations of implicit ratings were analyzed and compared with the explicit rating. we found that the time spent on a page, the amount of scrolling on a page and the combination of time and scrolling had a strong correlation with explicit interest, while individual scrolling methods and mouse-clicks were ineffective in predicting explicit interest.
869	430299	inproceedings	\N	sigir	\N	acm	7	\N	\N	2002	\N	2005-12-08 03:40:26	new york, ny, usa	methods and metrics for cold-start recommendations	we have developed a method for recommending items that combines content and collaborative data under a single probabilistic framework. we benchmark our algorithm against a na&iuml;ve bayes classifier on the   cold-start  problem, where we wish to recommend items that no one in the community has yet rated. we systematically explore three testing methodologies using a publicly available data set, and explain how these methods apply to specific real-world applications. we advocate heuristic recommenders when benchmarking to give competent baseline performance. we introduce a new performance metric, the croc curve, and demonstrate empirically that the various components of our testing strategy combine to obtain deeper understanding of the performance characteristics of recommender systems. though the emphasis of our testing is on  cold-start  recommending, our methods for recommending and evaluation are general.
870	431197	article	evolutionary computation, ieee transactions on	evolutionary computation, ieee transactions on	\N	\N	15	6	1	2002	\N	2005-12-09 09:55:41	\N	the particle swarm - explosion, stability, and convergence in a multidimensional complex space	the particle swarm is an algorithm for finding optimal regions of complex search spaces through the interaction of individuals in a population of particles. this paper analyzes a particle's trajectory as it moves in discrete time (the algebraic view), then progresses to the view of it in continuous time (the analytical view). a five-dimensional depiction is developed, which describes the system completely. these analyses lead to a generalized model of the algorithm, containing a set of coefficients to control the system's convergence tendencies. some results of the particle swarm optimizer, implementing modifications derived from the analysis, suggest methods for altering the original algorithm in ways that eliminate problems and increase the ability of the particle swarm to find optima of some well-studied test functions
871	431383	inproceedings	\N	\N	\N	\N	9	\N	\N	-1	\N	2005-12-09 13:43:24	\N	data analysis with bayesian networks a bootstrap approach	in recent years there has been significant  progress in algorithms and methods for inducing  bayesian networks from data. however, in complex  data analysis problems, we need to go beyond  being satisfied with inducing networks with  high scores. we need to provide confidence measures  on features of these networks: is the existence  of an edge between two nodes warranted?  is the markov blanket of a given node robust?  can we say something about the ordering of the  variables? we should be able to address these  questions, even when the amount of data is not  enough to induce a high scoring network. in this  paper we propose efron's bootstrap as a computationally  efficient approach for answering these  questions. in addition, we propose to use these  confidence measures to induce better structures  from the data, and to detect the presence of latent  variables.
872	431442	article	nat rev genet.	\N	\N	\N	13	6	8	2005	aug	2005-12-09 13:43:26	\N	{dna} methylation and human disease	dna methylation is a crucial epigenetic modification of the genome that is involved in regulating many cellular processes. these include embryonic development, transcription, chromatin structure, x chromosome inactivation, genomic imprinting and chromosome stability. consistent with these important roles, a growing number of human diseases have been found to be associated with aberrant dna methylation. the study of these diseases has provided new and fundamental insights into the roles that dna methylation and other epigenetic modifications have in development and normal cellular homeostasis.
873	431502	article	journal of the american statistical association	\N	\N	\N	10	97	457	2002	\N	2005-12-09 13:43:27	\N	comparison of discrimination methods for the classification of tumors using gene expression data	a reliable and precise classification of tumors is essential for successful treatment of cancer. cdna microarrays and high-density oligonucleotide chips are novel biotechnologies which are being used increasingly in cancer research. by allowing the monitoring of expression levels for thousands of genes simultaneously, such techniques may lead to a more complete understanding of the molecular variations among tumors and hence to a finer and more informative classification. the ability to successfully distinguish between tumor classes (already known or yet to be discovered) using gene expression data is an important aspect of this novel approach to cancer classification. in this talk, we compare the performance of different discrimination methods for the classification of tumors based on gene expression profiles. these methods include: nearest-neighbor classifiers, linear discriminant analysis, and classification trees. in our comparison, we also consider recent machine learning approaches for aggregating predictors such as bagging and boosting. the methods are applied to three recently published datasets: the leukemia (all/aml) dataset of golub et al. (1999), the lymphoma dataset of alizadeh et al. (2000), and the 60 cancer cell line (nci 60) dataset of ross et al. (2000).
874	431521	article	cancer cell	\N	\N	\N	6	1	\N	2002	\N	2005-12-09 13:43:27	\N	gene expression correlates of clinical prostate cancer behavior	prostate tumors are among the most heterogeneous of cancers, both histologically and clinically. microarray expression analysis was used to determine whether global biological differences underlie common pathological features of prostate cancer and to identify genes that might anticipate the clinical behavior of this disease. while no expression correlates of age, serum prostate specific antigen (psa), and measures of local invasion were found, a set of genes was identified that strongly correlated with the state of tumor differentiation as measured by gleason score. moreover, a model using gene expression data alone accurately predicted patient outcome following prostatectomy. these results support the notion that the clinical behavior of prostate cancer is linked to underlying gene expression differences that are detectable at the time of diagnosis.
875	431525	article	nature	\N	\N	\N	6	415	\N	2002	\N	2005-12-09 13:43:27	\N	gene expression profiling predicts clinical outcome of breast cancer	breast cancer patients with the same stage of disease can have markedly different treatment responses and overall outcome. the strongest predictors for metastases (for example, lymph node status and histological grade) fail to classify accurately breast tumours according to their clinical behaviour. chemotherapy or hormonal therapy reduces the risk of distant metastases by approximately one-third; however, 70-80\\% of patients receiving this treatment would have survived without it. none of the signatures of breast cancer gene expression reported to date allow for patient-tailored therapy strategies. here we used dna microarray analysis on primary breast tumours of 117 young patients, and applied supervised classification to identify a gene expression signature strongly predictive of a short interval to distant metastases ('poor prognosis' signature) in patients without tumour cells in local lymph nodes at diagnosis (lymph node negative). in addition, we established a signature that identifies tumours of brca1 carriers. the poor prognosis signature consists of genes regulating cell cycle, invasion, metastasis and angiogenesis. this gene expression profile will outperform all currently used clinical parameters in predicting disease outcome. our findings provide a strategy to select patients who would benefit from adjuvant therapy.
876	431532	article	proc. natl. acad. sci. u.s.a.	\N	\N	\N	5	\N	98	2001	\N	2005-12-09 13:43:27	\N	image metrics in the statistical analysis of {dna} microarray data	dna microarrays represent an important new method for determining the complete expression profile of a cell. in ``spotted'' microarrays, slides carrying spots of target dna are hybridized to fluorescently labeled cdna from experimental and control cells and the arrays are imaged at two or more wavelengths. in this paper, we perform statistical analysis on images of microarrays and show that quantitating the amount of fluorescent dna bound to microarrays is subject to considerable uncertainty because of large and small-scale intensity fluctuations within spots, nonadditive background, and fabrication artifacts. pixel-by-pixel analysis of individual spots can be used to estimate these sources of error and establish the precision and accuracy with which gene expression ratios are determined. simple weighting schemes based on these estimates are effective in improving significantly the quality of microarray data as it accumulates in a multiexperiment database. we propose that error estimates from image-based metrics should be one component in an explicitly probabilistic scheme for the analysis of dna microarray data.
877	431542	article	biostatistics	\N	\N	\N	18	\N	2	2001	\N	2005-12-09 13:43:27	\N	experimental design for gene expression microarrays	we examine experimental design issues arising with gene expression microarray technology. microarray experiments have multiple sources of variation, and experimental plans should ensure that effects of interest are not confounded with ancillary effects. a commonly used design is shown to violate this principle and to be generally inefficient. we explore the connection between microarray designs and classical block design and use a family of anova models as a guide to choosing a design. we combine principles of good design and a-optimality to give a general set of recommendations for design with microarrays. these recommendations are illustrated in detail for one kind of experimental objective, where we also give the results of a computer search for good designs. 10.1093/biostatistics/2.2.183
878	431561	article	proc. natl. acad. sci. u.s.a.	\N	\N	\N	5	98	\N	2001	\N	2005-12-09 13:43:27	\N	predicting the clinical status of human breast cancer by using gene expression profiles	10.1073/pnas.201162998 prognostic and predictive factors are indispensable tools in the treatment of patients with neoplastic disease. for the most part, such factors rely on a few specific cell surface, histological, or gross pathologic features. gene expression assays have the potential to supplement what were previously a few distinct features with many thousands of features. we have developed bayesian regression models that provide predictive capability based on gene expression data derived from dna microarray analysis of a series of primary breast cancer samples. these patterns have the capacity to discriminate breast tumors on the basis of estrogen receptor status and also on the categorized lymph node status. importantly, we assess the utility and validity of such models in predicting the status of tumors in crossvalidation determinations. the practical value of such approaches relies on the ability not only to assess relative probabilities of clinical outcomes for future samples but also to provide an honest assessment of the uncertainties associated with such predictive classifications on the basis of the selection of gene subsets for each validation analysis. this latter point is of critical importance in the ability to apply these methodologies to clinical assessment of tumor phenotype.
879	431562	article	j. comp. biol.	\N	\N	\N	12	\N	8	2001	\N	2005-12-09 13:43:27	\N	assessing gene significance from {cdna} microarray expression data via mixed models	the determination of a list of differentially expressed genes is a basic objective in many cdna microarray experiments. we present a statistical approach that allows direct control over the percentage of false positives in such a list and, under certain reasonable assumptions, improves on existing methods with respect to the percentage of false negatives. the method accommodates a wide variety of experimental designs and can simultaneously assess significant differences between multiple types of biological samples. two interconnected mixed linear models are central to the method and provide a flexible means to properly account for variability both across and within genes. the mixed model also provides a convenient framework for evaluating the statistical power of any particular experimental design and thus enables a researcher to a priori select an appropriate number of replicates. we also suggest some basic graphics for visualizing lists of significant genes. analyses of published experiments studying human cancer and yeast cells illustrate the results.
880	431565	article	bioinf: bioinformatics	\N	\N	\N	\N	17	\N	2001	\N	2005-12-09 13:43:27	\N	validating clustering for gene expression data	motivation: many clustering algorithms have been proposed for the analysis of gene expression data, but little guidance is available to help choose among them. we provide a systematic framework for assessing the results of clustering algorithms. clustering algorithms attempt to partition the genes into groups exhibiting similar patterns of variation in expression level. our methodology is to apply a clustering algorithm to the data from all but one experimental condition. the remaining condition is used to assess the predictive power of the resulting clusters-meaningful clusters should exhibit less variation in the remaining condition than clusters formed by chance. results: we successfully applied our methodology to compare six clustering algorithms on four gene expression data sets. we found our quantitative measures of cluster quality to be positively correlated with external standards of cluster quality.
881	431569	article	proc. natl. acad. sci. u.s.a.	\N	\N	\N	5	\N	97	2000	\N	2005-12-09 13:43:27	\N	singular value decomposition for genome-wide expression data processing and modeling	we describe the use of singular value decomposition in transforming genome-wide expression data from genes 3 arrays space to reduced diagonalized ''eigengenes'' 3 ''eigenarrays'' space, where the eigengenes (or eigenarrays) are unique orthonormal superpositions of the genes (or arrays). normalizing the data by filtering out the eigengenes (and eigenarrays) that are inferred to represent noise or experimental artifacts enables meaningful comparison of the expression of different genes across different arrays in different experiments. sorting the data according to the eigengenes and eigenarrays gives a global picture of the dynamics of gene expression, in which individual genes and arrays appear to be classified into groups of similar regulation and function, or similar cellular state and biological phenotype, respectively. after normalization and sorting, the significant eigengenes and eigenarrays can be associated with observed genome-wide effects of regulators, or with measured samples, in which these regulators are overactive or underactive, respectively.
882	431582	article	j. comp. biol.	\N	\N	\N	18	\N	7	2000	\N	2005-12-09 13:43:27	\N	analysis of variance for gene expression microarray data	spotted {cdna} microarrays are emerging as a powerful and cost-effective tool for large-scale analysis of gene expression. microarrays can be used to measure the relative quantities of specific {mrnas} in two or more tissue samples for thousands of genes simultaneously. while the power of this technology has been recognized, many open questions remain about appropriate analysis of microarray data. one question is how to make valid estimates of the relative expression for genes that are not biased by ancillary sources of variation. recognizing that there is inherent "noise" in microarray data, how does one estimate the error variation associated with an estimated change in expression, i.e., how does one construct the error bars? we demonstrate that {anova} methods can be used to normalize microarray data and provide estimates of changes in gene expression that are corrected for potential confounding effects. this approach establishes a framework for the general analysis and interpretation of microarray data.
883	431585	article	proc. natl. acad. sci. u.s.a.	\N	\N	\N	5	\N	97	2000	\N	2005-12-09 13:43:28	\N	importance of replication in microarray gene expression studies: statistical methods and evidence from repetitive {cdna} hybridizations	we present statistical methods for analyzing replicated cdna microarray expression data and report the results of a controlled experiment. the study was conducted to investigate inherent variability in gene expression data and the extent to which replication in an experiment produces more consistent and reliable findings. we introduce a statistical model to describe the probability that mrna is contained in the target sample tissue, converted to probe, and ultimately detected on the slide. we also introduce a method to analyze the combined data from all replicates. of the 288 genes considered in this controlled experiment, 32 would be expected to produce strong hybridization signals because of the known presence of repetitive sequences within them. results based on individual replicates, however, show that there are 55, 36, and 58 highly expressed genes in replicates 1, 2, and 3, respectively. on the other hand, an analysis by using the combined data from all 3 replicates reveals that only 2 of the 288 genes are incorrectly classified as expressed. our experiment shows that any single microarray output is subject to substantial variability. by pooling data from replicates, we can provide a more reliable analysis of gene expression data. therefore, we conclude that designing experiments with replications will greatly reduce misclassification rates. we recommend that at least three replicates be used in designing experiments by using cdna microarrays, particularly when gene expression data from single specimens are being analyzed.
884	431587	misc	\N	\N	\N	\N	\N	\N	\N	2000	\N	2005-12-09 13:43:28	\N	principal components analysis to summarize microarray experiments: application to sporulation time series	a series of microarray experiments produces observations of differential expression for thousands of genes across multiple conditions. it is often not clear whether a set of experiments are measuring fundamentally different gene expression states or are measuring similar states created through different mechanisms. it is useful, therefore, to define a core set of independent features for the expression states that allow them to be compared directly. principal components analysis (pca) is a statistical technique for determining the key variables in a multidimensional data set that explain the differences in the observations, and can be used to simplify the analysis and visualization of multidimensional data sets. we show that application of pca to expression data (where the experimental conditions are the variables, and the gene expression measurements are the observations) allows us to summarize the ways in which gene responses vary under different conditions. examination of the components also provides insight into the underlying factors that are measured in the experiments. we applied pca to the publicly released yeast sporulation data set (chu et al. 1998). in that work, 7 different measurements of gene expression were made over time. pca on the time-points suggests that much of the observed variability in the experiment can be summarized in just 2 components--i.e. 2 variables capture most of the information. these components appear to represent (1) overall induction level and (2) change in induction level over time. we also examined the clusters proposed in the original paper, and show how they are manifested in principal component space. our results are available on the internet at http: inverted question markwww.smi.stanford.edu/project/helix/pcarray .
885	431588	article	nat genet.	\N	\N	\N	\N	24	3	2000	mar	2005-12-09 13:43:28	\N	systematic variation in gene expression patterns in human cancer cell lines	we used cdna microarrays to explore the variation in expression of approximately 8,000 unique genes among the 60 cell lines used in the national cancer institute's screen for anti-cancer drugs. classification of the cell lines based solely on the observed patterns of gene expression revealed a correspondence to the ostensible origins of the tumours from which the cell lines were derived. the consistent relationship between the gene expression patterns and the tissue of origin allowed us to recognize outliers whose previous classification appeared incorrect. specific features of the gene expression patterns appeared to be related to physiological properties of the cell lines, such as their doubling time in culture, drug metabolism or the interferon response. comparison of gene expression patterns in the cell lines to those observed in normal breast tissue or in breast tumour specimens revealed features of the expression patterns in the tumours that had recognizable counterparts in specific cell lines, reflecting the tumour, stromal and inflammatory components of the tumour tissue. these results provided a novel molecular characterization of this important group of human cell lines and their relationships to tumours in vivo.
886	431591	article	nature genetics	\N	\N	\N	8	24	\N	2000	mar	2005-12-09 13:43:28	\N	a gene expression database for the molecular pharmacology of cancer	we used cdna microarrays to assess gene expression profiles in 60 human cancer cell lines used in a drug discovery screen by the national cancer institute. using these data, we linked bioinformatics and chemoinformatics by correlating gene expression and drug activity patterns in the nci60 lines. clustering the cell lines on the basis of gene expression yielded relationships very different from those obtained by clustering the cell lines on the basis of their response to drugs. gene-drug relationships for the clinical agents 5-fluorouracil and l-asparaginase exemplify how variations in the transcript levels of particular genes relate to mechanisms of drug sensitivity and resistance. this is the first study to integrate large databases on gene expression and molecular pharmacology.
887	431595	misc	\N	\N	\N	\N	\N	\N	\N	2000	\N	2005-12-09 13:43:28	\N	estimating the number of clusters in a dataset via the gap statistic	we propose a method (the \\gap statistic&#034;) for estimating the number of clusters (groups) in a set of data. the technique uses the output of any clustering algorithm (e.g. k-means or hierarchical), comparing the change in within cluster dispersion to that expected under an appropriate reference null distribution. some theory is developed for the proposal and a simulation study that shows that the gap statistic usually outperforms other methods that have been proposed in the literature. we also briey explore application of the same technique to the problem for estimating the number of linear principal components. 1 introduction  cluster analysis is an important tool for \\unsupervised&#034; learning  the problem of nding groups in data without the help of a response variable. a major challenge in cluster analysis is estimation of the optimal number of \\clusters&#034;. figure 1 (top right) shows a typical plot of an error measure w k (the within cluster dispersion dened below) for a clustering pr...
888	431610	article	nature genetics chipping forecast	\N	\N	\N	4	21	\N	1999	\N	2005-12-09 13:43:28	\N	high density synthetic oligonucleotide arrays	experimental genomics involves taking advantage of sequence information to investigate and understand the workings of genes, cells and organisms. we have developed an approach in which sequence information is used directly to design high- density, two-dimensional arrays of synthetic oligonucleotides. the genechip(r) probe arrays are made using spatially patterned, light-directed combinatorial chemical synthesis, and contain up to hundreds of thousands of different oligonucleotides on a small glass surface. the arrays have been designed and used for quantitative and highly parallel measurements of gene expression, to discover polymorphic loci and to detect the presence of thousands of alternative alleles. here, we describe the fabrication of the arrays, their design and some specific: applications to high-throughput genetic and cellular analysis.
889	431627	unpublished	\N	\N	\N	\N	\N	\N	\N	1998	\N	2005-12-09 13:43:28	\N	{semi-supervised} support vector machines	we introduce a semi-supervised support vector machine (s  3  vm)  method. given a training set of labeled data and a working set  of unlabeled data, s  3  vm constructs a support vector machine using  both the training and working sets. we use s  3  vm to solve  the transduction problem using overall risk minimization (orm)  posed by vapnik. the transduction problem is to estimate the  value of a classification function at the given points in the working  set. this contrasts with the standard inductive learning problem  of estimating the classification function at all possible values and  then using the fixed function to deduce the classes of the working  set data. we propose a general s  3  vm model that minimizes both  the misclassification error and the function capacity based on all  the available data. we show how the s  3  vm model for 1-norm linear  support vector machines can be converted to a mixed-integer  program and then solved exactly using integer programming. results  of s  3  vm and the standard 1-norm support vector machine  approach are compared on eleven data sets. our computational  results support the statistical learning theory results showing that  incorporating working data improves generalization when insu#-  cient training information is available. in every case, s  3  vm either  improved or showed no significant di#erence in generalization compared  to the traditional approach.  # this paper has been accepted for publication in proceedings of neural information processing systems, denver, 1998.  1
890	431628	article	ieee transactions on pattern analysis and machine inteligence	\N	\N	\N	12	20	3	1998	mar	2005-12-09 13:43:28	\N	a hierarchical latent variable model for data visualization	visualization has proven to be a powerful and widely-applicable tool for the analysis and interpretation of multi-variate data. most visualization algorithms aim to find a projection from the data space down to a two-dimensional visualization space. however, for complex data sets living in a high-dimensional space it is unlikely that a single two-dimensional projection can reveal all of the interesting structure. we therefore introduce a hierarchical visualization algorithm which allows the complete data set to be visualized at the top level, with clusters and sub-clusters of data points visualized at deeper levels. the algorithm is based on a hierarchical mixture of latent variable models, whose parameters are estimated using the expectation-maximization algorithm. we demonstrate the principle of the approach on a toy data set, and we then apply the algorithm to the visualization of a synthetic data set in 12 dimensions obtained from a simulation of multi-phase ,ows in oil pipelines, and to data in 36 dimensions derived from satellite images. a matlab* software implementation of the algorithm is publicly available from the world-wide web.^l a hierarchical latent variable model for data visualization
891	431650	book	\N	\N	\N	springer verlag	\N	\N	\N	1997	\N	2005-12-09 13:43:28	new york	matrix analysis	{linear algebra and matrix theory have long been fundamental tools in mathematical disciplines as well as fertile fields for research. in this book the authors present classical and recent results of matrix analysis that have proved to be important to applied mathematics. facts about matrices, beyond those found in an elementary linear algebra course, are needed to understand virtually any area of mathematical science, but the necessary material has appeared only sporadically in the literature and in university curricula. as interest in applied mathematics has grown, the need for a text and reference offering a broad selection of topics in matrix theory has become apparent, and this book meets that need.    this volume reflects two concurrent views of matrix analysis. first, it encompasses topics in linear algebra that have arisen out of the needs of mathematical analysis. second, it is an approach to real and complex linear algebraic problems that does not hesitate to use notions from analysis. both views are reflected in its choice and treatment of topics.}
892	431667	article	proc. natl. acad. sci. u.s.a.	\N	\N	\N	5	94	\N	1997	\N	2005-12-09 13:43:28	\N	yeast microarrays for genome wide parallel genetic and gene expression analysis	{{w}e have developed high-density {dna} microarrays of yeast {orf}s. {t}hese microarrays can monitor hybridization to {orf}s for applications such as quantitative differential gene expression analysis and screening for sequence polymorphisms. {a}utomated scripts retrieved sequence information from public databases to locate predicted {orf}s and select appropriate primers for amplification. {t}he primers were used to amplify yeast {orf}s in 96-well plates, and the resulting products were arrayed using an automated micro arraying device. {a}rrays containing up to 2,479 yeast {orf}s were printed on a single slide. {t}he hybridization of fluorescently labeled samples to the array were detected and quantitated with a laser confocal scanning microscope. {a}pplications of the microarrays are shown for genetic and gene expression analysis at the whole genome level.}
893	431678	inproceedings	\N	proceedings, 13th intl.\\ conf.\\ on machine learning	\N	morgan kaufmann	6	\N	\N	1996	\N	2005-12-09 13:43:29	san mateo, ca	simplified support vector decision rules	a support vector machine (svm) is a universal learning machine whose decision surface is parameterized by a set of support vectors, and by a set of corresponding weights. an svm is also characterized by a kernel  function. choice of the kernel determines whether the resulting svm is a polynomial classifier, a two-layer neural network, a radial basis function machine, or some other learning machine. svms are currently considerably slower in test phase than other approaches with similar...
894	431690	techreport	\N	\N	\N	\N	\N	\N	11	1995	\N	2005-12-09 13:43:29	z{\\"u}rich, switzerland	a comparison of selection schemes used in genetic algorithms	genetic algorithms are a common probabilistic optimization method based on the model of natural evolution. one important operator in these algorithms is the selection scheme for which in this paper a new description model is introduced. with this a mathematical analysis of tournament selection, ranking selection and truncation selection is carried out that allows an exact prediction of the fitness values after selection. furthermore several new properties of selection schemes are derived and...
895	431698	inproceedings	\N	{ijcai}	\N	\N	8	\N	\N	1995	\N	2005-12-09 13:43:29	\N	a study of {cross-validation} and bootstrap for accuracy estimation and model selection	we review accuracy estimation methods and compare the two most common methods: crossvalidation and bootstrap. recent experimental results on artificial data and theoretical results in restricted settings have shown that for selecting a good classifier from a set of classifiers (model selection), ten-fold cross-validation may be better than the more expensive leaveone -out cross-validation. we report on a largescale experiment---over half a million runs of c4.5 and a naive-bayes algorithm---to estimate the effects of different parameters on these algorithms on real-world datasets. for crossvalidation, we vary the number of folds and whether the folds are stratified or not; for bootstrap, we vary the number of bootstrap samples. our results indicate that for real-word datasets similar to ours, the best method to use for model selection is ten-fold stratified cross validation, even if computation power allows using more folds. 1 introduction it can not be emphasized enough that no claim ...
896	431734	article	neural computation	\N	\N	\N	32	4	3	1992	\N	2005-12-09 13:43:29	\N	{b}ayesian interpolation	although bayesian analysis has been in use since laplace, the bayesian method of model--comparison has only recently been developed in depth.  in this paper, the bayesian approach to regularisation and model--comparison is demonstrated by studying the inference problem of interpolating noisy data. the concepts and methods described are quite general and can be applied to many other problems.  regularising constants are set by examining their posterior probability distribution. alternative regularisers (priors) and alternative basis sets are objectively compared by evaluating the evidence for them. `occam&#039;s razor&#039; is automatically embodied by this framework.  the way in which bayes infers the values of regularising constants and noise levels has an elegant interpretation in terms of the effective number of parameters determined by the data set. this framework is due to gull and skilling.  1 data modelling and occam&#039;s razor in science, a central task is to develop and compare models to a...
897	431738	book	\N	\N	\N	commonwealth scientific and industrial research organization.	\N	\N	\N	1992	\N	2005-12-09 13:43:29	victoria, australia	a review of evolutionary artificial neural networks	research on potential interactions between connectionist learning systems, i.e., artificial neural networks (anns), and evolutionary search procedures, like genetic algorithms (gas), has attracted a lot of attention recently. evolutionary anns (eanns) can be considered as the combination of anns and evolutionary search procedures. this paper first distinguishes among three kinds of evolution in eanns, i.e., the evolution of connection weights, of architectures and of learning rules. then it reviews each kind of evolution in detail and analyses critical issues related to different evolutions. the review shows that although a lot of work has been done on the evolution of connection weights and of architectures, few attempts have been made to understand the evolution of learning rules. interactions among different evolutions are seldom mentioned in current research. however, the evolution of learning rules and its interactions with other kinds of evolution play a vital role in eanns. as the final part, this paper briefly describes a general framework for eanns, which not only includes the aforementioned three kinds of evolution, but also considers interactions among them.
898	431744	article	science	\N	\N	\N	6	251	4995	1991	feb	2005-12-09 13:43:29	\N	light-directed, spatially addressable parallel chemical synthesis	solid-phase chemistry, photolabile protecting groups, and photolithography have been combined to achieve light-directed, spatially addressable parallel chemical synthesis to yield a highly diverse set of chemical products. binary masking, one of many possible combinatorial synthesis strategies, yields 2n compounds in n chemical steps. an array of 1024 peptides was synthesized in ten steps, and its interaction with a monoclonal antibody was assayed by epifluorescence microscopy. high-density arrays formed by light-directed synthesis are potentially rich sources of chemical diversity for discovering new ligands that bind to biological receptors and for elucidating principles governing molecular interactions. the generality of this approach is illustrated by the light-directed synthesis of a dinucleotide. spatially directed synthesis of complex compounds could also be used for microfabrication of devices. 10.1126/science.1990438
899	431745	inproceedings	\N	fga1	\N	\N	24	\N	\N	1991	\N	2005-12-09 13:43:29	\N	a comparative analysis of selection schemes used in genetic algorithms	this paper considers a number of selection schemes commonly used in modern genetic algorithms. specifically, proportionate reproduction, ranking selection, tournament selection, and genitor (or Â«steady state") selection are compared on the basis of solutions to deterministic difference or differential equations, which are verified through computer simulations. the analysis provides convenient approximate or exact solutions as well as useful convergence time and growth ratio estimates. the paper recommends practical application of the analyses and suggests a number of paths for more detailed analytical investigation of selection techniques.
900	432019	article	ieee	\N	\N	\N	\N	\N	\N	2006	\N	2005-12-09 15:07:58	\N	{rfid} security and privacy: a research survey	this article surveys recent technical research on the problems of privacy and security for {rfid} {(radio} frequency {identification).} {rfid} tags are small, wireless devices that help identify objects and people. thanks to dropping cost, they are likely to proliferate into the billions in the next several years â€“ and eventually into the trillions. {rfid} tags track objects in supply chains, and are working their way into the pockets, belongings and even the bodies of consumers. this survey examines approaches proposed by scientists for privacy protection and integrity assurance in {rfid} systems, and treats the social and technical context of their work. while geared toward the non-specialist, the survey may also serve as a reference for specialist readers.
901	435447	misc	\N	\N	\N	\N	\N	\N	\N	2004	\N	2005-12-12 02:07:19	\N	an ontology-driven framework for data transformation in scientific workflows	ecologists spend considerable effort integrating heterogeneous data for statistical analyses and simulations, for example, to run and test predictive models. our research is focused on reducing this effort by providing data integration and transformation tools, allowing researchers to focus on &#8220;real science,&#8221; that is, discovering new knowledge through analysis and modeling. this paper defines a generic framework for transforming heterogeneous data within scientific workflows. our approach relies on a formalized ontology, which serves as a simple, unstructured global schema. in the framework, inputs and outputs of services within scientific workflows can have structural types and separate semantic types (expressions of the target ontology). in addition, a registration mapping can be defined to relate input and output structural types to their corresponding semantic types. using registration mappings, appropriate data transformations can then be generated for each desired service composition. here, we describe our proposed framework and an initial implementation for services that consume and produce xml data.
902	436280	article	protein engineering	\N	\N	\N	5	14	1	2001	jan	2005-12-12 15:36:53	laboratoire de physique quantique, umr 5626 of cnrs, irsamc, universit\\'{e} paul sabatier, 118 route de narbonne, 31062 toulouse cedex, france.	conformational change of proteins arising from normal mode calculations.	a normal mode analysis of 20 proteins in `open' or `closed' forms was performed using simple potential and protein models. the quality of the results was found to depend upon the form of the protein studied, normal modes obtained with the open form of a given protein comparing better with the conformational change than those obtained with the closed form. moreover, when the motion of the protein is a highly collective one, then, in all cases considered, there is a single low-frequency normal mode whose direction compares well with the conformational change. when it is not, in most cases there is still a single low-frequency normal mode giving a good description of the pattern of the atomic displacements, as they are observed experimentally during the conformational change. hence a lot of information on the nature of the conformational change of a protein is often found in a single low-frequency normal mode of its open form. since this information can be obtained through the normal mode analysis of a model as simple as that used in the present study, it is likely that the property captured by such an analysis is for the most part a property of the shape of the protein itself. one of the points that has to be clarified now is whether or not amino acid sequences have been selected in order to allow proteins to follow a single normal mode direction, as least at the very beginning of their conformational change. 10.1093/protein/14.1.1
903	436449	inproceedings	\N	game developers conference	\N	\N	\N	\N	\N	2004	mar	2005-12-13 04:18:29	\N	why we play games: four keys to more emotion without story	people play games to change or structure their internal experiences. adults in this study, enjoy filling their heads with thoughts and emotions unrelated to work or school, others enjoy the challenge and chance to test their abilities. games offer an efficiency and order in playing that they want in life. they value the sensations from doing new things such as dirt-bike racing or flying, that they otherwise lack the skills, resources, or social permission to do. a few like to escape the real world; others enjoy escaping its social norms. nearly all enjoy the feeling of challenge and complete absorption. the exciting and relaxing effects of games is very appealing and some apply its therapeutic benefits to â€œget perspective,â€ calm down after a hard day, or build self-esteem.  direct observation reveals details about player emotion. we find emotion in playerâ€™s visceral, behavioral, cognitive, and social responses to games. players play to experience these body sensations that result from and drive their actions. some crave the increased heart rate of excitement from a race, the skin prickling sensation from wonder, or the tension of frustration followed by feelings of fiero. for others it is simply the exchange of worries and thought and feelings for relaxation and contentment or a feeling of achievement knowing they did it right.
904	436785	article	nucleic acids research	nucl. acids res.	\N	oxford university press	\N	33	20	2005	jan	2006-09-20 20:30:34	\N	evolving gene/transcript definitions significantly alter the interpretation of {genechip} data	genome-wide expression profiling is a powerful tool for implicating novel gene ensembles in cellular mechanisms of health and disease. the most popular platform for genome-wide expression profiling is the affymetrix {genechip}. however, its selection of probes relied on earlier genome and transcriptome annotation which is significantly different from current knowledge. the resultant informatics problems have a profound impact on analysis and interpretation the data. here, we address these critical issues and offer a solution. we identified several classes of problems at the individual probe level in the existing annotation, under the assumption that current genome and transcriptome databases are more accurate than those used for {genechip} design. we then reorganized probes on more than a dozen popular {genechips} into gene-, transcript- and exon-specific probe sets in light of up-to-date genome, {cdna}/{est} clustering and single nucleotide polymorphism information. comparing analysis results between the original and the redefined probe sets reveals ?30–50\\% discrepancy in the genes previously identified as differentially expressed, regardless of analysis method. our results demonstrate that the original affymetrix probe set definitions are inaccurate, and many conclusions derived from past {genechip} analyses may be significantly flawed. it will be beneficial to re-analyze existing {genechip} data with updated probe set definitions.
905	437260	unpublished	\N	\N	\N	\N	\N	\N	\N	2005	dec	2005-12-14 08:37:13	\N	the hive mind: folksonomies and {user-based} tagging	there is a revolution happening on the internet that is alive and building momentum with each passing tag. with the advent of social software and web 2.0, we usher in a new era of internet order. one in which the user has the power to effect their own online experience, and contribute to others'.
906	437770	article	nucleic acids res	\N	\N	\N	6	31	13	2003	jul	2005-12-14 14:36:49	department of computer science, university of british columbia, vancouver, bc v6t 1z4, canada.	{rnasoft}: a suite of {rna} secondary structure prediction and design software tools.	{dna} and {rna} strands are employed in novel ways in the construction of nanostructures, as molecular tags in libraries of polymers and in therapeutics. new software tools for prediction and design of molecular structure will be needed in these applications. the {rnasoft} suite of programs provides tools for predicting the secondary structure of a pair of {dna} or {rna} molecules, testing that combinatorial tag sets of {dna} and {rna} molecules have no unwanted secondary structure and designing {rna} strands that fold to a given input secondary structure. the tools are based on standard thermodynamic models of {rna} secondary structure formation. {rnasoft} can be found online at {http://www.rnasoft}.ca.
907	439126	inproceedings	principles of programming languages	\N	\N	\N	\N	\N	\N	2006	jan	2005-12-15 19:50:24	\N	modular {set-based} analysis from contracts	in plt scheme, programs consist of modules with contracts. the latter describe the inputs and outputs of functions and objects via predicates. a run-time system enforces these predicates; if a predicate fails, the enforcer raises an exception that blames a specific module with an explanation of the fault.in this paper, we show how to use such module contracts to turn set-based analysis into a fully modular parameterized analysis. using this analysis, a static debugger can indicate for any given contract check whether the corresponding predicate is always satisfied, partially satisfied, or (potentially) completely violated. the static debugger can also predict the source of potential errors, i.e., it is sound with respect to the blame assignment of the contract system.
908	439131	article	j. comput. chem.	\N	\N	wiley subscription services, inc., a wiley company	17	26	16	2005	dec	2005-12-15 21:09:26	department of cell and molecular biology, uppsala university, husargatan 3, box 596, s-75124 uppsala, sweden; stockholm bioinformatics center, scfab, stockholm university, se-10691 stockholm, sweden; max-planck institut f\\"{u}r polymerforschung, ackermannweg 10, d-55128 mainz, germany; groningen biomolecular sciences and biotechnology institute, university of groningen, nijenborgh 4, nl-9747 ag groningen, the netherlands	{gromacs}: fast, flexible, and free	this article describes the software suite {gromacs} (groningen {machine} for chemical simulation) that was developed at the university of groningen, the netherlands, in the early 1990s. the software, written in {ansi} c, originates from a parallel hardware project, and is well suited for parallelization on processor clusters. by careful optimization of neighbor searching and of inner loop performance, {gromacs} is a very fast program for molecular dynamics simulation. it does not have a force field of its own, but is compatible with {gromos}, {opls}, {amber}, and {encad} force fields. in addition, it can handle polarizable shell models and flexible constraints. the program is versatile, as force routines can be added by the user, tabulated functions can be specified, and analyses can be easily customized. nonequilibrium dynamics and free energy determinations are incorporated. interfaces with popular quantum-chemical packages ({mopac}, {games}-{uk}, {gaussian}) are provided to perform mixed {mm}/{qm} simulations. the package includes about 100 utility and analysis programs. {gromacs} is in the public domain and distributed (with source code and documentation) under the {gnu} general public license. it is maintained by a group of developers from the universities of groningen, uppsala, and stockholm, and the max planck institute for polymer research in mainz. its web site is http://www.gromacs.org. {\\copyright} 2005 wiley periodicals, inc. j comput chem 26: 1701–1718, 2005
909	440893	unpublished	\N	\N	\N	\N	\N	\N	\N	2005	dec	2005-12-18 05:56:40	\N	matching as nonparametric preprocessing for reducing model dependence in parametric causal inference	(version: 11/15/2005) although political science articles rarely include causal estimates from more than a few model specifications, authors usually choose these from numerous trial runs readers never see. given the typically large variation in estimates across choices of control variables, functional forms, and other modeling assumptions, how can researchers ensure that the few estimates presented are accurate or representative? how do readers know that publications are not merely demonstrations that the author found it possible to find a specification that fits his or her favorite hypothesis? matching methods, which offer the promise of causal inference with fewer assumptions, is one possible way forward, but the literature suffers from conflicting approaches to estimation, uncertainty, theoretical results, and practical advice. we propose a unified approach that makes it possible for researchers to preprocess data (such as with the easy-to-use software we offer) and then to apply whatever familiar parametric techniques they would have used anyway. instead of replacing existing methods, we use matching to make parametric models work better by giving more accurate and considerably less model-dependent causal inferences.
910	441049	proceedings	mobile computing systems and applications, 2003. proceedings. fifth ieee workshop on	\N	\N	\N	11	\N	\N	2003	\N	2005-12-18 16:52:20	\N	design and evaluation of a metropolitan area multitier wireless ad hoc network architecture	few real-world applications of mobile ad hoc networks have been developed or deployed outside the military environment, and no traces of actual node movement in a real ad hoc network have been available. we propose a novel commercial application of ad hoc networking, we describe and evaluate a multitier ad hoc network architecture and routing protocol for this system, and we document a new source of real mobility traces to support detailed simulation of ad hoc network applications on a large scale. the proposed system, which we call ad hoc city, is a multitier wireless ad hoc network routing architecture for general-purpose wide-area communication. the backbone network in this architecture is itself also a mobile multihop network, composed of wireless devices mounted on mobile fleets such as city buses or delivery vehicles. we evaluate our proposed design through simulation based on traces of the actual movement of the fleet of city buses in the seattle, washington metropolitan area, on their normal routes providing passenger bus service throughout the city.
911	445554	article	computer	\N	\N	ieee computer society press	8	38	5	2005	\N	2005-12-20 16:16:05	los alamitos, ca, usa	intel virtualization technology	once confined to specialized server and mainframe systems, virtualization is now supported in off-the-shelf systems based on intel architecture hardware. intel virtualization technology provides hardware support for processor virtualization, enabling simplifications of virtual machine monitor software. resulting {vmms} can support a wider range of legacy and future operating systems while maintaining high performance.
912	446839	article	media culture society	\N	\N	\N	18	27	6	2005	nov	2005-12-21 17:10:50	\N	what does the photoblog want?	theoretical accounts of photography have persistently emphasized, departed from and returned to the issue of the real, thereby positioning the real behind or at the   heart of what photography purportedly is and does. but these familiar and familiarizing consistencies in the writing about photography do not make photographs   less of a paradox at the level of being (what they are), or less equivocal at the level of their expressive content (what they mean or know). digital photography  problematizes the issues yet further even while writing about photography reasserts the familiar pieties. this article presents the results of an ethnographic study of photoblogs as a way of addresssing impasses in the literature on photography and   digital photography. blogs have become popular in the last three years as an internet-based technology for writing the self. photoblogs are a type of blog that   adds photographs to text and hyperlinks in the telling of stories. in this article, i argue that photoblogs are (1) entities that identify the repetitions  which paralyse writing about photography and (2) entities that want to position photographs as something more than an outcome, photobloggers as something  more than selves (or authors) and the photoblog as something more than technology. 10.1177/0163443705057675
913	451503	unpublished	\N	\N	\N	\N	\N	\N	\N	2000	\N	2005-12-27 18:26:28	\N	fixed point calculus	fixed point calculus is about the solution of recursive equations defined by a monotonic endofunction on a partially ordered set. this tutorial discusses applications of fixed point calculus in the construction of computer programs, beginning with standard applications and progressing to recent research. the basic properties of least and greatest fixed points are presented. well-foundedness and inductive properties of relations are expressed in terms of fixed points. a class of fixed point equations, called ``hylo'' equations, is introduced. a methodology of recursive program design based on the use of hylo equations is presented. current research on generalisations of well-foundedness and inductive properties of relations, making these properties relative to a datatype, is introduced.
914	454305	book	\N	\N	springer series in statistics	springer verlag	\N	\N	\N	2001	\N	2006-01-02 16:18:12	\N	monte {c}arlo {s}trategies in {s}cientific {c}omputing	{a large number of scientists and engineers employ monte carlo simulation and related global optimization techniques (such as simulated annealing) as an essential tool in their work. for such scientists, there is a need to keep up to date with several recent advances in monte carlo methodologies such as cluster methods, data- augmentation, simulated tempering and other auxiliary variable methods. there is also a trend in moving towards a population-based approach. all these advances in one way or another were motivated by the need to sample from very complex distribution for which traditional methods would tend to be trapped in local energy minima. it is our aim to provide a self-contained and up to date treatment of the monte carlo method to this audience.                                                                              the monte carlo method is a computer-based statistical sampling approach for solving numerical problems    concerned with a complex system. the methodology was initially developed in the field of statistical physics during the early days of electronic computing (1945-55) and has now been adopted by researchers in  almost all scientific fields. the fundamental idea for constructing markov chain based monte carlo algorithms was introduced in the 1950s. this idea was later extended to handle more and more complex     physical systems. in the 1980s, statisticians and computer scientists developed monter carlo-based algorithms for a wide variety of integration and optimization tasks. in the 1990s, the method began to play an important role in computational biology. over the past fifty years, reasearchers in diverse scientific fields have studied the monte carlo method and contributed to its development. today, a large number of  scientisits and engineers employ monte carlo techniques as an essential tool in their work. for such    scientists, there is a need to keep up-to-date with recent advances in monte carlo methodologies.}
915	454555	article	genome research	genome research	\N	cold spring harbor laboratory press	7	12	8	2002	aug	2006-01-02 18:02:59	howard hughes medical institute and department of genetics, washington university school of medicine, st. louis, missouri 63110, usa.	automated de novo identification of repeat sequence families in sequenced genomes	an international, peer-reviewed genome sciences journal featuring outstanding original research that offers novel insights into the biology of all organisms
916	456530	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2006-01-05 23:03:05	\N	{treefam}: a curated database of phylogenetic trees of animal gene families.	treefam is a database of phylogenetic trees of gene families found in animals. it aims to develop a curated resource that presents the accurate evolutionary history of all animal gene families, as well as reliable ortholog and paralog assignments. curated families are being added progressively, based on seed alignments and trees in a similar fashion to pfam. release 1.1 of treefam contains curated trees for 690 families 25 and automatically generated trees for another 11 646 families. these represent over 128 000 genes from nine fully sequenced animal genomes and over 45 000 other animal proteins from uniprot; similar to 40 - 85% of proteins encoded in the fully sequenced animal genomes are included in treefam. treefam is freely available at http://www.treefam.org and http://treefam.genomics.org.cn.
917	456945	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2006-01-05 23:03:31	\N	{snpdetector}: a software tool for sensitive and accurate {snp} detection	identification of single nucleotide polymorphisms (snps) and mutations is important for the discovery of genetic predisposition to complex diseases. pcr resequencing is the method of choice for de novo snp discovery. however, manual curation of putative snps has been a major bottleneck in the application of this method to high-throughput screening. therefore it is critical to develop a more sensitive and accurate computational method for automated snp detection. we developed a software tool, snpdetector, for automated identification of snps and mutations in fluorescence-based resequencing reads. snpdetector was designed to model the process of human visual inspection and has a very low false positive and false negative rate. we demonstrate the superior performance of snpdetector in snp and mutation analysis by comparing its results with those derived by human inspection, polyphred (a popular snp detection tool), and independent genotype assays in three large-scale investigations. the first study identified and validated inter- and intra-subspecies variations in 4,650 traces of 25 inbred mouse strains that belong to either the mus musculus species or the m. spretus species. unexpected heterozgyosity in cast/ei strain was observed in two out of 1,167 mouse snps. the second study identified 11,241 candidate snps in five encode regions of the human genome covering 2.5 mb of genomic sequence. approximately 50&#37; of the candidate snps were selected for experimental genotyping; the validation rate exceeded 95&#37;. the third study detected enu-induced mutations (at 0.04&#37; allele frequency) in 64,896 traces of 1,236 zebra fish. our analysis of three large and diverse test datasets demonstrated that snpdetector is an effective tool for genome-scale research and for large-sample clinical studies. snpdetector runs on unix/linux platform and is available publicly (http://lpg.nci.nih.gov).
918	457047	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2006-01-05 23:03:38	\N	coauthorship networks and patterns of scientific collaboration	10.1073/pnas.0307545100 by using data from three bibliographic databases in biology, physics, and mathematics, respectively, networks are constructed in which the nodes are scientists, and two scientists are connected if they have coauthored a paper. we use these networks to answer a broad variety of questions about collaboration patterns, such as the numbers of papers authors write, how many people they write them with, what the typical distance between scientists is through the network, and how patterns of collaboration vary between subjects and over time. we also summarize a number of recent results by other authors on coauthorship patterns.
919	457991	article	american journal of physics	\N	\N	aapt	8	69	1	2001	jan	2006-01-06 13:12:32	\N	an introduction to {pound-drever}-hall laser frequency stabilization	this paper is an introduction to an elegant and powerful technique in modern optics: pound&#150;drever&#150;hall laser frequency stabilization. this introduction is primarily meant to be conceptual, but it includes enough quantitative detail to allow the reader to immediately design a real setup, suitable for research or industrial application. the intended audience is both the researcher learning the technique for the first time and the teacher who wants to cover modern laser locking in an upper-level physics or electrical engineering course. &copy;2001 american association of physics teachers.
920	459612	article	trends cogn sci	\N	\N	\N	5	9	4	2005	apr	2006-01-08 04:19:09	psychology department, harvard university, cambridge, ma 02138, usa. saxe@mit.edu	against simulation: the argument from error.	according to simulation theory, to understand what is going on in another person's mind, the observer uses his or her own mind as a model of the other mind. recently, philosophers and cognitive neuroscientists have proposed that mirror neurones (which fire in response to both executing and observing a goal directed action) provide a plausible neural substrate for simulation, a mechanism for directly perceiving, or 'resonating' with, the contents of other minds. this article makes the case against simulation theory, using evidence from cognitive neuroscience, developmental psychology, and social psychology. in particular, the errors that adults and children make when reasoning about other minds are not consistent with the 'resonance' versions of simulation theory.
921	460108	article	computers \\& education	\N	\N	\N	14	34	3-4	2000	apr	2006-01-09 10:34:34	\N	designing argumentation for conceptual development	if virtual learning environments are to support real learning, they must promote effective teaching-learning processes and interactions. in this paper we describe a collaborative, computer-based framework for argumentation that supports the dialogue process in ways which stimulate belief revision leading to conceptual change and development in science. this pedagogy is specified as a prescriptive 'dialogue game', which models features of the tutorial process. within this scheme, the learner adopts the role of an 'explainer' whilst the system plays a facilitating role, and these participants collaborate to develop a shared explanatory model of a qualitative, causal domain. the design framework includes an abstract world model of a qualitative causal system, some 'commonsense' reasoning rules, an interaction language and dialogue strategies and tactics, that are co-ordinated within a facilitating dialogue game. a prototype {college} (computer based lab for language games in education) system implements the framework and operates as a dialogue modelling work-bench for demonstrating, investigating and developing the approach. an empirical study showed that students revised their beliefs and improved their explanatory models, and held to their revised and improved conceptions in a delayed post-test. in using {college} to simulate these dialogues, we found that the tutor's low-level tactical pedagogies emerged and developed reactively during the dialogues, in response to conceptual difficulties experienced by the students.
922	461067	article	genome research	\N	\N	\N	6	15	11	2005	nov	2006-01-10 15:08:25	molecular biology and genetics and computational biology, cornell university, ithaca, new york 14853, usa. ac347@cornell.edu	ascertainment bias in studies of human genome-wide polymorphism	10.1101/gr.4107905 large-scale {snp} genotyping studies rely on an initial assessment of nucleotide variation to identify sites in the {dna} sequence that harbor variation among individuals. this {\\^{a}??snp} discovery\\^{a}?? sample may be quite variable in size and composition, and it has been well established that properties of the {snps} that are found are influenced by the discovery sampling effort. the international {hapmap} project relied on nearly any piece of information available to identify {snps\\^{a}}??including {bac} end sequences, shotgun reads, and differences between public and private sequences\\^{a}??and even made use of chimpanzee data to confirm human sequence differences. in addition, the ascertainment criteria shifted from using only {snps} that had been validated in population samples, to double-hit {snps}, to finally accepting {snps} that were singletons in small discovery samples. in contrast, perlegen's primary discovery was a resequencing-by-hybridization effort using the 24 people of diverse origin in the polymorphism discovery resource. here we take these two data sets and contrast two basic summary statistics, heterozygosity and , as well as the site frequency spectra, for 500-kb windows spanning the genome. the magnitude of disparity between these samples in these measures of variability indicates that population genetic analysis on the raw genotype data is ill advised. given the knowledge of the discovery samples, we perform an ascertainment correction and show how the post-correction data are more consistent across these studies. however, discrepancies persist, suggesting that the heterogeneity in the {snp} discovery process of the {hapmap} project resulted in a data set resistant to complete ascertainment correction. ascertainment bias will likely erode the power of tests of association between {snps} and complex disorders, but the effect will likely be small, and perhaps more importantly, it is unlikely that the bias will introduce false-positive inferences.
923	462159	electronic	\N	\N	\N	\N	\N	\N	\N	2000	apr	2006-01-11 16:06:50	\N	a theory of universal artificial intelligence based on algorithmic complexity	decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental prior probability distribution is known. solomonoff's theory of universal induction formally solves the problem of sequence prediction for unknown prior distribution. we combine both ideas and get a parameterless theory of universal artificial intelligence. we give strong arguments that the resulting aixi model is the most intelligent unbiased agent possible. we outline for a number of problem classes, including sequence prediction, strategic games, function minimization, reinforcement and supervised learning, how the aixi model can formally solve them. the major drawback of the aixi model is that it is uncomputable. to overcome this problem, we construct a modified algorithm aixi-tl, which is still effectively more intelligent than any other time t and space l bounded agent. the computation time of aixi-tl is of the order tx2^l. other discussed topics are formal definitions of intelligence order relations, the horizon problem and relations of the aixi theory to other ai approaches.
924	463013	book	\N	\N	\N	academic press	\N	\N	\N	2004	sep	2006-01-12 13:27:27	\N	handbook of {mri} pulse sequences	{this indispensable guide gives concise yet comprehensive descriptions of the pulse sequences commonly used on modern mri scanners.  the book consists of a total of 65 self-contained sections, each focused on a single subject.  written primarily for scientists, engineers, radiologists, and graduate students who are interested in an in-depth understanding of various mri pulse sequences, it serves readers with a diverse set of backgrounds by providing both non-mathematical and mathematical descriptions. <br><br>the book is divided into five parts.  part i of the book describes two mathematical tools, fourier transforms and the rotating reference frame, that are useful for understanding mri pulse sequences.  the second part is devoted to a wide variety of radiofrequency (rf) pulses, and the third part focuses on gradient waveforms.  data acquisition, image reconstruction, and physiological monitoring related to pulse sequence design form the subject of part iv of the book.   once this foundation is established, part v of the book describes the underlying principles, implementation, and selected applications of many pulse sequences commonly in use today. <br><br>the extensive topic coverage and cross-referencing makes this book ideal for beginners learning the building blocks of mri pulse sequence design, as well as for experienced professionals who are seeking deeper knowledge of a particular technique.<br><br>\\&\\#183;explains pulse sequences, their components, and the associated image reconstruction methods commonly used in mri<br>\\&\\#183;provides self-contained sections for individual techniques<br>\\&\\#183;can be used as a quick reference guide or as a resource for deeper study<br>\\&\\#183;includes both non-mathematical and mathematical descriptions <br>\\&\\#183;contains numerous figures, tables, references, and worked example problems}
925	463317	book	\N	\N	\N	harvard university press	\N	\N	\N	1996	apr	2006-01-12 17:38:23	\N	aramis, or, the love of technology	a guided-transportation system intended for paris, aramis represented a major advance in personal rapid transit: it combined the efficiency of a subway with the flexibility of an automobile. but in the end, its electronic couplings proved too complex and expensive, the political will failed, and the project died in 1987. the story of aramis is told by several different parties, none of which take precedence over any other: a young engineer and his professor, who act as detective to ferret out the reasons for the project's failure; company executives and elected officials; a sociologist; and finally aramis itself, who delivers a passionate plea: technological innovation has needs and desires, especially a desire to be born, but cannot live without the sustained commitment of those who have created it.
926	463786	article	science (new york, n.y.)	\N	\N	\N	5	299	5611	2003	feb	2006-01-13 02:00:45	department of ecology and evolutionary biology, brown university, providence, ri 02912, usa.	the endocrine regulation of aging by insulin-like signals.	reduced signaling of insulin-like peptides increases the life-span of nematodes, flies, and rodents. in the nematode and the fly, secondary hormones downstream of insulin-like signaling appear to regulate aging. in mammals, the order in which the hormones act is unresolved because insulin, insulin-like growth factor-1, growth hormone, and thyroid hormones are interdependent. in all species examined to date, endocrine manipulations can slow aging without concurrent costs in reproduction, but with inevitable increases in stress resistance. despite the similarities among mammals and invertebrates in insulin-like peptides and their signal cascade, more research is needed to determine whether these signals control aging in the same way in all the species by the same mechanism.
927	464432	article	cell	\N	\N	\N	14	124	1	2006	jan	2006-01-13 14:47:00	\N	synaptic protein synthesis associated with memory is regulated by the {risc} pathway in drosophila	{summarylong}-lasting forms of memory require protein synthesis, but how the pattern of synthesis is related to the storage of a memory has not been determined. here we show that neural activity directs the {mrna} of the drosophila ca2+, {calcium/calmodulin}-dependent kinase {ii} ({camkii}), to postsynaptic sites, where it is rapidly translated. these features of {camkii} synthesis are recapitulated during the induction of a long-term memory and produce patterns of local protein synthesis specific to the memory. we show that {mrna} transport and synaptic protein synthesis are regulated by components of the {risc} pathway, including the {sde3} helicase armitage, which is specifically required for long-lasting memory. armitage is localized to synapses and lost in a memory-specific pattern that is inversely related to the pattern of synaptic protein synthesis. therefore, we propose that degradative control of the {risc} pathway underlies the pattern of synaptic protein synthesis associated with a stable memory. {introductionit} has long been known that the establishment of long-lasting forms of memory requires protein synthesis, a feature of memory common to vertebrates and invertebrates (bailey et al., 2004 and kelleher et al., 2004). a number of cellular changes accompany protein synthesis; these changes include the modification of synapse and circuit function and, in net effect, behavior. but it is not yet known how protein synthesis is deployed across the nervous system and how it contributes to the formation of a particular memory. of special interest is protein synthesis localized to the synapse, as this might confer selective synaptic change and the stable modification of a circuit. while regulators of such synthesis are known (reviewed by richter and lorenz, 2002), the molecular events underlying synaptic protein synthesis during the establishment of a memory remain {unresolved.a} well-defined system for the study of memory is the olfactory/electric shock paradigm of drosophila (tully and quinn, 1985). a memory of odor associated with electric shock can be induced in phases that include short-term ({stm}) and long-term ({ltm}), phases that are distinguished by their dependence on training protocol, genetic pathway, and protein synthesis. a requirement for protein synthesis in olfactory {ltm} was demonstrated long ago (tully et al., 1994) and reinforced by the identification of longterm memory mutants as genes with functions in {mrna} transport and translation (dubnau et al., 2003). this system presents an opportunity to determine how and where protein synthesis is deployed and what mechanisms regulate it during the formation of a {memory.to} approach these questions, we devised fluorescent reporters of synaptic protein synthesis using sequences of the drosophila {calcium/calmodulin}-dependent kinase {ii} ({camkii}), the homolog of the mammalian {?camkii}, which is synthesized at synapses and required for memory (reviewed in kelleher et al., 2004). we show that the induction of a long-term memory in drosophila is accompanied by transport of {mrna} to synapses and patterns of synaptic protein synthesis that have features of memory specificity. these events are regulated by components of the {rna} interference ({risc}) pathway. at least one {risc} factor, armitage (cook et al., 2004), is localized to synapses and degraded by the proteasome in response to neural activity or the induction of an {ltm}. therefore, degradative control of {risc} underlies the pattern of synaptic protein synthesis associated with the establishment of a stable memory. results {mrna} determinants of {camkii} synaptic {localizationsince} the mammalian {?camkii} is found at synapses, where its synthesis is regulated by neural activity, our attention turned to the {camkii} gene of drosophila. drosophila {camkii} has a role in neuromuscular synaptic plasticity and memory in the courtship-conditioning paradigm (griffith et al., 1993 and koh et al., 1999). {camkii} is localized to both pre- and postsynaptic sites in the adult brain (figures {1b}–{1d} and {1j}). we focused on the olfactory system because of its well-described neural components, circuitry, and paradigms for the establishment of memory (davis, 2004). this system consists of sensory neurons and interneurons that form an early receptive and processing circuit with synapses organized in bilaterally symmetric centers known as the antennal lobes (figures {1a} and {1b}). the first-order interneurons (projection neurons; {pns}) collect sensory input in a stereotyped array of multisynaptic structures known as glomeruli (figure {1b}), where the {pn} dendritic synapses collect cholinergic input via nicotinic acetylcholine receptors ({nachrs}; figure {1k}). the {pns} direct-output to two brain centers via branching axons that project to the  ” calyx” of the mushroom body and to the lateral horn. these terminals release acetylcholine from choline acyltransferase ({chat})-positive boutons (figures {1i} and {1j}). the {pn} dendrites also form reciprocal synapses with local interneurons (ng et al., 2002 and wilson et al., 2004). on the {pn} dendrites, {camkii} was localized in postsynaptic puncta with the markers discs large ({dlg}) and {ard} (a {nachr} ?-subunit; figures {1c} and {1k}). {camkii} was also concentrated at the {pn} presynaptic boutons in the calyx and lateral horn (figure {1j}). thus, within the same neuron, {camkii} is concentrated at both pre- and postsynaptic sites. the mouse {?camkii} {mrna} displays dendritic localization and activity-dependent synaptic translation, features conferred by sequences in its {3?utr} (rook et al., 2000 and richter and lorenz, 2002). to determine whether this is the case for drosophila {camkii}, the {3?utr} was inserted downstream of the {eyfp} coding sequence in the reporter, {uas}-{eyfp3}?{utr} (figure {1f}). an additional pair of constructs was made bearing a translational fusion of {eyfp} to {camkii}, with the {3?utrcamkii} present ({uas}-{camkii}::{eyfp3}?{utr}; figure {1h}) or absent ({uas}-{camkii}::{eyfpnut}; figure {1g}).when expressed specifically in {pns} using the {gal4}, {uas} binary system (brand and perrimon, 1993), {eyfp3}?{utr} fluorescence was strikingly localized to synapses on the {pn} dendritic and axonal termini (figure {1f}) and colocalized with {chat} at the presynaptic boutons (figure {1m}) and with the {nachr} subunit {ard} on dendritic branches in glomeruli (figure {1k}). this distribution roughly matched that of {camkii} protein (figures {1j} and {1k}), though {eyfp} was somewhat more diffuse along the dendritic branches; presumably {eyfp} does not bind to the postsynaptic apparatus as {camkii} does (koh et al., 1999). in contrast, a cytoplasmic {eyfp} reporter lacking {camkii} sequences was distributed poorly to the axons and dendrites (figure {1e}).the {camkii}::{eyfp} fusion protein synthesized from {mrna} harboring the {3?utr} ({camkii}::{eyfp3}?{utr}) displayed synaptic localization in axons and dendrites like that of {eyfp3}?{utr} (figure {1h}) but was notably more concentrated in synaptic puncta (data not shown). the same fusion protein made from {mrna} lacking the {3?utr} ({camkii}::{eyfpnut}) was strongly localized to axonal presynaptic sites (figures {1g}' and {1n}) but found at a very low level on the antennal lobe dendrites (figure {1g}), where it was localized to synaptic puncta (figure {1l}). thus the {camkii} {3?utr} is necessary and sufficient for the robust localization of {camkii} to dendritic arbors but not required for axonal localization. {activity-dependent} synaptic protein {synthesisto} determine how neural activity might affect {camkii} expression, brains were explanted into bath culture with acetylcholine ({ach}) or nicotine (an agonist of {nachrs}). after 20 min, the tissue was examined by {anti-camkii} immunohistochemistry and quantitative confocal microscopy (figure {2a}). on average (n = 10), when cholinergic synapses were activated, {camkii} immunofluorescence in the antennal lobe increased by 3- to 4-fold (figures {2a}' and {2o}; nicotine, p = 0.0014; {ach}, p = 0.007). in a time-course experiment, an increase in {camkii} level was detected within 5–10 min of nicotine exposure (data not shown). the increase was widespread in the brain and reflected in a 4-fold increase of {camkii} protein on western blot analysis (figure {2d}). the {camkii} increase was also specific, as the levels of synaptic proteins {dlg} and {ard} were unchanged (figures {2b}, {2c}, and {2o}). consistent with the notion that this regulation occurs via translational control, the effect of cholinergic stimulation was blocked by the ribosomal inhibitor anisomycin but not by actinomycin d, an inhibitor of transcription (figures {2a}, {2d}, and {2o}). these results and the requirement of the {camkii} {3?utr} for dendritic localization suggest that cholinergic activity may induce the translation of {camkii} {mrna} at postsynaptic sites. this was examined by monitoring {eyfp3}?{utr} reporter expression in explant culture (figures {2e} and {2f}). a 5 min nicotine incubation increased {eyfp3}?{utr} expression by 30\\% and, after 20 min, by 250\\% (figures {2f} and {2p}). the induced {eyfp} protein was found in large punctae (figures {2k}–{2n}). cholinergic stimulation did not increase {eyfp3}?{utr} expression at the presynaptic terminals in the calyx (data not shown). in contrast, {camkii}::{eyfpnut} expression was only slightly increased by nicotine or {ach} exposure (figures {2g}, {2h}, and {2p}). nicotine incubation did not alter the expression of cytoplasmic {eyfp}, {cd8}::{gfp}, or an {egfp} construct harboring an ?1-tubulin {3?utr} ({hh::egfp}-{3?utrtub}; figures {2i}, {2j}, and {2p}). these observations indicate that the localization and rapid induction of {camkii} in dendrites is due to {3?utr}-dependent synaptic protein synthesis. pattern of synaptic protein synthesis associated with a {long-term} {memoryin} drosophila, an olfactory {ltm} is induced by  ” spaced training,” a protocol where an odor ({cs}+) and electric shock ({us}) are presented coincidentally at temporally spaced intervals. a second odor ({cs}?) follows the {cs}+ odor in each interval without coincident shock. an {ltm} appears after several hours and lasts beyond 24 hr, as assayed by tactic behavior in a t-maze (tully and quinn, 1985). we followed this protocol and used {eyfp3}?{utr} to report synaptic protein synthesis in animals that developed an olfactory {ltm}. our analysis focused on the antennal lobe glomeruli because these structures can be reproducibly identified and display clustered synaptic activity (ng et al., 2002 and wang et al., 2003). furthermore, the first-order antennal lobe synapses might participate in an early stage of memory storage (yu et al., 2004), including the storage of {ltm} (muller, 2000). our analysis (figure 3) revealed an odorant-specific pattern of synaptic protein synthesis associated with the induction of a long-term memory. animals harboring the {uas}-{eyfp3}?{utr} reporter driven by the {pn}-specific {gh146}-{gal4} were trained and analyzed at times from 4 to 24 hr posttraining. the brains of trained and untrained animals were processed for microscopy in parallel. for each glomerulus, a z stack of 6–8 confocal microscopic images was recorded and analyzed via a thresholding protocol that isolated pixel groups corresponding to synaptic puncta (see supplemental experimental procedures in the supplemental data available with this article online). an average glomerulus intensity change ({?f}/f) was calculated for 5–8 brains in each experiment. each experiment was repeated five times. {ltm} was in all cases verified by t-maze {performance.the} analysis was restricted to a set of glomeruli that included those with a primary response to the odorants octanol ({oct}) and methylcyclohexanol ({mch}). only particular glomeruli displayed a training-dependent increase in {eyfp3}?{utr} fluorescence, while others did not; their identities depended on the odorant ({cs}+) paired with shock. when {oct} was the {cs}+, only glomeruli d and {dl3} displayed increased fluorescence, by 115\\% and 108\\%, respectively (figure 3). when {mch} was the {cs}+, fluorescence increased significantly in glomeruli {da1} and {va1} by 95\\% and 70\\%, respectively ({anova} with bonferroni correction, p < 0.05). there were modest but possibly insignificant increases in glomeruli {dm6} and {vc2}. the glomerulus-specific increases were noted as early as 4 hr posttraining and were not observed when odorant and/or electric shock was unpaired or left out or when temporal spacing was not employed ( ” massed training”). in animals that expressed a cytoplasmic {eyfp} reporter or {camkii}::{eyfpnut} (figure 3; data not shown), which lack the {camkii} {3?utr}, there were no significant fluorescence changes. this analysis revealed that an odor-specific induction of synaptic protein synthesis occurred when conditioned and unconditioned stimuli were presented coincidentally and with temporal spacing, the experience that establishes an {ltm}. this plasticity was evidently maintained for at least 24 hr. {activity-dependent} dendritic {mrna} transport and {localizationif} {camkii} was synthesized at the synapse, its {mrna} would be localized there. to address this question, we utilized an {mrna} tracking system based on the bacteriophage coat protein {ms2} and its {rna} binding site (rook et al., 2000). the fusion protein {ms2}::{gfp}::nls is concentrated in the nucleus by nuclear localization signals (nls) but can be diverted elsewhere by binding to an {ms2} binding site ({ms2}-bs) tagged {mrna}. we tagged three {mrnas}: the drosophila {camkii} {cdna}, its {3?utr} alone, and the mouse {?camkii} {3?utr}, which mediates dendritic localization and synaptic translation in the mouse (aakalu et al., 2001 and rook et al., {2000).gfp} fluorescence was examined when {ms2}::{gfp}::nls was expressed in projection neurons ({pns}) with or without an {ms2}-bs tagged {mrna}. punctate fluorescence was observed in the dendrites when an {ms2}-bs-tagged {mrna} was coexpressed with {ms2}::{gfp}::nls but not with {ms2}::{gfp}::nls alone (figures {4a} and {4b}). for example, the tagged drosophila {camkii} {mrna} increased the intensity of glomerular fluorescence by 200\\% (figure {4i}; p < 0.001). in dendrites, particular {mrnas}, including the mouse {?camkii}, are localized to particles containing the motor protein kinesin (kanai et al., 2004). consistent with this observation, the {gfp}-positive dendritic puncta were labeled with an antibody against the major kinesin heavy chain, {khc} (figure {4h}; pearson's coefficient = 0.75; brendza et al., 2002). we wondered whether the synaptic {camkii} expression induced by cholinergic activity (figure {2a}) might be associated with enhanced dendritic localization of {camkii} {mrna}, as was found for the mouse {?camkii} and arc {mrnas} (rook et al., 2000 and steward et al., 1998). when explanted into media with nicotine or {ach}, brains harboring {ms2}::{gfp}::nls and the tagged drosophila {camkii} {mrna} displayed a striking increase in dendritic {gfp} fluorescence (figures {4b}' and {4i}; p < 0.001). the effect with cholinergic stimulation was similar with the tagged mouse {?camkii} {3?utr}: a 70\\%–73\\% increase relative to culture without nicotine (figures {4g} and {4i}). the activity-enhanced dendritic {mrna} transport was blocked by anisomycin (figure {4d}) but not by actinomycin d (figure {4c}). we suppose that mainly existing {mrna} can be translocated during the short period of culture. thus, in drosophila, like mammals (krichevsky and kosik, 2001), neural activity increases the rate of {mrna} movement to the synapse by a protein synthesis-dependent {mechanism.we} then asked whether the induction of an {ltm} might affect {mrna} transport to the synapse. when animals expressing {ms2}::{gfp}::nls and drosophila {ms2bs-camkii} were subjected to the spaced training protocol, the number of {gfp}-labeled puncta in dendrites was substantially increased (80\\% relative to untrained; n = 10, p < 0.001; figures {4a}, {4f}, and {4i}). the pattern of dendritic punctae did not display evident glomerular specificity, as observed for synaptic protein synthesis (figure 3). however, the induced punctae were distributed along the dendritic branches, making a determination of glomerular specificity uncertain. these observations reveal that a coordinated program for synaptic gene expression occurs during the storage of a memory. regulation of {mrna} transport and synaptic protein synthesis by the {risc} {pathwaythe} {rna} interference ({risc}) pathway silences gene expression by the targeted degradation of {mrnas} or their nondestructive silencing (sontheimer, 2005). in drosophila, {risc}-mediated translational silencing controls oskar expression in the developing oocyte. an {sde3}-class {rna} helicase, armitage (armi; cook et al., 2004) acts as part of {risc} (tomari et al., 2004) to control oskar translation and regulate cytoskeletal organization, possibly via control of kinesin heavy chain (khc) translation. both the oskar and khc {3?utrs} have putative binding sites for the {microrna} ({mirna}) {mir}-280. the {camkii} {3?utr} has a remarkably similar {mir}-280 binding site (figure {5a}). this site and a nearby site for {mir}-289 satisfy the predictive rule that 7 of 8 nucleotides at the 5? end of an {mirna} are cognate to a target {mrna} (lai et al., 2003 and stark et al., 2003). kinesin is also a component of the {rna}-containing dendritic particles that bring {mrna} to the synapse (kanai et al., 2004; figure {4h}). staufen, likewise a mediator of {rna} transport (ferrandon et al., 1994 and tang et al., 2001), has binding sites for {mir}-280 and {mir}-305 in its {mrna} {3?utr} (rajewsky and socci, 2004). we thus explored the role of {risc} in {camkii}, {khc}, and staufen expression. dicer-2 is one of two drosophila ribonucleases that produce short {rna} components of {risc} (pham et al., 2004). {camkii} synaptic expression was dramatically increased in a dicer-2 mutant, particularly in the antennal lobe and mushroom body (figures {5e} and {5f}). in contrast, there was no difference in the expression of the cell adhesion protein fasciclin {ii} in the same animals (figures {5e}' and {5f}'). in western analysis (figure {5g}, left panel), there was a striking 25-fold increase in {camkii} protein in dicer-2 mutant brains. synaptic {camkii} expression was also elevated in aubergine and armitage mutant brains (figures {5g} and s1). the aubergine locus encodes an argonaute protein involved in {risc} assembly and function (cook et al., 2004 and tomari et al., 2004). the level of staufen protein was also increased in the armitage mutant brain (figure {5g}, right {panel).whether} the {mirna} binding sites in the {camkii} {3?utr} might be involved in {risc}-mediated regulation was examined with the {eyfp3}?{utr} transgene. when expressed in the {pns}, {eyfp} fluorescence in glomeruli was 80\\% greater in armi than in the wild-type (figures {5j} and {5o}). the {eyfp3}?{utr} fluorescence was localized to large dendritic puncta like those found in brains explanted into nicotine-containing media (figures {5l} and {5m}). indeed, {eyfp3}?{utr} expression in armi brains did not increase further upon explant with nicotine (figures {5l} and {5n}), consistent with the notion that cholinergic activation might act via antagonism of {risc}. the expression of {camkii}::{eyfpnut}, which lacks the {3?utr}, increased slightly (15\\%) in the armi mutant background (figure {5i}), while other control constructs, such as {cd8}::{gfp}, were essentially unchanged (figure {5h}). in addition, {rt}-{pcr} analysis of wild-type and armi mutant brains did not reveal a difference in the levels of transgenic {mrnas} (data not shown). there was also a substantial increase in {eyfp3}?{utr} and {camkii}::{eyfp3}?{utr} synaptic fluorescence in dicer-2 and aubergine mutants (figure s1 and data not shown). therefore we conclude that {risc} regulates {camkii} expression by a posttranscriptional mechanism, utilizing sites in the {camkii} {3?utr}.armitage expression was found in multiple neuronal populations in the brain, including the {pns} and mushroom-body kenyon cells. it was distributed in puncta in cell bodies and dendrites and to axon termini (figure {5b}). a {gfp}::armi fusion protein, when expressed in the {pns}, displayed a similar punctate distribution (figures {5c} and {6a}–{6e}) that overlapped synaptic puncta containing {camkii} (arrowhead, figure {5c}'). the {gfp}::armi fusion protein retains armi+ activity (cook et al., 2004) such that neurons with high levels of {gfp}::armi expression would have increased armi+ activity. several observations indicate that a posttranscriptional autoregulatory circuit modulates armi expression (figure s2). nonetheless, strong transgenic expression of {gfp}::armi reduced the level of {camkii} expression, as revealed by western blot analysis (figure {5p}) and immunohistochemistry. neurons that expressed a high level of {gfp}::armi displayed reduced expression of both {camkii} and {khc} (figures {5q}, {5r}, and s2). a control {uas}-{cd8}-{gfp} transgene was unaffected by {gfp}::armi expression (figure {5s}).since armi regulates {khc} and staufen expression, we considered the possibility that it might also regulate the dendritic transport of {camkii} {mrna}. when examined with the {ms2}::{gfp} system, armi72.1 homozygotes indeed displayed a 78\\% increase in fluorescence by dendritic {gfp}-positive puncta, compared to an armi+ control (figures {4e} and {4i}; n = 7, p < 0.05). therefore, armi regulation of synaptic protein synthesis reflects a coordinated program with multiple {mirna} targets, affecting both {mrna} transport and translation at the synapse, where armi protein is found. neural activity induces rapid {proteasome-mediated} degradation of {armitageif} {mrna} silencing by {risc} plays a role in {ltm}, we would expect this pathway to be somehow regulated by neural function. given the inverse relationship between {camkii} expression and armi+ activity, we considered whether armi might be a regulatory target. as shown in figure 6, the level of {gfp}::armi fluorescence rapidly decreased (by 3.5-fold) in brains explanted into nicotine-containing medium. there was a correlated increase in {camkii} expression (by 4.5-fold) in the {pn} dendritic arbors of the antennal lobe. a short incubation with nicotine (5 min) resulted in the complete disappearance of armi protein in western analysis (figure {6g}, top panel). the {gfp}::armi protein was also eliminated upon explant with nicotine (figure {6g}, bottom panel). in contrast, the {camkii} protein level increased (figure {6h}, right panel) and ?1-tubulin was unchanged (figure {6g}). two experiments were performed to determine whether the activity-induced elimination of armi required the proteasome. first, {gfp}::armi was expressed along with a transgenic dominant-negative mutant of the proteasome ? subunit ({dts5}; speese et al., 2003). when the {dts5} transgene was present, the level of {gfp}::armi fluorescence was elevated by 3.2-fold (figures {6a} and {6b}; p < 0.0001, n = 8). in contrast, the {dts5} transgene did not alter the level of {cd8}::{gfp} (not shown). second, incubation with the proteasome inhibitor lactacystin blocked the nicotine-induced loss of {gfp}::armi (figures {6d} and {6e}) and degradation of endogenous armi protein (figure {6h}, left panel). preincubation with lactacystin also blocked nicotine-induced synaptic {camkii} synthesis, as determined by both western analysis (figure {6h}, right panel) and by immunohistochemistry (figure {6e}). thus, cholinergic activity evidently acts via the proteasome to induce the degradation of armitage and synaptic synthesis of {camkii}. a degradative pathway for {ltma} key question is whether this degradative pathway has a role in synaptic protein synthesis associated with {ltm}. animals expressing the {gfp}::armi protein in projection neurons were subjected to olfactory spaced training and analyzed by the same microscopic methods used to assess {ltm}-associated {eyfp3}?{utr} expression (figure 3). the {gfp}::armi protein was found concentrated in synaptic puncta in the glomeruli (figures {5c} and {6j}). when examined at either 3 or 24 hr posttraining, {gfp}::armi fluorescence was significantly reduced in many glomeruli and most strongly reduced in the glomeruli that had displayed the greatest increase in {eyfp3}?{utr} expression (figure 3 and figure {6i}, and {6j}). fluorescence in glomeruli {da1} and {va1} decreased by 3.1- and 3.8-fold, respectively, when the odorant {mch} was paired with shock. when the odorant {oct} was paired with shock, the d and {dl3} glomeruli displayed the most significant decreases (2-fold). more modest losses of {gfp} fluorescence were observed in other glomeruli (figures {6i} and {6j}). these observations reveal an inverse relationship between synaptic armi protein and {camkii} synthesis during the establishment of an {ltm}. since these changes were still present at 24 hr posttraining, the change was evidently maintained long-term, perhaps for the term of the {memory.given} the role of armi in the synaptic synthesis of {camkii}, we wondered whether either of these genes might be required to form an olfactory {ltm}. several armi hypomorphic alleles display normal adult viability and behavior, including normal odor and shock sensitivity. given their normal performance in these tests, we examined armi animals for {stm} and {ltm}. the animals (armi72.1/armi72.1 or {armi72.1/df}({3l})e1) displayed normal memory in the short-term paradigm (figure {6k}; armi?, {pi} = 0.55; armi+, {pi} = 0.58; p > 0.1) but were profoundly deficient in {ltm} (armi?, {pi} = 0.085 versus armi+, {pi} = 0.31; p < 0.05). expression of the {gfp}::armi transgene rescued the armi72.1/armi72.1 {ltm} deficiency to a normal value (figure {6k}). we achieved a nearly complete and tissue-specific loss of {camkii} by use of a construct that generates a {camkii} hairpin {rna} ({uas}-{camkiihpn}; figure {6l}). animals expressing {uas}-{camkiihpin} in all {camkii}-positive neurons (with the {camkii}-{gal4} driver) retained normal short-term memory ({pi} = 0.653; {camkii}-{gal4} alone, {pi} = 0.64), but displayed a near-complete loss of {ltm} ({pi} = 0.07 versus {camkii}-{gal4} alone, {pi} = 0.3; p < 0.001). thus, both {camkii} and armitage are required for {ltm} but not for {stm}. {discussionit} has long been known that the establishment of long-lasting forms of memory requires protein synthesis, a feature of memory common to both vertebrates and invertebrates. of special interest is protein synthesis localized to the synapse, which might result in selective synaptic change and the stable modification of a neural circuit (bailey et al., 2004 and kelleher et al., 2004). yet how and where synaptic protein synthesis occurs in relation to the establishment and maintenance of a memory has not been determined. here we report that memory-specific patterns of synaptic protein synthesis occur with the induction of a long-term memory in drosophila. these patterns appear to be controlled by the proteasome-mediated degradation of a {risc} pathway component, armitage, to regulate the transport of {mrna} to synapses and its translation once {there.to} visualize synaptic protein synthesis, we devised fluorescent reporters based on the drosophila {camkii} gene, which has well-described roles in synaptic plasticity and memory (griffith et al., 1993 and koh et al., 1999). the {3?utr} of {camkii} shares regulatory motifs with the mammalian {?camkii} {mrna}, which mediate dendritic {mrna} localization and neural activity-dependent translation (reviewed in richter and lorenz, 2002). we found that the {3?utr} of drosophila {camkii} was also necessary and sufficient for {mrna} localization to dendrites and synaptic translation (figure 1 and figure 4). this {3?utr} sufficed for the enhanced dendritic {mrna} transport and translation induced by cholinergic stimulation (figure 2 and figure 4). hence we find a simple parallel between the synaptic regulation of {camkii} in drosophila and {mammals.when} these fluorescent reporters were utilized in vivo, the induction of synaptic protein synthesis was observed in several drosophila brain centers following the spaced training paradigm of repetitive odor paired with electric shock that establishes a long-term memory ({ltm}; figure 3). there were local patterns of memory specificity identifiable in glomeruli of the antennal lobe where synapses of similar function are clustered. when the odorant {oct} was paired with electric shock, protein synthesis was induced selectively in the d and {dl3} glomeruli. when the odorant {mch} was paired with shock, the {da1} and {va1} glomeruli displayed the most robust enhancement of synaptic protein synthesis. notably, the animals were exposed to both odorants during training; the pattern of synthesis depended on coincidence with shock. there was no significant induction of protein synthesis when exposure to odor and shock was nonoverlapping, with either stimulus presented alone, or in the absence of temporal spacing ( ” massed training”). thus, an odor-specific pattern of synaptic protein synthesis was induced under conditions that produce an {ltm}.experiments in the honeybee suggest that the antennal lobe is a  ” way station” for memory where stimuli are integrated to yield plasticity more labile than a short-term memory (menzel and giurfa, 1999). a long-term memory can be formed in the honeybee antennal lobe in a spaced training paradigm (muller, 2000). the experiments of yu et al. (2004) revealed plasticity in the drosophila antennal lobe, where particular glomeruli acquired enhanced synaptic activity after a single episode of paired odor and shock. remarkably, the enhanced synaptic protein synthesis we observed with spaced training occurred in essentially the same glomeruli that displayed enhanced synaptic activity in the {stm} protocol (yu et al., 2004). these glomeruli are distinct from those that display the greatest odor or electric shock-evoked synaptic activity (ng et al., 2002, wang et al., 2003 and wilson et al., 2004). therefore, we suppose that the mechanism that integrates a single paired odor and shock to produce new synaptic activity might also generate the trigger for synaptic protein synthesis when the paired stimuli are repeated with temporal spacing. we believe this trigger includes the proteasome-mediated degradation of the {risc} factor armitage (figure 7). though these  ” memory traces” have been recorded in the antennal lobe, there is still no evidence for their role in memory. the mushroom body, on the other hand, is required for {ltm} (pascual and preat, 2001). our current methods cannot resolve patterns of synaptic protein synthesis in the mushroom body because it lacks the stereotyped synaptic architecture of the antennal lobe. when determined, a global brain map of synaptic protein synthesis will provide significant insights into the mechanisms of memory {storage.synaptic} protein synthesis and dendritic {mrna} transport are well studied for the mammalian {?camkii} gene, which bears recognition motifs in its {3?utr} for {cpeb} and other proteins with transport and translation control functions (richter and lorenz, 2002). the presence of potential recognition motifs for the {cpeb}, pumilio, and staufen proteins in the drosophila {camkii} {3?utr} (our unpublished observations) suggests that these mechanisms are conserved in drosophila. indeed, staufen, orb (a {cpeb} family member), and pumilio have been identified as {ltm}-deficient mutants (dubnau et al., 2003). the roles of these genes remain to be fully {explored.we} focused instead on the {risc} pathway because of apparent binding motifs for {micrornas} {mir}-280 and {mir}-289 in the {camkii} {3?utr} (figure {5a}). these sites are similar to those in the {3?utrs} of oskar and kinesin heavy chain (khc), which are targets for translational silencing by armitage and other {risc} components in the oocyte (cook et al., 2004). we found armitage in synaptic puncta on dendrites, colocalized with {camkii} (figure 5). when the level of armitage was decreased or increased by mutation or transgenic expression, {camkii} synaptic expression was modulated in a reciprocal and cell-autonomous fashion (figure 5 and figure 6). this regulation could be recapitulated by an {eyfp} reporter bearing the {camkii} {3?utr}. mutants for the {risc} components aubergine and dicer-2 displayed similar phenotypes (figures 5 and s1). it therefore seems likely that multiple tiers of control regulate {camkii} like oskar, where two systems ({bruno/cup} and {risc}) act on distinct sites in its {3?utr} (cook et al., 2004 and webster et al., {1997).a} second avenue for {risc} control of {camkii} synthesis is via {mrna} transport. by tagging {camkii} {mrna} with a {gfp} reporter, we observed dendritic punctae whose frequency and intensity increased under the same conditions that induced synaptic protein synthesis: cholinergic activation and olfactory spaced training (figure 4). the induction of {mrna} transport required new protein synthesis but not transcription. armitage was also found to regulate the frequency and intensity of the {gfp}-tagged dendritic puncta (figure 4). two proteins that play a role in {mrna} transport, kinesin heavy chain ({khc}) and staufen, recapitulate this pattern of regulation by cholinergic stimulation and armitage (figure 5). both of their {mrnas} bear targets for {mirna} regulation in the {3?utr} (figure 5; cook et al., 2004 and rajewsky and socci, 2004). our studies leave open the possibility that the enhanced synaptic localization of {camkii} {mrna} underlies the induction of its synaptic translation. however, the presence of {mirna} binding sites in the {camkii} {3?utr}, the localization of armitage with {camkii} in synaptic punctae, and the rapid induction of {camkii} synthesis by cholinergic activity all suggest that {risc} acts at the synapse. furthermore, local translational control may be required to impose the specificity that was not evident in the pattern of {mrna} transport associated with the induction of an {ltm} (figure {4).a} link between the induction of memory and synaptic protein synthesis is the proteasome-mediated degradation of armitage. in explant culture, cholinergic induction of {camkii} synthesis was accompanied by the rapid degradation of armi; both events were blocked by inhibition of the proteasome (figure 6). the relationship between armi degradation and {camkii} synaptic translation was recapitulated in the brain as animals formed and maintained an {ltm}. the same glomeruli that displayed the greatest increase in {camkii} synthesis displayed the largest decline in synaptic armi (figure 6). this reciprocal relationship between the armi and {camkii} proteins was detected as early as 3 hr after training and maintained for at least 24 hr posttraining. the training-induced change of synaptic armi was therefore  ” locked in,” possibly for the term of the memory, consistent with a role in maintaining an alteration of synaptic {function.therefore} we propose a new mechanism for stable memory in which an integrated sensory trigger induces the proteasome-mediated degradation of a {risc} factor, releasing synaptic protein synthesis and {mrna} transport from {microrna} suppression (figure 7). we suppose that this mechanism is triggered with neuronal specificity in order to produce memory-specific patterns of protein synthesis. whether this specificity is required for memory or extends to the level of a single synapse are questions that remain to be addressed. experimental procedures drosophila stocks and {geneticsfly} stocks were maintained at {25°c} on standard cornmeal agar medium under a constant 24 hr light/dark cycle. aubergine ({aubhn} and {aubqc42}) and {df(3l})e1 strains were obtained from the bloomington stock center (indiana). transformant lines carrying {uas}-{gfp}::armi, and armi mutant lines were generously provided by h. cook and w. theurkauf (u. mass., worcester, {ma}). the {dcr-2l811ex} mutant was a gift of drs. r. carthew and h. {ruohola-baker}. the {uas}-{dts5} strain (speese et al., 2003) was crossed to {elav-gal4}, {uas}-{gfp}::armi or {gh146}-{gal4}, {uas}-{gfp}::armi animals and grown at {17°c} (permissive temperature), as described in the text. imunnohistochemical {methodsadult} brains were dissected and processed for immunohistochemistry as described by kunes et al. (1993). antibodies were used at the following dilutions: mouse {?camkii} (1:100; takamatsu et al., 2003), rabbit {?camkii} (1:4000; koh et al., 1999 and takamatsu et al., 2003), {?ard} (1:50), {?-elav} (1:100), {?-armi} (1:200) and {?-khc} (1:50), ?-mouse cy3 (1:100), ?-rat cy5 (1:200), and ?-rabbit cy5 (1:500). explant {protocoladult} heads were dissected to remove the brain in {ahl} medium (wang et al., 2003) and incubated as described. nicotine ([?] nicotine, sigma) was used at 10 {mm}, adjusted to {ph} 7.0–7.4. acetylcholine (acetylcholine chloride, 99\\%, sigma) was used at 50 {?m}. the explants were incubated at {22°c} as indicated, washed three times in {ahl}, and fixed in 4\\% paraformaldehyde. for proteasome inhibition, specimens were incubated for 40 min in lactacystin (100 {?m}, sigma) prior to further manipulation. for inhibition of translation or transcription, explants were incubated for 30–60 min in anisomycin (25 {?m}) and actinomycin d (50 {?m}, {a.g}. scientific) prior to nicotine incubation. behavioral {assaysshort} and longterm olfactory memory tests were performed in a t-maze apparatus as described by tully and quinn (1985). most transgenic lines were backcrossed to canton s (c. quinn, {mit}) for 3–4 generations prior to use. animals were used at 2–4 days posteclosion. fluorescence {measurementsimage} quantification and statistical analysis are described in the supplemental experimental procedures. {acknowledgmentswe} thank drs. s. ohsako, k. kosik, e. gundelfinger, l. griffith, r. carthew, h. {ruohola-baker}, h. cook, and w. theurkauf for generously sharing reagents. we thank drs. t. preat, s. waddell, and w. quinn for advice on behavioral assays.
928	465715	article	vision res	\N	\N	\N	13	41	6	2001	mar	2006-01-15 12:24:48	department of psychology, university of texas at austin, austin, tx 78712, usa. geisler@psy.utexas.edu	edge co-occurrence in natural images predicts contour grouping performance.	the human brain manages to correctly interpret almost every visual image it receives from the environment. underlying this ability are contour grouping mechanisms that appropriately link local edge elements into global contours. although a general view of how the brain achieves effective contour grouping has emerged, there have been a number of different specific proposals and few successes at quantitatively predicting performance. these previous proposals have been developed largely by intuition and computational trial and error. a more principled approach is to begin with an examination of the statistical properties of contours that exist in natural images, because it is these statistics that drove the evolution of the grouping mechanisms. here we report measurements of both absolute and bayesian edge co-occurrence statistics in natural images, as well as human performance for detecting natural-shaped contours in complex backgrounds. we find that contour detection performance is quantitatively predicted by a local grouping rule derived directly from the co-occurrence statistics, in combination with a very simple integration rule (a transitivity rule) that links the locally grouped contour elements into longer contours.
929	466050	article	nature	\N	\N	nature publishing group	3	430	6995	2004	jul	2006-01-16 14:22:19	\N	evolutionary changes in cis and trans gene regulation	differences in gene expression are central to evolution. such differences can arise from cis-regulatory changes that affect transcription initiation, transcription rate and/or transcript stability in an allele-specific manner, or from trans-regulatory changes that modify the activity or expression of factors that interact with cis-regulatory sequences1, 2. both cis- and trans-regulatory changes contribute to divergent gene expression, but their respective contributions remain largely unknown3. here we examine the distribution of cis- and trans-regulatory changes underlying expression differences between closely related drosophila species, d. melanogaster and d. simulans, and show functional cis-regulatory differences by comparing the relative abundance of species-specific transcripts in f1 hybrids4, 5. differences in trans-regulatory activity were inferred by comparing the ratio of allelic expression in hybrids with the ratio of gene expression between species. of 29 genes with interspecific expression differences, 28 had differences in cis-regulation, and these changes were sufficient to explain expression divergence for about half of the genes. trans-regulatory differences affected 55% (16 of 29) of genes, and were always accompanied by cis-regulatory changes. these data indicate that interspecific expression differences are not caused by select trans-regulatory changes with widespread effects, but rather by many cis-acting changes spread throughout the genome.
930	466470	article	genome research	\N	\N	\N	6	15	12	2005	dec	2006-01-16 22:30:16	department of genetics, university of cambridge, cambridge, cb2 3eh, united kingdom. ma11@gen.cam.ac.uk	drosophila melanogaster: a case study of a model genomic sequence and its consequences	10.1101/gr.3726705 the sequencing and annotation of the  genome, first published in 2000 through collaboration between celera genomics and the  genome projects, has provided a number of important contributions to genome research. by demonstrating the utility of methods such as whole-genome shotgun sequencing and genome annotation by a community  ” jamboree,” the  genome established the precedents for the current paradigm used by most genome projects. subsequent releases of the initial genome sequence have been improved by the berkeley  genome project and annotated by {flybase}, the  community database, providing one of the highest-quality genome sequences and annotations for any organism. we discuss the impact of the growing number of genome sequences now available in the genus on current  research, and some of the biological questions that these resources will enable to be solved in the future.
931	466558	book	\N	\N	\N	belknap press	\N	\N	\N	2004	may	2006-01-17 00:45:14	\N	strangers to ourselves : discovering the adaptive unconscious	"know thyself," a precept as old as socrates, is still good advice. but is introspection the best path to self-knowledge? wilson makes the case for better ways of discovering our unconscious selves. if you want to know who you are or what you feel or what you're like, wilson advises, pay attention to what you actually do and what other people think about you. showing us an unconscious more powerful than freud's, and even more pervasive in our daily life, <{i>strangers} to ourselves</i> marks a revolution in how we know ourselves.
932	467056	article	artificial intelligence	\N	\N	\N	27	94	1-2	1997	\N	2006-01-17 13:41:05	\N	on the emergence of social conventions: modeling, analysis, and simulations	we define the notion of social conventions in a standard game-theoretic framework, and identify various criteria of consistency of such conventions with the principle of individual rationality. we then investigate the emergence of such conventions in a stochastic setting; we do so within a stylized framework currently popular in economic circles, namely that of stochastic games. this framework comes in several forms; in our setting agents interact with each other through a random process, and accumulate information about the system. as they do so, they continually reevaluate their current choice of strategy in light of the accumulated information. we introduce a simple and natural strategy-selection rule, called highest cumulative reward (hcr). we show a class of games in which hcr guarantees eventual convergence to a rationally acceptable social convention. most importantly, we investigate the efficiency with which such social conventions are achieved. we give an analytic lower bound on this rate, and then present results about how hcr works out in practice. specifically, we pick one of the most basic games, namely a basic coordination game (as defined by lewis), and through extensive computer simulations determine not only the effect of applying hcr, but also the subtle effects of various system parameters, such as the amount of memory and the frequency of update performed by all agents.
933	467098	book	\N	\N	\N	harper torchbooks	\N	\N	\N	1982	feb	2006-01-17 15:09:13	\N	the question concerning technology, and other essays	{"to read heidegger is to set out on an adventure. the essays in this volume--intriguing, challenging, and often baffling to the reader--call him always to abandon all superficial scanning and to enter wholeheartedly into the serious pursuit of thinking....</p><p>"heidegger is not a 'primitive' or a 'romanitic.' he is not one who seeks escape from the burdens and responsibilities of contemporary life into serenity, either through the re-creating of some idyllic past or through the exalting of some simple experience. finally, heidegger is not a foe of technology and science. he neither disdains nor rejects them as though they were only destructive of human life.</p><p>"the roots of heidegger's hinking lie deep in the western philosophical tradition. yet that thinking is unique in many of its aspects, in its language, and in its leterary expression. in the development of this thought heidegger has been taught chiefly by the greeks, by german idealism, by phenomenology, and by the scholastic theological tradition. in him these and other elements have been fused by his genius of sensitivity and intellect into a very individual philosophical expression." --william lovitt, <i>from the introduction</i>}
934	467313	article	genome biology	\N	\N	\N	\N	6	5	2005	\N	2006-01-17 19:11:25	bioinformatics department, centro de investigaci\\'{o}n pr\\'{\\i}ncipe felipe, autopista del saler 16, 46013 valencia, spain. hdopazo@ochoa.fib.es	genome-scale evidence of the nematode-arthropod clade	{background}:the issue of whether coelomates form a single clade, the coelomata, or whether all animals that moult an exoskeleton (such as the coelomate arthropods and the pseudocoelomate nematodes) form a distinct clade, the ecdysozoa, is the most puzzling issue in animal systematics and a major open-ended subject in evolutionary biology. previous single-gene and genome-scale analyses designed to resolve the issue have produced contradictory results. here we present the first genome-scale phylogenetic evidence that strongly supports the ecdysozoa {hypothesis.results}:through the most extensive phylogenetic analysis carried out to date, the complete genomes of 11 eukaryotic species have been analyzed in order to find homologous sequences derived from 18 human chromosomes. phylogenetic analysis of datasets showing an increased adjustment to equal evolutionary rates between nematode and arthropod sequences produced a gradual change from support for coelomata to support for ecdysozoa. transition between topologies occurred when fast-evolving sequences of caenorhabditis elegans were removed. when chordate, nematode and arthropod sequences were constrained to fit equal evolutionary rates, the ecdysozoa topology was statistically accepted whereas coelomata was {rejected.conclusions}:the reliability of a monophyletic group clustering arthropods and nematodes was unequivocally accepted in datasets where traces of the long-branch attraction effect were removed. this is the first phylogenomic evidence to strongly support the 'moulting clade' hypothesis.
935	467844	techreport	\N	\N	\N	\N	\N	\N	99-87	2001	\N	2006-01-18 01:03:02	\N	estimating the support of a {high-dimensional} distribution	suppose you are given some dataset drawn from an underlying probability distribution p and you want to estimate a "simple" subset s of input space such that the probability that a test point drawn from p lies outside of s is bounded by some a priori specified values between 0 and {1.we} propose a method to approach this problem by trying to estimate a function f which is positive on s and negative on the complement. the functional form of f is given by a kernel expansion in terms of a potentially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space. the expansion coefficients are found by solving a quadratic programming problem, which we do by carrying out sequential optimization over pairs of input patterns. we also provide a preliminary theoretical analysis of the statistical performance of our {algorithm.the} algorithm is a natural extension of the support vector algorithm to the case of unlabelled data.
936	468425	inproceedings	\N	chi	\N	acm press/addison-wesley publishing co.	7	\N	\N	1995	\N	2006-01-18 12:11:53	new york, ny, usa	{tilebars}: visualization of term distribution information in full text information access	the field of information retrieval has traditionally focused on textbases consisting of titles and abstracts. as a consequence, many underlying assumptions must be altered for retrieval from full-length text collections. this paper argues for making use of text structure when retrieving from full text documents, and presents a visualization paradigm, called tilebars, that demonstrates the usefulness of explicit term distribution information in boolean-type queries. tilebars simultaneously and compactly indicate relative document length, query term frequency, and query term distribution. the patterns in a column of tilebars can be quickly scanned and deciphered, aiding users in making judgments about the potential relevance of the retrieved documents. keywords: information retrieval, full-length text, visualization. introduction information access systems have traditionally focused on retrieval of documents consisting of titles and abstracts. as a consequence, the underlying assumpt...
937	468934	inproceedings	\N	proceedings of the xxi\\$^{st}\\$international conference on software engineering. icse' 99	\N	acm	12	\N	\N	1999	may	2006-01-18 12:33:25	los angeles, california	{n degrees} of separation: multi-dimensional separation of concerns	done well, separation of concerns can provide many software engineering benefits, including reduced complexity, improved reusability, and simpler evolution. the choice of boundaries for separate concerns depends on both requirements on the system and on the kind(s) of decomposition and composition a given formalism supports. the predominant methodologies and formalisms available, however, support only orthogonal separations of concerns, along single dimensions of composition and decomposition. these characteristics lead to a number of well-known and difficult problems.  this paper describes a new paradigm for modeling and implementing software artifacts, one that permits separation of overlapping concerns along multiple dimensions of composition and decomposition. this approach addresses numerous problems throughout the software lifecycle in achieving wellengineered, evolvable, flexible software artifacts and traceability across artifacts.  keywords  hypermodules; hyperslices; software...
938	469005	article	international journal of group decision and negotiation	\N	\N	\N	16	10	2	2001	\N	2006-01-18 12:33:25	\N	automated negotiation: prospects, methods and challenges	the remainder of this paper is structured as follows. section 2 presents a generic framework for automated negotiation. this framework is then used to structure the subsequent discussion and analysis of the various negotiation techniques; section 3 deals with game theoretic techniques, section 4 with heuristic techniques, and section 5 with argumentation-based techniques. finally, section 6 outlines some of the major challenges that need to be addressed before automated negotiation becomes pervasive.
939	469344	article	computer	\N	\N	\N	6	33	10	2000	\N	2006-01-18 15:28:29	\N	an empirical comparison of seven programming languages	80 implementations of the same set of requirements are compared for several properties, such as run time, memory consumption, source text length, comment density, program structure, reliability, and the amount of effort required for writing them. the results indicate that, for the given programming problem, which regards string manipulation and search in a dictionary, \\&quot;scripting languages\\&quot; (perl, python, rexx, tcl) are more productive than \\&quot;conventional languages \\&quot; (c, c++, java). in terms of...
940	469350	article	information retrieval	\N	\N	kluwer academic publishers	33	2	4	2000	\N	2006-01-18 15:36:43	\N	learning algorithms for keyphrase extraction	many academic journals ask their authors to provide a list of about five to fifteen keywords, to appear on the first page of each article. since these key words are often phrases of two or more words, we prefer to call them keyphrases. there is a wide variety of tasks for which keyphrases are useful, as we discuss in this paper. we approach the problem of automatically extracting keyphrases from text as a supervised learning task. we treat a document as a set of phrases, which the learning algorithm must learn to classify as positive or negative examples of keyphrases. our first set of experiments applies the c4.5 decision tree induction algorithm to this learning task. we evaluate the performance of nine different configurations of c4.5. the second set of experiments applies the genex algorithm to the task. we developed the genex algorithm specifically for automatically extracting keyphrases from text. the experimental results support the claim that a custom-designed algorithm (genex), incorporating specialized procedural domain knowledge, can generate better keyphrases than a generalpurpose algorithm (c4.5). subjective human evaluation of the keyphrases generated by extractor suggests that about 80 % of the keyphrases are acceptable to human readers. this level of performance should be satisfactory for a wide variety of applications.
941	469427	article	nature reviews. genetics	\N	\N	nature publishing group	11	7	2	2006	feb	2006-03-01 16:51:43	\N	towards multidimensional genome annotation.	our information about the gene content of organisms continues to grow as more genomes are sequenced and gene products are characterized. sequence-based annotation efforts have led to a list of cellular components, which can be thought of as a one-dimensional annotation. with growing information about component interactions, facilitated by the advancement of various high-throughput technologies, systemic, or two-dimensional, annotations can be generated. knowledge about the physical arrangement of chromosomes will lead to a three-dimensional spatial annotation of the genome and a fourth dimension of annotation will arise from the study of changes in genome sequences that occur during adaptive evolution. here we discuss all four levels of genome annotation, with specific emphasis on two-dimensional annotation methods.
942	469428	article	nature reviews genetics	\N	\N	nature publishing group	10	7	2	2006	feb	2006-01-21 06:09:24	\N	literature mining for the biologist: from information retrieval to biological discovery	for the average biologist, hands-on literature mining currently means a keyword search in {pubmed}. however, methods for extracting biomedical facts from the scientific literature have improved considerably, and the associated tools will probably soon be used in many laboratories to automatically annotate and analyse the growing number of system-wide experimental data sets. owing to the increasing body of text and the open-access policies of many journals, literature mining is also becoming useful for both hypothesis generation and biological discovery. however, the latter will require the integration of literature and high-throughput data, which should encourage close collaborations between biologists and computational linguists.
943	469592	book	\N	\N	\N	{university of chicago press}	\N	\N	\N	1998	dec	2006-01-18 19:04:02	\N	the human condition	{<div>a work of striking originality bursting with unexpected insights, <i>the human condition</i> is in many respects more relevant now than when it first appeared in 1958. in her study of the state of modern humanity, hannah arendt considers humankind from the perspective of the actions of which it is capable. the problems arendt identified then--diminishing human agency and political freedom, the paradox that as human powers increase through technological and humanistic inquiry, we are less equipped to control the consequences of our actions--continue to confront us today. this new edition, published to coincide with the fortieth anniversary of its original publication, contains an improved and expanded index and a new introduction by noted arendt scholar margaret canovan which incisively analyzes the book's argument and examines its present relevance. a classic in political and social theory, <i>the human condition</i> is a work that has proved both timeless and perpetually timely.<br><br>hannah arendt (1906-1975) was one of the leading social theorists in the united states. her <i>lectures on kant's political philosophy</i> and <i>love and saint augustine</i> are also published by the university of chicago press. <br><br></div>}
944	471849	article	annual review of sociology	\N	\N	\N	\N	22	1	1996	\N	2006-01-20 01:51:43	\N	{computer} {networks} {as} {social} {networks}: collaborative work, telework, and virtual community	when computer networks link people as well as machines, they become social networks. such computer-supported social networks (cssns) are becoming important bases of virtual communities, computer-supported cooperative work, and telework. computer-mediated communication such as electronic mail and computerized conferencing is usually text-based and asynchronous. it has limited social presence, and on-line communications are often more uninhibited, creative, and blunt than in-person communication. nevertheless, cssns sustain strong, intermediate, and weak ties that provide information and social support in both specialized and broadly based relationships. cssns foster virtual communities that are usually partial and narrowly focused, although some do become encompassing and broadly based. cssns accomplish a wide variety of cooperative work, connecting workers within and between organizations who are often physically dispersed. cssns also link teleworkers from their homes or remote work centers to main organizational offices. although many relationships function off-line as well as on-line, cssns have developed their own norms and structures. the nature of the medium both constrains and facilitates social control. cssns have strong societal implications, fostering situations that combine global connectivity, the fragmentation of solidarities, the de-emphasis of local organizations (in the neighborhood and workplace), and the increased importance of home bases.
945	472947	article	protein science :  a publication of the protein society.	\N	\N	\N	10	12	6	2003	jun	2006-01-20 23:01:19	sackler institute of molecular medicine, department of human genetics and molecular medicine, sackler school of medicine, tel aviv university, tel aviv 69978, israel.	reducing the computational complexity of protein folding via fragment folding and assembly.	understanding, and ultimately predicting, how a {1-d} protein chain reaches its native {3-d} fold has been one of the most challenging problems during the last few decades. data increasingly indicate that protein folding is a hierarchical process. hence, the question arises as to whether we can use the hierarchical concept to reduce the practically intractable computational times. for such a scheme to work, the first step is to cut the protein sequence into fragments that form local minima on the polypeptide chain. the conformations of such fragments in solution are likely to be similar to those when the fragments are embedded in the native fold, although alternate conformations may be favored during the mutual stabilization in the combinatorial assembly process. two elements are needed for such cutting: (1) a library of (clustered) fragments derived from known protein structures and (2) an assignment algorithm that selects optimal combinations to "cover" the protein sequence. the next two steps in hierarchical folding schemes, not addressed here, are the combinatorial assembly of the fragments and finally, optimization of the obtained conformations. here, we address the first step in a hierarchical protein-folding scheme. the input is a target protein sequence and a library of fragments created by clustering building blocks that were generated by cutting all protein structures. the output is a set of cutout fragments. we briefly outline a graph theoretic algorithm that automatically assigns building blocks to the target sequence, and we describe a sample of the results we have obtained.
946	475048	article	phys. rev. e	\N	\N	\N	\N	66	\N	2002	jul	2006-01-22 14:43:13	\N	spread of epidemic disease on networks	the study of social networks, and in particular the spread of disease on networks, has attracted considerable recent attention in the physics community. in this paper, we show that a large class of standard epidemiological models, the so-called susceptible/infective/removed Ã´ÂÂ°Â”sirÃ´ÂÂ°Â• models can be solved exactly on a wide variety of networks. in addition to the standard but unrealistic case of fixed infectiveness time and fixed and uncorrelated probability of transmission between all pairs of individuals, we solve cases in which times and probabilities are nonuniform and correlated. we also consider one simple case of an epidemic in a structured population, that of a sexually transmitted disease in a population divided into men and women. we confirm the correctness of our exact solutions with numerical simulations of sir epidemics on networks.
947	475980	article	annu rev phys chem	\N	\N	\N	-77	51	\N	2000	\N	2006-01-22 16:33:01	\N	generalized born models of macromolecular solvation effects.	it would often be useful in computer simulations to use a simple description of solvation effects, instead of explicitly representing the individual solvent molecules. continuum dielectric models often work well in describing the thermodynamic aspects of aqueous solvation, and approximations to such models that avoid the need to solve the poisson equation are attractive because of their computational efficiency. here we give an overview of one such approximation, the generalized born model, which is simple and fast enough to be used for molecular dynamics simulations of proteins and nucleic acids. we discuss its strengths and weaknesses, both for its fidelity to the underlying continuum model and for its ability to replace explicit consideration of solvent molecules in macromolecular simulations. we focus particularly on versions of the generalized born model that have a pair-wise analytical form, and therefore fit most naturally into conventional molecular mechanics calculations.
948	476071	article	genome res	\N	\N	\N	-2393	13	11	2003	nov	2006-01-22 16:33:03	\N	parameter estimation in biochemical pathways: a comparison of global optimization methods.	here we address the problem of parameter estimation (inverse problem) of nonlinear dynamic biochemical pathways. this problem is stated as a nonlinear programming ({nlp}) problem subject to nonlinear differential-algebraic constraints. these problems are known to be frequently ill-conditioned and multimodal. thus, traditional (gradient-based) local optimization methods fail to arrive at satisfactory solutions. to surmount this limitation, the use of several state-of-the-art deterministic and stochastic global optimization methods is explored. a case study considering the estimation of 36 parameters of a nonlinear biochemical dynamic model is taken as a benchmark. only a certain type of stochastic algorithm, evolution strategies ({es}), is able to solve this problem successfully. although these stochastic methods cannot guarantee global optimality with certainty, their robustness, plus the fact that in inverse problems they have a known lower bound for the cost function, make them the best available candidates.
949	477344	article	acm transactions on computer-human interaction	\N	\N	acm	33	12	3	2005	sep	2006-01-23 00:27:27	new york, ny, usa	social matching	social matching systems bring people together in both physical and online spaces. they have the potential to increase social interaction and foster collaboration. however, social matching systems lack a clear intellectual foundation: the nature of the design space, the key research challenges, and the roster of appropriate methods are all ill-defined. this article begins to remedy the situation. it clarifies the scope of social matching systems by distinguishing them from other recommender systems and related systems and techniques. it identifies a set of issues that characterize the design space of social matching systems and shows how existing systems explore different points within the design space. it also reviews selected social science results that can provide input into system design. most important, the article presents a research agenda organized around a set of claims. the claims embody our understanding of what issues are most important to investigate, our beliefs about what is most likely to be true, and our suggestions of specific research directions to pursue.
950	477418	book	\N	\N	\N	psychology press	\N	\N	\N	1986	sep	2006-01-23 04:28:45	\N	the ecological approach to visual perception	the great perception theorist j. j. gibson (gibson, 1979) brought about radical changes in the ways we think about perception with his theories of ecological optics, affordances and direct perception.  gibson assumed that we perceive in order to operate on the environment. perception is designed for action. gibson called the perceivable possibilities for action affordances. he claimed that we perceive affordance properties of the environment in a direct and immediate way. this theory is clearly attractive from the perspective of visualization. the goal of most visualization is decision making. in short, gibson claims that we perceive possibilities for action. i.e. surfaces for walking, handles for pulling, space for navigation, tools for manipulating, etc. in general, our whole evolution has been geared toward perceiving useful possibilities for action.  affordance example: (norman, 1988)  you are approaching a door through which you eventually want to pass. the door, and the manner in which it is secured to the wall, permits opening by pushing it from its 'closed' position. we say that the door affords (or allows, or is for) opening by pushing. on approaching that door you observe a flat plate fixed to it at waist height on the 'non-hinge' side, and possibly some sticky finger marks on its otherwise polished surface. you deduce that the door is meant to be pushed open: you therefore push on the plate, whereupon the door opens and you pass through. here, there is a perceived affordance, triggered by the sight of the plate and the finger marks, that is identical with the actual affordance. note that the affordance we discuss is neither the door nor the plate: it is a property of the door (the door affords opening by pushing).
951	477435	inproceedings	\N	proceedings of the 1st acm sigsoft workshop on self-managed systems	woss	acm	5	\N	\N	2004	\N	2006-01-23 05:18:28	new york, ny, usa	a survey of self-management in dynamic software architecture specifications	as dynamic software architecture use becomes more widespread, a variety of formal specification languages have been developed to gain a better understanding of the foundations of this type of software evolutionary change. in this paper we survey 14 formal specification approaches based on graphs, process algebras, logic, and other formalisms. our survey will evaluate the ability of each approach to specify self-managing systems as well as the ability to address issues regarding expressiveness and scalability. based on the results of our survey we will provide recommendations on future directions for improving the specification of dynamic software architectures, specifically self-managed architectures.
952	477678	book	\N	\N	information science and knowledge management	springer	\N	\N	\N	2005	aug	2006-01-23 15:37:16	\N	citation analysis in research evaluation	this book deals with the evaluation of scholarly research performance, and focuses on the contribution of scholarly work to the advancement of scholarly knowledge. its principal question is: how can citation analysis be used properly as a tool in the assessment of such a contribution? citation analysis involves the construction and application of a series of indicators of the 'impact', 'influence' or 'quality' of scholarly work, derived from references cited in footnotes or bibliographies of scholarly research publications. it describes primarily the use of data extracted from the science citation index and the web of science, published by the institute for scientific information ({isi})/thomson scientific. but many aspects to which this book dedicates attention relate to citation analysis in general, it provides a wide range of important facts, and corrects a number of common misunderstandings about citation analysis. it introduces basic notions and distinctions, and deals both with theoretical and technical aspects, and with its applicability in various policy contexts, at the level of individual scholars, research groups, departments, institutions, national scholarly systems, disciplines or subfields, and scholarly journals. although the major part of the analysis relates to the basic science – a domain in which citation analysis is used most frequently – this book also addresses its uses and limits in the applied and technical sciences, social sciences and humanities. it reveals the enormous potential of quantitative, bibliometric analyses of the scholarly literature for a deeper understanding of scholarly activity and performance, and highlights their policy relevance. but this book is also critical, underlines the limits of citation analysis in research evaluation, and issues warnings for potential misuse. it proposes criteria for proper use of citation analysis as a research evaluation tool. in order to be used properly as a research evaluation tool, it is essential that all participants have insight into the nature of citation analysis, how its indicators are constructed and calculated, what the various theoretical positions state about what they measure, and what are their potentialities and limitations, particularly in relation to peer review. this book aims at providing such insight.
953	478558	article	evolution	\N	\N	\N	14	52	4	1998	\N	2006-01-23 21:09:29	\N	the population genetics of adaptation: the distribution of factors fixed during adaptive evolution	we know very little about the genetic basis of adaptation. indeed, we can make no theoretical predictions, however heuristic, about the distribution of phenotypic effects among factors fixed during adaptation nor about the expected 'size' of the largest factor fixed. study of this problem requires taking into account that populations gradually approach a phenotypic optimum during adaptation via the stepwise substitution of favorable mutations. using fisher's geometric model of adaptation, i analyze this approach to the optimum, and derive an approximate solution to the size distribution of factors fixed during adaptation. i further generalize these results to allow the input of any distribution of mutational effects. the distribution of factors fixed during adaptation assumes a pleasingly simple, exponential form. this result is remarkably insensitive to changes in the fitness function and in the distribution of mutational effects. an exponential trend among factors fixed appears to be a general property of adaptation toward a fixed optimum.
954	478599	book	\N	\N	\N	{open court publishing company}	\N	\N	\N	1993	dec	2006-01-23 22:58:52	\N	scientific reasoning: the bayesian approach	{in this clearly reasoned defense of bayes's theorem \\&\\#151; that probability can be used to reasonably justify scientific theories \\&\\#151; colin howson and peter urbach examine the way in which scientists appeal to probability arguments, and demonstrate that the classical approach to statistical inference is full of flaws. arguing the case for the bayesian method with little more than basic algebra, the authors show that it avoids the difficulties of the classical system. the book also refutes the major criticisms leveled against bayesian logic, especially that it is too subjective. this newly updated edition of this classic textbook is also suitable for college courses.}
955	478739	article	journal of biomedical informatics	\N	\N	\N	10	37	1	2004	feb	2006-01-24 12:21:24	columbia genome center, columbia university, new york, ny 10032, usa. ar345@columbia.edu	{geneways}: a system for extracting, analyzing, visualizing, and integrating molecular pathway data.	the immense growth in the volume of research literature and experimental data in the field of molecular biology calls for efficient automatic methods to capture and store information. in recent years, several groups have worked on specific problems in this area, such as automated selection of articles pertinent to molecular biology, or automated extraction of information using natural-language processing, information visualization, and generation of specialized knowledge bases for molecular biology. geneways is an integrated system that combines several such subtasks. it analyzes interactions between molecular substances, drawing on multiple sources of information to infer a consensus view of molecular networks. geneways is designed as an open platform, allowing researchers to query, review, and critique stored information.
956	480467	book	\N	\N	\N	{libraries unlimited}	\N	\N	\N	2003	nov	2006-01-25 15:01:39	\N	the organization of information : second edition (library and information science text series)	{the extensively revised and completely updated second edition of this popular textbook provides lis practitioners and students with a vital guide to the organization of information. after a broad overview of the concept and its role in human endeavors, taylor proceeds to a detailed and insightful discussion of such basic retrieval tools as bibliographies, catalogs, indexes, finding aids, registers, databases, major bibliographic utilities, and other organizing entities. after tracing the development of the organization of recorded information in western civilization from 2000 b.c.e. to the present, the author addresses topics that include encoding standards (marc, sgml, and various dtds), metadata (description, access, and access control), verbal subject analysis including controlled vocabularies and ontologies, classification theory and methodology, arrangement and display, and system design.}
957	482141	article	nature neuroscience	\N	\N	nature publishing group	6	9	2	2006	feb	2006-01-30 15:13:55	\N	activity in prefrontal cortex during dynamic selection of action sequences.	completing everyday tasks often requires the execution of action sequences matched to a particular problem. to study the neural processes underlying these behaviors, we trained monkeys to produce a series of eye movements according to a sequence that changed unpredictably from one block of trials to the next. we then applied a decoding algorithm to estimate which sequence was being represented by the ensemble activity in prefrontal cortex. we found that the sequence predicted by this analysis changed gradually from the sequence that had been correct in the previous block to the sequence that was correct in the current block, closely following the fraction of executed movements that were consistent with the corresponding sequence. thus, the neural activity dynamically tracked the monkeys' uncertainty about the correct sequence of actions. these results are consistent with prefrontal involvement in representing subjective knowledge of the correct action sequence.
958	483203	article	j mol biol	\N	\N	\N	35	145	1	1981	jan	2006-01-27 23:46:01	\N	helix to helix packing in proteins.	analysis of the pattern of residue to residue contacts at the interface of 50 helix to helix packings observed in ten proteins of known structure supports a model for helix to helix packing in which the ridges and grooves on the helix surface intercalate. these ridges are formed by rows of residues whose separation in sequence is usually four, occasionally three and rarely one. the model explains the observed predominance of packings whose interhelical angle is ~ -50 [deg]. of the 50 packings, 38 agree with the model and the general features of another ten packings are described by an extension to the model in which ridges can pack across each other if a small side-chain occurs at the place where they cross.
959	484900	article	journal of molecular biology	\N	\N	\N	16	356	4	2006	mar	2006-01-29 20:15:09	\N	conformational diversity of ligands bound to proteins	the phenomenon of molecular recognition, which underpins almost all biological processes, is dynamic, complex and subtle. establishing an interaction between a pair of molecules involves mutual structural rearrangements guided by a highly convoluted energy landscape, the accurate mapping of which continues to elude us. increased understanding of the degree to which the conformational space of a ligand is restricted upon binding may have important implications for docking studies, structure refinement and for function prediction methods based on geometrical comparisons of ligands or their binding sites. here, we present an analysis of the conformational variability exhibited by three of the most ubiquitous biological ligands in nature, {atp}, {nad} and {fad}. first, we demonstrate qualitatively that these ligands bind to proteins in widely varying conformations, including several cases in which parts of the molecule assume energetically unfavourable orientations. next, by comparing the distribution of bound ligand shapes with the set of all possible molecular conformations, we provide a quantitative assessment of previous observations that ligands tend to unfold when binding to proteins. we show that, while extended forms of ligands are indeed common in ligand–protein structures, instances of ligands in almost maximally compact arrangements can also be found. thirdly, we compare the conformational variation in two sets of ligand molecules, those bound to homologous proteins, and those bound to unrelated proteins. although most superfamilies bind ligands in a fairly conserved manner, we find several cases in which significant variation in ligand configuration is observed.
960	485372	article	j. am. chem. soc.	journal of the american chemical society	\N	american chemical society	7	128	4	2006	jan	2006-01-30 01:59:41	contribution from the department of biochemistry and biophysics, university of north carolina, campus box 7260, chapel hill, north carolina 27599.	computational design of a single amino acid sequence that can switch between two distinct protein folds	the functions of many proteins are mediated by specific conformational changes, and therefore the ability to design primary sequences capable of secondary and tertiary changes is an important step toward the creation of novel functional proteins. to this end, we have developed an algorithm that can optimize a single amino acid sequence for multiple target structures. the algorithm consists of an outer loop, in which sequence space is sampled by a monte carlo search with simulated annealing, and an inner loop, in which the effect of a given mutation is evaluated on the various target structures by using the rotamer packing routine and composite energy function of the protein design software, {rosettadesign}. we have experimentally tested the method by designing a peptide, sw2, which can be switched from a {2cys}-{2his} zinc finger-like fold to a trimeric coiled-coil fold, depending upon the {ph} or the presence of transition metals. physical characterization of sw2 confirms that it is able to reversibly adopt each intended target fold.
961	487356	inproceedings	\N	ifip international conference on network and parallel computing	lecture notes in computer science	springer verlag	11	3779	\N	2005	\N	2006-01-31 14:54:48	\N	globus toolkit version 4: software for {service-oriented} systems	abstract&nbsp;&nbsp;the globus toolkit (gt) has been developed since the late 1990s to support the development of service-oriented distributed computing applications and infrastructures. core gt components address, within a common framework, fundamental issues relating to security, resource access, resource management, data movement, resource discovery, and so forth. these components enable a broader â€œglobus ecosystemâ€ of tools and components that build on, or interoperate with, gt functionality to provide a wide range of useful application-level functions. these tools have in turn been used to develop a wide range of both â€œgridâ€ infrastructures and distributed applications. i summarize here the principal characteristics of the recent web services-based gt4 release, which provides significant improvements over previous releases in terms of robustness, performance, usability, documentation, standards compliance, and functionality. i also introduce the new â€œdev.globusâ€ community development process, which allows a larger community to contribute to the development of globus software.
962	489193	article	bioinformatics	\N	\N	oxford university press	6	22	2	2006	jan	2006-02-02 05:48:58	\N	the {swiss}-{model} workspace: a web-based environment for protein structure homology modelling	motivation: homology models of proteins are of great interest for planning and analysing biological experiments when no experimental three-dimensional structures are available. building homology models requires specialized programs and up-to-date sequence and structural databases. integrating all required tools, programs and databases into a single web-based workspace facilitates access to homology modelling from a computer with web connection without the need of downloading and installing large program packages and {databases.results}: {swiss}-{model} workspace is a web-based integrated service dedicated to protein structure homology modelling. it assists and guides the user in building protein homology models at different levels of complexity. a personal working environment is provided for each user where several modelling projects can be carried out in parallel. protein sequence and structure databases necessary for modelling are accessible from the workspace and are updated in regular intervals. tools for template selection, model building and structure quality evaluation can be invoked from within the workspace. workflow and usage of the workspace are illustrated by modelling human cyclin a1 and human transmembrane protease {3.availability}: the {swiss}-{model} workspace can be accessed freely at {http://swissmodel.expasy.org/workspace/contact}:{torsten.schwede}@{unibas.chsupplementary} information: supplementary data are available at bioinformatics online.
963	491420	article	ieee transactions on visualization and computer graphics	\N	\N	\N	11	8	1	2002	\N	2006-02-03 12:35:45	\N	{themeriver}: visualizing thematic changes in large document collections	the themeriver visualization depicts thematic variations over time within a large collection of documents. the thematic changes are shown in the context of a time line and corresponding external events. the focus on temporal thematic change within a context framework allows a user to discern patterns that suggest relationships or trends. for example, the sudden change of thematic strength following an external event may indicate a causal relationship. such patterns are not readily accessible in other visualizations of the data. we use a river metaphor to convey several key notions. the document collection's time line, selected thematic content, and thematic strength are indicated by the river's directed flow, composition, and changing width, respectively. the directed flow from left to right is interpreted as movement through time and the horizontal distance between two points on the river defines a time interval. at any point in time, the vertical distance, or width, of the river indicates the collective strength of the selected themes. colored {\\\\grqq}currentsâ€ flowing within the river represent individual themes. a current's vertical width narrows or broadens to indicate decreases or increases in the strength of the individual theme.
964	491903	article	the vldb journal	\N	\N	springer-verlag new york, inc.	14	13	1	2004	\N	2006-02-03 12:37:36	\N	retrieval effectiveness of an ontology-based model for information selection	technology in the field of digital media generates huge amounts of nontextual information, audio, video, and images, along with more familiar textual information. the potential for exchange and retrieval of information is vast and daunting. the key problem in achieving efficient and user-friendly retrieval is the development of a search mechanism to guarantee delivery of minimal irrelevant information (high precision) while insuring relevant information is not overlooked (high recall). the traditional solution employs keyword-based search. the only documents retrieved are those containing user-specified keywords. but many documents convey desired semantic information without containing these keywords. this limitation is frequently addressed through query expansion mechanisms based on the statistical co-occurrence of terms. recall is increased, but at the expense of deteriorating precision. one can overcome this problem by indexing documents according to context and meaning rather than keywords, although this requires a method of converting words to meanings and the creation of a meaning-based index structure. we have solved the problem of an index structure through the design and implementation of a concept-based model using domain-dependent ontologies. an ontology is a collection of concepts and their interrelationships that provide an abstract view of an application domain. with regard to converting words to meaning, the key issue is to identify appropriate concepts that both describe and identify documents as well as language employed in user requests. this paper describes an automatic mechanism for selecting these concepts. an important novelty is a scalable disambiguation algorithm that prunes irrelevant concepts and allows relevant ones to associate with documents and participate in query generation. we also propose an automatic query expansion mechanism that deals with user requests expressed in natural language. this mechanism generates database queries with appropriate and relevant expansion through knowledge encoded in ontology form. focusing on audio data, we have constructed a demonstration prototype. we have experimentally and analytically shown that our model, compared to keyword search, achieves a significantly higher degree of precision and recall. the techniques employed can be applied to the problem of information selection in all media types.
965	493773	book	\N	\N	\N	wiley	\N	\N	\N	2004	jan	2006-02-04 11:07:59	\N	secrets and lies: digital security in a networked world	{whom can you trust? try bruce schneier, whose rare gift for common sense  makes his book <i>secrets and lies: digital security in a networked world</i> both  enlightening and practical. he's worked in cryptography and electronic security  for years, and has reached the depressing conclusion that even the loveliest  code and toughest hardware still will yield to attackers who exploit human  weaknesses in the users. the book is neatly divided into three parts, covering  the turn-of-the-century landscape of systems and threats, the technologies used  to protect and intercept data, and strategies for proper implementation of  security systems. moving away from blind faith in prevention, schneier advocates  swift detection and response to an attack, while maintaining firewalls and  other gateways to keep out the amateurs.<p>  newcomers to the world of schneier will be surprised at how funny he can be,  especially given a subject commonly perceived as quiet and dull. whether he's  analyzing the security issues of the rebels and the death star in <i>star  wars</i> or poking fun at the giant software and e-commerce companies that  consistently sacrifice security for sexier features, he's one of the few tech  writers who can provoke laughter consistently. while moderately pessimistic on  the future of systems vulnerability, he goes on to relieve the reader's tension  by comparing our electronic world to the equally insecure paper world we've  endured for centuries--a little smart-card fraud doesn't seem so bad after all.  despite his unfortunate (but brief) shill for his consulting company in the  book's afterword, you can trust schneier to dish the dirt in <i>secrets and  lies</i>. <i>--rob lightner</i>      } {bestselling author bruce schneier offers his expert guidance on achieving  security on a network internationally recognized computer security expert  bruce schneier offers a practical, straightforward guide to achieving  security throughout computer networks. schneier uses his extensive field  experience with his own clients to dispel the myths that often mislead it  managers as they try to build secure systems. this practical guide  provides readers with a better understanding of why protecting information is harder in the digital world, what they need to know to protect digital  information, how to assess business and corporate security needs, and much more. * walks the reader through the real choices they have now for  digital security and how to pick and choose the right one to meet their  business needs * explains what cryptography can and can't do in achieving  digital security  }
966	494263	article	bioinformatics	\N	\N	\N	8	21	suppl 1	2005	jun	2006-02-06 16:38:41	program in molecular and computational biology, university of southern california los angeles, ca 90089, usa.	mining coherent dense subgraphs across massive biological networks for functional discovery	motivation: the rapid accumulation of biological network data translates into an urgent need for computational methods for graph pattern mining. one important problem is to identify recurrent patterns across multiple networks to discover biological modules. however, existing algorithms for frequent pattern mining become very costly in time and space as the pattern sizes and network numbers increase. currently, no efficient algorithm is available for mining recurrent patterns across large collections of genome-wide {networks.results}: we developed a novel algorithm, {codense}, to efficiently mine frequent coherent dense subgraphs across large numbers of massive graphs. compared with previous methods, our approach is scalable in the number and size of the input graphs and adjustable in terms of exact or approximate pattern mining. applying {codense} to 39 co-expression networks derived from microarray datasets, we discovered a large number of functionally homogeneous clusters and made functional predictions for 169 uncharacterized yeast {genes.availability}: {http://zhoulab.usc.edu/codense}/contact: xjzhou@usc.edu
967	494558	article	cognitive psychology	\N	\N	\N	57	8	\N	1976	\N	2006-02-06 22:43:10	\N	basic objects in natural categories	notes that categorizations which humans make of the concrete world are not arbitrary but highly determined; in taxonomies of concrete objects, there is 1 level of abstraction at which the most basic category cuts are made. {b}asic categories are those which carry the most information, possess the highest category cue validity, and are, thus, the most differentiated from one another. {t}he 4 experiments of {p}art 1, with approximately 502 {s}s, define basic objects by demonstrating that in taxonomies of common concrete nouns in {e}nglish based on class inclusion, basic objects are the most inclusive categories whose members (a) possess significant numbers of attributes in common, (b) have motor programs which are similar to one another, (c) have similar shapes, and (d) can be identified from averaged shapes of members of the class. {t}he 8 experiments of {p}art 2, with approximately 502 {s}s, explored implications of the structure of categories. {b}asic objects were shown to be the most inclusive categories for which a concrete image of the category as a whole could be formed, to be the 1st categorizations made during perception of the environment, to be the earliest categories sorted and earliest named by children, and to be the categories most codable, most coded, and most necessary in language.
968	495430	book	\N	\N	\N	\N	\N	\N	\N	2006	jan	2006-02-07 10:40:10	\N	programming languages: application and interpretation	contents preface iii acknowledgments vii i prelude 1 1 modeling languages 3 1.1 modeling meaning 1.2 modeling syntax 1.3 a primer on parsers 1.4 primus inter parsers ii rudimentary interpreters 11 2 interpreting arithmetic 13 3 substitution 15 3.1 defining substitution 3.2 calculating with with 3.3 the scope of with expressions 3.4 what kind of redundancy do identifiers eliminate? 3.5 are names necessary? 4 an introduction to functions 27 4.1 enriching the language with functions 4.2 the scope of substitution 4.3 the scope of function definitions 5 deferring substitution 33 5.1 the substitution repository 5.2 deferring substitution correctly ix x contents 5.3 fixing the interpreter 6 first-class functions 41 6.1 a taxonomy of functions 6.2 enriching the language with functions 6.3 making with redundant 6.4 implementing functions using deferred substitutions 6.5 some perspective on scope 6.5.1 filtering and sorting lists 6.5.2 differentiation 6.5.3 callbacks 6.6 eagerness and laziness 6.7 standardizing terminology iii laziness 57 7 programming with laziness 59 7.1 haskell 7.1.1 expressions and definitions 7.1.2 lists 7.1.3 polymorphic type inference
969	498337	article	journal of fluid mechanics	\N	\N	\N	33	177	\N	1987	\N	2006-02-08 08:11:47	\N	turbulence statistics in fully developed channel flow at low reynolds number	a direct numerical simulation of a turbulent channel flow is performed. the unsteady navier-stokes equations are solved numerically at a reynolds number of 3300, based on the mean centreline velocity and channel half-width, with about 4 &times; 106 grid points (192 &times; 129 &times; 160 in x, y, z). all essential turbulence scales are resolved on the computational grid and no subgrid model is used. a large number of turbulence statistics are computed and compared with the existing experimental data at comparable reynolds numbers. agreements as well as discrepancies are discussed in detail. particular attention is given to the behaviour of turbulence correlations near the wall. in addition, a number of statistical correlations which are complementary to the existing experimental data are reported for the first time.
970	498573	article	cell	\N	\N	\N	10	122	4	2005	aug	2006-02-08 09:14:44	whitehead institute for biomedical research, nine cambridge center, cambridge, massachusetts 02142, usa.	genome-wide map of nucleosome acetylation and methylation in yeast.	eukaryotic genomes are packaged into nucleosomes whose position and chemical modification state can profoundly influence regulation of gene expression. we profiled nucleosome modifications across the yeast genome using chromatin immunoprecipitation coupled with dna microarrays to produce high-resolution genome-wide maps of histone acetylation and methylation. these maps take into account changes in nucleosome occupancy at actively transcribed genes and, in doing so, revise previous assessments of the modifications associated with gene expression. both acetylation and methylation of histones are associated with transcriptional activity, but the former occurs predominantly at the beginning of genes, whereas the latter can occur throughout transcribed regions. most notably, specific methylation events are associated with the beginning, middle, and end of actively transcribed genes. these maps provide the foundation for further understanding the roles of chromatin in gene expression and genome maintenance.
971	498972	article	neuron	\N	\N	\N	10	20	5	1998	may	2006-02-08 17:17:28	howard hughes medical institute and sloan center for theoretical neurobiology, the salk institute for biological studies, la jolla, california 92037, usa.	efficient discrimination of temporal patterns by motion-sensitive neurons in primate visual cortex.	although motion-sensitive neurons in macaque middle temporal ({mt}) area are conventionally characterized using stimuli whose velocity remains constant for 1-3 s, many ecologically relevant stimuli change on a shorter time scale (30-300 ms). we compared neuronal responses to conventional (constant-velocity) and time-varying stimuli in alert primates. the responses to both stimulus ensembles were well described as rate-modulated poisson processes but with very high precision (approximately 3 ms) modulation functions underlying the time-varying responses. information-theoretic analysis revealed that the responses encoded only approximately 1 bit/s about constant-velocity stimuli but up to 29 bits/s about the time-varying stimuli. analysis of local field potentials revealed that part of the residual response variability arose from "noise" sources extrinsic to the neuron. our results demonstrate that extrastriate neurons in alert primates can encode the fine temporal structure of visual stimuli.
972	499778	article	nature	\N	\N	nature publishing group	7	393	6685	1998	jun	2006-02-09 03:04:28	sanger centre, wellcome trust genome campus, hinxton, uk. stcole@pasteur.fr	deciphering the biology of mycobacterium tuberculosis from the complete genome sequence	countless millions of people have died from tuberculosis, a chronic infectious disease caused by the tubercle bacillus. the complete genome sequence of the best-characterized strain of mycobacterium tuberculosis, {h37rv}, has been determined and analysed in order to improve our understanding of the biology of this slow-growing pathogen and to help the conception of new prophylactic and therapeutic interventions. the genome comprises 4,411,529 base pairs, contains around 4,000 genes, and has a very high guanine + cytosine content that is reflected in the biased amino-acid content of the proteins. m. tuberculosis differs radically from other bacteria in that a very large portion of its coding capacity is devoted to the production of enzymes involved in lipogenesis and lipolysis, and to two new families of glycine-rich proteins with a repetitive structure that may represent a source of antigenic variation.
973	500745	inproceedings	\N	proceedings of the 24th annual international {acm sigir} conference on research and development in information retrieval	\N	acm press	8	\N	\N	2001	\N	2006-02-10 18:19:27	\N	document language models, query models, and risk minimization for information retrieval	we present a framework for information retrieval that combines document models and query models using a probabilistic ranking function based on bayesian decision theory. the framework suggests an operational retrieval model that extends recent developments in the language modeling approach to information retrieval.  a language model for each document is estimated, as well as a language model for each query, and the retrieval problem is cast in terms of risk minimization. the query language model can be exploited to model user preferences, the context of a query, synonomy and word senses. while recent work has incorporated word translation models for this purpose, we introduce a new method using markov chains defined on a set of documents to estimate the query models.  the markov chain method has connections to algorithms from link analysis and social networks.  the new approach is evaluated on trec collections and compared to the basic language modeling approach and vector space models together with query expansion using rocchio.  significant improvements are obtained over standard query expansion methods for strong baseline tf-idf systems, with the greatest improvements attained for short queries on web data.
974	501140	article	bioinformatics	\N	\N	oxford university press	4	22	3	2006	feb	2006-03-19 20:20:29	\N	comparison of bayesian and maximum-likelihood inference of population genetic parameters	comparison of the performance and accuracy of different inference methods, such as maximum likelihood ({ml}) and bayesian inference, is difficult because the inference methods are implemented in different programs, often written by different authors. both methods were implemented in the program {migrate}, that estimates population genetic parameters, such as population sizes and migration rates, using coalescence theory. both inference methods use the same markov chain monte carlo algorithm and differ from each other in only two aspects: parameter proposal distribution and maximization of the likelihood function. using simulated datasets, the bayesian method generally fares better than the {ml} approach in accuracy and coverage, although for some values the two approaches are equal in {performance.motivation}: the markov chain monte carlo-based {ml} framework can fail on sparse data and can deliver non-conservative support intervals. a bayesian framework with appropriate prior distribution is able to remedy some of these {problems.results}: the program {migrate} was extended to allow not only for {ml}(-) maximum likelihood estimation of population genetics parameters but also for using a bayesian framework. comparisons between the bayesian approach and the {ml} approach are facilitated because both modes estimate the same parameters under the same population model and {assumptions.availability}: the program is available from {http://popgen.csit.fsu.edu/contact}:beerli@csit.fsu.edu
975	501539	article	mach. learn.	machine learning	\N	kluwer academic publishers	29	62	1-2	2006	feb	2007-11-06 17:49:26	hingham, ma, usa	markov logic networks	we propose a simple approach to combining first-order logic and probabilistic graphical models in a single representation. a markov logic network ({mln}) is a first-order knowledge base with a weight attached to each formula (or clause). together with a set of constants representing objects in the domain, it specifies a ground markov network containing one feature for each possible grounding of a first-order formula in the {kb}, with the corresponding weight. inference in {mlns} is performed by {mcmc} over the minimal subset of the ground network required for answering the query. weights are efficiently learned from relational databases by iteratively optimizing a pseudo-likelihood measure. optionally, additional clauses are learned using inductive logic programming techniques. experiments with a real-world database and knowledge base in a university domain illustrate the promise of this approach.
976	502008	inproceedings	\N	proceedings of the fourth international joint conference on autonomous agents and multiagent systems	aamas	acm	7	\N	\N	2005	\N	2006-02-11 19:44:24	new york, ny, usa	agent-organized networks for dynamic team formation	many multi-agent systems consist of a complex network of autonomous yet interdependent agents. examples of such networked multi-agent systems include supply chains and sensor networks. in these systems, agents have a select set of other agents with whom they interact based on environmental knowledge, cognitive capabilities, resource limitations, and communications constraints. previous findings have demonstrated that the structure of the artificial social network governing the agent interactions is strongly correlated with organizational performance. as multi-agent systems are typically embedded in dynamic environments, we wish to develop distributed, on-line network adaptation mechanisms for discovering effective network structures. therefore, within the context of dynamic team formation, we propose several strategies for agent-organized networks ({aons}) and evaluate their effectiveness for increasing organizational performance.
977	502645	article	health information \\& libraries journal	\N	\N	blackwell publishing ltd	9	23	1	2006	mar	2008-12-20 02:03:25	\N	developing efficient search strategies to identify reports of adverse effects in medline and embase	abstract objective:? this study aimed to assess the performance, in terms of sensitivity and precision, of different approaches to searching medline and embase to identify studies of adverse effects. methods:? five approaches to searching for adverse effects evidence were identified: approach 1, using specified adverse effects; approach 2, using subheadings/qualifiers; approach 3, using text words; approach 4, using indexing terms; approach 5, searching for specific study designs. the sensitivity and precision of these five approaches, and combinations of these approaches, were compared in a case study using a systematic review of the adverse effects of seven anti-epileptic drugs. results:? the most sensitive search strategy in medline (97.0\\%) required a combination of terms for specified adverse effects, floating subheadings, and text words for 'adverse effects'. in embase, a combination of terms for specified adverse effects and text words for 'adverse effects' provided the most sensitive search strategy (98.6\\%). both these search strategies yielded low precision (2.8\\%). conclusions:? a highly sensitive search in either database requires a combination of approaches, and has low precision. this suggests that better reporting and indexing of adverse effects is required and that an effective generic search filter may not yet be feasible.
978	503602	article	sigops oper. syst. rev.	sosp	\N	acm	15	17	5	1983	oct	2006-02-13 09:15:39	new york, ny, usa	hints for computer system design	experience with the design and implementation of a number of computer systems, and study of many other systems, has led to some general hints for system design which are described here. they are illustrated by a number of examples, ranging from hardware such as the alto and the dorado to applications programs such as bravo and star.
979	503636	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2006-02-13 09:53:54	\N	{self-managing} systems: a control theory foundation	summary form only given. the high cost of ownership of computing systems has resulted in a number of industry initiatives to reduce the burden of operations and management by making systems more self-managing. a major challenge in realizing self-managing systems is understanding how automated actions affect system behavior, especially system stability. other disciplines such as mechanical, electrical, and aeronautical engineering make use of control theory to design feedback systems. the talk uses control theory as a way to identify a number of requirements for and challenges in building self-managing, or autonomic, systems. in essence, the autonomic computing architecture describes feedback control loops for self-managing systems. the talk has three goals: (1) educating systems oriented computer science researchers and practitioners on the concepts and techniques needed to apply control theory to computing systems; (2) describing how control theory can aid in building self-managing systems and identifying the challenges in doing so; (3) describing a deployable testbed for autonomic computing that is intended to foster research that addresses the challenges identified.
980	503695	book	\N	\N	\N	\N	\N	\N	\N	1871	\N	2006-02-13 10:12:58	\N	principles of economics	in the beginning, there was menger. it was this book that reformulated, and really rescued, economic science. it kicked off the marginalist revolution, which corrected theoretical errors of the old classical school. these errors concerned value theory, and they had sewn enough confusion to make the dangerous ideology of marxism seem more plausible than it really was.  menger set out to elucidate the precise nature of economic value, and root economics firmly in the real-world actions of individual human beings.  for this reason, carl menger (1840-1921) was the founder of the austrian school of economics. it is the book that mises said turned him into a real economist. what's striking is how nearly a century and a half later, the book still retains its incredible power, both in its prose and its relentless logic.  the mises institute's new edition features a new foreword by peter g. klein, which summarizes menger's contribution and places him in the history of ideas. he also explains his continued relevance.  economics students still say that it is the best introduction to economic logic ever written. the book also deserves the status as a seminal contribution to science in general. truly, no one can claim to be well read in economics without having mastered menger's argument.  328 pp. (pb)  isbn 978-1-933550-12-1 paperback
981	503699	book	\N	\N	\N	princeton university press	\N	\N	\N	1947	\N	2006-02-13 10:12:58	\N	theory of games and economic behavior	this is the classic work upon which modern-day game theory is based. what began more than sixty years ago as a modest proposal that a mathematician and an economist write a short paper together blossomed, in 1944, when princeton university press published _theory of games and economic behavior_. in it, john von neumann and oskar morgenstern conceived a groundbreaking mathematical theory of economic and social organization, based on a theory of games of strategy. not only would this revolutionize economics, but the entirely new field of scientific inquiry it yielded--game theory--has since been widely used to analyze a host of real-world phenomena from arms races to optimal policy choices of presidential candidates, from vaccination policy to major league baseball salary negotiations. and it is today established throughout both the social sciences and a wide range of other sciences. this sixtieth anniversary edition includes not only the original text but also an introduction by harold kuhn, an afterword by ariel rubinstein, and reviews and articles on the book that appeared at the time of its original publication in the _new york times_, tthe _american economic review_, and a variety of other publications. together, these writings provide readers a matchless opportunity to more fully appreciate a work whose influence will yet resound for generations to come.
982	503703	book	\N	\N	\N	w. w. norton and company	\N	\N	\N	2003	\N	2006-02-13 10:12:58	\N	globalization and its discontents	{due to massive media coverage, many people are familiar with the controversy and organized resistance that globalization has generated around the world, yet explaining what globalization actually means in practice is a complicated task. for those wanting to learn more, this book is an excellent place to start. an experienced economist, joseph stiglitz had a brilliant career in academia before serving for four years on president clinton's council of economic advisors and then three years as chief economist and senior vice president of the world bank. his book clearly explains the functions and powers of the main institutions that govern globalization--the international monetary fund, the world bank, and the world trade organization--along with the ramifications, both good and bad, of their policies. he strongly believes that globalization can be a positive force around the world, particularly for the poor, but only if the imf, world bank, and wto dramatically alter the way they operate, beginning with increased transparency and a greater willingness to examine their own actions closely. of his time at the world bank, he writes, "decisions were made on the basis of what seemed a curious blend of ideology and bad economics, dogma that sometimes seemed to be thinly veiling special interests.... open, frank discussion was discouraged--there was no room for it." the book is not entirely critical, however: "those who vilify globalization too often overlook its benefits," stiglitz writes, explaining how globalization, along with foreign aid, has improved the living standards of millions around the world. with this clear and balanced book, stiglitz has contributed significantly to the debate on this important topic. <i>--shawn carkonen</i> } {this powerful, unsettling book gives us a rare glimpse behind the closed doors of global financial institutions by the winner of the 2001 nobel prize in economics.  <p>when it was first published, this national bestseller quickly became a touchstone in the globalization debate. renowned economist and nobel prize winner joseph e. stiglitz had a ringside seat for most of the major economic events of the last decade, including stints as chairman of the council of economic advisers and chief economist at the world bank. particularly concerned with the plight of the developing nations, he became increasingly disillusioned as he saw the international monetary fund and other major institutions put the interests of wall street and the financial community ahead of the poorer nations.  <p>those seeking to understand why globalization has engendered the hostility of protesters in seattle and genoa will find the reasons here. while this book includes no simple formula on how to make globalization work, stiglitz provides a reform agenda that will provoke debate for years to come. rarely do we get such an insider's analysis of the major institutions of globalization as in this penetrating book. with a new foreword for this paperback edition.}
983	503720	article	american economic review	\N	\N	\N	15	70	3	1980	\N	2006-02-13 10:12:59	\N	on the impossibility of informationally efficient markets	if competitive equilibrium is defined as a situation in which prices are such that all arbitrage profits are eliminated, it is not clear whether it is possible that a competitive economy will always be in equilibrium. clearly not, for then those who arbitrage make no return from their costly activity. hence the assumptions that all markets, including that for information, are always in equilibrium and always perfectly arbitraged are inconsistent when arbitrage is costly. a model has been proposed in which there is an equilibrium degree of disequilibrium: prices reflect the information of informed individuals but only partially, so that those who expend resources to obtain information do receive compensation. the model is the simplest one in which prices perform a well-articulated role in conveying information from the informed to the uninformed. when informed individuals observe information that the return to a security is going to be high, they bid its price up, and conversely when t
984	503721	article	econometrica	\N	\N	\N	20	50	\N	1982	\N	2006-02-13 10:12:59	\N	autoregressive conditional heteroscedasticity with estimates of the variance of united kingdom inflation	traditional econometric models assume a constant one-period forecast variance. to generalize this implausible assumption, a new class of stochastic processes called autoregressive conditional heteroscedastic (arch) processes are introduced in this paper. these are mean zero, serially uncorrelated processes with nonconstant variances conditional on the past, but constant unconditional variances. for such processes, the recent past gives information about the one-period forecast variance. a regression model is then introduced with disturbances following an arch process. maximum likelihood estimators are described and a simple scoring iteration formulated. ordinary least squares maintains its optimality properties in this set-up, but maximum likelihood is more efficient. the relative efficiency is calculated and can be infinite. to test whether the disturbances follow an arch process, the lagrange multiplier procedure is employed. the test is based simply on the autocorrelation of the squared ols residuals. this model is used to estimate the means and variances of inflation in the u.k. the arch effect is found to be significant and the estimated variances increase substantially during the chaotic seventies.
985	503727	article	review of financial studies	\N	\N	\N	16	6	\N	1993	\N	2006-02-13 10:12:59	\N	a closed-form solution for options with stochastic volatility with applications to bond and currency options	i use a new technique to derive a closed-form solution for the price of a european call option on an asset with stochastic volatility. the model allows arbitrary correlation between volatility and spot-asset returns. i introduce stochastic interest rates and show how to apply the model to bond options and foreign currency options. simulations show that correlation between volatility and the spot asset's price is important for explaining return skewness and strike-price biases in the black-scholes (1973) model. the solution technique is based on characteristic functions and can be applied to other problems.
986	503737	article	quantitative finance	\N	\N	\N	13	1	\N	2001	\N	2006-02-13 10:12:59	\N	empirical properties of asset returns: stylized facts and statistical issues	we present a set of stylized empirical facts emerging from the statistical analysis of price variations in various types of financial markets. we first discuss some general issues common to all statistical studied of financial time series. various statistical properties of asset returns are then described: distributional properties, tail properties and extreme fluctuations, pathwise regularity, linear and nonlinear dependence of returns in time and across stocks. our description emphasizes properties common to a wide variety of markets and instruments. we then show how these statistical properties invalidate many of the common statistical approaches used to study financial data sets and examine some of the statistical problems encountered in each case.
987	503758	book	\N	\N	\N	blackwell	\N	\N	\N	2000	\N	2006-02-13 10:13:00	\N	market microstructure theory	{written by one of the leading authorities in market microstructure research, this book provides a comprehensive guide to the theoretical work in this important area of finance.after an introduction to the general issues and problems in market microstructure, the book examines the main theoretical models developed to address inventory-based issues. there is then an extensive examination and discussion of the information-based models, with particular attention paid to the linkage with rational expectations model and learning models. the concluding chapters are concerned with price dynamics and with applications of the various models to specific microstructure problems including:- liquidity.- multi-market trading.- market structure.- market design market microstructure theory includes extensive appendices developing bayesian learning and the rational expectations framework.}
988	503787	article	journal of political economy	\N	\N	\N	18	101	1	1993	\N	2006-02-13 10:13:00	\N	allocative efficiency of markets with {zero-intelligence} traders: market as a partial substitute for individual rationality	this paper reports market experiments in which human traders are replaced by 'zero-intelligence' programs that submit random bids and offers. imposing a budget constraint (i.e., n ot permitting traders to sell below their costs or buy above their valu es) is sufficient to raise the allocative efficiency of these auctions close to 100 percent. allocative efficiency of a double auction deri ves largely from its structure, independent of traders' motivation, intelligence, or learning. adam smith's invisible hand may be more powerful than some may have thought; it can generate aggregate rationality not only from individual rationality but also from individual irrationality.
989	503791	book	\N	\N	\N	cambridge: mit press	\N	\N	\N	1996	\N	2006-02-13 10:13:00	\N	growing artificial societies: social science from the bottom up	how do social structures and group behaviors arise from the interaction of individuals? in this groundbreaking study, joshua m. epstein and robert l. axtell approach this age-old question with cutting-edge computer simulation techniques. such fundamental collective behaviors as group formation, cultural transmission, combat, and trade are seen to "emerge" from the interaction of individual agents following simple local rules. in their computer model, epstein and axtell begin the development of a "bottom up" social science. their program, named sugarscape, simulates the behavior of artificial people (agents) located on a landscape of a generalized resource (sugar). agents are born onto the sugarscape with a vision, a metabolism, a speed, and other genetic attributes. their movement is governed by a simple local rule: "look around as far as you can; find the spot with the most sugar; go there and eat the sugar." every time an agent moves, it burns sugar at an amount equal to its metabolic rate. agents die if and when they burn up all their sugar. a remarkable range of social phenomena emerge. for example, when seasons are introduced, migration and hibernation can be observed. agents are accumulating sugar at all times, so there is always a distribution of wealth. next, epstein and axtell attempt to grow a "proto-history" of civilization. it starts with agents scattered about a twin-peaked landscape; over time, there is self-organization into spatially segregated and culturally distinct "tribes" centered on the peaks of the sugarscape. population growth forces each tribe to disperse into the sugar lowlands between the mountains. there, the two tribes interact, engaging in combat and competing for cultural dominance, to produce complex social histories with violent expansionist phases, peaceful periods, and so on. the proto-history combines a number of ingredients, each of which generates insights of its own. one of these ingredients is sexual reproduction. in some runs, the population becomes thin, birth rates fall, and the population can crash. alternatively, the agents may over-populate their environment, driving it into ecological collapse. when epstein and axtell introduce a second resource (spice) to the sugarscape and allow the agents to trade, an economic market emerges. the introduction of pollution resulting from resource-mining permits the study of economic markets in the presence of environmental factors. growing artificial societies is also available in cd-rom format which includes about fifty animations that develop the scenarios described in the text. this study is part of the 2050 project, a joint venture of the santa fe institute, the world resources institute, and the brookings institution. the project is an international effort to identify conditions for a sustainable global system in the middle of the next century and to design policy actions to help achieve such a system. robert l. axtell is a research associate in the brookings foreign policy studies program. they are both members of the santa fe institute. joshua m. epstein is senior fellow in the brookings foreign policy studies program and teaches at princeton university. he is author of strategy and force planning: the case of the persian gulf (1987). the calculus of conventional war (1985), and the 1987 defense budget and the 1988 defense budget.
990	503792	book	\N	\N	\N	princeton university press	\N	\N	\N	1997	\N	2006-02-13 10:13:00	\N	the complexity of cooperation: {agent—based} models of competition and collaboration	robert axelrod is widely known for his groundbreaking work in game theory and complexity theory. he is a leader in applying computer modeling to social science problems. his book _the evolution of cooperation_ has been hailed as a seminal contribution and has been translated into eight languages since its initial publication. the _complexity of cooperation_ is a sequel to that landmark book. it collects seven essays, originally published in a broad range of journals, and adds an extensive new introduction to the collection, along with new prefaces to each essay and a useful new appendix of additional resources. written in axelrod's acclaimed, accessible style, this collection serves as an introductory text on complexity theory and computer modeling in the social sciences and as an overview of the current state of the art in the field.  the articles move beyond the basic paradigm of the prisoner's dilemma to study a rich set of issues, including how to cope with errors in perception or implementation, how norms emerge, and how new political actors and regions of shared culture can develop. they use the shared methodology of agent-based modeling, a powerful technique that specifies the rules of interaction between individuals and uses computer simulation to discover emergent properties of the social system. _the complexity of cooperation_ is essential reading for all social scientists who are interested in issues of cooperation and complexity
991	503802	article	nature	\N	\N	\N	2	397	\N	1999	\N	2006-02-13 10:13:00	\N	scaling and criticality in a stochastic multi-agent model of a financial market	{financial prices have been found to exhibit some universal characteristics1, 2, 3, 4, 5, 6 that resemble the scaling laws characterizing physical systems in which large numbers of units interact. this raises the question of whether scaling in finance emerges in a similar way â€” from the interactions of a large ensemble of market participants. however, such an explanation is in contradiction to the prevalent 'efficient market hypothesis'7 in economics, which assumes that the movements of financial prices are an immediate and unbiased reflection of incoming news about future earning prospects. within this hypothesis, scaling in price changes would simply reflect similar scaling in the 'input' signals that influence them. here we describe a multi-agent model of financial markets which supports the idea that scaling arises from mutual interactions of participants. although the 'news arrival process' in our model lacks both power-law scaling and any temporal dependence in volatility, we find that it generates such behaviour as a result of interactions between agents.}
992	504229	article	nat genet	\N	\N	\N	3	29	2	2001	oct	2006-02-13 20:04:44	whitehead institute/massachusetts institute of technology, center for genome research, cambridge, massachusetts, usa. mjdaly@genome.wi.mit.edu	high-resolution haplotype structure in the human genome.	linkage disequilibrium ({ld}) analysis is traditionally based on individual genetic markers and often yields an erratic, non-monotonic picture, because the power to detect allelic associations depends on specific properties of each marker, such as frequency and population history. ideally, {ld} analysis should be based directly on the underlying haplotype structure of the human genome, but this structure has remained poorly understood. here we report a high-resolution analysis of the haplotype structure across 500 kilobases on chromosome 5q31 using 103 single-nucleotide polymorphisms ({snps}) in a european-derived population. the results show a picture of discrete haplotype blocks (of tens to hundreds of kilobases), each with limited diversity punctuated by apparent sites of recombination. in addition, we develop an analytical model for {ld} mapping based on such haplotype blocks. if our observed structure is general (and published data suggest that it may be), it offers a coherent framework for creating a haplotype map of the human genome.
993	506682	article	annual review of neuroscience	\N	\N	\N	23	28	1	2005	\N	2006-10-20 02:43:07	beckman vision center, university of california-san francisco, san francisco, ca 94143, usa. sincichl@vision.ucsf.edu	{the} {circuitry} {of} v1 {and} v2: integration of color, form, and motion	primary and secondary visual cortex (v1 and v2) form the foundation of the cortical visual system. v1 transforms information received from the lateral geniculate nucleus ({lgn}) and distributes it to separate domains in v2 for transmission to higher visual areas. during the past 20 years, schemes for the functional organization of v1 and v2 have been based on a tripartite framework developed by livingstone \\& hubel (1988). since then, new anatomical data have accumulated concerning v1's input, its internal circuitry, and its output to v2. these new data, along with physiological and imaging studies, now make it likely that the visual attributes of color, form, and motion are not neatly segregated by v1 into different stripe compartments in v2. instead, there are just two main streams, originating from cytochrome oxidase patches and interpatches, that project to v2. each stream is composed of a mixture of magno, parvo, and konio geniculate signals. further studies are required to elucidate how the patches and interpatches differ in the output they convey to extrastriate cortex.
994	507239	article	pattern recognition	\N	\N	\N	20	35	4	2002	apr	2006-03-14 20:38:57	\N	a survey on the use of pattern recognition methods for abstraction, indexing and retrieval of images and video	the need for content-based access to image and video information from media archives has captured the attention of researchers in recent years. research efforts have led to the development of methods that provide access to image and video data. these methods have their roots in pattern recognition. the methods are used to determine the similarity in the visual information content extracted from low level features. these features are then clustered for generation of database indices. this paper presents a comprehensive survey on the use of these pattern recognition methods which enable image and video retrieval by content.
995	507409	book	\N	\N	\N	springer	\N	\N	\N	2003	feb	2006-02-17 04:30:37	\N	combinatorial optimization (3 volume, {a,b}, \\& c)	{this book offers an in-depth overview of polyhedral methods and efficient algorithms in combinatorial optimization.these methods form a broad, coherent and powerful kernel in combinatorial optimization, with strong links to discrete mathematics, mathematical programming and computer science. in eight parts, various areas are treated, each starting with an elementary introduction to the area, with short, elegant proofs of the principal results, and each evolving to the more advanced methods and results, with full proofs of some of the deepest theorems in the area. over 4000 references to further research are given, and historical surveys on the basic subjects are presented.}
996	507754	article	phys. rev. lett.	\N	\N	\N	\N	77	\N	1996	\N	2006-02-17 14:32:40	\N	generalized gradient approximation made simple	generalized gradient approximations (ggaâ€™s) for the exchange-correlation energy improve upon the local spin density (lsd) description of atoms, molecules, and solids. we present a simple derivation of a simple gga, in which all parameters (other than those in lsd) are fundamental constants. only general features of the detailed construction underlying the perdew-wang 1991 (pw91) gga are invoked. improvements over pw91 include an accurate description of the linear response of the uniform electron gas, correct behavior under uniform scaling, and a smoother potential.
997	511998	article	ieee transactions on systems, man, and cybernetics part b: cybernetics	\N	\N	\N	12	26	1	1996	\N	2006-02-20 01:32:34	\N	the {a}nt {s}ystem: {o}ptimization by a colony of cooperating agents	an analogy with the way ant colonies function has suggested the definition of a new computational paradigm, which we call ant system. we propose it as a viable new approach to stochastic combinatorial optimization. the main characteristics of this model are positive feedback, distributed computation, and the use of a constructive greedy heuristic. positive feedback accounts for rapid discovery of good solutions, distributed computation avoids premature convergence, and the greedy heuristic...
998	512091	techreport	\N	\N	\N	\N	\N	\N	MSR-TR-98-14	1998	apr	2006-02-20 01:32:35	\N	sequential minimal optimization: a fast algorithm for training support vector machines	this paper proposes a new algorithm for training support vector machines: sequential minimal optimization, or smo. training a support vector machine requires the solution of a very large quadratic programming (qp) optimization problem. smo breaks this large qp problem into a series of smallest possible qp problems. these small qp problems are solved analytically, which avoids using a time-consuming numerical qp optimization as an inner loop. the amount of memory required for smo is linear in the training set size, which allows smo to handle very large training sets. because matrix computation is avoided, smo scales somewhere between linear and quadratic in the training set size for various test problems, while the standard chunking svm algorithm scales somewhere between linear and cubic in the training set size. smoâ€™s computation time is dominated by svm evaluation, hence smo is fastest for linear svms and sparse data sets. on real- world sparse data sets, smo can be more than 1000 times faster than the chunking algorithm.
999	513563	electronic	\N	\N	\N	\N	\N	\N	\N	2006	feb	2006-02-20 19:36:30	\N	algebraic quantum field theory	algebraic quantum field theory provides a general, mathematically precise description of the structure of quantum field theories, and then draws out consequences of this structure by means of various mathematical tools -- the theory of operator algebras, category theory, etc.. given the rigor and generality of aqft, it is a particularly apt tool for studying the foundations of qft. this paper is a survey of aqft, with an orientation towards foundational topics. in addition to covering the basics of the theory, we discuss issues related to nonlocality, the particle concept, the field concept, and inequivalent representations. we also provide a detailed account of the analysis of superselection rules by s. doplicher, r. haag, and j. e. roberts (dhr); and we give an alternative proof of doplicher and roberts' reconstruction of fields and gauge group from the category of physical representations of the observable algebra. the latter is based on unpublished ideas due to roberts and the abstract duality theorem for symmetric tensor *-categories, a self-contained proof of which is given in the appendix.
1000	513921	article	science education	\N	\N	\N	18	88	\N	2004	\N	2006-02-21 00:25:41	\N	{tapping} into argumentation: developments in the application of toulmin's argument pattern for studying science discourse	this paper reports some methodological approaches to the analysis of argumentation discourse developed as part of the two-and-a-half year project titled ?enhancing the quality of argument in school scienc'' supported by the economic and social research council in the united kingdom. in this project researchers collaborated with middle-school science teachers to develop models of instructional activities in an effort to make argumentation a component of instruction. we begin the paper with a brief theoretical justification for why we consider argumentation to be of significance to science education. we then contextualize the use of toulmin's argument pattern in the study of argumentation discourse and provide a justification for the methodological outcomes our approach generates. we illustrate how our work refines and develops research methodologies in argumentation analysis. in particular, we present two methodological approaches to the analysis of argumentation resulting in whole-class as well as small-group student discussions. for each approach, we illustrate our coding scheme and some results as well as how our methodological approach has enabled our inquiry into the quality of argumentation in the classroom. we conclude with some implications for future research in argumentation in science education.
1001	515187	article	genome research	\N	\N	\N	9	15	11	2005	nov	2006-02-21 21:52:24	department of biological statistics and computational biology, cornell university, ithaca, new york 14853, usa. rasmus@binf.ku.dk	genomic scans for selective sweeps using {snp} data	detecting selective sweeps from genomic {snp} data is complicated by the intricate ascertainment schemes used to discover {snps}, and by the confounding influence of the underlying complex demographics and varying mutation and recombination rates. current methods for detecting selective sweeps have little or no robustness to the demographic assumptions and varying recombination rates, and provide no method for correcting for ascertainment biases. here, we present several new tests aimed at detecting selective sweeps from genomic {snp} data. using extensive simulations, we show that a new parametric test, based on composite likelihood, has a high power to detect selective sweeps and is surprisingly robust to assumptions regarding recombination rates and demography (i.e., has low type i error). our new test also provides estimates of the location of the selective sweep(s) and the magnitude of the selection coefficient. to illustrate the method, we apply our approach to data from the seattle {snp} project and to chromosome 2 data from the {hapmap} project. in chromosome 2, the most extreme signal is found in the lactase gene, which previously has been shown to be undergoing positive selection. evidence for selective sweeps is also found in many other regions, including genes known to be associated with disease risk such as {dpp10} and {col4a3}.
1002	515943	article	british journal of cancer	\N	\N	nature publishing group	4	94	6	2006	mar	2006-02-24 23:04:23	\N	{micrornas} in cell proliferation, cell death, and tumorigenesis.	{micrornas} ({mirnas}) are a recently discovered class of approximately 18-24 nucleotide {rna} molecules that negatively regulate target {mrnas}. all studied multicellular eukaryotes utilise {mirnas} to regulate basic cellular functions including proliferation, differentiation, and death. it is now apparent that abnormal {mirna} expression is a common feature of human malignancies. in this review, we will discuss how {mirnas} influence tumorigenesis by acting as oncogenes and tumour suppressors.
1003	516625	article	nature	\N	\N	nature publishing group	4	440	7087	2006	apr	2006-04-19 19:06:36	\N	designed divergent evolution of enzyme function	it is generally believed that proteins with promiscuous functions divergently evolved to acquire higher specificity and activity, and that this process was highly dependent on the ability of proteins to alter their functions with a small number of amino acid substitutions (plasticity). the application of this theory of divergent molecular evolution to promiscuous enzymes may allow us to design enzymes with more specificity and higher activity. many structural and biochemical analyses have identified the active or binding site residues important for functional plasticity (plasticity residues). to understand how these residues contribute to molecular evolution, and thereby formulate a design methodology, plasticity residues were probed in the active site of the promiscuous sesquiterpene synthase gamma-humulene synthase. identified plasticity residues were systematically recombined based on a mathematical model in order to construct novel terpene synthases, each catalysing the synthesis of one or a few very different sesquiterpenes. here we present the construction of seven specific and active synthases that use different reaction pathways to produce the specific and very different products. creation of these enzymes demonstrates the feasibility of exploiting the underlying evolvability of this scaffold, and provides evidence that rational approaches based on these ideas are useful for enzyme design.
1004	517198	article	nat rev mol cell biol	\N	\N	nature publishing group	5	7	3	2006	feb	2006-05-10 09:22:01	\N	a visual approach to proteomics	cryo-electron tomography is an emerging imaging technique that has unique potential for molecular cell biology. at the present resolution of 4–5 nm, large supramolecular structures can be studied in unperturbed cellular environments and, in the future, it will become possible to map molecular landscapes inside cells in a more comprehensive manner. 'visual proteomics' aims to complement and extend mass-spectrometry-based inventories, and to provide a quantitative description of the macromolecular interactions that underlie cellular functions.
1005	517200	article	nature reviews molecular cell biology	\N	\N	nature publishing group	9	7	3	2006	mar	2006-03-16 17:00:56	\N	structural systems biology: modelling protein interactions	much of systems biology aims to predict the behaviour of biological systems on the basis of the set of molecules involved. understanding the interactions between these molecules is therefore crucial to such efforts. although many thousands of interactions are known, precise molecular details are available for only a tiny fraction of them. the difficulties that are involved in experimentally determining atomic structures for interacting proteins make predictive methods essential for progress. structural details can ultimately turn abstract system representations into models that more accurately reflect biological reality.
1006	517203	article	nature reviews. molecular cell biology	\N	\N	nature publishing group	11	7	3	2006	mar	2006-02-27 18:03:41	\N	cell-signalling dynamics in time and space.	the specificity of cellular responses to receptor stimulation is encoded by the spatial and temporal dynamics of downstream signalling networks. temporal dynamics are coupled to spatial gradients of signalling activities, which guide pivotal intracellular processes and tightly regulate signal propagation across a cell. computational models provide insights into the complex relationships between the stimuli and the cellular responses, and reveal the mechanisms that are responsible for signal amplification, noise reduction and generation of discontinuous bistable dynamics or oscillations.
1007	517896	article	ieee communications surveys \\& tutorials	\N	\N	\N	21	7	2	2005	\N	2006-02-23 18:01:30	\N	a survey and comparison of {peer-to-peer} overlay network schemes	over the internet today, computing and communications environments are significantly more complex and chaotic than classical distributed systems, lacking any centralized organization or hierarchical control. there has been much interest in emerging peer-to-peer (p2p) network overlays because they provide a good substrate for creating large-scale data sharing, content distribution, and application-level multicast applications. these p2p overlay networks attempt to provide a long list of features, such as: selection of nearby peers, redundant storage, efficient search/location of data items, data permanence or guarantees, hierarchical naming, trust and authentication, and anonymity. p2p networks potentially offer an efficient routing architecture that is self-organizing, massively scalable, and robust in the wide-area, combining fault tolerance, load balancing, and explicit notion of locality. in this article we present a survey and comparison of various structured and unstructured p2p overlay networks. we categorize the various schemes into these two groups in the design spectrum, and discuss the application-level network performance of each group.
1008	517923	inproceedings	conference on applications, technologies, architectures, and protocols for computer communications (sigcomm 2005)	conference on applications, technologies, architectures, and protocols for computer communications (sigcomm 2005)	\N	acm	11	\N	\N	2005	\N	2006-02-23 18:01:30	\N	{opendht: a public dht service and its uses}	large-scale distributed systems are hard to deploy, and distributed hash tables ({dhts}) are no exception. to lower the barriers facing {dht}-based applications, we have created a public {dht} service called {opendht}. designing a {dht} that can be widely shared, both among mutually untrusting clients and among a variety of applications, poses two distinct challenges. first, there must be adequate control over storage allocation so that greedy or malicious clients do not use more than their fair share. second, the interface to the {dht} should make it easy to write simple clients, yet be sufficiently general to meet a broad spectrum of application requirements. in this paper we describe our solutions to these design challenges. we also report our early deployment experience with {opendht} and describe the variety of applications already using the system.
1009	518657	article	ca cancer j clin	\N	\N	\N	20	55	1	2005	\N	2006-02-23 18:37:38	\N	cancer statistics, 2005	each year, the american cancer society estimates the number of new cancer cases and deaths expected in the united states in the current year and compiles the most recent data on cancer incidence, mortality, and survival based on incidence data from the national cancer institute and mortality data from the national center for health statistics. incidence and death rates are age-standardized to the 2000 {us} standard million population. a total of 1,372,910 new cancer cases and 570,280 deaths are expected in the united states in 2005. when deaths are aggregated by age, cancer has surpassed heart disease as the leading cause of death for persons younger than 85 since 1999. when adjusted to delayed reporting, cancer incidence rates stabilized in men from 1995 through 2001 but continued to increase by 0.3\\% per year from 1987 through 2001 in women. the death rate from all cancers combined has decreased by 1.5\\% per year since 1993 among men and by 0.8\\% per year since 1992 among women. the mortality rate has also continued to decrease from the three most common cancer sites in men (lung and bronchus, colon and rectum, and prostate) and from breast and colorectal cancers in women. lung cancer mortality among women has leveled off after increasing for many decades. in analyses by race and ethnicity, african american men and women have 40\\% and 20\\% higher death rates from all cancers combined than white men and women, respectively. cancer incidence and death rates are lower in other racial and ethnic groups than in whites and african americans for all sites combined and for the four major cancer sites. however, these groups generally have higher rates for stomach, liver, and cervical cancers than whites. furthermore, minority populations are more likely to be diagnosed with advanced stage disease than are whites. progress in reducing the burden of suffering and death from cancer can be accelerated by applying existing cancer control knowledge across all segments of the population.
1010	518807	article	bmc bioinformatics	\N	\N	\N	\N	7	56	2006	\N	2006-02-23 20:26:38	santorstrasse 1. 39106 magdeburg. germany	a methodology for the structural and functional analysis of signaling and regulatory networks	background structural analysis of cellular interaction networks contributes to a deeper understanding of network-wide interdependencies, causal relationships, and basic functional capabilities. while the structural analysis of metabolic networks is a well-established field, similar methodologies have been scarcely developed and applied to signaling and regulatory networks. results we propose formalisms and methods, relying on adapted and partially newly introduced approaches, which facilitate a structural analysis of signaling and regulatory networks with focus on functional aspects. we use two different formalisms to represent and analyze interaction networks: interaction graphs and (logical) interaction hypergraphs. we show that, in interaction graphs, the determination of feedback cycles and of all the signaling paths between any pair of species is equivalent to the computation of elementary modes known from metabolic networks. knowledge on the set of signaling paths and feedback loops facilitates the computation of intervention strategies and the classification of compounds into activators, inhibitors, ambivalent factors, and non-affecting factors with respect to a certain species. in some cases, qualitative effects induced by perturbations can be unambiguously predicted from the network scheme. interaction graphs however, are not able to capture {and} relationships which do frequently occur in interaction networks. the consequent logical concatenation of all the arcs pointing into a species leads to boolean networks. for a boolean representation of cellular interaction networks we propose a formalism based on logical (or signed) interaction hypergraphs, which facilitates in particular a logical steady state analysis ({lssa}). {lssa} enables studies on the logical processing of signals and the identification of optimal intervention points (targets) in cellular networks. {lssa} also reveals network regions whose parametrization and initial states are crucial for the dynamic behavior. we have implemented these methods in our software tool {cellnetanalyzer} (successor of {fluxanalyzer}) and illustrate their applicability using a logical model of {t-cell} receptor signaling providing non-intuitive results regarding feedback loops, essential elements, and (logical) signal processing upon different stimuli. conclusion the methods and formalisms we propose herein are another step towards the comprehensive functional analysis of cellular interaction networks. their potential, shown on a realistic t-cell signaling model, makes them a promising tool.
1011	518808	electronic	\N	\N	\N	\N	\N	\N	\N	2006	jan	2006-02-23 20:26:42	\N	integrality gaps of semidefinite programs for vertex cover and relations to \\$\\ell\\_1\\$ embeddability of negative type metrics	we study various sdp formulations for {\\sc vertex cover} by adding different constraints to the standard formulation. we show that {\\sc vertex cover} cannot be approximated better than $2-o(1)$ even when we add the so called pentagonal inequality constraints to the standard sdp formulation, en route answering an open question of karakostas~\\cite{karakostas}. we further show the surprising fact that by strengthening the sdp with the (intractable) requirement that the metric interpretation of the solution is an $\\ell_1$ metric, we get an exact relaxation (integrality gap is 1), and on the other hand if the solution is arbitrarily close to being $\\ell_1$ embeddable, the integrality gap may be as big as $2-o(1)$. finally, inspired by the above findings, we use ideas from the integrality gap construction of charikar \\cite{char02} to provide a family of simple examples for negative type metrics that cannot be embedded into $\\ell_1$ with distortion better than $8/7-\\eps$. to this end we prove a new isoperimetric inequality for the hypercube.
1012	520148	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2006-02-24 23:23:22	\N	semantic integration and retrieval of multimedia metadata	the amount of digital media that has to be actually managed has already become unaffordable without fine-grained computerised support. this requires an extensive use of multimedia metadata. mpeg-7 is the greatest metadata framework created to date but it is based on xml schemas. therefore, its does not have formal semantics, which makes difficult to manage, extend and integrate it. consequently, there have been a lot attempts to move mpeg-7 to the semantic web. <br /> our approach contributes a complete and automatic mapping of the whole mpeg-7 standard to owl. it is based on a generic xml schema to owl mapping. the previous mapping is complemented with an xml metadata instances to rdf mapping that completes a transparent transfer of metadata from the xml to the semantic web domain. <br /> once in a semantic space, data integration, which is a crucial factor when several sources of information are available, is facilitated enormously. we have used the generated mpeg-7 owl ontology as an Â“upper-ontologyÂ” for multimedia metadata, where three different music schemas have been linked. thus, it has been possible to retrieve related information from instances of all the metadata sources. furthermore, detecting and merging instances from different sources has allowed us to enhance the description of audio files, both content-based and editorial data.
1013	520460	misc	\N	\N	\N	\N	\N	\N	\N	2004	\N	2006-02-25 06:20:47	\N	automatic web news extraction using tree edit distance	the web poses itself as the largest data repository ever available in the history of humankind. major efforts have been made in order to provide efficient access to relevant information within this huge repository of data. although several techniques have been developed to the problem of web data extraction, their use is still not spread, mostly because of the need for high human intervention and the low quality of the extraction results.in this paper, we present a domain-oriented approach to web data extraction and discuss its application to automatically extracting news from web sites. our approach is based on a highly efficient tree structure analysis that produces very effective results. we have tested our approach with several important brazilian on-line news sites and achieved very precise results, correctly extracting 87.71% of the news in a set of 4088 pages distributed among 35 different sites.
1014	523209	article	social problems	\N	\N	university of california press on behalf of the society for the study of social problems	12	35	3	1988	\N	2006-02-27 07:34:29	\N	mixing humans and nonhumans together: the sociology of a {door-closer}	is sociology the study of social questions, or is it the study of associations? in this paper the author takes the second position and extends the study of our associations to nonhumans. to make the argument clearer, the author chooses one very humble nonhuman, a door-closer, and analyzes how this "purely" technical artifact is a highly moral, highly social actor that deserves careful consideration. then the author proposes a vocabulary to follow human and nonhuman relations without stopping at artificial divides between what is purely technical and what is social. the author builds "its" or "his" own text in such a way that the text itself is a machine that exemplifies several of the points made by the author. in particular, the author is constructed and deconstructed several times to show how many social actors are inscribed or prescribed by machines and automatisms.
1015	523878	article	science	\N	\N	\N	2	309	5740	2005	sep	2006-02-27 23:54:14	\N	elucidation of the small {rna} component of the transcriptome	small {rnas} play important regulatory roles in most eukaryotes, but only a small proportion of these molecules have been identified. we sequenced more than two million small {rnas} from seedlings and the inflorescence of the model plant arabidopsis thaliana. known and new {micrornas} ({mirnas}) were among the most abundant of the nonredundant set of more than 75,000 sequences, whereas more than half represented lower abundance small interfering {rnas} ({sirnas}) that match repetitive sequences, intergenic regions, and genes. individual or clusters of highly regulated small {rnas} were readily observed. targets of antisense {rna} or {mirna} did not appear to be preferentially associated with {sirnas}. many genomic regions previously considered featureless were found to be sites of numerous small {rnas}.
1016	525214	article	the annals of statistics	\N	\N	\N	43	29	5	2001	\N	2006-03-01 12:45:42	\N	greedy function approximation: a gradient boosting machine	function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. a connection is made between stagewise additive expansions and steepest-descent minimization. a general gradient descent â€œboostingâ€ paradigm is developed for additive expansions based on any fitting criterion.specific algorithms are presented for least-squares, least absolute deviation, and huber-m loss functions for regression, and multiclass logistic likelihood for classification. special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such â€œtreeboostâ€ models are presented. gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. connections between this approach and the boosting methods of freund and shapire and friedman, hastie and tibshirani are discussed.
1017	525381	article	the journal of neuroscience	\N	\N	society for neuroscience	8	26	3	2006	jan	2006-03-01 14:47:16	division of applied mathematics, brown university, providence, rhode island 02912, usa. asohan@dam.brown.edu	spike count reliability and the poisson hypothesis	the variability of cortical activity in response to repeated presentations of a stimulus has been an area of controversy in the ongoing debate regarding the evidence for fine temporal structure in nervous system activity. we present a new statistical technique for assessing the significance of observed variability in the neural spike counts with respect to a minimal poisson hypothesis, which avoids the conventional but troubling assumption that the spiking process is identically distributed across trials. we apply the method to recordings of inferotemporal cortical neurons of primates presented with complex visual stimuli. on this data, the minimal poisson hypothesis is rejected: the neuronal responses are too reliable to be fit by a typical firing-rate model, even allowing for sudden, time-varying, and trial-dependent rate changes after stimulus onset. the statistical evidence favors a tightly regulated stimulus response in these neurons, close to stimulus onset, although not further away.
1018	525472	electronic	\N	\N	\N	\N	\N	\N	\N	2006	feb	2006-03-01 16:18:27	\N	preferential attachment in the growth of social networks: the case of wikipedia	we present an analysis of the statistical properties and growth of the free on-line encyclopedia wikipedia. by describing topics by vertices and hyperlinks between them as edges, we can represent this encyclopedia as a directed graph. the topological properties of this graph are in close analogy with that of the world wide web, despite the very different growth mechanism. in particular we measure a scaleâ€“invariant distribution of the inâ€“ and outâ€“ degree and we are able to reproduce these features by means of a simple statistical model. as a major consequence, wikipedia growth can be described by local rules such as the preferential attachment mechanism, though users can act globally on the network.
1019	525510	article	science, technology, and human values	\N	\N	\N	30	16	1	1991	\N	2006-03-01 16:48:54	\N	"the turn to technology in social studies of science"	this article examines how the special theoretical significance of the sociology of scientific knowledge (ssk) is affected by attempts to apply relativist-constructivism to technology. the article shows that the failure to confront key analytic ambivalences in the practice of ssk has compromised its original strategic significance. in particular, the construal of ssk as an explanatory formula diminishes its potential for profoundly reconceptualizing epistemic issues. a consideration of critiques of technological determinism, and of some empirical studies, reveals similar analytic ambivalences in the social study of technology (sst). the injunction to consider "technology as text" is critically examined. it is concluded that a reflexive interpretation of this slogan is necessary to recover some of the epistemological significance lost in the constructivist move from ssk to sst. 10.1177/016224399101600102
1020	527383	article	ieee trans. pattern anal. mach. intell.	\N	\N	ieee computer society	12	22	8	2000	\N	2006-03-02 20:07:53	washington, dc, usa	a bayesian computer vision system for modeling human interactions	we describe a real-time computer vision and machine learning system for modeling and recognizing human behaviors in a visual surveillance task. the system deals in particularly with detecting when interactions between people occur and classifying the type of interaction. examples of interesting interaction behaviors include following another person, altering one's path to meet another, and so forth. our system combines top-down with bottom-up information in a closed feedback loop, with both components employing a statistical bayesian approach. we propose and compare two different state-based learning architectures, namely, hmms and chmms for modeling behaviors and interactions. finally, a synthetic â€œalife-styleâ€ training system is used to develop flexible prior models for recognizing human interactions. we demonstrate the ability to use these a priori models to accurately classify real human behaviors and interactions with no additional tuning or training
1021	527579	article	plos comput biol	\N	\N	public library of science	\N	2	2	2006	feb	2006-03-03 01:04:50	\N	ten simple rules for getting grants	this piece follows an earlier editorial, {\\^a}Â€Âœten simple rules for getting published{\\^a}Â€Â{$[$}1{$]$}, which has generated significant interest, is well read, and continues to generate a variety of positive comments. that editorial was aimed at students in the early stages of a life of scientific paper writing. this interest has prompted us to try to help scientists in making the next academic career step{\\^a}Â€Â”becoming a young principal investigator. leo chalupa has joined us in putting together ten simple rules for getting grants, based on our many collective years of writing both successful and unsuccessful grants. while our grant writing efforts have been aimed mainly at united states government funding agencies, we believe the rules presented here are generic, transcending funding institutions and national boundaries.
1022	530837	phdthesis	\N	\N	\N	\N	\N	\N	\N	1999	\N	2006-09-24 17:45:46	\N	correlation-based feature selection for machine learning	a central problem in machine learning is identifying a representative set of features from which to construct a classification model for a particular task. this thesis addresses the problem of feature selection for machine learning through a correlation based approach. the central hypothesis is that good feature sets contain features that are highly correlated with the class, yet uncorrelated with each other. a feature evaluation formula, based on ideas from test theory, provides an operational ...
1023	531107	inproceedings	\N	nips	\N	\N	\N	\N	\N	2005	\N	2006-03-05 17:54:48	\N	infinite latent feature models and the indian buffet process.	we define a probability distribution over equivalence classes of binary matrices with a finite number of rows and an unbounded number of columns. this distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially infinite array of features. we derive the distribution by taking the limit of a distribution over n Ã— k binary matrices as k goes to infinity, a strategy inspired by the derivation of the chinese restaurant process {(aldous,} 1985; pitman, 2002) as the limit of a dirichlet-multinomial model. this strategy preserves the exchangeability of the rows of matrices. we define several simple generative processes that result in the same distribution over equivalence classes of binary matrices, one of which we call the indian buffet process. we illustrate the use of this distribution as a prior in an infinite latent feature model, deriving a markov chain monte carlo algorithm for inference in this model and applying this algorithm to an artificial dataset.
1024	531261	article	the journal of cell biology	\N	\N	rockefeller university press	9	172	1	2006	jan	2006-03-06 09:12:16	\N	seeing is believing? a beginners' guide to practical pitfalls in image acquisition	imaging can be thought of as the most direct of experiments. you see something; you report what you see. if only things were truly this simple. modern imaging technology has brought about a revolution in the kinds of questions we can approach, but this comes at the price of increasingly complex equipment. moreover, in an attempt to market competing systems, the microscopes have often been inappropriately described as easy to use and suitable for near-beginners. insufficient understanding of the experimental manipulations and equipment set-up leads to the introduction of errors during image acquisition. in this feature, i review some of the most common practical pitfalls faced by researchers during image acquisition, and how they can affect the interpretation of the experimental data.
1025	532004	article	bioinformatics	\N	\N	\N	6	21	19	2005	oct	2006-03-06 22:23:24	department of biochemistry, university of cambridge tennis court road, cambridge cb2 1ga, uk. rinaldo@cryst.bioc.cam.ac.uk	{choral}: a differential geometry approach to the prediction of the cores of protein structures.	{motivation}: although the cores of homologous proteins are relatively well conserved, amino acid substitutions lead to significant differences in the structures of divergent superfamilies. thus, the classification of amino acid sequence patterns and the selection of appropriate fragments of the protein cores of homologues of known structure are important for accurate comparative modelling. {results}: {choral} utilizes a knowledge-based method comprising an amalgam of differential geometry and pattern recognition algorithms to identify conserved structural patterns in homologous protein families. propensity tables are used to classify and to select patterns that most likely represent the structure of the core for a target protein. in our benchmark, {choral} demonstrates a performance equivalent to that of {modeller}.
1026	539713	article	\N	\N	\N	\N	11	17	4	2005	apr	2006-03-08 07:25:38	\N	toward integrating feature selection algorithms for classification and clustering	this paper introduces concepts and algorithms of feature selection, surveys existing feature selection algorithms for classification and clustering, groups and compares different algorithms with a categorizing framework based on search strategies, evaluation criteria, and data mining tasks, reveals unattempted combinations, and provides guidelines in selecting feature selection algorithms. with the categorizing framework, we continue our efforts toward-building an integrated system for intelligent feature selection. a unifying platform is proposed as an intermediate step. an illustrative example is presented to show how existing feature selection algorithms can be integrated into a meta algorithm that can take advantage of individual algorithms. an added advantage of doing so is to help a user employ a suitable algorithm without knowing details of each algorithm. some real-world applications are included to demonstrate the use of feature selection in data mining. we conclude this work by identifying trends and challenges of feature selection research and development.
1027	540510	article	the journal of neuroscience : the official journal of the society for neuroscience	\N	\N	\N	14	8	8	1988	aug	2006-03-08 17:58:39	philip bard laboratories of neurophysiology, department of neuroscience, johns hopkins university, school of medicine, baltimore, maryland 21205.	primate motor cortex and free arm movements to visual targets in three-dimensional space. i. relations between single cell discharge and direction of movement.	we describe the relations between the neuronal activity in primate motor cortex and the direction of arm movement in three-dimensional ({3-d}) space. the electrical signs of discharge of 568 cells were recorded while monkeys made movements of equal amplitude from the same starting position to 8 visual targets in a reaction time task. the layout of the targets in {3-d} space was such that the direction of the movement ranged over the whole {3-d} directional continuum in approximately equal angular intervals. we found that the discharge rate of 475/568 (83.6\\%) cells varied in an orderly fashion with the direction of movement: discharge rate was highest with movements in a certain direction (the cell's "preferred direction") and decreased progressively with movements in other directions, as a function of the cosine of the angle formed by the direction of the movement and the cell's preferred direction. the preferred directions of different cells were distributed throughout {3-d} space. these findings generalize to {3-d} space previous results obtained in {2-d} space (georgopoulos et al., 1982) and suggest that the motor cortex is a nodal point in the construction of patterns of output signals specifying the direction of arm movement in extrapersonal space.
1028	541415	inproceedings	\N	15th sosp	\N	acm sigops	11	\N	\N	1995	dec	2006-03-09 04:35:34	copper mountain, co	managing update conflicts in bayou, a weakly connected replicated storage system	bayou is a replicated, weakly consistent storage system designed for a mobile computing environment that includes portable machines with less than ideal network connectivity. to maximize availability, users can read and write any accessible replica. bayou's design has focused on supporting apphcation-specific mechanisms to detect and resolve the update conflicts that naturally arise in such a system, ensuring that replicas move towards eventual consistency, and defining a protocol by which the...
1029	541533	techreport	\N	\N	\N	\N	\N	\N	MSR-TR-2003-96	2004	jan	2006-03-09 04:35:37	\N	{c}onsensus on {transaction} {commit}	the distributed transaction commit problem requires reaching agreement on whether a transaction is committed or aborted. the classic two-phase commit protocol blocks if the coordinator fails. fault-tolerant consensus algorithms also reach agreement, but do not block whenever any majority of the processes are working. the paxos commit algorithm runs a paxos consensus algorithm on the commit/abort decision of each participant to obtain a transaction commit protocol that uses 2 f  &plus; 1 coordinators and makes progress if at least  f  &plus; 1 of them are working properly. paxos commit has the same stable-storage write delay, and can be implemented to have the same message delay in the fault-free case as two-phase commit, but it uses more messages. the classic two-phase commit algorithm is obtained as the special  f  &equals; 0 case of the paxos commit algorithm.
1030	543355	article	the computer journal	\N	\N	\N	10	41	8	1998	\N	2006-03-10 03:44:12	\N	how many clusters? which clustering method? answers via {model-based} cluster analysis	we consider the problem of determining the structure of clustered data, without prior knowledge of the number of clusters or any other information about their composition. data are represented by a mixture model in which each component corresponds to a different cluster. models with varying geometric properties are obtained through gaussian components with different parameterizations and cross-cluster constraints. noise and outliers can be modeled by adding a poisson process component....
1031	546174	article	proceedings of the national academy of sciences of the united states of america	\N	\N	\N	3	98	20	2001	sep	2006-03-10 14:44:56	cognition and neuroimaging laboratories, university of arizona, tucson, az 85721, usa. kmccabe@gmu.edu	a functional imaging study of cooperation in two-person reciprocal exchange.	cooperation between individuals requires the ability to infer each other's mental states to form shared expectations over mutual gains and make cooperative choices that realize these gains. from evidence that the ability for mental state attribution involves the use of prefrontal cortex, we hypothesize that this area is involved in integrating theory-of-mind processing with cooperative actions. we report data from a functional {mri} experiment designed to test this hypothesis. subjects in a scanner played standard two-person "trust and reciprocity" games with both human and computer counterparts for cash rewards. behavioral data shows that seven subjects consistently attempted cooperation with their human counterpart. within this group prefrontal regions are more active when subjects are playing a human than when they are playing a computer following a fixed (and known) probabilistic strategy. within the group of five noncooperators, there are no significant differences in prefrontal activation between computer and human conditions.
1032	546717	article	bmc bioinformatics	\N	\N	\N	\N	7	1	2006	mar	2006-03-10 19:35:41	\N	an improved map of conserved regulatory sites for saccharomyces cerevisiae	{background}:the regulatory map of a genome consists of the binding sites for proteins that determine the transcription of nearby genes. an initial regulatory map for s. cerevisiae was recently published using six motif discovery programs to analyze genome-wide chromatin immunoprecipitation data for 203 transcription factors. the programs were used to identify sequence motifs that were likely to correspond to the {dna}-binding specificity of the immunoprecipitated proteins. we report improved versions of two conservation-based motif discovery algorithms, {phylocon} and converge. using these programs, we create a refined regulatory map for s. cerevisiae by reanalyzing the same chromatin immunoprecipitation {data.results}:applying the same conservative criteria that were applied in the original study, we find that {phylocon} and converge each separately discover more known specificities than the combination of all six programs in the previous study. combining the results of {phylocon} and converge, we discover significant sequence motifs for 36 transcription factors that were previously missed. the new set of motifs identifies 636 more regulatory interactions than the previous one. the new network contains 28\\% more regulatory interactions among transcription factors, evidence of greater cross-talk between {regulators.conclusion}:combining two complementary computational strategies for conservation-based motif discovery improves the ability to identify the specificity of transcriptional regulators from genome-wide chromatin immunoprecipitation data. the increased sensitivity of these methods significantly expands the map of yeast regulatory sites without the need to alter any of the thresholds for statistical significance. the new map of regulatory sites reveals a more elaborate and complex view of the yeast genetic regulatory network than was observed previously.
1033	548293	inproceedings	\N	26th annual meeting of the association for computational linguistics: proceedings of the conference	\N	\N	8	\N	\N	1988	\N	2006-03-11 19:20:44	buffalo, new york	interpretation as abduction	abduction is inference to the best explanation. in the tacitus project at sri we have developed an approach to abductive inference, called â€œweighted abductionâ€, that has resulted in a significant simplification of how the problem of interpreting texts is conceptualized. the interpretation of a text is the minimal explanation of why the text would be true. more precisely, to interpret a text, one must prove the logical form of the text from what is already mutually known, allowing for coercions, merging redundancies where possible, and making assumptions where necessary. it is shown how such â€œlocal pragmaticsâ€ problems as reference resolution, the interpretation of compound nominals, the resolution of syntactic ambiguity and metonymy, and schema recognition can be solved in this manner. moreover, this approach of â€œinterpretation as abductionâ€ can be combined with the older view of â€œparsing as deductionâ€ to produce an elegant and thorough integration of syntax, semantics, and pragmatics, one that spans the range of linguistic phenomena from phonology to discourse structure. finally, we discuss means for making the abduction process efficient, possibilities for extending the approach to other pragmatics phenomena, and the semantics of the weights and costs in the abduction scheme.
1034	549085	article	current opinion in structural biology	\N	\N	\N	5	16	2	2006	apr	2006-03-12 15:00:19	center of excellence in bioinformatics, university at buffalo, 901 washington street, buffalo, ny, 14203, usa.	in quest of an empirical potential for protein structure prediction.	key to successful protein structure prediction is a potential that recognizes the native state from misfolded structures. recent advances in empirical potentials based on known protein structures include improved reference states for assessing random interactions, sidechain-orientation-dependent pair potentials, potentials for describing secondary or supersecondary structural preferences and, most importantly, optimization protocols that sculpt the energy landscape to enhance the correlation between native-like features and the energy. improved clustering algorithms that select native-like structures on the basis of cluster density also resulted in greater prediction accuracy. for template-based modeling, these advances allowed improvement in predicted structures relative to their initial template alignments over a wide range of target-template homology. this represents significant progress and suggests applications to proteome-scale structure prediction.
1035	549645	book	\N	\N	\N	milton keynes: open university press	\N	\N	\N	1999	\N	2006-03-13 13:56:19	\N	simulation for the social scientist	what can computer simulation contribute to the social sciences? which of the many approaches to simulation would be best for my social science project? how do i design, carry out and analyse the results from a computer simulation? interest in social simulation has been growing rapidly worldwide as a result of increasingly powerful hardware and software and a rising interest in the application of ideas of complexity, evolution, adaptation and chaos in the social sciences. simulation for the social scientist is a practical textbook on the techniques of building computer simulations to assist understanding of social and economic issues and problems. this authoritative book details all the common approaches to social simulation to provide social scientists with an appreciation of the literature and allow those with some programming skills to create their own simulations. new for this edition: a new chapter on designing multi-agent systems to support the fact that multi-agent modelling has become the most common approach to simulation new examples and guides to current software updated throughout to take new approaches into account the book is an essential tool for social scientists in a wide range of fields, particularly sociology, economics, anthropology, geography, organizational theory, political science, social policy, cognitive psychology and cognitive science. it will also appeal to computer scientists interested in distributed artificial intelligence, multi-agent systems and agent technologies.
1036	551078	article	int. j. comput. vision	\N	\N	\N	19	38	3	2000	\N	2006-03-14 12:23:42	\N	a theory of shape by space carving	. in this paper we consider the problem of computing the {3d} shape of an unknown, arbitrarily-shaped scene from multiple photographs taken at known but arbitrarily-distributed viewpoints. by studying the equivalence class of all {3d} shapes that reproduce the input photographs, we prove the existence of a special member of this class, the photo hull, that (1) can be computed directly from photographs of the scene, and (2) subsumes all other members of this class. we then give a provably-correct...
1037	551462	misc	\N	\N	\N	\N	\N	\N	\N	-1	\N	2006-03-14 14:00:31	\N	the design space of wireless sensor networks	in the recent past, wireless sensor networks have found their way into a wide variety of applications and systems with vastly varying requirements and characteristics. as a consequence, it is becoming increasingly difficult to discuss typical requirements regarding hardware issues and software support. this is particularly problematic in a multidisciplinary research area such as wireless sensor networks, where close collaboration between users, application domain experts, hardware designers,...
1038	554015	article	bioinformatics (oxford, england)	\N	\N	oxford university press	7	22	6	2006	mar	2006-04-04 00:31:57	\N	automated discovery of {3d} motifs for protein function annotation.	{motivation}: function inference from structure is facilitated by the use of patterns of residues ({3d} motifs), normally identified by expert knowledge, that correlate with function. as an alternative to often limited expert knowledge, we use machine-learning techniques to identify patterns of 3-10 residues that maximize function prediction. this approach allows us to test the assumption that residues that provide function are the most informative for predicting function. {results}: we apply our method, {gasps}, to the haloacid dehalogenase, enolase, amidohydrolase and crotonase superfamilies and to the serine proteases. the motifs found by {gasps} are as good at function prediction as {3d} motifs based on expert knowledge. the {gasps} motifs with the greatest ability to predict protein function consist mainly of known functional residues. however, several residues with no known functional role are equally predictive. for four groups, we show that the predictive power of our {3d} motifs is comparable with or better than approaches that use the entire fold ({combinatorial-extension}) or sequence profiles ({psi}-{blast}). {availability}: source code is freely available for academic use by contacting the authors. {supplementary} {information}: supplementary data are available at bioinformatics online.
1039	556109	inproceedings	\N	proceedings of the 17th international conference on machine learning	\N	\N	\N	\N	\N	2000	\N	2006-03-17 13:07:25	\N	practical reinforcement learning in continuous spaces	dynamic control tasks are good candidates for the application of reinforcement learning techniques. however, many of these tasks inherently have continuous state or action variables. this can cause problems for traditional reinforcement learning algorithms which assume discrete states and actions. in this paper, we introduce an algorithm that safely approximates the value function for continuous state control tasks, and that learns quickly from a small amount of data. we give experimental...
1040	556147	article	bmc bioinformatics	\N	\N	\N	\N	5	1	2004	oct	2006-03-17 14:57:11	center for studies in physics and biology, the rockefeller university, new york, ny 10021, usa. saurabh@lonnrot.rockefeller.edu	{phyme}: a probabilistic algorithm for finding motifs in sets of orthologous sequences.	this paper addresses the problem of discovering transcription factor binding sites in heterogeneous sequence data, which includes regulatory sequences of one or more genes, as well as their orthologs in other species. we propose an algorithm that integrates two important aspects of a motif's significance - overrepresentation and cross-species conservation - into one probabilistic score. the algorithm allows the input orthologous sequences to be related by any user-specified phylogenetic tree. it is based on the {expectation-maximization} technique, and scales well with the number of species and the length of input sequences. we evaluate the algorithm on synthetic data, and also present results for data sets from yeast, fly, and human. the results demonstrate that the new approach improves motif discovery by exploiting multiple species information.
1041	556200	inproceedings	\N	ieee symposium on foundations of computer science	\N	\N	8	\N	\N	2000	\N	2006-03-17 18:38:52	\N	opportunistic data structures with applications	there is an upsurging interest in designing succinct data structures for basic searching problems (see [munro99] and references therein). the motivation has to be found in the exponential increase of electronic data nowadays available which is even surpassing the significant increase in memory and disk storage capacities of current computers. space reduction is an attractive issue because it is also intimately related to performance improvements as noted by several authors (e.g. knuth [knuth3], bentley [bentley]). in designing these implicit data structures the goal is to reduce as much as possible the auxiliary information kept together with the input data without introducing a significant slowdown in the final query performance. yet input data are represented in their entirety thus taking no advantage of possible repetitiveness into them. the importance of those issues is well known to programmers who typically use various tricks to squeeze data as much as possible and still achieve good query performance. their approaches, thought, boil down to heuristics whose effectiveness is witnessed only by experimentation. in this paper, we address the issue of compressing and indexing data by studying it in a theoretical framework. we devise a novel data structure for indexing and searching whose space occupancy is a function of the entropy of the underlying data set. the novelty resides in the careful combination of a compression algorithm, proposed by burrows-wheeler [bw], with the structural properties of a well known indexing tool, the suffix array [mm93]. we call the data structure opportunistic since its space occupancy is decreased when the input is compressible at no significant slowdown in the query performance and without any assumption on a particular fixed distribution. more precisely, its space occupancy is optimal in a information-content sense because a text $t[1,u]$ is stored using $o(k(t)) + o(1)$ bits per input symbol, where $k(t)$ is the $k$th order entropy of $t$ (the bound holds for any fixed $k$). given an arbitrary string $p[1,p]$, the opportunistic data structure allows to search for the $occ$ occurrences of $p$ in $t$ requiring $o(p + occ \\log^\\epsilon u)$ time complexity (for any fixed $\\epsilon &lt;0$). if data are non compressible, then we achieve the best space bound currently known [gv00]; otherwise our solution improves the succinct suffix array in [gv00] and the classical suffix tree and suffix array data structures either in space or in query time complexity or both. it was a belief [witten:1999:mgc] that some space overhead should be paid to use full-text indices (i.e. suffix trees or suffix arrays) with respect to the word-based indices (i.e. inverted lists). the results in this paper show that no space overhead is needed at all, and as an application we improve space and query time complexity of the well-known glimpse tool [glimpse]. we finally investigate the modifiability of our opportunistic data structure by studying
1042	556513	article	nature	\N	\N	\N	3	418	\N	2002	\N	2006-03-18 21:52:24	\N	molecular evolution of {foxp2}, a gene involved in speech and language.	language is a uniquely human trait likely to have been a prerequisite for the development of human culture. the ability to develop articulate speech relies on capabilities, such as fine control of the larynx and mouth(1), that are absent in chimpanzees and other great apes. foxp2 is the first gene relevant to the human ability to develop language(2). a point mutation in foxp2 co-segregates with a disorder in a family in which half of the members have severe articulation difficulties accompanied by linguistic and grammatical impairment(3). this gene is disrupted by translocation in an unrelated individual who has a similar disorder. thus, two functional copies of foxp2 seem to be required for acquisition of normal spoken language. we sequenced the complementary dnas that encode the foxp2 protein in the chimpanzee, gorilla, orang-utan, rhesus macaque and mouse, and compared them with the human cdna. we also investigated intraspecific variation of the human foxp2 gene. here we show that human foxp2 contains changes in amino-acid coding and a pattern of nucleotide polymorphism, which strongly suggest that this gene has been the target of selection during recent human evolution.
1043	556578	article	science	\N	\N	american association for the advancement of science	3	303	5657	2004	jan	2006-03-19 08:44:22	\N	extensive gene traffic on the mammalian x chromosome	mammalian sex chromosomes have undergone profound changes since evolving from ancestral autosomes. by examining retroposed genes in the human and mouse genomes, we demonstrate that, during evolution, the mammalian x chromosome has generated and recruited a disproportionately high number of functional retroposed genes, whereas the autosomes experienced lower gene turnover. most autosomal copies originating from x-linked genes exhibited testis-biased expression. such export is incompatible with mutational bias and is likely driven by natural selection to attain male germline function. however, the excess recruitment is consistent with a combination of both natural selection and mutational bias.
1044	556694	article	current opinion in structural biology	\N	\N	\N	9	16	2	2006	apr	2006-03-19 16:29:37	department of computer science and genome center, kemper hall, university of california, davis, ca 95616, usa.	electrostatics calculations: latest methodological advances.	electrostatics plays a major role in the stabilization and function of biomolecules; as such, it remains a major focus of theoretical and computational studies of macromolecules. electrostatic interactions are long range, and strongly dependent on the solvent and ions surrounding the biomolecule under study. during the past year, progress has been reported in the treatment of electrostatics in explicit and implicit solvent models. interesting new developments of explicit solvent models include more efficient ewald summation methods, as well as alternative approaches based on reaction field theory, periodic images and euler summations. implicit solvent models remain divided into those that solve the poisson{\\^a}Â€Â“boltzmann equation numerically and those based on the generalized born formalism. both approaches are now included in molecular dynamics simulations and their accuracies may be assessed by direct comparison against experimental data. it is worth mentioning the recent development of web interfaces that facilitate access to and usage of existing tools for computing electrostatic interactions.
1045	557078	article	proceedings of the national academy of sciences of the united states of america	\N	\N	\N	3	102	43	2005	oct	2006-03-20 15:23:13	\N	biological imaging by soft x-ray diffraction microscopy	10.1073/pnas.0503305102 we have used the method of x-ray diffraction microscopy to image the complex-valued exit wave of an intact and unstained yeast cell. the images of the freeze-dried cell, obtained by using {750-ev} x-rays from different angular orientations, portray several of the cell's major internal components to 30-nm resolution. the good agreement among the independently recovered structures demonstrates the accuracy of the imaging technique. to obtain the best possible reconstructions, we have implemented procedures for handling noisy and incomplete diffraction data, and we propose a method for determining the reconstructed resolution. this work represents a previously uncharacterized application of x-ray diffraction microscopy to a specimen of this complexity and provides confidence in the feasibility of the ultimate goal of imaging biological specimens at 10-nm resolution in three dimensions. {er} -  
1046	557082	article	proceedings of the national academy of sciences of the united states of america	\N	\N	national academy of sciences	5	102	7	2005	feb	2007-03-03 20:14:09	center for microbial ecology, and department of crop and soil sciences, michigan state university, east lansing, mi 48824, usa.	genomic insights that advance the species definition for prokaryotes.	to help advance the species definition for prokaryotes, we have compared the gene content of 70 closely related and fully sequenced bacterial genomes to identify whether species boundaries exist, and to determine the role of the organism's ecology on its shared gene content. we found the average nucleotide identity (ani) of the shared genes between two strains to be a robust means to compare genetic relatedness among strains, and that ani values of approximately 94\\\\ corresponded to the traditional 70% dna-dna reassociation standard of the current species definition. at the 94% ani cutoff, current species includes only moderately homogeneous strains, e.g., most of the >4-mb genomes share only 65-90% of their genes, apparently as a result of the strains having evolved in different ecological settings. furthermore, diagnostic genetic signatures (boundaries) are evident between groups of strains of the same species, and the intergroup genetic similarity can be as high as 98-99% ani, indicating that justifiable species might be found even among organisms that are nearly identical at the nucleotide level. notably, a large fraction, e.g., up to 65%, of the differences in gene content within species is associated with bacteriophage and transposase elements, revealing an important role of these elements during bacterial speciation. our findings are consistent with a definition for species that would include a more homogeneous set of strains than provided by the current definition and one that considers the ecology of the strains in addition to their evolutionary distance.
1047	557230	article	circuits and systems for video technology, ieee transactions on	\N	\N	\N	11	8	5	1998	\N	2006-03-20 18:13:28	\N	relevance feedback: a power tool for interactive content-based image retrieval	content-based image retrieval ({cbir}) has become one of the most active research areas in the past few years. many visual feature representations have been explored and many systems built. while these research efforts establish the basis of {cbir}, the usefulness of the proposed approaches is limited. specifically, these efforts have relatively ignored two distinct characteristics of {cbir} systems: (1) the gap between high-level concepts and low-level features, and (2) the subjectivity of human perception of visual content. this paper proposes a relevance feedback based interactive retrieval approach, which effectively takes into account the above two characteristics in {cbir}. during the retrieval process, the user's high-level query and perception subjectivity are captured by dynamically updated weights based on the user's feedback. the experimental results over more than 70000 images show that the proposed approach greatly reduces the user's effort of composing a query, and captures the user's information need more precisely
1048	558931	article	molecular biology and evolution	\N	\N	\N	9	23	6	2006	jun	2006-03-21 18:59:02	department of ecology and evolutionary biology, university of michigan, ann arbor, mi 48109, usa.	low rates of expression profile divergence in highly expressed genes and tissue-specific genes during mammalian evolution.	evolutionary rates provide important information about the pattern and mechanism of evolution. although the rate of gene sequence evolution has been well studied, the rate of gene expression evolution is poorly understood. in particular, it is unclear whether the gene expression level and tissue specificity influence the divergence of expression profiles between orthologous genes. here we address this question using a microarray data set comprising the expression signals of 10,607 pairs of orthologous human and mouse genes from over 60 tissues per species. we show that the level of gene expression and the degree of tissue specificity are generally conserved between the human and mouse orthologs. the rate of gene expression profile change during evolution is negatively correlated with the level of gene expression, measured by either the average or the highest level among all tissues examined. this is analogous to the observation that the rate of gene (or protein) sequence evolution is negatively correlated with the gene expression level. the impacts of the degree of tissue specificity on the evolutionary rate of gene sequence and that of expression profile, however, are opposite. highly tissue-specific genes tend to evolve rapidly at the gene sequence level but slowly at the expression profile level. thus, different forces and selective constraints must underlie the evolution of gene sequence and that of gene expression.
1049	559470	inproceedings	\N	41st allerton conf. communication, control and computing	\N	\N	\N	\N	\N	2003	oct	2006-03-22 00:00:17	\N	{practical network coding}	we propose a distributed scheme for practical network coding that obviates  the need for centralized knowledge of the graph topology, the encoding functions,  and the decoding functions, and furthermore obviates the need for information to  be communicated synchronously through the network. the result is a practical  system for network coding that is robust to random packet loss and delay as well  as robust to any changes in the network topology or capacity due to joins, leaves,  node or link failures, congestion, and so on. we simulate such a practical network  coding system using the network topologies of several commercial internet service  providers, and demonstrate that it can achieve close to the theoretically optimal  performance.
1050	559842	book	\N	\N	\N	{the mit press}	\N	\N	\N	2006	mar	2006-03-22 10:04:12	\N	play between worlds : exploring online game culture	{in <i>play between worlds</i>, t. l. taylor examines multiplayer gaming life as it is lived on the borders, in the gaps--as players slip in and out of complex social networks that cross online and offline space. taylor questions the common assumption that playing computer games is an isolating and alienating activity indulged in by solitary teenage boys. massively multiplayer online games (mmogs), in which thousands of players participate in a virtual game world in real time, are in fact actively designed for sociability. games like the popular <i>everquest</i>, she argues, are fundamentally social spaces. \	<br /> <br /> taylor's detailed look at <i>everquest</i> offers a snapshot of multiplayer culture. drawing on her own experience as an <i>everquest</i> player (as a female gnome necromancer)--including her attendance at an <i>everquest</i> fan faire, with its blurring of online-and offline life--and extensive research, taylor not only shows us something about games but raises broader cultural issues. she considers "power gamers," who play in ways that seem closer to work, and examines our underlying notions of what constitutes play--and why play sometimes feels like work and may even be painful, repetitive, and boring. she looks at the women who play <i>everquest</i> and finds they don't fit the narrow stereotype of women gamers, which may cast into doubt our standardized and preconceived ideas of femininity. and she explores the questions of who owns game space--what happens when emergent player culture confronts the major corporation behind the game.}
1051	561425	electronic	\N	\N	\N	\N	\N	\N	\N	2006	mar	2006-03-23 16:27:57	\N	{ip} over {p2p}: enabling self-configuring virtual {ip} networks for grid computing	abstract â€” peer-to-peer (p2p) networks have mostly focused on task oriented networking, where networks are constructed for single applications, i.e. file-sharing, dns caching, etc. in this work, we introduce ipop, a system for creating virtual ip networks on top of a p2p overlay. ipop enables seamless access to grid resources spanning multiple domains by aggregating them into a virtual ip network that is completely isolated from the physical network. the virtual ip network provided by ipop supports deployment of existing ip-based protocols over a robust, self-configuring p2p overlay. we present implementation details as well as experimental measurement results taken from lan, wan, and planet-lab tests. i.
1052	561613	article	journal of information science	\N	\N	\N	13	31	6	2005	\N	2006-03-23 21:20:48	\N	what are communities of practice? a comparative review of four seminal works	10.1177/0165551505057016 this paper is a comparative review of four seminal works on communities of practice.                 it is argued that the ambiguities of the terms community and practice are a source                 of the concept's reusability allowing it to be reappropriated for different                 purposes, academic and practical. however, it is potentially confusing that the                 works differ so markedly in their conceptualizations of community, learning, power                 and change, diversity and informality. the three earlier works are underpinned by a                 common epistemological view, but lave and wenger's 1991 short monograph is                 often read as primarily about the socialization of newcomers into knowledge by a                 form of apprenticeship, while the focus in brown and duguid's article of                 the same year is, in contrast, on improvising new knowledge in an interstitial group                 that forms in resistance to management. wenger's 1998 book treats                 communities of practice as the informal relations and understandings that develop in                 mutual engagement on an appropriated joint enterprise, but his focus is the impact                 on individual identity. the applicability of the concept to the heavily                 individualized and tightly managed work of the twenty-first century is questionable.                 the most recent work by wenger Ã¢Â€Â“ this time with mcdermott and snyder as                 coauthors Ã¢Â€Â“ marks a distinct shift towards a managerialist stance. the                 proposition that managers should foster informal horizontal groups across                 organizational boundaries is in fact a fundamental redefinition of the concept.                 however it does identify a plausible, if limited, knowledge management (km) tool.                 this paper discusses different interpretations of the idea of                 'co-ordinating' communities of practice as a management ideology                 of empowerment.
1053	562684	article	proceedings of the national academy of sciences	\N	\N	national academy of sciences	5	100	3	2003	feb	2006-03-24 18:10:35	department of biology, university of california, riverside, ca 92521, usa. mark.springer@ucr.edu	placental mammal diversification and the {cretaceous–tertiary} boundary	competing hypotheses for the timing of the placental mammal radiation focus on whether extant placental orders originated and diversified before or after the {cretaceous-tertiary} ({k/t}) boundary. molecular studies that have addressed this issue suffer from single calibration points, unwarranted assumptions about the molecular clock, and/or taxon sampling that lacks representatives of all placental orders. we investigated this problem using the largest available molecular data set for placental mammals, which includes segments of 19 nuclear and three mitochondrial genes for representatives of all extant placental orders. we used the {thorne/kishino} method, which permits simultaneous constraints from the fossil record and allows rates of molecular evolution to vary on different branches of a phylogenetic tree. analyses that used different sets of fossil constraints, different priors for the base of placentalia, and different data partitions all support interordinal divergences in the cretaceous followed by intraordinal diversification mostly after the {k/t} boundary. four placental orders show intraordinal diversification that predates the {k/t} boundary, but only by an average of 10 million years. in contrast to some molecular studies that date the rat–mouse split as old as 46 million years, our results show improved agreement with the fossil record and place this split at 16–23 million years. to test the hypothesis that molecular estimates of cretaceous divergence times are an artifact of increased body size subsequent to the {k/t} boundary, we also performed analyses with a  ” {k/t} body size” taxon set. in these analyses, interordinal splits remained in the cretaceous.
1054	562885	article	journal of economic perspectives	\N	\N	\N	21	14	3	2000	\N	2006-03-25 08:14:19	\N	economic analysis of social interactions	this article discusses the need to broaden empirical analysis of the economics of social interactions in the {u.s}. in august 2000. economists have long been ambivalent about what social interactions constitute the proper domain of the discipline. the narrower view has been that economics is primarily the study of markets, a circumscribed class of institutions in which persons interact through an anonymous process of price formation. the broader view has been that economics is defined fundamentally by its concern with the allocation of resources and by its emphasis on the idea that people respond to incentives. in this view, economists may properly study how incentives shape all social interactions that affect the allocation of resources. the weak state of empirical research on social interactions should be a matter of concern both to economists with a policy focus and those with a theoretical focus. economic theorists need to know what classes of social interactions are prevalent in the real world. otherwise, theory risks becoming only a self-contained exercise in mathematical logic.
1055	563622	book	\N	\N	\N	{university of chicago press}	\N	\N	\N	2002	jun	2006-03-26 00:33:51	\N	food webs	{<div>food webs are diagrams depicting which species interact or in other words, who eats whom. an understanding of the structure and function of food webs is crucial for any study of how an ecosystem works, including attempts to predict which communities might be more vulnerable to disturbance and therefore in more immediate need of conservation.<br><br>although it was first published twenty years ago, stuart pimm's <i>food webs</i> remains the clearest introduction to the study of food webs. reviewing various hypotheses in the light of theoretical and empirical evidence, pimm shows that even the most complex food webs follow certain patterns and that those patterns are shaped by a limited number of biological processes, such as population dynamics and energy flow. pimm provides a variety of mathematical tools for unravelling these patterns and processes, and demonstrates their application through concrete examples. for this edition, he has written a new foreword covering recent developments in the study of food webs and demonstrates their continuing importance to conservation biology.<br><br><br></div>}
1056	563914	article	evolution	\N	\N	society for the study of evolution	22	18	4	1964	\N	2006-03-26 08:52:31	\N	butterflies and plants: a study in coevolution	the reciprocal evolutionary relationships of butterflies and their food plants have been examined on the basis of an extensive survey of patterns of plant utilization and information on factors affecting food plant choice. the evolution of secondary plant substances and the stepwise evolutionary responses to these by phytophagous organisms have clearly been the dominant factors in the evolution of butterflies and other phytophagous groups. furthermore, these secondary plant substances have probably been critical in the evolution of angiosperm subgroups and perhaps of the angiosperms themselves. the examination of broad patterns of coevolution permits several levels of predictions and shows promise as a route to the understanding of community evolution. little information useful for the reconstruction of phylogenies is supplied. it is apparent that reciprocal selective responses have been greatly underrated as a factor in the origination of organic diversity. the paramount importance of plant-herbivore interactions in generating terrestrial diversity is suggested. for instance, viewed in this framework the rich diversity of tropical communities may be traced in large part to the hospitality of warm climates toward poikilothermal phytophagous insects.
1057	566168	article	nature reviews. neuroscience	\N	\N	\N	12	4	4	2003	apr	2006-03-27 22:46:44	center for the neural basis of cognition and department of psychology, carnegie mellon university, 4400 fifth avenue, pittsburgh, pennsylvania 15213-2683, usa. jlm@cnbc.cmu.edu	the parallel distributed processing approach to semantic cognition.	how do we know what properties something has, and which of its properties should be generalized to other objects? how is the knowledge underlying these abilities acquired, and how is it affected by brain disorders? our approach to these issues is based on the idea that cognitive processes arise from the interactions of neurons through synaptic connections. the knowledge in such interactive and distributed processing systems is stored in the strengths of the connections and is acquired gradually through experience. degradation of semantic knowledge occurs through degradation of the patterns of neural activity that probe the knowledge stored in the connections. simulation models based on these ideas capture semantic cognitive processes and their development and disintegration, encompassing domain-specific patterns of generalization in young children, and the restructuring of conceptual knowledge as a function of experience.
1058	566988	article	neuroimage	\N	\N	\N	8	29	4	2006	feb	2006-03-28 21:44:32	\N	{fmri} resting state networks define distinct modes of long-distance interactions in the human brain.	functional magnetic resonance imaging ({fmri}) studies of the human brain have suggested that low-frequency fluctuations in resting {fmri} data collected using blood oxygen level dependent ({bold}) contrast correspond to functionally relevant resting state networks ({rsns}). whether the fluctuations of resting {fmri} signal in {rsns} are a direct consequence of neocortical neuronal activity or are low-frequency artifacts due to other physiological processes (e.g., autonomically driven fluctuations in cerebral blood flow) is uncertain. in order to investigate further these fluctuations, we have characterized their spatial and temporal properties using probabilistic independent component analysis ({pica}), a robust approach to {rsn} identification. here, we provide evidence that: i. {rsns} are not caused by signal artifacts due to low sampling rate (aliasing); ii. they are localized primarily to the cerebral cortex; iii. similar {rsns} also can be identified in perfusion {fmri} data; and iv. at least 5 distinct {rsn} patterns are reproducible across different subjects. the {rsns} appear to reflect "default" interactions related to functional networks related to those recruited by specific types of cognitive processes. {rsns} are a major source of non-modeled signal in {bold} {fmri} data, so a full understanding of their dynamics will improve the interpretation of functional brain imaging studies more generally. because {rsns} reflect interactions in cognitively relevant functional networks, they offer a new approach to the characterization of state changes with pathology and the effects of drugs.
1059	568376	book	\N	\N	\N	penguin	\N	\N	\N	2002	\N	2006-03-29 18:28:56	new york	the art of war	{<i>the art of war</i> is the swiss army knife of military theory--pop out a different tool for any situation.  folded into this small package are compact views on resourcefulness, momentum, cunning, the profit motive, flexibility, integrity, secrecy, speed, positioning, surprise, deception, manipulation, responsibility, and practicality.  thomas cleary's translation keeps the package tight, with crisp language and short sections. commentaries from the chinese tradition trail sun-tzu's words, elaborating and picking up on puzzling lines. take the solitary passage: "do not eat food for their soldiers."  elsewhere, sun-tzu has told us to plunder the enemy's stores, but now we're not supposed to eat the food?  the tang dynasty commentator du mu solves the puzzle nicely, "if the enemy suddenly abandons their food supplies, they should be tested first before eating, lest they be poisoned." most passages, however, are the pinnacle of succinct clarity: "lure them in with the prospect of gain, take them by confusion" or "invincibility is in oneself, vulnerability is in the opponent." sun-tzu's maxims are widely applicable beyond the military because they speak directly to the exigencies of survival. your new tools will serve you well, but don't flaunt them. remember sun-tzu's advice: "though effective, appear to be ineffective." <i>--brian bruya</i> } {<div>widely regarded as "the oldest military treatise in the world," this compact little book, written more than 2,500 years ago, today retains much of its original authoritative merit. american officers during world war ii read it closely. the japanese army studied the work for decades, and many twentieth-century chinese officers are said to have known the book by heart. maintaining that "all warfare is based on deception" and that "in war . . . let your great object be victory, not lengthy campaigns," the author adds: "that general is skillful in attack whose opponent does not know what to defend; and he is skillful in defense whose opponent does not know what to attack." principles of strategy, tactics, maneuvering, communication, and supplies; the use of terrain, fire, and the seasons of the year; the classification and utilization of spies; the treatment of soldiers, including captives, all have a modern ring to them. the author even provides rules for the "blitzkrieg," prefacing them with the words that "rapidity is the essence of war." still a valuable guide to the conduct of war, this volume will be indispensable to military students and of interest to all those fascinated by military history. unabridged republication of the edition published by the military service publishing company, harrisburg, pennsylvania, 1944.<br></div>} {sun tzu's classic treatise on the art of warfare  }
1060	571029	article	j. mach. learn. res.	\N	\N	mit press	44	5	\N	2004	\N	2006-03-30 19:51:14	cambridge, ma, usa	feature selection for unsupervised learning	in this paper, we identify two issues involved in developing an automated feature subset selection algorithm for unlabeled data: the need for finding the number of clusters in conjunction with feature selection, and the need for normalizing the bias of fe ature selection criteria with respect to dimension. we explore the feature selection problem and these issues through fssem (feature subset selection using expectation-maximization (em) clustering) and through two different performance criteria for evaluating candidate feature subsets: scatter separability and maximum likelihood. we present proofs on the dimensionality biases of these feature criteria, and present a cross-projection normalization scheme that can be applied to any criterion to ameliorate these biases. our experiments show the need for feature selection, the need for addressing these two issues, and the effectiveness of our proposed solutions.
1061	571073	inproceedings	\N	proceedings of the second conference on applied natural language processing	\N	association for computational linguistics	7	\N	\N	1988	\N	2006-03-30 19:51:18	morristown, nj, usa	a stochastic parts program and noun phrase parser for unrestricted text	alice!kwc it is well-known that part of speech depends on context. the word &amp;quot;table, &amp;quot; for example, can be a verb in some contexts (e.g., &amp;quot;he will table the motion&amp;quot;) and a noun in others (e.g., &amp;quot;the table is ready&amp;quot;). a program has been written which tags each word in an input sentence with the most likely part of speech. the program produces the following output for the two &amp;quot;table &amp;quot; sentences just mentioned:. he/pps will/md table/vb the/at motion/nn./.. the/at table/nn is/bez ready/jj./. (pps = subject pronoun; md = modal; vb = verb (no inflection); at = article; nn = noun; bez = present 3rd sg form of &amp;quot;to be&amp;quot;; jj = adjective; notation is borrowed from [francis and kucera,
1062	572520	article	magnetic resonance in medicine	\N	\N	\N	7	44	\N	2000	\N	2006-04-01 23:35:52	\N	in vivo fiber tractography using {dt--mri} data	fiber tract trajectories in coherently organized brain white matter pathways were computed from in vivo diffusion tensor magnetic resonance imaging (dt-mri) data. first, a continuous diffusion tensor field is constructed from this discrete, noisy, measured dt-mri data. then a frenet equation, describing the evolution of a fiber tract, was solved. this approach was validated using synthesized, noisy dt-mri data. corpus callosum and pyramidal tract trajectories were constructed and found to be consistent with known anatomy. the method's reliability, however, degrades where the distribution of fiber tract directions is nonuniform. moreover, background noise in diffusion-weighted mris can cause a computed trajectory to hop from tract to tract. still, this method can provide quantitative information with which to visualize and study connectivity and continuity of neural pathways in the central and peripheral nervous systems in vivo, and holds promise for elucidating architectural features in other fibrous tissues and ordered media. magn reson med 44:625-632, 2000. published 2000 wiley-liss, inc.
1063	572535	article	neurobiology	\N	\N	\N	5	96	\N	1999	\N	2006-04-01 23:35:53	\N	tracking neuronal fiber pathways in the living human brain	functional imaging with positron emission tomography and functional mri has revolutionized studies of the human brain. understanding the organization of brain systems, especially those used for cognition, remains limited, however, because no methods currently exist for noninvasive tracking of neuronal connections between functional regions [crick, f. \\\\\\\\& jones, e. (1993) nature (london) 361, 109-110]. detailed connectivities have been studied in animals through invasive tracer techniques, but these invasive studies cannot be done in humans, and animal results cannot always be extrapolated to human systems. we have developed noninvasive neuronal fiber tracking for use in living humans, utilizing the unique ability of mri to characterize water diffusion. we reconstructed fiber trajectories throughout the brain by tracking the direction of fastest diffusion (the fiber direction) from a grid of seed points, and then selected tracks that join anatomically or functionally (functional mri) defined regions. we demonstrate diffusion tracking of fiber bundles in a variety of white matter classes with examples in the corpus callosum, geniculo-calcarine, and subcortical association pathways. tracks covered long distances, navigated through divergences and tight curves, and manifested topological separations in the geniculo-calcarine tract consistent with tracer studies in animals and retinotopy studies in humans. additionally, previously undescribed topologies were revealed in the other pathways. this approach enhances the power of modern imaging by enabling study of fiber connections among anatomically and functionally defined brain regions in individual human subjects.
1064	572613	article	journal of magnetic resonance	\N	\N	\N	7	Series B 103	\N	1994	\N	2006-04-01 23:35:53	\N	estimation of the effective self-diffusion tensor from the {nmr} spin echo	the diagonal and off-diagonal elements of the effective self-diffusion tensor, deff, are related to the echo intensity in an nmr spin-echo experiment. this relationship is used to design experiments from which deff is estimated. this estimate is validated using isotropic and anisotropic media, i.e., water and skeletal muscle. it is shown that significant errors are made in diffusion nmr spectroscopy and imaging of anisotropic skeletal muscle when off-diagonal elements of deff are ignored, most notably the loss of information needed to determine fiber orientation. estimation of deff provides the theoretical basis for a new mri modality, diffusion tensor imaging, which provides information about tissue microstructure and its physiologic state not contained in scalar quantities such as t1, t2, proton density, or the scalar apparent diffusion constant.
1065	573506	book	\N	\N	\N	springer	\N	\N	\N	2003	jul	2007-05-06 16:29:43	\N	complex analysis	{the book provides an introduction to complex analysis for students with some familiarity with complex numbers from high school. the first part comprises the basic core of a course in complex analysis for junior and senior undergraduates. the second part includes various more specialized topics as the argument principle the poisson integral, and the riemann mapping theorem. the third part consists of a selection of topics designed to complete the coverage of all background necessary for passing ph.d. qualifying exams in complex analysis.}
1066	573510	inproceedings	modeling, analysis, and simulation of computer and telecommunication systems, 2005. 13th ieee international symposium on	modeling, analysis, and simulation of computer and telecommunication systems, 2005. 13th ieee international symposium on	\N	\N	7	\N	\N	2005	\N	2006-04-03 01:07:21	\N	{gps}: a general peer-to-peer simulator and its use for modeling {bittorrent}	{peer-to-peer} ({p2p}) systems have become popular over the past few years. however, their large scale and the open nature of the system makes studying them challenging. this paper presents an extensible framework for simulating {p2p} networks efficiently and accurately. efficiency is accomplished by using message level simulation rather than packet level simulation. moreover, accuracy is maintained by tracking the network infrastructure and using a flow model to accomplish accurate estimate of the message behavior. a second contribution of the paper is to model the {bittorrent} ({bt}) protocol. {bt} is a widely-used protocol that is significantly more complex than other {p2p} protocols because file download occurs in chunks from many other peers concurrently. thus, contrary to models of other {p2p} systems such as gnutella or freenet, which focus on finding the location of a file in the network, {bt}'s complexity occurs in downloading files (locating files in fact occurs out of band using websites that host the torrent files). we validate the model against a packet level simulator and also using a real, but small scale, {bittorrent} experiment. the simulator is object oriented and extensible for simulating other {p2p} protocols and applications.
1067	573991	book	\N	\N	\N	university of chicago press	\N	\N	\N	1993	\N	2006-04-03 12:37:41	chicago and london	{english verb classes and alternations a preliminary investigation}	{<div>in this rich reference work, beth levin classifies over 3,000 english verbs according to shared meaning and behavior. levin starts with the hypothesis that a verb's meaning influences its syntactic behavior and develops it into a powerful tool for studying the english verb lexicon. she shows how identifying verbs with similar syntactic behavior provides an effective means of distinguishing semantically coherent verb classes, and isolates these classes by examining verb behavior with respect to a wide range of syntactic alternations that reflect verb meaning. <br><br>the first part of the book sets out alternate ways in which verbs can express their arguments. the second presents classes of verbs that share a kernel of meaning and explores in detail the behavior of each class, drawing on the alternations in the first part. levin's discussion of each class and alternation includes lists of relevant verbs, illustrative examples, comments on noteworthy properties, and bibliographic references. the result is an original, systematic picture of the organization of the verb inventory. <br><br>easy to use, <i>english verb classes and alternations</i> sets the stage for further explorations of the interface between lexical semantics and syntax. it will prove indispensable for theoretical and computational linguists, psycholinguists, cognitive scientists, lexicographers, and teachers of english as a second language. <br><br><br></div>}
1068	574003	book	\N	\N	\N	athenaeum	\N	\N	\N	1972	\N	2006-04-03 12:37:42	frankfurt am main	semantic structures	this vol in the current studies in linguistics series contains an introduction, iii parts, & 11 chpts. the vol refines the theory of conceptual semantics within a broad study of conceptual structure & its lexical & syntactic expression in english, constructed in jackendoff's earlier works, semantics and cognition (cambridge: mit press, 1983) & consciousness and the computational mind (cambridge: mit press, 1987), the major premises of which are summarized herein. part i - basic machinery - contains (1) overview of conceptual semantics - presents an outline of i-conceptual (internal language/concepts) knowledge. some basic notions relating to lexicon & grammar are discussed & a new approach to the decomposition of lexical concepts that goes beyond simple binary feature treatments is outlined; (2) argument structure and thematic roles - systematically develops a description of how the lexical conceptual structure of a head is combined with its arguments & modifiers to form a phrasal conceptual structure; (3) multiple thematic roles for a single np - explores the theta-criterion & arguments suggesting that it should be weakened. the notion of argument binding is introduced & developed; & (4) unifying lexical entries - outlines abbreviatory conventions intended to facilitate the consolidation of multiple related uses of a single lexical item into one entry. part ii - mostly on the problem of meaning - contains (5) some further conceptual functions - examines basic verbs of manner of motion & configuration; (6) some featural elaborations of spatial functions - discusses the properties of distributive location & the restrictions it entails. properties of verbs of touching, verbs of attachment, & verbs of material composition are also discussed. it is shown that formalization of the conceptual structure of these classes is best accomplished not through the use of simple primitives but through development of primitive semantic fields into coherent feature systems; & (7) the action tier and the analysis of causation - adapts the phonological tier model to conceptual structure & outlines an analysis of causation. part iii - mostly on the problem of correspondence - contains (8) adjuncts that express an incorporated argument - provides a detailed technical discussion of adjuncts using several specific examples. a rule for the passive by-adjunct is also proposed; (9) adjuncts that express an argument of a modifying conceptual clause - examines the structure of three kinds of for-adjuncts: beneficiary, benefit, & exchange; (10) adjuncts that express arguments of a superordinate conceptual clause - examines unusual cases in which the adjunct rather than the verb determines the syntax of the verb phrase (vp). an alternative treatment of resultatives is proposed; & (11) toward a theory of linking - examines how conceptual semantics can improve the formulation of linking theory, & vice versa. the general outlines of a revised theory of linking are presented. bibliog. b. annesser murray
1069	574099	inproceedings	\N	{coling}-{acl}	\N	\N	6	\N	\N	1998	\N	2006-04-03 12:37:47	\N	automatic retrieval and clustering of similar words	bootstrapping semantics from text is one of the greatest challenges in natural language learning. earlier research showed that it is possible to automatically identify words that are semantically similar to a given word based on the syntactic collocation patterns of the words. we present an approach that goes a step further by obtaining a tree structure among the most similar words so that different senses of a given word can be identified with different subtrees. submission type: paper topic...
1070	574112	article	biometrics	\N	\N	\N	15	33	1	1977	mar	2006-04-03 12:37:47	\N	the measurement of observer agreement for categorical data	this paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies. the procedure essentially involves the construction of functions of the observed proportions which are directed at the extent to which the observers agree among themselves and the construction of test statistics for hypotheses involving these functions. tests for interobserver bias are presented in terms of first-order marginal homogeneity and measures of interobserver agreement are developed as generalized kappa-type statistics. these procedures are illustrated with a clinical diagnosis example from the epidemiological literature.
1071	576263	article	user modeling and user-adapted interaction	\N	\N	\N	18	14	2-3	2004	jun	2006-04-04 20:12:19	\N	user modelling for news web sites with word sense based techniques	siteif is a personal agent for a bilingual news web site that learns userâ€™s interests from the requested pages. in this paper we propose to use a word sense based document representation as a starting point to build a model of the userâ€™s interests. documents passed over are processed and relevant senses (disambiguated over wordnet) are extracted and then combined to form a semantic network. a filtering procedure dynamically predicts new documents on the basis of the semantic network. there are two main advantages of a sense-based approach: first, the model predictions, being based on senses rather than words, are more accurate; second, the model is language independent, allowing navigation in multilingual sites. we report the results of a comparative experiment that has been carried out to give a quantitative estimation of these improvements.
1072	576317	article	ieee transactions on medical imaging	\N	\N	\N	8	20	11	2001	nov	2006-04-04 22:08:52	\N	spatial transformations of diffusion tensor magnetic resonance images	the authors address the problem of applying spatial transformations (or "image warps") to diffusion tensor magnetic resonance images. the orientational information that these images contain must be handled appropriately when they are transformed spatially during image registration. the authors present solutions for global transformations of three-dimensional images up to 12-parameter affine complexity and indicate how their methods can be extended for higher order transformations. several approaches are presented and tested using synthetic data. one method, the preservation of principal direction algorithm, which takes into account shearing, stretching and rigid rotation, is shown to be the most effective. additional registration experiments are performed on human brain data obtained from a single subject, whose head was imaged in three different orientations within the scanner. all of the authors' methods improve the consistency between registered and target images over naive warping algorithms.
1073	576653	article	journal of research in personality	\N	\N	\N	21	40	2	2006	apr	2006-04-05 10:56:02	\N	do bilinguals have two personalities? a special case of cultural frame switching	four studies examined and empirically documented cultural frame switching ({cfs}; hong, chiu, \\& kung, 1997) in the domain of personality. specifically, we asked whether {spanish-english} bilinguals show different personalities when using different languages? if so, are the two personalities consistent with cross-cultural differences in personality? to generate predictions about the specific cultural differences to expect, study 1 documented personality differences between {us} and mexican monolinguals. studies 2-4 tested {cfs} in three samples of {spanish-english} bilinguals, located in the {us} and mexico. findings replicated across all three studies, suggesting that language activates {cfs} for extraversion, agreeableness, and conscientiousness. further analyses suggested the findings were not due to anomalous items or translation effects. results are discussed in terms of the interplay between culture and self.
1074	576758	book	\N	\N	\N	springer-verlag	\N	\N	\N	1985	\N	2006-04-05 13:55:04	new york	computational geometry: an introduction	from the reviews: "this book offers a coherent treatment, at the graduate textbook level, of the field that has come to be known in the last decade or so as computational geometry. ... ... the book is well organized and lucidly written; a timely contribution by two founders of the field. it clearly demonstrates that computational geometry in the plane is now a fairly well-understood branch of computer science and mathematics. it also points the way to the solution of the more challenging problems in dimensions higher than two." #mathematical reviews#1 "... this remarkable book is a comprehensive and systematic study on research results obtained especially in the last ten years. the very clear presentation concentrates on basic ideas, fundamental combinatorial structures, and crucial algorithmic techniques. the plenty of results is clever organized following these guidelines and within the framework of some detailed case studies. a large number of figures and examples also aid the understanding of the material. therefore, it can be highly recommended as an early graduate text but it should prove also to be essential to researchers and professionals in applied fields of computer-aided design, computer graphics, and robotics." #biometrical journal#2
1075	576807	article	automatica	\N	\N	\N	17	38	1	2002	jan	2006-04-05 13:55:04	\N	the explicit linear quadratic regulator for constrained systems	for discrete-time linear time invariant systems with constraints on inputs and states, we develop an algorithm to determine explicitly, the state feedback control law which minimizes a quadratic performance criterion. we show that the control law is piece-wise linear and continuous for both the finite horizon problem (model predictive control) and the usual infinite time measure (constrained linear quadratic regulation). thus, the on-line control computation reduces to the simple evaluation of an explicitly defined piecewise linear function. by computing the inherent underlying controller structure, we also solve the equivalent of the hamilton-jacobi-bellman equation for discrete-time linear constrained systems. control based on on-line optimization has long been recognized as a superior alternative for constrained systems. the technique proposed in this paper is attractive for a wide range of practical problems where the computational complexity of on-line optimization is prohibitive. it also provides an insight into the structure underlying optimization-based controllers.
1076	577491	article	sigir forum	proceedings of the 15th australasian database conference - volume 27	adc	acm	7	36	2	2002	sep	2006-04-05 23:57:05	new york, ny, usa	a taxonomy of web search	classic {ir} (information retrieval) is inherently predicated on users searching for information, the so-called "information need". but the need behind a web search is often not informational -- it might be navigational (give me the url of the site i want to reach) or transactional (show me sites where i can perform a certain transaction, e.g. shop, download a file, or find a map). we explore this taxonomy of web searches and discuss how global search engines evolved to deal with web-specific needs.
1077	578311	book	\N	\N	\N	morgan kaufmann	\N	\N	\N	2006	apr	2006-04-06 12:30:14	\N	the persona lifecycle: keeping people in mind throughout product design (interactive technologies)	{if you design and develop products for people, this book is for you. the persona lifecycle addresses the how of creating effective personas and using those personas to design products that people love. it doesnt just describe the value of personas; it offers detailed techniques and tools related to planning, creating, communicating, and using personas to create great product designs. moreover, it provides rich examples, samples, and illustrations to imitate and model. perhaps most importantly, it positions personas not as a panacea, but as a method used to complement other user-centered design (ucd) techniques including scenario-based design, cognitive walkthroughs and user testing. <br><br>john pruitt is the user research manager for the tablet \\& mobile pc division at microsoft corporation. tamara adlin is a customer experience manager at amazon.com. for the past six years, john and tamara have been researching and using personas, leading workshops, and teaching courses at professional conferences and universities. they developed the persona lifecycle model to communicate the value and practical application of personas to product design and development professionals.<br><br><b>features</b><br>* presentation and discussion of the complete lifecycle of personas, to guide the designer at each stage of product development.<br>* a running case study with rich examples and samples that demonstrate how personas can be used in building a product end-to-end. <br>* recommended best practices in techniques, tools, and innovative methods.<br>* hundreds of relevant stories, commentary, opinions, and case studies from user experience professionals across a variety of domains and industries.}
1078	579603	article	ieee transactions on knowledge and data engineering	\N	\N	\N	9	17	6	2005	jun	2006-04-07 18:09:32	\N	a statistical model for user preference	modeling user preference is one of the challenging issues in intelligent information systems. extensive research has been performed to automatically analyze user preference and to utilize it. one problem still remains: the representation of preference, usually given by measure of vector similarity or probability, does not always correspond to common sense of preference. this problem gets worse in the case of negative preference. to overcome this problem, this paper presents a preference model using mutual information in a statistical framework. this paper also presents a method that combines information of joint features and alleviates problems arising from sparse data. experimental results, compared with the previous recommendation models, show that the proposed model has the highest accuracy in recommendation tests.
1079	579981	electronic	\N	\N	\N	\N	\N	\N	\N	2006	apr	2006-04-08 08:08:23	\N	graph theory and networks in biology	in this paper, we present a survey of the use of graph theoretical techniques in biology. in particular, we discuss recent work on identifying and modelling the structure of bio-molecular networks, as well as the application of centrality measures to interaction networks and research on the hierarchical structure of such networks and network motifs. work on the link between structural network properties and dynamics is also described, with emphasis on synchronization and disease propagation.
1080	580617	article	molecular ecology	\N	\N	\N	20	15	6	2006	mar	2006-04-15 07:17:04	northwest fisheries science center, 2725 montlake blvd east, seattle, wa 98112 usa, ;  laboratoire decologie alpine (leca), gnomique des populations et biodiversit, universit joseph fourier, grenoble, france	what is a population? an empirical evaluation of some genetic methods for identifying the number of gene pools and their degree of connectivity	we review commonly used population definitions under both the ecological paradigm (which emphasizes demographic cohesion) and the evolutionary paradigm (which emphasizes reproductive cohesion) and find that none are truly operational. we suggest several quantitative criteria that might be used to determine when groups of individuals are different enough to be considered 'populations'. units for these criteria are migration rate (m) for the ecological paradigm and migrants per generation (nm) for the evolutionary paradigm. these criteria are then evaluated by applying analytical methods to simulated genetic data for a finite island model. under the standard parameter set that includes lÂ =Â 20 high mutation (microsatellite-like) loci and samples of sÂ =Â 50 individuals from each of nÂ =Â 4 subpopulations, power to detect departures from panmixia was very high (223c100%; pÂ &lt;Â 0.001) even with high gene flow (nmÂ = 25). a new method, comparing the number of correct population assignments with the random expectation, performed as well as a multilocus contingency test and warrants further consideration. use of low mutation (allozyme-like) markers reduced power more than did halving s or l. under the standard parameter set, power to detect restricted gene flow below a certain level x (h0: nmÂ &lt;Â x) can also be high, provided that true nmÂ 2264 0.5x. developing the appropriate test criterion, however, requires assumptions about several key parameters that are difficult to estimate in most natural populations. methods that cluster individuals without using a priori sampling information detected the true number of populations only under conditions of moderate or low gene flow (nmÂ 2264Â 5), and power dropped sharply with smaller samples of loci and individuals. a simple algorithm based on a multilocus contingency test of allele frequencies in pairs of samples has high power to detect the true number of populations even with nmÂ =Â 25 but requires more rigorous statistical evaluation. the ecological paradigm remains challenging for evaluations using genetic markers, because the transition from demographic dependence to independence occurs in a region of high migration where genetic methods have relatively little power. some recent theoretical developments and continued advances in computational power provide hope that this situation may change in the future.
1081	581179	article	journal of theoretical biology	\N	\N	\N	30	22	3	1969	mar	2006-04-10 17:29:40	\N	metabolic stability and epigenesis in randomly constructed genetic nets.	proto-organisms probably were randomly aggregated nets of chemical reactions. the hypothesis that contemporary organisms are also randomly constructed molecular automata is examined by modeling the gene as a binary (on-off) device and studying the behavior of large, randomly constructed nets of these binary â€œgenesâ€. the results suggest that, if each â€œgeneâ€ is directly affected by two or three other â€œgenesâ€, then such random nets: behave with great order and stability; undergo behavior cycles whose length predicts cell replication time as a function of the number of genes per cell; possess different modes of behavior whose number per net predicts roughly the number of cell types in an organism as a function of its number of genes; and under the stimulus of noise are capable of differentiating directly from any mode of behavior to at most a few other modes of behavior. cellular differentation is modeled as a markov chain among the modes of behavior of a genetic net. the possibility of a general theory of metabolic behavior is suggested.
1082	582065	article	physics of life reviews	\N	\N	\N	20	2	4	2005	dec	2006-04-11 16:41:44	\N	ant colony optimization: introduction and recent trends	ant colony optimization is a technique for optimization that was introduced in the early 1990's. the inspiring source of ant colony optimization is the foraging behavior of real ant colonies. this behavior is exploited in artificial ant colonies for the search of approximate solutions to discrete optimization problems, to continuous optimization problems, and to important problems in telecommunications, such as routing and load balancing. first, we deal with the biological inspiration of ant colony optimization algorithms. we show how this biological inspiration can be transfered into an algorithm for discrete optimization. then, we outline ant colony optimization in more general terms in the context of discrete optimization, and present some of the nowadays best-performing ant colony optimization variants. after summarizing some important theoretical results, we demonstrate how ant colony optimization can be applied to continuous optimization problems. finally, we provide examples of an interesting recent research direction: the hybridization with more classical techniques from artificial intelligence and operations research.
1083	582131	inproceedings	\N	proceedings of the ninth international conference on artificial neural networks	\N	\N	5	\N	\N	1999	\N	2006-04-11 18:20:50	\N	products of experts	it is possible to combine multiple probabilistic models of the same data by multiplying the probabilities together and then renormalizing. this is a very efficient way to model high-dimensional data which simultaneously satisfies many different low dimensional constraints. each individual expert model can focus on giving high probability to data vectors that satisfy just one of the constraints. data vectors that satisfy this one constraint but violate other constraints will be ruled out by their low probability under the other expert models. training a product of models appears difficult because, in addition to maximizing the probabilities that the individual models assign to the observed data, it is necessary to make the models disagree on unobserved regions of the data space. however, if the individual models are tractable there is a fairly efficient way to train a product of models. this training algorithm suggests a biologically plausible way of learning neural population codes.
1084	582156	inproceedings	\N	computer speech and language	\N	\N	41	10	\N	1996	\N	2006-04-11 18:20:50	\N	a maximum entropy approach to adaptive statistical language modeling	an adaptive statistical languagemodel is described, which successfullyintegrates long distancelinguistic information with other knowledge sources. most existing statistical language models exploit only the immediate history of a text. to extract information from further back in the document's history, we propose and use trigger pairs as the basic information bearing elements. this allows the model to adapt its expectations to the topic of discourse. next, statistical evidence from multiple...
1085	582356	article	science	\N	\N	american association for the advancement of science	3	312	5770	2006	apr	2006-04-11 21:40:10	\N	the competitive advantage of sanctioning institutions	understanding the fundamental patterns and determinants of human cooperation and the maintenance of social order in human societies is a challenge across disciplines. the existing empirical evidence for the higher levels of cooperation when altruistic punishment is present versus when it is absent systematically ignores the institutional competition inherent in human societies. whether punishment would be deliberately adopted and would similarly enhance cooperation when directly competing with nonpunishment institutions is highly controversial in light of recent findings on the detrimental effects of punishment. we show experimentally that a sanctioning institution is the undisputed winner in a competition with a sanction-free institution. despite initial aversion, the entire population migrates successively to the sanctioning institution and strongly cooperates, whereas the sanction-free society becomes fully depopulated. the findings demonstrate the competitive advantage of sanctioning institutions and exemplify the emergence and manifestation of social order driven by institutional selection.
1086	584206	article	acm comput. surv.	\N	\N	acm press	34	34	4	2002	dec	2006-04-12 16:46:59	new york, ny, usa	a survey of web metrics	the unabated growth and increasing significance of the world wide web has resulted in a flurry of research activity to improve its capacity for serving information more effectively. but at the heart of these efforts lie implicit assumptions about ``quality'' and ``usefulness'' of web resources and services. this observation points towards measurements and models that quantify various attributes of web sites. the science of measuring all aspects of information, especially its storage and retrieval or informetrics has interested information scientists for decades before the existence of the web. is web informetrics any different, or is it just an application of classical informetrics to a new medium? in this article, we examine this issue by classifying and discussing a wide ranging set of web metrics. we present the origins, measurement functions, formulations and comparisons of well-known web metrics for quantifying web graph properties, web page significance, web page similarity, search and retrieval, usage characterization and information theoretic properties. we also discuss how these metrics can be applied for improving web information access and use.
1087	584533	article	\N	\N	\N	\N	\N	\N	\N	2006	apr	2006-04-12 20:36:50	\N	the free will theorem	abstract&nbsp;&nbsp;on the basis of three physical axioms, we prove that if the choice of a particular type of spin 1 experiment is not a function of the information accessible to the experimenters, then its outcome is equally not a function of the information accessible to the particles. we show that this result is robust, and deduce that neither hidden variable theories nor mechanisms of the grw type for wave function collapse can be made relativistic and causal. we also establish the consistency of our axioms and discuss the philosophical implications.
1088	584547	article	nature	\N	\N	nature publishing group	4	440	7086	2006	apr	2006-07-20 17:52:45	\N	{sted} microscopy reveals that synaptotagmin remains clustered after synaptic vesicle exocytosis	synaptic transmission is mediated by neurotransmitters that are stored in synaptic vesicles and released by exocytosis upon activation. the vesicle membrane is then retrieved by endocytosis, and synaptic vesicles are regenerated and re-filled with neurotransmitter1. although many aspects of vesicle recycling are understood, the fate of the vesicles after fusion is still unclear. do their components diffuse on the plasma membrane, or do they remain together? this question has been difficult to answer because synaptic vesicles are too small (40 nm in diameter) and too densely packed to be resolved by available fluorescence microscopes. here we use stimulated emission depletion ({sted})2 to reduce the focal spot area by about an order of magnitude below the diffraction limit, thereby resolving individual vesicles in the synapse. we show that synaptotagmin i, a protein resident in the vesicle membrane, remains clustered in isolated patches on the presynaptic membrane regardless of whether the nerve terminals are mildly active or intensely stimulated. this suggests that at least some vesicle constituents remain together during recycling. our study also demonstrates that questions involving cellular structures with dimensions of a few tens of nanometres can be resolved with conventional far-field optics and visible light.
1089	585952	inproceedings	iccv	\N	\N	\N	\N	1	\N	2005	\N	2006-04-14 02:26:18	\N	creating efficient codebooks for visual recognition	visual codebook based quantization of robust appearance descriptors extracted from local image patches is an effective means of capturing image statistics for texture analysis and scene classification. codebooks are usually constructed by using a method such as k-means to cluster the descriptor vectors of patches sampled either densely ('textons') or sparsely ('bags of features' based on key-points or salience measures) from a set of training images. this works well for texture analysis in homogeneous images, but the images that arise in natural object recognition tasks have far less uniform statistics. we show that for dense sampling, k-means over-adapts to this, clustering centres almost exclusively around the densest few regions in descriptor space and thus failing to code other informative regions. this gives suboptimal codes that are no better than using randomly selected centres. we describe a scalable acceptance-radius based clusterer that generates better codebooks and study its performance on several image classification tasks. we also show that dense representations outperform equivalent keypoint based ones on these tasks and that {svm} or mutual information based feature selection starting from a dense codebook further improves the performance.
1090	587700	article	ieee transactions on software engineering	\N	\N	ieee	16	30	5	2004	may	2006-04-15 17:05:35	\N	{qos}-aware middleware for web services composition	the paradigmatic shift from a web of manual interactions to a web of programmatic interactions driven by web services is creating unprecedented opportunities for the formation of online business-to-business ({b2b}) collaborations. in particular, the creation of value-added services by composition of existing ones is gaining a significant momentum. since many available web services provide overlapping or identical functionality, albeit with different quality of service ({qos}), a choice needs to be made to determine which services are to participate in a given composite service. this paper presents a middleware platform which addresses the issue of selecting web services for the purpose of their composition in a way that maximizes user satisfaction expressed as utility functions over {qos} attributes, while satisfying the constraints set by the user and by the structure of the composite service. two selection approaches are described and compared: one based on local (task-level) selection of services and the other based on global allocation of tasks to services using integer programming.
1091	590914	article	internet computing, ieee	\N	\N	\N	5	9	5	2005	\N	2006-04-19 07:22:50	\N	guest editors' introduction: social networks and social networking	this issue's theme includes three articles on research activities that have drawn on ideas from social networking to drive innovative designs. the focus covers the design, development, and study of social technologies at the level of individuals, groups, and organizations. although the tools described here are all intended for individuals, each article highlights how new technologies and technical competencies will further push our understanding of human social-networking drives and desires.
1092	593144	article	ieee transactions on acoustics, speech and signal processing	\N	\N	\N	1	35	3	1987	mar	2006-04-20 23:51:24	\N	estimation of probabilities from sparse data for the language model component of a speech recognizer	the description of a novel type of m-gram language model is given. the model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data. this solution compares favorably to other proposed methods. while the method has been developed for and successfully implemented in the ibm real time speech recognizers, its generality makes it applicable in other areas where the problem of estimating probabilities from sparse data arises.
1093	593175	techreport	\N	\N	\N	\N	\N	\N	\N	-1	aug	2006-04-20 23:51:24	\N	introduction to wordnet an online lexical database	wordnet is an on-line lexical reference system whose design is inspired by current psycholinguistic theories of human lexical memory. english nouns, verbs, and adjectives are organized into synonym sets, each representing one underlying lexical concept. different relations link the synonym sets. standard alphabetical procedures for organizing lexical information put together words that are spelled alike and scatter words with similar or related meanings haphazardly through the list. unfortunately, there is no obvious alternative, no other simple way for lexicographers to keep track of what has been done or for readers to find the word they are looking for. but a frequent objection to this solution is that finding things on an alphabetical list can be tedious and time-consuming. many people who would like to refer to a dictionary decide not to bother with it because finding the information would interrupt their work and break their train of thought. in this age of computers, however, there is an answer to that complaint. one obvious reason to resort to on-line dictionaries - lexical databases that can be read by computers - is that computers can search such alphabetical lists much faster than people can. a dictionary entry can be available as soon as the target word is selected or typed into the keyboard. moreover, since dictionaries are printed from tapes that are read by computers, it is a relatively simple matter to convert those tapes into the appropriate kind of lexical database. putting conventional dictionaries on line seems a simple and natural marriage of the old and the new. once computers are enlisted in the service of dictionary users, however, it quickly becomes apparent that it is grossly inefficient to use these powerful machines as little more than rapid page-turners. the challenge is to think what further use to make of them. wordnet is a proposal for a more effective combination of traditional lexicographic information and modern high-speed computation. this, and the accompanying four papers, is a detailed report of the state of wordnet as of 1990. in order to reduce the unnecessary repetition, the papers are written to be read consecutively.
1094	594772	article	science	\N	\N	\N	4	296	5566	2002	apr	2006-04-21 22:08:34	ghost lab, laboratory for cellular and molecular immunology, national institute of allergy and infectious diseases, national institutes of health, bethesda, md 20892, usa. pcm@helix.nih.gov	the danger model: a renewed sense of self	10.1126/science.1071059
1095	594821	article	bmc bioinformatics	\N	\N	\N	\N	6 Suppl 1	Suppl 1	2005	\N	2006-04-21 23:25:38	the mitre corporation, 202 burlington road, bedford, ma 01730, usa. lynette@mitre.org	overview of {biocreative}: critical assessment of information extraction for biology.	the goal of the first {biocreative} challenge (critical assessment of information extraction in biology) was to provide a set of common evaluation tasks to assess the state of the art for text mining applied to biological problems. the results were presented in a workshop held in granada, spain march 28-31, 2004. the articles collected in this {bmc} bioinformatics supplement entitled "a critical assessment of text mining methods in molecular biology" describe the {biocreative} tasks, systems, results and their independent evaluation. {biocreative} focused on two tasks. the first dealt with extraction of gene or protein names from text, and their mapping into standardized gene identifiers for three model organism databases (fly, mouse, yeast). the second task addressed issues of functional annotation, requiring systems to identify specific text passages that supported gene ontology annotations for specific proteins, given full text articles. the first {biocreative} assessment achieved a high level of international participation (27 groups from 10 countries). the assessment provided state-of-the-art performance results for a basic task (gene name finding and normalization), where the best systems achieved a balanced 80\\% precision / recall or better, which potentially makes them suitable for real applications in biology. the results for the advanced task (functional annotation from free text) were significantly lower, demonstrating the current limitations of text-mining approaches where knowledge extrapolation and interpretation are required. in addition, an important contribution of {biocreative} has been the creation and release of training and test data sets for both tasks. there are 22 articles in this special issue, including six that provide analyses of results or data quality for the data sets, including a novel inter-annotator consistency assessment for the test set used in task 2.
1096	594822	article	bmc bioinformatics	\N	\N	\N	\N	6 Suppl 1	Suppl 1	2005	\N	2006-04-21 23:26:55	the mitre corporation, 202 burlington road, bedford, ma 01730, usa. asy@mitre.org	{biocreative} task {1a}: gene mention finding evaluation.	background: the biological research literature is a major repository of knowledge. as the amount of literature increases, it will get harder to find the information of interest on a particular topic. there has been an increasing amount of work on text mining this literature, but comparing this work is hard because of a lack of standards for making comparisons. to address this, we worked with colleagues at the protein design group, cnb-csic, madrid to develop biocreative (critical assessment for information extraction in biology), an open common evaluation of systems on a number of biological text mining tasks. we report here on task 1a, which deals with finding mentions of genes and related entities in text. "finding mentions" is a basic task, which can be used as a building block for other text mining tasks. the task makes use of data and evaluation software provided by the (us) national center for biotechnology information (ncbi). results: 15 teams took part in task 1a. a number of teams achieved scores over 80% f-measure (balanced precision and recall). the teams that tried to use their task 1a systems to help on other biocreative tasks reported mixed results. conclusion: the 80% plus f-measure results are good, but still somewhat lag the best scores achieved in some other domains such as newswire, due in part to the complexity and length of gene names, compared to person or organization names in newswire.
1097	595408	article	genome research	\N	\N	\N	\N	\N	\N	-1	\N	2006-04-22 19:28:26	\N	genomewide computational prediction of transcriptional regulatory modules reveals new insights into human gene expression	the identification of regulatory regions is one of the most important and challenging problems toward the functional annotation of the human genome. in higher eukaryotes, transcription-factor (tf) binding sites are often organized in clusters called cis-regulatory modules (crm). while the prediction of individual tf-binding sites is a notoriously difficult problem, crm prediction has proven to be somewhat more reliable. starting from a set of predicted binding sites for more than 200 tf families documented in transfac, we describe an algorithm relying on the principle that crms generally contain several phylogenetically conserved binding sites for a few different tfs. the method allows the prediction of more than 118,000 crms within the human genome. a subset of these is shown to be bound in vivo by tfs using chip-chip. their analysis reveals, among other things, that crm density varies widely across the genome, with crm-rich regions often being located near genes encoding transcription factors involved in development. predicted crms show a surprising enrichment near the 3â€² end of genes and in regions far from genes. we document the tendency for certain tfs to bind modules located in specific regions with respect to their target genes and identify tfs likely to be involved in tissue-specific regulation. the set of predicted crms, which is made available as a public database called premod (http://genomequebec.mcgill.ca/premod), will help analyze regulatory mechanisms in specific biological systems.
1098	595633	article	phys rev e stat nonlin soft matter phys	\N	\N	\N	\N	71	6 Pt 1	2005	jun	2006-04-22 21:27:30	\N	duplication-divergence model of protein interaction network	we investigate a very simple model describing the evolution of protein-protein interaction networks via duplication and divergence. the model exhibits a remarkably rich behavior depending on a single parameter, the probability to retain a duplicated link during divergence. when this parameter is large, the network growth is not self-averaging and an average node degree increases algebraically. the lack of self-averaging results in a great diversity of networks grown out of the same initial condition. when less than a half of links are (on average) preserved after divergence, the growth is self-averaging, the average degree increases very slowly or tends to a constant, and a degree distribution has a power-law tail. the predicted degree distributions are in a very good agreement with the distributions observed in real protein networks.
1099	598208	electronic	\N	\N	\N	\N	\N	\N	\N	2006	apr	2006-04-24 16:23:40	\N	complexity and philosophy	the science of complexity is based on a new way of thinking that stands in sharp contrast to the philosophy underlying newtonian science, which is based on reductionism, determinism, and objective knowledge. this paper reviews the historical development of this new world view, focusing on its philosophical foundations. determinism was challenged by quantum mechanics and chaos theory. systems theory replaced reductionism by a scientifically based holism. cybernetics and postmodern social science showed that knowledge is intrinsically subjective. these developments are being integrated under the header of "complexity science". its central paradigm is the multi-agent system. agents are intrinsically subjective and uncertain about their environment and future, but out of their local interactions, a global organization emerges. although different philosophers, and in particular the postmodernists, have voiced similar ideas, the paradigm of complexity still needs to be fully assimilated by philosophy. this will throw a new light on old philosophical issues such as relativism, ethics and the role of the subject.
1100	598868	article	neuroimage	\N	\N	\N	5	28	3	2005	nov	2006-04-24 22:12:48	department of radiology, university of pennsylvania, 3600 market street, suite 380, philadelphia, pa 19104, usa. christos@rad.upenn.edu	classifying spatial patterns of brain activity with machine learning methods: application to lie detection	patterns of brain activity during deception have recently been characterized with {fmri} on the multi-subject average group level. the clinical value of {fmri} in lie detection will be determined by the ability to detect deception in individual subjects, rather than group averages. high-dimensional non-linear pattern classification methods applied to functional magnetic resonance ({fmri}) images were used to discriminate between the spatial patterns of brain activity associated with lie and truth. in 22 participants performing a forced-choice deception task, 99\\% of the true and false responses were discriminated correctly. predictive accuracy, assessed by cross-validation in participants not included in training, was 88\\%. the results demonstrate the potential of non-linear machine learning techniques in lie detection and other possible clinical applications of {fmri} in individual subjects, and indicate that accurate clinical tests could be based on measurements of brain function with {fmri}.
1101	599904	article	science	\N	\N	\N	6	227	\N	1985	mar	2006-04-25 15:41:26	\N	{rapid and sensitive protein similarity searches}	an algorithm was developed which facilitates the search for similarities between newly determined amino acid sequences and sequences already available in databases. because of the algorithm's efficiency on many microcomputers, sensitive protein database searches may now become a routine procedure for molecular biologists. the method efficiently identifies regions of similar sequence and then scores the aligned identical and differing residues in those regions by means of an amino acid replacability matrix. this matrix increases sensitivity by giving high scores to those amino acid replacements which occur frequently in evolution. the algorithm has been implemented in a computer program designed to search protein databases very rapidly. for example, comparison of a 200-amino-acid sequence to the 500,000 residues in the national biomedical research foundation library would take less than 2 minutes on a minicomputer, and less than 10 minutes on a microcomputer (ibm pc).
1102	600388	article	molecular systems biology	\N	\N	nature publishing group	\N	2	1	2006	apr	2006-04-25 16:38:51	\N	deciphering principles of transcription regulation in eukaryotic genomes	transcription regulation has been responsible for organismal complexity and diversity in the course of biological evolution and adaptation, and it is determined largely by the context-dependent behavior of cis-regulatory elements ({cres}). therefore, understanding principles underlying {cre} behavior in regulating transcription constitutes a fundamental objective of quantitative biology, yet these remain poorly understood. here we present a deterministic mathematical strategy, the motif expression decomposition ({med}) method, for deriving principles of transcription regulation at the single-gene resolution level. {med} operates on all genes in a genome without requiring any a priori knowledge of gene cluster membership, or manual tuning of parameters. applying {med} to saccharomyces cerevisiae transcriptional networks, we identified four functions describing four different ways that {cres} can quantitatively affect gene expression levels. these functions, three of which have extrema in different positions in the gene promoter (short-, mid-, and long-range) whereas the other depends on the motif orientation, are validated by expression data. we illustrate how nature could use these principles as an additional dimension to amplify the combinatorial power of a small set of {cres} in regulating transcription.
1103	600541	techreport	\N	\N	\N	cornell university	\N	\N	\N	1987	\N	2006-04-25 17:39:02	ithaca, ny, usa	term weighting approaches in automatic text retrieval	the experimental evidence accumulated over the past 20 years indicates that textindexing systems based on the assignment of appropriately weighted single terms produce retrieval results that are superior to those obtainable with other more elaborate text representations. these results depend crucially on the choice of effective term weighting systems. this paper summarizes the insights gained in automatic term weighting, and provides baseline single term indexing models with which other more elaborate content analysis procedures can be compared.
1104	603893	article	evolution	\N	\N	\N	16	37	6	1983	nov	2006-04-27 00:44:42	\N	the measurement of selection on correlated characters	multivariate statistical methods are derived for measuring selection solely from observed changes in the distribution of phenotypic characters in a population within a generation. selective effects are readily detectable in characters that do not change with age, such as meristic traits or adult characters in species with determinate growth. ontogenetic characters, including allometric growth rates, can be analyzed in longitudinal studies where individuals are followed through time. following an approach pioneered by pearson (1903), this analysis helps to reveal the target(s) of selection, and to quantify its intensity, without identifying the selective agent(s). by accounting for indirect selection through correlated characters, separate forces of directional and stabilizing (or disruptive) selection acting directly on each character can be measured. these directional and stabilizing selection coefficients are respectively the parameters that describe the best linear and quadratic approximations to the selective surface of individual fitness as a function of the phenotypic characters. the theory is illustrated by estimating selective forces on morphological characters influencing survival in pentatomid bugs and in house sparrows during severe weather conditions.
1105	604304	incollection	\N	spinning the semantic web: bringing the world wide web to its full potential	\N	mit press	\N	\N	\N	2002	\N	2006-04-27 10:33:28	\N	ontologies come of age	ontologies have moved beyond the domains of library science, philosophy, and knowledge representation. they are now the concerns of marketing departments, {ceos}, and mainstream business. research analyst companies such as forrester research report on the critical roles of ontologies in support of browsing and search for e-commerce and in support of interoperability for facilitation of knowledge management and configuration. one now sees ontologies used as central controlled vocabularies that are integrated into catalogues, databases, web publications, knowledge management applications, etc. large ontologies are essential components in many online applications including search (such as yahoo and lycos), e-commerce (such as amazon and {ebay}), configuration (such as dell and {pc}-order), etc. one also sees ontologies that have long life spans, sometimes in multiple projects (such as {umls}, {sic} codes, etc.). such diverse usage generates many implications for ontology environments. in this paper, we will discuss ontologies and requirements in their current instantiations on the web today. we will describe some desirable properties of ontologies. we will also discuss how both simple and complex ontologies are being and may be used to support varied applications. we will conclude with a discussion of emerging trends in ontologies and their environments and briefly mention our evolving ontology evolution environment.
1106	608465	inproceedings	\N	chi	\N	acm	9	\N	\N	2006	\N	2006-05-01 07:18:57	new york, ny, usa	the frame of the game: blurring the boundary between fiction and reality in mobile experiences	mobile experiences that take place in public settings such as on city streets create new opportunities for interweaving the fictional world of a performance or game with the everyday physical world. a study of a touring performance reveals how designers generated excitement and dramatic tension by implicating bystanders and encouraging the (apparent) crossing of normal boundaries of behaviour. the study also shows how designers dealt with associated risks through a process of careful orchestration. consequently, we extend an existing framework for designing spectator interfaces with the concept of performance frames, enabling us to distinguish audience from bystanders. we conclude that using ambiguity to blur the frame can be a powerful design tactic, empowering players to willingly suspend disbelief, so long as a safety-net of orchestration ensures that they do not stray into genuine difficulty.
1107	608690	article	proc natl acad sci u s a	\N	\N	\N	5	103	14	2006	apr	2006-05-01 12:08:50	department of biochemistry, university of washington, box 357350, j-567 health sciences, seattle, wa 98195-7350.	physically realistic homology models built with {rosetta} can be more accurate than their templates.	we have developed a method that combines the rosetta de novo protein folding and refinement protocol with distance constraints derived from homologous structures to build homology models that are frequently more accurate than their templates. we test this method by building complete-chain models for a benchmark set of 22 proteins, each with 1 or 2 candidate templates, for a total of 39 test cases. we use structure-based and sequence-based alignments for each of the test cases. all atoms, including hydrogens, are represented explicitly. the resulting models contain approximately the same number of atomic overlaps as experimentally determined crystal structures and maintain good stereochemistry. the most accurate models can be identified by their energies, and in 22 of 39 cases a model that is more accurate than the template over aligned regions is one of the 10 lowest-energy models.
1108	608904	article	ieee tse	\N	\N	\N	16	27	4	2001	\N	2006-05-01 15:05:57	\N	software reflexion models: bridging the gap between design and implementation	the artifacts constituting a software system often drift apart over time. we have developed the software reflexion model technique to help engineers perform various software engineering tasks by exploiting, rather than removing, the drift between design and implementation. more specifically, the technique helps an engineer compare artifacts by summarizing where one artifact (such as a design) is consistent with and inconsistent with another artifact (such as source). the technique can be applied to help a software engineer evolve a structural mental model of a system to the point that it is &ldquo;good enough&rdquo; to be used for reasoning about a task at hand. the software reflexion model technique has been applied to support a variety of tasks, including design conformance, change assessment, and an experimental reengineering of the million-lines-of-code microsoft excel product. we provide a formal characterization of the reflexion model technique, discuss practical aspects of the approach, relate experiences of applying the approach and tools, and place the technique into the context of related work
1109	612089	article	proceedings of the national academy of sciences of the united states of america	\N	\N	national academy of sciences	5	103	15	2006	apr	2006-05-03 05:43:29	department of physics of complex systems, the weizmann institute of science, rehovot 76100, israel.	thousands of samples are needed to generate a robust gene list for predicting outcome in cancer.	10.1073/pnas.0601231103 predicting at the time of discovery the prognosis and metastatic potential of cancer is a major challenge in current clinical research. numerous recent studies searched for gene expression signatures that outperform traditionally used clinical parameters in outcome prediction. finding such a signature will free many patients of the suffering and toxicity associated with adjuvant chemotherapy given to them under current protocols, even though they do not need such treatment. a reliable set of predictive genes also will contribute to a better understanding of the biological mechanism of metastasis. several groups have published lists of predictive genes and reported good predictive performance based on them. however, the gene lists obtained for the same clinical types of patients by different groups differed widely and had only very few genes in common. this lack of agreement raised doubts about the reliability and robustness of the reported predictive gene lists, and the main source of the problem was shown to be the small number of samples that were used to generate the gene lists. here, we introduce a previously undescribed mathematical method, probably approximately correct (pac) sorting, for evaluating the robustness of such lists. we calculate for several published data sets the number of samples that are needed to achieve any desired level of reproducibility. for example, to achieve a typical overlap of 50% between two predictive lists of genes, breast cancer studies would need the expression profiles of several thousand early discovery patients.
1110	612596	article	human molecular genetics	\N	\N	oxford university press	12	15	suppl 1	2006	apr	2006-05-03 18:17:46	\N	non-coding {rna}	the term non-coding {rna} ({ncrna}) is commonly employed for {rna} that does not encode a protein, but this does not mean that such {rnas} do not contain information nor have function. although it has been generally assumed that most genetic information is transacted by proteins, recent evidence suggests that the majority of the genomes of mammals and other complex organisms is in fact transcribed into {ncrnas}, many of which are alternatively spliced and/or processed into smaller products. these {ncrnas} include {micrornas} and {snornas} (many if not most of which remain to be identified), as well as likely other classes of yet-to-be-discovered small regulatory {rnas}, and tens of thousands of longer transcripts (including complex patterns of interlacing and overlapping sense and antisense transcripts), most of whose functions are unknown. these {rnas} (including those derived from introns) appear to comprise a hidden layer of internal signals that control various levels of gene expression in physiology and development, including chromatin architecture/epigenetic memory, transcription, {rna} splicing, editing, translation and turnover. {rna} regulatory networks may determine most of our complex characteristics, play a significant role in disease and constitute an unexplored world of genetic variation both within and between species.
1111	613092	article	journal of artificial societies and social simulation	\N	\N	\N	\N	\N	3	2002	\N	2006-05-04 09:42:04	\N	opinion dynamics and bounded confidence models, analysis and simulation	abstract when does opinion formation within an interacting group lead to consensus, polarization or fragmentation? the article investigates various models for the dynamics of continuous opinions by analytical methods as well as by computer simulations. section 2 develops within a unified framework the classical model of consensus formation, the variant of this model due to friedkin and johnsen, a time-dependent version and a nonlinear version with bounded confidence of the agents. section 3 presents for all these models major analytical results. section 4 gives an extensive exploration of the nonlinear model with bounded confidence by a series of computer simulations. an appendix supplies needed mathematical definitions, tools, and theorems.
1112	613130	article	software process: improvement and practice	\N	\N	\N	26	10(2)	\N	2005	\N	2006-05-04 10:43:56	\N	staged configuration through specialization and multilevel configuration of feature models	feature modeling is a key technique for capturing commonalities and variabilities in system families and product lines. in this article, we propose a cardinality-based notation for feature modeling, which integrates a number of existing extensions of previous approaches. we then introduce and motivate the novel concept of staged configuration. staged configuration can be achieved by the stepwise specialization of feature models or by multilevel configuration, where the configuration choices available in each stage are defined by separate feature models. staged configuration is important because, in a realistic development process, different groups and different people make product configuration choices in different stages. finally, we also discuss how multilevel configuration avoids a breakdown between the different abstraction levels of individual features. this problem, sometimes referred to as 'analysis paralysis', easily occurs in feature modeling because features can denote entities at arbitrary levels of abstraction within a system family. copyright Â© 2005 john wiley & sons, ltd.
1113	619738	inproceedings	\N	proc.\\ 11th annual conference on computational learning theory	\N	acm press	11	\N	\N	1998	\N	2006-05-09 01:04:56	new york, ny	improved boosting algorithms using confidence-rated predictions	we describe several improvements to freund and schapireâ€˜s adaboost boosting algorithm, particularly in a setting in which hypotheses may assign confidences to each of their predictions. we give a simplified analysis of adaboost in this setting, and we show how this analysis can be used to find improved parameter settings as well as a refined criterion for training weak hypotheses. we give a specific method for assigning confidences to the predictions of decision trees, a method closely related to one used by quinlan. this method also suggests a technique for growing decision trees which turns out to be identical to one proposed by kearns and mansour. we focus next on how to apply the new boosting algorithms to multiclass classification problems, particularly to the multi-label case in which each example may belong to more than one class. we give two boosting methods for this problem, plus a third method based on output coding. one of these leads to a new method for handling the single-label case which is simpler but as effective as techniques suggested by freund and schapire. finally, we give some experimental results comparing a few of the algorithms discussed in this paper.
1114	619753	article	journal of molecular evolution	\N	\N	\N	14	21	\N	1985	\N	2006-05-09 01:07:38	\N	dating of the human-ape splitting by a molecular clock of mitochondrial {dna}	a new statistical method for estimating divergence dates of species from dna sequence data by a molecular clock approach is developed. this method takes into account effectively the information contained in a set of dna sequence data. the molecular clock of mitochondrial dna (mtdna) was calibrated by setting the date of divergence between primates and ungulates at the cretaceous-tertiary boundary (65 million years ago), when the extinction of dinosaurs occurred. a generalized least-squares method was applied in fitting a model to mtdna sequence data, and the clock gave dates of 92.3 +/- 11.7, 13.3 +/- 1.5, 10.9 +/- 1.2, 3.7 +/- 0.6, and 2.7 +/- 0.6 million years ago (where the second of each pair of numbers is the standard deviation) for the separation of mouse, gibbon, orangutan, gorilla, and chimpanzee, respectively, from the line leading to humans. although there is some uncertainty in the clock, this dating may pose a problem for the widely believed hypothesis that the pipedal creature australopithecus afarensis, which lived some 3.7 million years ago at laetoli in tanzania and at hadar in ethiopia, was ancestral to man and evolved after the human-ape splitting. another likelier possibility is that mtdna was transferred through hybridization between a proto-human and a proto-chimpanzee after the former had developed bipedalism.
1115	622020	book	\N	\N	\N	basic books	\N	\N	\N	1996	mar	2006-05-10 22:13:22	\N	fluid concepts and creative analogies: computer models of the fundamental mechanisms of thought	{douglas hofstadter, best known for his masterpiece <i>godel, escher, bach: an eternal golden braid,</i> tackles the subject of artificial intelligence and machine learning in his thought-provoking work <i>fluid concepts and creative analogies,</i> written in conjunction with the fluid analogies research group at the university of michigan. driven to discover whether computers can be made to "think" like humans, hofstadter and his colleagues created a variety of computer programs that extrapolate sequences, apply pattern-matching strategies, make analogies, and even act "creative." as always, hofstadter's work requires devotion on the part of the reader, but rewards him with fascinating insights into the nature of both human and machine intelligence.} {"will change your idea of what it is to be creative and even what it is to be human."--(william poundstone, <i>new york times book review</i>) }
1116	624835	article	\N	genetic and evolutionary computation	\N	\N	8	\N	\N	1999	\N	2006-05-12 10:54:26	\N	three ways to grow designs: a comparison of embryogenies for an evolutionary design problem	this paper explores the use of growth processes, or embryogenies, to map genotypes to phenotypes within evolutionary systems. following a summary of the significant features of embryogenies, the three main types of embryogenies in evolutionary computation are then identified and explained: external, explicit and implicit. an experimental comparison between these three different embryogenies and an evolutionary algorithm with no embryogeny is performed. the problem set to the four evolutionary systems is to evolve tessellating tiles. in order to assess the scalability of the embryogenies, the problem is increased in difficulty by enlarging the size of tiles to be evolved. the results are surprising, with the implicit embryogeny outperforming all other techniques by showing no significant increase in the size of the genotypes or decrease in accuracy of evolution as the scale of the problem is increased.
1117	624990	misc	\N	\N	\N	\N	\N	\N	\N	-1	\N	2006-05-12 13:14:08	\N	an experimental study of the skype {peer-to-peer} {voip} system	despite its popularity, relatively little is known about the trafc characteristics of the skype voip system and how they differ from other p2p systems. we describe an experimental study of skype voip trafc conducted over a ve month period, where over 82 milÂ­ lion datapoints were collected regarding the population of online clients, the number of supernodes, and their trafc characteristics. this data was collected from september 1, 2005 to january 14, 2006. experiments on this data were done in a blackÂ­box manner, i.e., without knowing the internals or specics of the skype system or messages, as skype encrypts all user trafc and signaling trafc payloads. the results indicate that although the structure of the skype system appears to be similar to other p2p systems, particuÂ­ larly kazaa, there are several signicant differences in trafc. the number of active clients shows diurnal and workÂ­week behavior, correlating with normal working hours regardless of geography. the population of supernodes in the system tends to be relatively stable; thus node churn, a signicant concern in other systems, seems less problematic in skype. the typical bandwidth load on a supernode is relatively low, even if the supernode is relaying voip trafc. the paper aims to aid further understanding of a signicant, successful p2p voip system, as well as provide experimental data that may be useful for future design and modeling of such sysÂ­ tems. these results also imply that the nature of a voip p2p system like skype differs fundamentally from earlier p2p systems that are oriented toward leÂ­sharing, and music and video download appliÂ­ cations, and deserves more attention from the research community.
1118	625347	article	genome res	\N	\N	\N	\N	\N	\N	2006	may	2006-05-12 17:41:10	genome exploration research group, riken genomic sciences centre (gsc), riken yokohama institute, 1-7-22 suehiro-cho, tsurumi-ku, yokohama, kanagawa, 230-0045, japan;	evolutionary turnover of mammalian transcription start sites.	alignments of homologous genomic sequences are widely used to identify functional genetic elements and study their evolution. most studies tacitly equate homology of functional elements with sequence homology. this assumption is violated by the phenomenon of turnover, in which functionally equivalent elements reside at locations that are nonorthologous at the sequence level. turnover has been demonstrated previously for transcriptionfactor-binding sites. here, we show that transcription start sites of equivalent genes do not always reside at equivalent locations in the human and mouse genomes. we also identify two types of partial turnover, illustrating evolutionary pathways that could lead to complete turnover. these findings suggest that the signals encoding transcription start sites are highly flexible and evolvable, and have cautionary implications for the use of sequence-level conservation to detect gene regulatory elements.
1119	626055	article	neuroimage	\N	\N	\N	12	20	3	2003	nov	2006-05-13 14:38:39	wellcome department of imaging neuroscience, functional imaging laboratory, 12 queen square, london wc1n 3bg, uk. odavid@fil.ion.ucl.ac.uk	a neural mass model for {meg}/{eeg}: coupling and neuronal dynamics.	although meg/eeg signals are highly variable, systematic changes in distinct frequency bands are commonly encountered. these frequency-specific changes represent robust neural correlates of cognitive or perceptual processes (for example, alpha rhythms emerge on closing the eyes). however, their functional significance remains a matter of debate. some of the mechanisms that generate these signals are known at the cellular level and rest on a balance of excitatory and inhibitory interactions within and between populations of neurons. the kinetics of the ensuing population dynamics determine the frequency of oscillations. in this work we extended the classical nonlinear lumped-parameter model of alpha rhythms, initially developed by lopes da silva and colleagues [kybernetik 15 (1974) 27], to generate more complex dynamics. we show that the whole spectrum of meg/eeg signals can be reproduced within the oscillatory regime of this model by simply changing the population kinetics. we used the model to examine the influence of coupling strength and propagation delay on the rhythms generated by coupled cortical areas. the main findings were that (1) coupling induces phase-locked activity, with a phase shift of 0 or [pi] when the coupling is bidirectional, and (2) both coupling and propagation delay are critical determinants of the meg/eeg spectrum. in forthcoming articles, we will use this model to (1) estimate how neuronal interactions are expressed in meg/eeg oscillations and establish the construct validity of various indices of nonlinear coupling, and (2) generate event-related transients to derive physiologically informed basis functions for statistical modelling of average evoked responses.
1120	631023	book	\N	\N	\N	{new press}	\N	\N	\N	1999	jun	2006-05-14 16:42:05	\N	globalization and its discontents: essays on the new mobility of people and money	{groundbreaking essays on the new global economy from an "expert observer" (forecast). saskia sassen is an internationally recognized expert on globalization whose writings have appeared in journals and magazines worldwide. now available in paperback, globalization and its discontents is a collection of sassen's essays dealing with topics such as the "global city," gender and migration (reconceived as the globalization of labor), information technology, and the new dynamics of inequality.  sassen brings together cultural and literary studies, feminist theory, political economics, sociology, and political science, showing how vast the chasm between metropolitan business centers and low-income inner cities has become. incisive and original, she takes on common political, cultural, and economic misconceptions of globalization and offers a thoughtful, provocative new look at our increasingly global society. }
1121	634084	article	cognition	\N	\N	\N	\N	20	\N	1988	\N	2006-05-14 19:23:53	\N	connectionism and cognitive architecture: a critical analysis	(from the chapter) when taken as a way of modeling cognitive architecture, connectionism really does represent an approach that is quite different from that of the classical cognitive science that it seeks to replace. /// first, we discuss some methodological questions about levels of explanation that have become enmeshed in the substantive controversy over connectionism. second, we try to say what it is that makes connectionist and classical theories of mental structure incompatible. third, we review and extend some of the traditional arguments for the classical architecture. we hope to make it clear how various aspects of the classical doctrine cohere and why rejecting the classical picture of reasoning leads connectionists to say the very implausible things they do about logic and semantics. finally, we return to the question what makes the connectionist approach appear attractive to so many people. in doing so we'll consider some arguments that have been offered in favor of connectionist networks as general models of cognitive processing. /// topics discussed include: the nature of the dispute (complex mental representations, structure-sensitive operations)  the need for symbol systems: productivity, systematicity, and inferential coherence (productivity of thought, systematicity of cognitive representation and inference)  and concluding comments: connectionism as a theory of implementation. ((c) 1997 apa/psycinfo, all rights reserved)
1122	634897	book	\N	\N	\N	morgan kaufmann	\N	\N	\N	1993	sep	2007-02-04 17:01:50	\N	usability engineering	{an authoritative text by one of the premier researchers in usability engineering in the 1990s, jakob nielsen's <i>usability engineering</i> provides a landmark guide to software design that has helped bring this area of research into the mainstream of computing. "usability" is the measurement of how easy or difficult it is to be productive with a piece of software. it often looks at the user interface--what elements appear onscreen and how efficient, confusing, and/or intuitive they are for beginning, intermediate, and advanced users. "usability engineering" is the formal study of usability. it grew out of research on human factors, which looked at the way people interact with their environment.<p> the best thing about this book is its concise, cut-to-the-chase approach when defining usability and ways to measure and improve it. as the author notes, in the old days of computing, documents that attempted to define usability might have over 1,000 rules. the author offers just a handful of guiding principles for creating better software that apply even today. (published just before the internet revolution, this book's principles still hold true for web designers, as well as those who create more traditional applications.)<p> throughout this text, the author argues for the benefits of improved software usability. with software use as with all things, time is money and making more efficient interfaces translates into lower personnel costs and more productivity. the book also does a fine job of integrating usability design into the software development process, with guides for planning, working with end users, and running tests with users (whether on videotape or in person). the 50-page bibliography attests to the author's previous research on usability.<p> for anyone who needs to create better, more efficient software, <i>usability engineering</i> can help. this clear and intelligent guide to the science of usability engineering has helped enhance the potential of computers to work with end users more efficiently. in the new century, software developers will undoubtedly seek new advances in usability, in part because of the groundwork laid by books like this one. <i>--richard dragan</i><p> <b>topics covered</b>: usability basics, measuring usability, types of users, history of user interfaces, the usability engineering lifecycle, design techniques, heuristics and hints for improving usability, testing, managing user tests, assessing usability, interface standards, internationalization, and computer-aided usability engineering (cause) tools.} {written by the author of the best-selling <b>hypertext \\& hypermedia,</b> this book is an excellent guide to the methods of usability engineering. the book provides the tools needed to avoid usability surprises and improve product quality. step-by-step information on which method to use at various stages during the development lifecycle are included, along with detailed information on how to run a usability test and the unique issues relating to international usability.<br><br>* emphasizes cost-effective methods that developers can implement immediately<br>* instructs readers about which methods to use when, throughout the development lifecycle, which ultimately helps in cost-benefit analysis. <br>* shows readers how to avoid the four most frequently listed reasons for delay in software projects.<br>* includes detailed information on how to run a usability test.<br>* covers unique issues of international usability.<br>* features an extensive bibliography allowing readers to find additional information.<br>* written by an internationally renowned expert in the field and the author of the best-selling hypertext \\& hypermedia.}
1123	635513	article	nature	\N	\N	macmillan magazines ltd.	8	405	6783	2000	may	2006-05-15 12:39:38	\N	consequences of changing biodiversity	human alteration of the global environment has triggered the sixth major extinction event in the history of life and caused widespread changes in the global distribution of organisms. these changes in biodiversity alter ecosystem processes and change the resilience of ecosystems to environmental change. this has profound consequences for services that humans derive from ecosystems. the large ecological and societal consequences of changing biodiversity should be minimized to preserve options for future solutions to global environmental problems.
1124	643915	misc	\N	\N	\N	\N	\N	\N	\N	1998	\N	2006-05-18 10:45:02	\N	course notes: reinforcement learning i: an introduction	introduction richard s. sutton and andrew g. barto c fl all rights reserved [in which we try to give a basic intuitive sense of what reinforcement learning is and how it differs and relates to other fields, e.g., supervised learning and neural networks, genetic algorithms and artificial life, control theory. intuitively, {rl} is trial and error (variation and selection, search) plus learning (association, memory). we argue that {rl} is the only field that seriously addresses the special features of ...
1125	653744	inproceedings	\N	international conference on functional programming	\N	\N	6	\N	\N	1999	\N	2006-05-18 19:31:06	\N	when is a functional program not a functional program?	in an impure functional language, there are programs whose behaviour is completely functional (in that they behave extensionally on inputs), but the functions they compute cannot be written in the purely functional fragment of the language. that is, the class of programs with functional behaviour is more expressive than the usual class of pure functional programs. in this paper we introduce this extended class of functional programs by means of examples in standard ml, and explore what they...
1126	654294	inproceedings	\N	\N	\N	\N	7	\N	\N	-1	\N	2006-05-18 19:53:14	\N	the structure of the information visualization design space	research on information visualization has reached the point where a number of successful point designs have been proposed and a variety of techniques have been discovered. it is now appropriate to describe and analyze portions of the design space so as to understand the differences among designs and to suggest new possibilities. this paper proposes an organization of the information visualization literature and illustrates it with a series of examples. the result is a framework for designing new visualizations and augmenting existing designs.
1127	658235	article	systematic biology	\N	\N	oxford university press	15	53	5	2004	oct	2006-05-19 19:17:43	departamento de bioqu\\'{\\i}mica, gen\\'{e}tica e inmunolog\\'{\\i}a, facultad de biolog\\'{\\i}a, universidad de vigo, vigo 36200, spain. dposada@uvigo.es	model selection and model averaging in phylogenetics: advantages of akaike information criterion and bayesian approaches over likelihood ratio tests.	model selection is a topic of special relevance in molecular phylogenetics that affects many, if not all, stages of phylogenetic inference. here we discuss some fundamental concepts and techniques of model selection in the context of phylogenetics. we start by reviewing different aspects of the selection of substitution models in phylogenetics from a theoretical, philosophical and practical point of view, and summarize this comparison in table format. we argue that the most commonly implemented model selection approach, the hierarchical likelihood ratio test, is not the optimal strategy for model selection in phylogenetics, and that approaches like the akaike information criterion (aic) and bayesian methods offer important advantages. in particular, the latter two methods are able to simultaneously compare multiple nested or nonnested models, assess model selection uncertainty, and allow for the estimation of phylogenies and model parameters using all available models (model-averaged inference or multimodel inference). we also describe how the relative importance of the different parameters included in substitution models can be depicted. to illustrate some of these points, we have applied aic-based model averaging to 37 mitochondrial dna sequences from the subgenus ohomopterus (genus carabus) ground beetles described by sota and vogler (2001).
1128	658841	article	nat rev neurosci	\N	\N	nature publishing group	12	7	6	2006	jun	2006-06-01 21:48:17	\N	the role of the basal ganglia in habit formation	many organisms, especially humans, are characterized by their capacity for intentional, goal-directed actions. however, similar behaviours often proceed automatically, as habitual responses to antecedent stimuli. how are goal-directed actions transformed into habitual responses? recent work combining modern behavioural assays and neurobiological analysis of the basal ganglia has begun to yield insights into the neural basis of habit formation.
1129	663686	electronic	http://ssrn.com/abstract=555883	\N	\N	\N	\N	\N	\N	2004	\N	2006-05-22 00:30:00	\N	a piece of place: modeling the digital on the real in second life	digital worlds exist as synthetic models and have no need for the constraints of the real world.  this freedom allows digital worlds a vast design space of representational choices, ranging from near correspondence to the real world to complete abstraction.  the digital world second life was designed to allow its residents enormous creative freedom and to be as broadly appealing as possible.  second life chose to mirror the real world in many important aspects in order to provide a place that felt familiar and comfortable, while granting freedoms not possible in the real world.  this article will cover the environment of second life, the reasons for the choice and the challenges that arose.
1130	663718	book	\N	\N	\N	\N	\N	\N	\N	-1	\N	2006-05-22 00:58:44	\N	massively multiplayer online roleplaying games the people the addiction and the playing experience	{this book is about the fastest growing form of electronic game in the world&#151;the massively multiplayer online role playing game (mmorpg). it introduces these self-contained three-dimensional virtual worlds, often inhabited by thousands of players, and describes their evolution and sometimes become addicted to it. it also delves into the psychology of the people who inhabit the game universe and explores the development of the unique cultures, economies, moral codes, and slang in these virtual communities. it explains how the games are built, the spin-offs that players create to enhance their game lives, and peeks at the future of mmorpgs as they evolve from a form of amusement to an educational, scientific, and business tool.  <p>   based on hundreds of interviews over a three-year period, the work explores reasons people are attracted to and addicted to these games. it also surveys many existing and upcoming games, identifying their unique features and attractions. two appendices list online addiction organizations and mmorpg information sites.}
1131	663920	article	the american economic review	\N	\N	\N	32	91	5	2001	\N	2006-05-22 04:58:50	\N	the colonial origins of comparative development: an empirical investigation	we exploit differences in european mortality rates to estimate the effect of institutions on economic performance. europeans adopted very different colonization policies in different colonies, with different associated institutions. in places where europeans faced high mortality rates, they could not settle and were more likely to set up extractive institutions. these institutions persisted to the present. exploiting differences in european mortality rates as an instrument for current institutions, we estimate large effects of institutions on income per capita. once the effect of institutions is controlled for, countries in africa or those closer to the equator do not have lower incomes.
1132	665925	article	annual review of biochemistry	\N	\N	\N	25	75	1	2006	mar	2006-05-23 05:04:56	department of cellular and molecular pharmacology, university of california, san francisco, california 94143.	domains, motifs, and scaffolds: the role of modular interactions in the evolution and wiring of cell signaling circuits	{abstractliving} cells display complex signal processing behaviors, many of which are mediated by networks of proteins specialized for signal transduction. here we focus on the question of how the remarkably diverse array of eukaryotic signaling circuits may have evolved. many of the mechanisms that connect signaling proteins into networks are highly modular: the core catalytic activity of a signaling protein is physically and functionally separable from molecular domains or motifs that determine its linkage to both inputs and outputs. this high degree of modularity may make these systems more evolvable—in principle, novel circuits, and therefore highly innovative regulatory behaviors, can arise from relatively simple genetic events such as recombination, deletion, or insertion. in support of this hypothesis, recent studies show that such modular systems can be exploited to engineer nonnatural signaling proteins and pathways with novel behavior.
1133	665989	book	\N	\N	\N	{john wiley \\& sons}	\N	\N	\N	2006	feb	2006-05-23 10:22:25	\N	mobile interaction design	{<i>mobile interaction design</i> shifts the design perspective away from the technology and concentrates on usability; in other words the book concentrates on developing interfaces and devices with a great deal of sensitivity to human needs, desires and capabilities.     <ul>     <li>presents key interaction design ideas and successes in an accessible, relevant way     <li>exercises, case studies and study questions make this book ideal for students.     <li>provides ideals and techniques which will enable designers to create the next generation of effective mobile applications.     <li>critiques current mobile interaction design (bloopers) to help designers avoid pitfalls.     <li>design challenges and worked examples are given to reinforce ideas.     <li>discusses the new applications and gadgets requiring knowledgeable and inspired thinking about usability and design.     <li>authors have extensive experience in mobile interaction design, research, industry and teaching     </ul>} {mobile interaction design shifts the design perspective away from the technology and concentrates on usability; in other words the book concentrates on developing interfaces and devices with a great deal of sensitivity to human needs, desires and capabilities. presents key interaction design ideas and successes in an accessible, relevant way  exercises, case studies and study questions make this book ideal for students.  provides ideals and techniques which will enable designers to create the next generation of effective mobile applications.  critiques current mobile interaction design (bloopers) to help designers avoid pitfalls.  design challenges and worked examples are given to reinforce ideas.  discusses the new applications and gadgets requiring knowledgeable and inspired thinking about usability and design.  authors have extensive experience in mobile interaction design, research, industry and teaching}
1134	666883	article	computer	\N	\N	\N	9	39	5	2006	\N	2006-05-24 00:24:05	\N	the problem with threads	for concurrent programming to become mainstream, we must discard threads as a programming model. nondeterminism should be judiciously and carefully introduced where needed, and it should be explicit in programs.
1135	667832	article	proceedings of the national academy of sciences of the united states of america	\N	\N	\N	5	103	21	2006	may	2006-05-24 10:03:35	\N	hotspots for copy number variation in chimpanzees and humans.	copy number variation is surprisingly common among humans and can be involved in phenotypic diversity and variable susceptibility to complex diseases, but little is known of the extent of copy number variation in nonhuman primates. we have used two array-based comparative genomic hybridization platforms to identify a total of 355 copy number variants ({cnvs}) in the genomes of 20 wild-born chimpanzees (pan troglodytes) and have compared the identified chimpanzee {cnvs} to known human {cnvs} from previous studies. many {cnvs} were observed in the corresponding regions in both chimpanzees and humans; especially those {cnvs} of higher frequency. strikingly, these loci are enriched 20-fold for ancestral segmental duplications, which may facilitate {cnv} formation through nonallelic homologous recombination mechanisms. therefore, some of these regions may be unstable "hotspots" for the genesis of copy number variation, with recurrent duplications and deletions occurring across and within species.
1136	668238	article	protein science : a publication of the protein society	\N	\N	\N	12	11	11	2002	nov	2006-05-24 13:42:31	howard hughes medical institute center for single molecule biophysics, department of physiology \\& biophysics, state university of new york at buffalo, buffalo, new york 14214, usa.	distance-scaled, finite ideal-gas reference state improves structure-derived potentials of mean force for structure selection and stability prediction.	the distance-dependent structure-derived potentials developed so far all employed a reference state that can be characterized as a residue (atom)-averaged state. here, we establish a new reference state called the distance-scaled, finite ideal-gas reference (dfire) state. the reference state is used to construct a residue-specific all-atom potential of mean force from a database of 1011 nonhomologous (less than 30% homology) protein structures with resolution less than 2 a. the new all-atom potential recognizes more native proteins from 32 multiple decoy sets, and raises an average z-score by 1.4 units more than two previously developed, residue-specific, all-atom knowledge-based potentials. when only backbone and c(beta) atoms are used in scoring, the performance of the dfire-based potential, although is worse than that of the all-atom version, is comparable to those of the previously developed potentials on the all-atom level. in addition, the dfire-based all-atom potential provides the most accurate prediction of the stabilities of 895 mutants among three knowledge-based all-atom potentials. comparison with several physical-based potentials is made.
1137	673554	article	journal of artificial intelligence research	\N	\N	\N	15	7	\N	1997	\N	2006-05-28 21:39:44	\N	identifying hierarchical structure in sequences a lineartime algorithm	sequitur is an algorithm that infers a hierarchical structure from a sequence of discrete symbols by replacing repeated phrases with a grammatical rule that generates the phrase, and continuing this process recursively. the result is a hierarchical representation of the original sequence, which offers insights into its lexical structure. the algorithm is driven by two constraints that reduce the size of the grammar, and produce structure as a by-product. sequitur breaks new ground by operating incrementally. moreover, the method's simple structure permits a proof that it operates in space and time that is linear in the size of the input. our implementation can process 50,000 symbols per second and has been applied to an extensive range of real world sequences. 1. introduction many sequences of discrete symbols exhibit natural hierarchical structure. text is made up of paragraphs, sentences, phrases, and words. music is composed from major sections, motifs, bars, and notes. records of ...
1138	680937	article	nature	\N	\N	\N	5	409	6822	2001	feb	2006-06-02 04:56:09	\N	a map of human genome sequence variation containing 1.42 million single nucleotide polymorphisms	we describe a map of 1.42 million single nucleotide polymorphisms (snps) distributed throughout the human genome, providing an average density on available sequence of one snp every 1.9 kilobases. these snps were primarily discovered by two projects: the snp consortium and the analysis of clone overlaps by the international human genome sequencing consortium. the map integrates all publicly available snps with described genes and other genomic features. we estimate that 60,000 snps fall within exon (coding and untranslated regions), and 85% of exons are within 5 kb of the nearest snp. nucleotide diversity varies greatly across the genome, in a manner broadly consistent with a standard population genetic model of human history. this high-density snp map provides a public resource for defining haplotype variation across the genome, and should help to identify biomedically important genes for diagnosis and therapy.
1139	681467	inproceedings	\N	proceedings of the eleventh acm sigkdd international conference on knowledge discovery in data mining	kdd	acm	6	\N	\N	2005	\N	2006-06-02 13:14:11	new york, ny, usa	evaluating similarity measures: a large-scale study in the orkut social network	online information services have grown too large for users to navigate without the help of automated tools such as collaborative filtering, which makes recommendations to users based on their collective past behavior. while many similarity measures have been proposed and individually evaluated, they have not been evaluated relative to each other in a large real-world environment. we present an extensive empirical comparison of six distinct measures of similarity for recommending online communities to members of the orkut social network. we determine the usefulness of the different recommendations by actually measuring users' propensity to visit and join recommended communities. we also examine how the ordering of recommendations influenced user selection, as well as interesting social issues that arise in recommending communities within a real social network.
1140	681886	article	ieee journal on selected areas in communications	\N	\N	\N	19	23	2	2005	feb	2006-06-02 18:14:38	\N	cognitive radio: brain-empowered wireless communications	cognitive radio is viewed as a novel approach for improving the utilization of a precious natural resource: the radio electromagnetic spectrum. the cognitive radio, built on a software-defined radio, is defined as an intelligent wireless communication system that is aware of its environment and uses the methodology of understanding-by-building to learn from the environment and adapt to statistical variations in the input stimuli, with two primary objectives in mind: \\&middot; highly reliable communication whenever and wherever needed; \\&middot; efficient utilization of the radio spectrum. following the discussion of interference temperature as a new metric for the quantification and management of interference, the paper addresses three fundamental cognitive tasks. 1) radio-scene analysis. 2) channel-state estimation and predictive modeling. 3) transmit-power control and dynamic spectrum management. this work also discusses the emergent behavior of cognitive radio.
1141	683079	article	nature genetics	\N	\N	nature publishing group	6	34	3	2003	jun	2006-06-04 01:12:10	whitehead institute/mit center for genome research, cambridge, massachusetts, usa.	{pgc}-1?-responsive genes involved in oxidative phosphorylation are coordinately downregulated in human diabetes	{dna} microarrays can be used to identify gene expression changes characteristic of human disease. this is challenging, however, when relevant differences are subtle at the level of individual genes. we introduce an analytical strategy, gene set enrichment analysis, designed to detect modest but coordinate changes in the expression of groups of functionally related genes. using this approach, we identify a set of genes involved in oxidative phosphorylation whose expression is coordinately decreased in human diabetic muscle. expression of these genes is high at sites of insulin-mediated glucose disposal, activated by {pgc}-1 and correlated with total-body aerobic capacity. our results associate this gene set with clinically important variation in human metabolism and illustrate the value of pathway relationships in the analysis of genomic profiling experiments.
1142	685683	article	biomedical digital libraries	\N	\N	\N	\N	3	\N	2006	mar	2006-06-25 11:19:38	\N	scopus database: a review.	the scopus database provides access to stm journal articles and the references included in those articles, allowing the searcher to search both forward and backward in time. the database can be used for collection development as well as for research. this review provides information on the key points of the database and compares it to web of science. neither database is inclusive, but complements each other. if a library can only afford one, choice must be based in institutional needs.
1143	691332	article	human-computer interaction	\N	\N	\N	53	21	1	2006	\N	2006-06-09 18:46:22	\N	pattern languages in {hci}: a critical review	this article presents a critical review of patterns and pattern languages in human-computer interaction ({hci}). in recent years, patterns and pattern languages have received considerable attention in {hci} for their potential as a means for developing and communicating information and knowledge to support good design. this review examines the background to patterns and pattern languages in {hci}, and seeks to locate pattern languages in relation to other approaches to interaction design. the review explores four key issues: what is a pattern? what is a pattern language? how are patterns and pattern languages used? and how are values reflected in the pattern-based approach to design? following on from the review, a future research agenda is proposed for patterns and pattern languages in {hci}.
1144	691971	misc	\N	\N	\N	\N	\N	\N	\N	-1	\N	2006-06-10 22:05:27	\N	an adaptive system for the personalized access to news	personalization is one of the keys for the success of web services. in this paper we present sean (server for adaptive news), an adaptive system for the personalized access to news servers on the www. the aims of the system are (i) to select the sections (topics) and news in the server that are most relevant for each user, (ii) to customize the detail level of each news item to the user's characteristics and (iii) to select the advertisements that are most appropriate for each page and user. in the paper we discuss the functionalities of the system and we present the choices we made in its design. in particular, we focus on the techniques we adopted for structuring the news archive, for creating and maintaining the user model and for generating the personalized hypertext for browsing the news server.
1145	694515	article	computer supported cooperative work (cscw). the journal of collaborative computing	\N	\N	\N	13	11	3-4	2002	\N	2006-06-13 10:17:34	\N	the problem with ``awareness'': introductory remarks on ``awareness in {cscw}''	at a very early stage in the course of {cscw}, it became evident that categories such as `conversation' or `workflow' were quite insufficient for characterizing and understanding the ways in which cooperative work is coordinated and integrated. it quickly became obvious that cooperating actors somehow, while doing their individual bits, take heed of the context of their joint effort. more specifically, the early harvest of ethnographic field studies in {cscw} (e.g., harper et al., 1989b; harper et al., 1989a; heath and luff, 1991) indicated that cooperating actors align and integrate their activities with those of their colleagues in a seemingly `seamless' manner, that is, without interrupting each other, for instance by asking, suggesting, requesting, ordering, reminding, etc. others of this or that. as a placeholder for these elusive practices of taking heed of what is going on in the setting which seem to play a key role in cooperative work, the term `awareness' was soon adopted.
1146	694702	article	sigmobile mob. comput. commun. rev.	\N	\N	acm	15	9	2	2005	apr	2006-06-13 12:08:24	new york, ny, usa	access and mobility of wireless {pda} users	in this paper, we analyze the mobility patterns of users of wireless hand-held {pdas} in a campus wireless network using an eleven week trace of wireless network activity. our study has two goals. first, we characterize the high-level mobility and access patterns of hand-held {pda} users and compare these characteristics to previous workload mobility studies focused on laptop users. second, we develop two wireless network topology models for use in wireless mobility studies: an evolutionary topology model based on user proximity and a campus waypoint model that serves as a trace-based complement to the random waypoint model. we use our evolutionary topology model as a case study for preliminary evaluation of three ad hoc routing algorithms on the network topologies created by the access and mobility patterns of users of modern wireless {pdas}. based upon the mobility characteristics of our trace-based campus waypoint model, we find that commonly parameterized synthetic mobility models have overly aggressive mobility characteristics for scenarios where user movement is limited to walking. mobility characteristics based on realistic models can have significant implications for evaluating systems designed for mobility. when evaluated using our evolutionary topology model, for example, popular ad hoc routing protocols were very successful at adapting to user mobility, and user mobility was not a key factor in their performance.
1147	695505	article	acm trans. graph.	\N	\N	acm	7	24	3	2005	jul	2006-06-14 08:46:37	new york, ny, usa	image completion with structure propagation	in this paper, we introduce a novel approach to image completion, which we call structure propagation. in our system, the user manually specifies important missing structure information by extending a few curves or line segments from the known to the unknown regions. our approach synthesizes image patches along these user-specified curves in the unknown region using patches selected around the curves in the known region. structure propagation is formulated as a global optimization problem by enforcing structure and consistency constraints. if only a single curve is specified, structure propagation is solved using dynamic programming. when multiple intersecting curves are specified, we adopt the belief propagation algorithm to find the optimal patches. after completing structure propagation, we fill in the remaining unknown regions using patch-based texture synthesis. we show that our approach works well on a number of examples that are challenging to state-of-the-art techniques.
1148	697330	article	american journal of physiology - heart and circulatory physiology	\N	\N	american physiological society	10	278	6	2000	jun	2006-06-15 18:58:39	department of internal medicine, university of virginia health sciences center, charlottesville 22908, usa.	physiological time-series analysis using approximate entropy and sample entropy	entropy, as it relates to dynamical systems, is the rate of information production. methods for estimation of the entropy of a system represented by a time series are not, however, well suited to analysis of the short and noisy data sets encountered in cardiovascular and other biological studies. pincus introduced approximate entropy (apen), a set of measures of system complexity closely related to entropy, which is easily applied to clinical cardiovascular and other time series. apen statistics, however, lead to inconsistent results. we have developed a new and related complexity measure, sample entropy (sampen), and have compared apen and sampen by using them to analyze sets of random numbers with known probabilistic character. we have also evaluated cross-apen and cross-sampen, which use cardiovascular data sets to measure the similarity of two distinct time series. sampen agreed with theory much more closely than apen over a broad range of conditions. the improved accuracy of sampen statistics should make them useful in the study of experimental clinical cardiovascular and other biological time series.
1149	697912	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2006-06-16 08:59:47	\N	supporting ethnographic studies of ubiquitous computing in the wild	ethnography has become a staple feature of it research over the last twenty years, shaping our understanding of the social character of computing systems and informing their design in a wide variety of settings. the emergence of ubiquitous computing raises new challenges for ethnography however, distributing interaction across a burgeoning array of small, mobile devices and online environments which exploit invisible sensing systems. understanding interaction requires ethnographers to reconcile interactions that are, for example, distributed across devices on the street with online interactions in order to assemble coherent understandings of the social character and purchase of ubiquitous computing systems. we draw upon four recent studies to show how ethnographers are replaying system recordings of interaction alongside existing resources such as video recordings to do this and identify key challenges that need to be met to support ethnographic study of ubiquitous computing in the wild.
1150	699834	article	genome biology	\N	\N	\N	\N	7	6	2006	jun	2006-07-06 12:18:32	\N	what properties characterize the hub proteins of the protein-protein interaction network of saccharomyces cerevisiae?	most proteins interact with only a few other proteins while a small number of proteins (hubs) have many interaction partners. hub proteins and non-hub proteins differ in several respects; however, understanding is not complete about what properties characterize the hubs and set them apart from proteins of low connectivity. therefore, we have investigated what differentiates hubs from non-hubs and static hubs (party hubs) from dynamic hubs (date hubs) in the protein-protein interaction network of saccharomyces cerevisiae. the many interactions of hub proteins can only partly be explained by bindings to similar proteins or domains. it is evident that domain repeats, which are associated with binding, are enriched in hubs. moreover, there is an over representation of multi-domain proteins and long proteins among the hubs. in addition, there are clear differences between party hubs and date hubs. fewer of the party hubs contain long disordered regions compared to date hubs, indicating that these regions are important for flexible binding but less so for static interactions. furthermore, party hubs interact to a large extent with each other, supporting the idea of party hubs as the cores of highly clustered functional modules. in addition, hub proteins, and in particular party hubs, are more often ancient. finally, the more recent paralogs of party hubs are underrepresented. our results indicate that multiple and repeated domains are enriched in hub proteins and, further, that long disordered regions, which are common in date hubs, are particularly important for flexible binding.
1151	707182	book	\N	\N	\N	\N	\N	\N	\N	1988	\N	2006-06-22 11:45:02	\N	the sources of innovation	{it has long been assumed that new product innovations are typically developed by product manufacturers, an assumption that has inevitably had a major impact on innovation-related research and activities ranging from how firms organize their research and development to how governments measure innovation.  in this synthesis of his seminal research, von hippel challenges that basic assumption and demonstrates that innovation occurs in different places in different industries. presenting a series of studies showing that end-users, material suppliers, and others are the typical sources of innovation in some fields, von hippel explores why this variation in the "functional" sources of innovation occurs and how it might be predicted.  he also proposes and tests some implications of replacing a manufacturer-as-innovator assumption with a view of the innovation process as predictably distributed across users, manufacturers, and suppliers.  innovation, he argues, will take place where there is greatest economic benefit to the innovator.}
1152	707725	inproceedings	\N	chi	\N	acm press	9	\N	\N	2006	\N	2006-06-22 21:26:31	new york, ny, usa	talk to me: foundations for successful individual-group interactions in online communities	"people come to online communities seeking information, encouragement, and conversation. when a community re- sponds, participants benefit and become more committed. yet interactions often fail. in a longitudinal sample of 6,172 messages from 8 usenet newsgroups, 27% of posts re- ceived no response. the information context, postersâ€™ prior engagement in the community, and the content of their posts all influenced the likelihood that they received a re- ply, and, as a result, their willingness to continue active participation. posters were less likely to get a reply if they were newcomers. posting on-topic, introducing oneself via autobiographical testimonials, asking questions, using less complex language and other features of the messages, in- creased replies. results suggest ways that developers might increase the ability of online communities to support successful individual-group interactions"
1153	708641	article	image processing, ieee transactions on	\N	\N	\N	16	9	12	2000	\N	2006-06-23 17:13:17	\N	optimization of mutual information for multiresolution image registration	we propose a new method for the intermodal registration of images using a criterion known as mutual information. our main contribution is an optimizer that we specifically designed for this criterion. we show that this new optimizer is well adapted to a multiresolution approach because it typically converges in fewer criterion evaluations than other optimizers. we have built a multiresolution image pyramid, along with an interpolation process, an optimizer, and the criterion itself, around the unifying concept of spline-processing. this ensures coherence in the way we model data and yields good performance. we have tested our approach in a variety of experimental conditions and report excellent results. we claim an accuracy of about a hundredth of a pixel under ideal conditions. we are also robust since the accuracy is still about a tenth of a pixel under very noisy conditions. in addition, a blind evaluation of our results compares very favorably to the work of several other researchers.
1154	712068	book	\N	\N	\N	{peter lang publishing}	\N	\N	\N	-1	\N	2006-06-27 09:34:26	\N	gatewatching: collaborative online news production (digital formations)	{<i>gatewatching: collaborative online news production</i> is the first comprehensive study of the latest wave of online news publications. the book investigates the collaborative publishing models of key news websites, ranging from the worldwide <i>indymedia</i> network to the massively successful technology news site <i>slashdot,</i> and further to the multitude of weblogs that have emerged in recent years. building on collaborative approaches borrowed from the open source software development community, this book illustrates how gatewatching provides an alternative to gatekeeping and other traditional journalistic models of reporting, and has enabled millions of users around the world to participate in the online news publishing process.}
1155	713345	article	information processing letters	\N	\N	elsevier north-holland, inc.	6	90	5	2004	jun	2006-06-28 08:01:29	amsterdam, the netherlands, the netherlands	bipartite structure of all complex networks	the analysis and modelling of various complex networks has received much attention in the last few years. some such networks display a natural bipartite structure: two kinds of nodes coexist with links only between nodes of different kinds. this bipartite structure has not been deeply studied until now, mainly because it appeared to be specific to only a few complex networks. however, we show here that all complex networks can be viewed as bipartite structures sharing some important statistics, like degree distributions. the basic properties of complex networks can be viewed as consequences of this underlying bipartite structure. this leads us to propose the first simple and intuitive model for complex networks which captures the main properties met in practice.
1156	714224	article	nature neuroscience	\N	\N	nature publishing group	7	9	7	2006	jun	2006-07-31 01:40:55	\N	optimal decision making and the anterior cingulate cortex	learning the value of options in an uncertain environment is central to optimal decision making. the anterior cingulate cortex ({acc}) has been implicated in using reinforcement information to control behavior. here we demonstrate that the {acc}'s critical role in reinforcement-guided behavior is neither in detecting nor in correcting errors, but in guiding voluntary choices based on the history of actions and outcomes. {acc} lesions did not impair the performance of monkeys (macaca mulatta) immediately after errors, but made them unable to sustain rewarded responses in a reinforcement-guided choice task and to integrate risk and payoff in a dynamic foraging task. these data suggest that the {acc} is essential for learning the value of actions.
1157	732865	article	library \\& information science research	\N	\N	\N	15	28	2	2006	FebFeb	2006-07-03 14:05:01	\N	visualization-based information retrieval on the web	the application of visualization techniques to information retrieval ({ir}) has resulted in the development of innovative systems and interfaces that are now available for public use. visualization tools have emerged in research environments and more recently on the web to retrieve information. questions arise in regard to the utility of web-based {ir} visualization tools for assisting users not only in manipulating search output, but also in managing the information retrieval process. to understand how web-based visualization tools enable visual information retrieval, this article reviews some of the human perceptual theory behind the graphical interface of information visualization systems, analyzes iconic representations and information density on visualization displays, and examines information retrieval tasks that have been used in visualization system user research. this article is timely since it addresses new technologies for web information retrieval and discusses future information visualization user research directions.
1158	732884	article	management science	\N	\N	\N	19	48	9	2003	\N	2006-07-03 14:42:19	\N	{a structural approach to assessing innovation: construct development of innovation locus, type, and characteristics}	we take a structural approach to assessing innovation. we develop a comprehensive set of measures to assess an innovation's locus, type, and characteristics. we find that the concepts of competence destroying and competence enhancing are composed of two distinct constructs that, although correlated, separately characterize an innovation: new competence acquisition and competence enhancement/destruction. we develop scales to measure these constructs and show that new competence acquisition and competence enhancing/destroying are different from other innovation characteristics including core/peripheral and incremental/radical, as well as architectural and generational innovation types. we show that innovations can be evaluated distinctively on these various dimensions with generally small correlations between them. we estimate the impact these different innovation characteristics and types have on time to introduction and perceived commercial success. our results indicate the importance of taking a structural approach to describing innovations and to the differential importance of innovation locus, type, and characteristics on innovation outcomes. our results also raise intriguing questions regarding the locus of competence acquisition (internal vs. external) and both innovation outcomes. 10.1287/mnsc.48.9.1103.174
1159	735298	inbook	\N	\N	\N	springer: berlin	\N	\N	\N	1979	\N	2006-07-04 00:19:50	\N	{monte carlo methods in statistical physics}	{this book provides an introduction to monte carlo simulations in classical statistical physics and is aimed both at students beginning work in the field and at more experienced researchers who wish to learn more about monte carlo methods. the material covered includes methods for both equilibrium and out of equilibrium systems, and common algorithms like the metropolis and heat-bath algorithms are discussed in detail, as well as more sophisticated ones such as continuous time monte carlo, cluster algorithms, multigrid methods, entropic sampling and simulated tempering. data analysis techniques are also explained starting with straightforward measurement and error-estimation techniques and progressing to topics such as the single and multiple histogram methods and finite size scaling. the last few chapters of the book are devoted to implementation issues, including discussions of such topics as lattice representations, efficient implementation of data structures, multispin coding, parallelization of monte carlo algorithms, and random number generation. at the end of the book the authors give a number of example programs demonstrating the applications of these techniques to a variety of well-known models.}
1160	735744	article	nucleic acids research	\N	\N	oxford university press	2	32	suppl 2	2004	jul	2006-07-04 04:34:17	department of biochemistry and molecular biophysics, center for computational biology, washington university in st louis, 700 s. euclid avenue, campus box 8036, st louis, mo 63110, usa.	{pdb2pqr}: an automated pipeline for the setup of {poisson–boltzmann} electrostatics calculations	continuum solvation models, such as {poisson–boltzmann} and generalized born methods, have become increasingly popular tools for investigating the influence of electrostatics on biomolecular structure, energetics and dynamics. however, the use of such methods requires accurate and complete structural data as well as force field parameters such as atomic charges and radii. unfortunately, the limiting step in continuum electrostatics calculations is often the addition of missing atomic coordinates to molecular structures from the protein data bank and the assignment of parameters to biomolecular structures. to address this problem, we have developed the {pdb2pqr} web service (http://agave.wustl.edu/pdb2pqr/). this server automates many of the common tasks of preparing structures for continuum electrostatics calculations, including adding a limited number of missing heavy atoms to biomolecular structures, estimating titration states and protonating biomolecules in a manner consistent with favorable hydrogen bonding, assigning charge and radius parameters from a variety of force fields, and finally generating '{pqr}' output compatible with several popular computational biology packages. this service is intended to facilitate the setup and execution of electrostatics calculations for both experts and non-experts and thereby broaden the accessibility to the biological community of continuum electrostatics analyses of biomolecular systems.
1161	737763	article	the embo journal	\N	\N	nature publishing group	9	23	20	2004	oct	2006-07-04 10:23:30	institute of molecular biology and genetics and school of biological science, seoul national university, seoul, korea.	{microrna} genes are transcribed by {rna} polymerase {ii}.	{micrornas} ({mirnas}) constitute a large family of noncoding {rnas} that function as guide molecules in diverse gene silencing pathways. current efforts are focused on the regulatory function of {mirnas}, while little is known about how these unusual genes themselves are regulated. here we present the first direct evidence that {mirna} genes are transcribed by {rna} polymerase {ii} (pol {ii}). the primary {mirna} transcripts ({pri-mirnas}) contain cap structures as well as {poly(a}) tails, which are the unique properties of class {ii} gene transcripts. the treatment of human cells with alpha-amanitin decreased the level of {pri-mirnas} at a concentration that selectively inhibits pol {ii} activity. furthermore, chromatin immunoprecipitation analyses show that pol {ii} is physically associated with a {mirna} promoter. we also describe, for the first time, the detailed structure of a {mirna} gene by determining the promoter and the terminator of mir-23a approximately 27a approximately 24-2. these data indicate that pol {ii} is the main, if not the only, {rna} polymerase for {mirna} gene transcription. our study offers a basis for understanding the structure and regulation of {mirna} genes.
1162	739398	article	science	\N	\N	\N	\N	\N	\N	2006	jun	2006-07-05 02:44:02	department of biochemistry and molecular pharmacology, university of massachusetts medical school, worcester, ma 01605, usa; department of animal molecular genetics, institute of molecular genetics, moscow 123182, russia.	a distinct small {rna} pathway silences selfish genetic elements in the germline.	in the drosophila germline, repeat-associated small interfering {rnas} ({rasirnas}) ensure genomic stability by silencing endogenous selfish genetic elements such as retrotransposons and repetitive sequences. while small interfering {rnas} ({sirnas}) derive from both the sense and antisense strands of their double-stranded {rna} precursors, {rasirnas} arise mainly from the antisense strand. {rasirna} production appears not to require dicer-1, which makes {micrornas}, or dicer-2, which makes {sirnas}, and {rasirnas} lack the 2',3' hydroxy termini characteristic of animal {sirna} and {mirna}. unlike {sirnas} and {mirnas}, {rasirnas} function through the piwi, rather than the ago, argonaute protein subfamily. our data suggest that {rasirnas} protect the fly germline through a silencing mechanism distinct from both the {mirna} and {rnai} pathways.
1163	739792	proceedings	pervasive computing and communications, 2006. percom 2006. fourth annual ieee international conference on	\N	\N	\N	13	\N	\N	2006	\N	2006-07-05 06:31:22	\N	a collaborative web browsing system for multiple mobile users	in mobile computing environments, handheld devices with low functionality restrict the services provided for mobile users. we propose a new concept of collaborative browsing, where mobile users collaboratively browse web pages designed for desktop {pc}. in collaborative browsing, a web page is divided into multiple components, and each is distributed to a different device. in mobile computing environments, the number of handheld devices, their capabilities, and other conditions can vary widely amongst mobile users who want to browse content. therefore, we developed a page partitioning method for collaborative browsing, which divides a web page into multiple components. moreover, we designed and implemented a collaborative web browsing system in which users can search and browse their target information by discussing and watching partial pages displayed on multiple devices.
1164	745563	article	proceedings of the national academy of sciences of the u.s.a.	\N	\N	\N	5	100	1	2003	\N	2006-07-07 13:36:59	\N	functional connectivity in the resting brain: a network analysis of the default mode hypothesis	10.1073/pnas.0135058100 functional imaging studies have shown that certain brain regions, including posterior cingulate cortex (pcc) and ventral anterior cingulate cortex (vacc), consistently show greater activity during resting states than during cognitive tasks. this finding led to the hypothesis that these regions constitute a network supporting a default mode of brain function. in this study, we investigate three questions pertaining to this hypothesis: does such a resting-state network exist in the human brain? is it modulated during simple sensory processing? how is it modulated during cognitive processing? to address these questions, we defined pcc and vacc regions that showed decreased activity during a cognitive (working memory) task, then examined their functional connectivity during rest. pcc was strongly coupled with vacc and several other brain regions implicated in the default mode network. next, we examined the functional connectivity of pcc and vacc during a visual processing task and show that the resultant connectivity maps are virtually identical to those obtained during rest. last, we defined three lateral prefrontal regions showing increased activity during the cognitive task and examined their resting-state connectivity. we report significant inverse correlations among all three lateral prefrontal regions and pcc, suggesting a mechanism for attenuation of default mode network activity during cognitive processing. this study constitutes, to our knowledge, the first resting-state connectivity analysis of the default mode and provides the most compelling evidence to date for the existence of a cohesive default mode network. our findings also provide insight into how this network is modulated by task demands and what functions it might subserve.
1165	748961	article	plos comput biol	\N	\N	public library of science	\N	2	7	2006	jul	2006-07-10 12:22:42	\N	identification of {genome-scale} metabolic network models using experimentally measured flux profiles	genome-scale metabolic network models can be reconstructed for well-characterized organisms using genomic annotation and literature information. however, there are many instances in which model predictions of metabolic fluxes are not entirely consistent with experimental data, indicating that the reactions in the model do not match the active reactions in the in vivo system. we introduce a method for determining the active reactions in a genome-scale metabolic network based on a limited number of experimentally measured fluxes. this method, called optimal metabolic network identification ({omni}), allows efficient identification of the set of reactions that results in the best agreement between in silico predicted and experimentally measured flux distributions. we applied the method to intracellular flux data for evolved escherichia coli mutant strains with lower than predicted growth rates in order to identify reactions that act as flux bottlenecks in these strains. the expression of the genes corresponding to these bottleneck reactions was often found to be downregulated in the evolved strains relative to the wild-type strain. we also demonstrate the ability of the {omni} method to diagnose problems in e. coli strains engineered for metabolite overproduction that have not reached their predicted production potential. the {omni} method applied to flux data for evolved strains can be used to provide insights into mechanisms that limit the ability of microbial strains to evolve towards their predicted optimal growth phenotypes. when applied to industrial production strains, the {omni} method can also be used to suggest metabolic engineering strategies to improve byproduct secretion. in addition to these applications, the method should prove to be useful in general for reconstructing metabolic networks of ill-characterized microbial organisms based on limited amounts of experimental data. one of the major uses of in silico models in biology is to identify discrepancies between model predictions and experimental data and use these discrepancies to drive discovery of novel biological mechanisms. however, models only allow for identification of the discrepancies; they do not necessarily provide any assistance in discovering what are the missing or incorrect functionalities in the model that cause these discrepancies. herrg\\r{a}rd et al. describe a new in silico method, optimal metabolic network identification, or {omni}, that performs this discovery process in an efficient and systematic manner for genome-scale metabolic networks. given a preliminary metabolic network model and experimentally determined metabolic flux data, {omni} finds the changes that need to be made to the model so that its predictions match the experimental data as well as possible. herrg\\r{a}rd et al. apply the method to identify metabolic bottlenecks in experimentally evolved escherichia coli strains and to diagnose problems in strains designed through metabolic engineering strategies to overproduce specific desirable byproducts. the {omni} method can also be adapted to number of other settings, including identification of novel biochemical pathways in ill-characterized organisms based on limited amounts of experimental data.
1166	750011	article	genetics	\N	\N	genetics society of america	14	151	4	1999	apr	2006-07-11 06:07:08	\N	preservation of duplicate genes by complementary, degenerative mutations.	the origin of organismal complexity is generally thought to be tightly coupled to the evolution of new gene functions arising subsequent to gene duplication. under the classical model for the evolution of duplicate genes, one member of the duplicated pair usually degenerates within a few million years by accumulating deleterious mutations, while the other duplicate retains the original function. this model further predicts that on rare occasions, one duplicate may acquire a new adaptive function, resulting in the preservation of both members of the pair, one with the new function and the other retaining the old. however, empirical data suggest that a much greater proportion of gene duplicates is preserved than predicted by the classical model. here we present a new conceptual framework for understanding the evolution of duplicate genes that may help explain this conundrum. focusing on the regulatory complexity of eukaryotic genes, we show how complementary degenerative mutations in different regulatory elements of duplicated genes can facilitate the preservation of both duplicates, thereby increasing long-term opportunities for the evolution of new gene functions. the duplication-degeneration-complementation ({ddc}) model predicts that (1) degenerative mutations in regulatory elements can increase rather than reduce the probability of duplicate gene preservation and (2) the usual mechanism of duplicate gene preservation is the partitioning of ancestral functions rather than the evolution of new functions. we present several examples (including analysis of a new engrailed gene in zebrafish) that appear to be consistent with the {ddc} model, and we suggest several analytical and experimental approaches for determining whether the complementary loss of gene subfunctions or the acquisition of novel functions are likely to be the primary mechanisms for the preservation of gene duplicates. for a newly duplicated paralog, survival depends on the outcome of the race between entropic decay and chance acquisition of an advantageous regulatory mutation. sidow 1996(p. 717) on one hand, it may fix an advantageous allele giving it a slightly different, and selectable, function from its original copy. this initial fixation provides substantial protection against future fixation of null mutations, allowing additional mutations to accumulate that refine functional differentiation. alternatively, a duplicate locus can instead first fix a null allele, becoming a pseudogene. walsh 1995 (p. 426) duplicated genes persist only if mutations create new and essential protein functions, an event that is predicted to occur rarely. nadeau and sankoff 1997 (p. 1259) thus overall, with complex metazoans, the major mechanism for retention of ancient gene duplicates would appear to have been the acquisition of novel expression sites for developmental genes, with its accompanying opportunity for new gene roles underlying the progressive extension of development itself. cooke et al. 1997 (p. 362)
1167	753025	article	\N	\N	\N	\N	\N	\N	\N	2003	\N	2006-07-11 19:55:04	\N	web metasearch: rank vs. score based rank aggregation methods	given a set of rankings, the task of ranking fusion is the problem of combining these lists in such a way to optimize the performance of the combination. the ranking fusion problem is encountered in many situations and, e.g., metasearch is a prominent one. it deals with the problem of combining the result lists returned by multiple search engines in response to a given query, where each item in a result list is ordered with respect to a search engine and a relevance score. several ranking...
1168	753171	article	bmc public health	\N	\N	biomed central ltd	\N	6	1	2006	apr	2006-07-12 01:10:49	preventive and behavioral medicine, university of massachusetts medical school, worcester, ma, usa. nancy.lapelle@umassmed.edu	identifying strategies to improve access to credible and relevant information for public health professionals: a qualitative study	movement towards evidence-based practices in many fields suggests that public health ({ph}) challenges may be better addressed if credible information about health risks and effective {ph} practices is readily available. however, research has shown that many {ph} information needs are unmet. in addition to reviewing relevant literature, this study performed a comprehensive review of existing information resources and collected data from two representative {ph} groups, focusing on identifying current practices, expressed information needs, and ideal systems for information access.
1169	754255	article	science	\N	\N	\N	1	309	5732	2005	jul	2006-07-12 03:46:29	hubrecht laboratory, centre for biomedical genetics, 3584 ct utrecht, the netherlands.	{microrna} expression in zebrafish embryonic development	{micrornas} ({mirnas}) are small noncoding {rnas}, about 21 nucleotides in length, that can regulate gene expression by base-pairing to partially complementary {mrnas}. regulation by {mirnas} can play essential roles in embryonic development. we determined the temporal and spatial expression patterns of 115 conserved vertebrate {mirnas} in zebrafish embryos by microarrays and by in situ hybridizations, using locked-nucleic acid-modified oligonucleotide probes. most {mirnas} were expressed in a highly tissue-specific manner during segmentation and later stages, but not early in development, which suggests that their role is not in tissue fate establishment but in differentiation or maintenance of tissue identity. 10.1126/science.1114519
1170	754945	book	\N	\N	\N	cambridge university press	\N	\N	\N	2005	may	2006-07-12 16:13:58	\N	microeconometrics : methods and applications	the authors provide a comprehensive text on microeconometrics, the analysis of individual-level data on the economic behaviour of individuals or firms using regression methods for cross section and panel data.
1171	756195	inproceedings	\N	proceedings of the 5th acm/ieee-cs joint conference on digital libraries	jcdl	acm	1	\N	\N	2005	\N	2006-07-13 05:37:06	new york, ny, usa	link prediction approach to collaborative filtering	recommender systems can provide valuable services in a digital library environment, as demonstrated by its commercial success in book, movie, and music industries. one of the most commonly-used and successful recommendation algorithms is collaborative filtering, which explores the correlations within user-item interactions to infer user interests and preferences. however, the recommendation quality of collaborative filtering approaches is greatly limited by the data sparsity problem. to alleviate this problem we have previously proposed graph-based algorithms to explore transitive user-item associations. in this paper, we extend the idea of analyzing user-item interactions as graphs and employ link prediction approaches proposed in the recent network modeling literature for making collaborative filtering recommendations. we have adapted a wide range of linkage measures for making recommendations. our preliminary experimental results based on a book recommendation dataset show that some of these measures achieved significantly better performance than standard collaborative filtering algorithms.
1172	759594	article	international journal of computer vision	\N	\N	springer netherlands	21	48	3	2002	jul	2006-07-15 04:43:40	\N	{multi-frame} correspondence estimation using subspace constraints	when a rigid scene is imaged by a moving camera, the set of all displacements of all points across multiple frames often resides in a low-dimensional linear subspace. linear subspace constraints have been used successfully in the past for recovering 3d structure and 3d motion information from multiple frames (e.g., by using the factorization method of tomasi and kanade (1992, international journal of computer vision, 9:137â€“154)). these methods assume that the 2d correspondences have been precomputed. however, correspondence estimation is a fundamental problem in motion analysis. in this paper we show how the multi-frame subspace constraints can be used for constraining the 2d correspondence estimation process itself. we show that the multi-frame subspace constraints are valid not only for affine cameras, but also for a variety of imaging models, scene models, and motion models. the multi-frame subspace constraints are first translated from constraints on correspondences to constraints directly on image measurements (e.g., image brightness quantities). these brightness-based subspace constraints are then used for estimating the correspondences, by requiring that all corresponding points across all video frames reside in the appropriate low-dimensional linear subspace. the multi-frame subspace constraints are geometrically meaningful, and are {not} violated at depth discontinuities, nor when the camera-motion changes abruptly. these constraints can therefore replace {heuristic} constraints commonly used in optical-flow estimation, such as spatial or temporal smoothness.
1173	761824	book	\N	\N	\N	hyperion	\N	\N	\N	2006	jul	2006-07-17 12:47:38	\N	the long tail : why the future of business is selling less of more	examines the rise of the niche in today's economy, thanks to a breakdown in the barrier between supply and demand and the increasing availability of everything to everyone, and assesses the implications of this new economic model for business.
1174	765744	article	genome biology	\N	\N	\N	\N	7	7	2006	jul	2006-08-13 20:20:42	\N	shuffling of cis-regulatory elements is a pervasive feature of the vertebrate lineage	{background}: all vertebrates share a remarkable degree of similarity in their development as well as in the basic functions of their cells. despite this, attempts at unearthing genome-wide regulatory elements conserved throughout the vertebrate lineage using {blast}-like approaches have thus far detected noncoding conservation in only a few hundred genes, mostly associated with regulation of transcription and development. {results}: we used a unique combination of tools to obtain regional global-local alignments of orthologous loci. this approach takes into account shuffling of regulatory regions that are likely to occur over evolutionary distances greater than those separating mammalian genomes. this approach revealed one order of magnitude more vertebrate conserved elements than was previously reported in over 2,000 genes, including a high number of genes found in the membrane and extracellular regions. our analysis revealed that 72\\% of the elements identified have undergone shuffling. we tested the ability of the elements identified to enhance transcription in zebrafish embryos and compared their activity with a set of control fragments. we found that more than 80\\% of the elements tested were able to enhance transcription significantly, prevalently in a tissue-restricted manner corresponding to the expression domain of the neighboring gene. {conclusion}: our work elucidates the importance of shuffling in the detection of cis-regulatory elements. it also elucidates how similarities across the vertebrate lineage, which go well beyond development, can be explained not only within the realm of coding genes but also in that of the sequences that ultimately govern their expression.
1175	768502	article	trends genet	\N	\N	\N	6	19	\N	2003	\N	2006-07-21 16:50:18	\N	{genomic clocks and evolutionary timescales}	for decades, molecular clocks have helped to illuminate the evolutionary timescale of life, but now genomic data pose a challenge for time estimation methods. it is unclear how to integrate data from many genes, each potentially evolving under a different model of substitution and at a different rate. current methods can be grouped by the way the data are handled (genes considered separately or combined into a â€˜supergeneâ€™) and the way gene-specific rate models are applied (global versus local clock). there are advantages and disadvantages to each of these approaches, and the optimal method has not yet emerged. fortunately, time estimates inferred using many genes or proteins have greater precision and appear to be robust to different approaches.
1176	768516	misc	\N	\N	\N	\N	\N	\N	\N	2002	\N	2006-07-21 16:54:22	\N	an energy-efficient {mac} protocol for wireless sensor networks	this paper proposes {s-mac
1177	768767	article	american journal of human genetics	\N	\N	\N	15	78	4	2006	apr	2008-10-06 20:27:47	department of statistics, university of washington, seattle, 98195-4322, usa. pscheet@alum.wustl.edu	a fast and flexible statistical model for large-scale population genotype data: applications to inferring missing genotypes and haplotypic phase.	we present a statistical model for patterns of genetic variation in samples of unrelated individuals from natural populations. this model is based on the idea that, over short regions, haplotypes in a population tend to cluster into groups of similar haplotypes. to capture the fact that, because of recombination, this clustering tends to be local in nature, our model allows cluster memberships to change continuously along the chromosome according to a hidden markov model. this approach is flexible, allowing for both "block-like" patterns of linkage disequilibrium ({ld}) and gradual decline in {ld} with distance. the resulting model is also fast and, as a result, is practicable for large data sets (e.g., thousands of individuals typed at hundreds of thousands of markers). we illustrate the utility of the model by applying it to dense single-nucleotide-polymorphism genotype data for the tasks of imputing missing genotypes and estimating haplotypic phase. for imputing missing genotypes, methods based on this model are as accurate or more accurate than existing methods. for haplotype estimation, the point estimates are slightly less accurate than those from the best existing methods (e.g., for unrelated centre {d'etude} du polymorphisme humain individuals from the {hapmap} project, switch error was 0.055 for our method vs. 0.051 for {phase}) but require a small fraction of the computational cost. in addition, we demonstrate that the model accurately reflects uncertainty in its estimates, in that probabilities computed using the model are approximately well calibrated. the methods described in this article are implemented in a software package, {fastphase}, which is available from the stephens lab web site.
1178	769669	article	science	science	\N	american association for the advancement of science	3	308	5725	2005	may	2006-07-22 07:13:53	\N	deterministic coupling of single quantum dots to single nanocavity modes	we demonstrate a deterministic approach to the implementation of solid-state cavity quantum electrodynamics ({qed}) systems based on a precise spatial and spectral overlap between a single self-assembled quantum dot and a photonic crystal membrane nanocavity. by fine-tuning nanocavity modes with a high quality factor into resonance with any given quantum dot exciton, we observed clear signatures of cavity {qed} (such as the purcell effect) in all fabricated structures. this approach removes the major hindrances that had limited the application of solid-state cavity {qed} and enables the realization of experiments previously proposed in the context of quantum information processing.
1179	772702	article	annual review of microbiology	\N	\N	\N	34	55	1	2001	\N	2006-07-25 12:59:25	\N	quorum sensing in bacteria	â–ª abstractâ€‚ quorum sensing is the regulation of gene expression in response to fluctuations in cell-population density. quorum sensing bacteria produce and release chemical signal molecules called autoinducers that increase in concentration as a function of cell density. the detection of a minimal threshold stimulatory concentration of an autoinducer leads to an alteration in gene expression. gram-positive and gram-negative bacteria use quorum sensing communication circuits to regulate a diverse array of physiological activities. these processes include symbiosis, virulence, competence, conjugation, antibiotic production, motility, sporulation, and biofilm formation. in general, gram-negative bacteria use acylated homoserine lactones as autoinducers, and gram-positive bacteria use processed oligo-peptides to communicate. recent advances in the field indicate that cell-cell communication via autoinducers occurs both within and between bacterial species. furthermore, there is mounting data suggesting that bacterial autoinducers elicit specific responses from host organisms. although the nature of the chemical signals, the signal relay mechanisms, and the target genes controlled by bacterial quorum sensing systems differ, in every case the ability to communicate with one another allows bacteria to coordinate the gene expression, and therefore the behavior, of the entire community. presumably, this process bestows upon bacteria some of the qualities of higher organisms. the evolution of quorum sensing systems in bacteria could, therefore, have been one of the early steps in the development of multicellularity.
1180	774482	article	nature neuroscience	\N	\N	nature publishing group	6	9	8	2006	aug	2006-07-31 20:15:15	\N	midbrain dopamine neurons encode decisions for future action.	current models of the basal ganglia and dopamine neurons emphasize their role in reinforcement learning. however, the role of dopamine neurons in decision making is still unclear. we recorded from dopamine neurons in monkeys engaged in two types of trial: reference trials in an instructed-choice task and decision trials in a two-armed bandit decision task. we show that the activity of dopamine neurons in the decision setting is modulated according to the value of the upcoming action. moreover, analysis of the probability matching strategy in the decision trials revealed that the dopamine population activity and not the reward during reference trials determines choice behavior. because dopamine neurons do not have spatial or motor properties, we conclude that immediate decisions are likely to be generated elsewhere and conveyed to the dopamine neurons, which play a role in shaping long-term decision policy through dynamic modulation of the efficacy of basal ganglia synapses.
1181	776652	article	bioinformatics	\N	\N	\N	9	22	14	2006	\N	2006-07-27 21:07:04	\N	{on counting position weight matrix matches in a sequence, with application to discriminative motif finding}	motivation and results: the position weight matrix ({pwm}) is a popular method to model transcription factor binding sites. a fundamental problem in cis-regulatory analysis is to "count" the occurrences of a {pwm} in a {dna} sequence. we propose a novel probabilistic score to solve this problem of counting {pwm} occurrences. the proposed score has two important properties: (1) it gives appropriate weights to both strong and weak occurrences of the {pwm}, without using thresholds. (2) for any given {pwm}, this score can be computed while allowing for occurrences of other, a priori known {pwms}, in a statistically sound framework. additionally, the score is efficiently differentiable with respect to the {pwm} parameters, which has important consequences for designing search algorithms. the second problem we address is to find, ab initio, {pwms} that have high counts in one set of sequences, and low counts in another. we develop a novel algorithm to solve this "discriminative motif-finding problem", using the proposed score for counting a {pwm} in the sequences. the algorithm is a local search technique that exploits derivative information on an objective function to enhance speed and performance. it is extensively tested on synthetic data, and shown to perform better than other discriminative as well as non-discriminative {pwm} finding algorithms. it is then applied to cis-regulatory modules involved in development of the fruitfly embryo, to elicit known and novel motifs. we finally use the algorithm on genes predictive of social behavior in the honey bee, and find interesting motifs. availability: the program is available upon request from the author. contact: sinhas@cs.uiuc.edu
1182	778023	article	science	\N	\N	american association for the advancement of science	3	313	5786	2006	jul	2006-07-28 16:18:56	\N	reducing the dimensionality of data with neural networks	high-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. gradient descent can be used for fine-tuning the weights in such "autoencoder" networks, but this works well only if the initial weights are close to a good solution. we describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.
1183	781534	article	strat. mgmt. j.	\N	\N	john wiley \\& sons, ltd.	23	21	4	2000	apr	2006-08-01 10:52:21	the robert h. smith school of business, the university of maryland, college park, maryland, u.s.a.; the amos tuck school of business, dartmouth college, hanover, new hampshire, u.s.a.	knowledge flows within multinational corporations	pursuing a nodal (i.e., subsidiary) level of analysis, this paper advances and tests an overarching theoretical framework pertaining to intracorporate knowledge transfers within multinational corporations ({mncs}). we predicted that (i) knowledge outflows from a subsidiary would be positively associated with value of the subsidiary's knowledge stock, its motivational disposition to share knowledge, and the richness of transmission channels; and (ii) knowledge inflows into a subsidiary would be positively associated with richness of transmission channels, motivational disposition to acquire knowledge, and the capacity to absorb the incoming knowledge. these predictions were tested empirically with data from 374 subsidiaries within 75 {mncs} headquartered in the {u.s}., europe, and japan. except for our predictions regarding the impact of source unit's motivational disposition on knowledge outflows, the data provide either full or partial support to all of the other elements of our theoretical framework. copyright {\\copyright} 2000 john wiley \\& sons, ltd.
1184	781872	article	reviews of modern physics	\N	\N	american physical society	38	76	4	2005	feb	2008-07-15 18:44:20	\N	decoherence, the measurement problem, and interpretations of quantum mechanics	environment-induced decoherence and superselection have been a subject of intensive research over the past two decades, yet their implications for the foundational problems of quantum mechanics, most notably the quantum measurement problem, have remained a matter of great controversy. this paper is intended to clarify key features of the decoherence program, including its more recent results, and to investigate their application and consequences in the context of the main interpretive approaches of quantum mechanics.
1185	781890	article	rev. mod. phys.	\N	\N	american physical society	8	71	2	1999	mar	2006-09-07 14:20:36	\N	scaling, universality, and renormalization: three pillars of modern critical phenomena	this brief overview is designed to introduce some of the advances that have occurred in our understanding of phase transitions and critical phenomena. the presentation is organized around three simple questions: (i) what are the basic phenomena under consideration? (ii) why do we care? (iii) what do we actually do? to answer the third question, the author shall briefly review scaling, universality, and renormalization, three of the many important themes which have served to provide the framework of much of our current understanding of critical phenomena. the style is that of a colloquium, not that of a mini-review article.
1186	787025	article	qualitative research	\N	\N	\N	16	6	1	2006	\N	2006-08-05 23:01:34	\N	{integrating quantitative and qualitative research: how is it done?}	this article seeks to move beyond typologies of the ways in which quantitative and qualitative research are integrated to an examination of the ways that they are combined in practice. the article is based on a content analysis of 232 social science articles in which the two were combined. an examination of the research methods and research designs employed suggests that on the quantitative side structured interview and questionnaire research within a cross-sectional design tends to predominate, while on the qualitative side the semi-structured interview within a cross-sectional design tends to predominate. an examination of the rationales that are given for employing a mixed-methods research approach and the ways it is used in practice indicates that the two do not always correspond. the implications of this finding for how we think about mixed-methods research are outlined.
1187	789655	article	bmc bioinformatics	\N	\N	\N	\N	7	1	2006	aug	2006-08-10 19:59:05	\N	automatic document classification of biological literature	{background}:document classification is a wide-spread problem with many applications, from organizing search engine snippets to spam filtering. we previously described textpresso, a text-mining system for biological literature, which marks up full text according to a shallow ontology that includes terms of biological interest. this project investigates document classification in the context of biological literature, making use of the textpresso markup of a corpus of caenorhabditis elegans {literature.results}:we present a two-step text categorization algorithm to classify a corpus of c. elegans papers. our classification method first uses a support vector machine-trained classifier, followed by a novel, phrase-based clustering algorithm. this clustering step autonomously creates cluster labels that are descriptive and understandable by humans. this clustering engine performed better on a standard test-set (reuters 21578) compared to previously published results (f-value of 0.55 vs. 0.49), while producing cluster descriptions that appear more useful. a web interface allows researchers to quickly navigate through the hierarchy and look for documents that belong to a specific {concept.conclusion}:we have demonstrated a simple method to classify biological documents that embodies an improvement over current methods. while the classification results are currently optimized for caenorhabditis elegans papers by human-created rules, the classification engine can be adapted to different types of documents. we have demonstrated this by presenting a web interface that allows researchers to quickly navigate through the hierarchy and look for documents that belong to a specific concept.
1188	791438	article	proceedings of the national academy of sciences	\N	\N	national academy of sciences	5	103	32	2006	aug	2006-08-09 21:03:53	josephine bay paul center, marine biological laboratory at woods hole, 7 mbl street, woods hole, ma 02543.	microbial diversity in the deep sea and the underexplored  ” rare biosphere”	the evolution of marine microbes over billions of years predicts that the composition of microbial communities should be much greater than the published estimates of a few thousand distinct kinds of microbes per liter of seawater. by adopting a massively parallel tag sequencing strategy, we show that bacterial communities of deep water masses of the north atlantic and diffuse flow hydrothermal vents are one to two orders of magnitude more complex than previously reported for any microbial environment. a relatively small number of different populations dominate all samples, but thousands of low-abundance populations account for most of the observed phylogenetic diversity. this  ” rare biosphere” is very ancient and may represent a nearly inexhaustible source of genomic innovation. members of the rare biosphere are highly divergent from each other and, at different times in earth's history, may have had a profound impact on shaping planetary processes.
1189	796286	article	curr opin chem biol	\N	\N	\N	5	5	1	2001	feb	2006-08-10 17:39:32	department of biochemistry and molecular biophysics, howard hughes medical institute, columbia university, 630 west 168th street, new york, ny 10032, usa.	protein structure prediction.	the prediction of protein structure, based primarily on sequence and structure homology, has become an increasingly important activity. homology models have become more accurate and their range of applicability has increased. progress has come, in part, from the flood of sequence and structure information that has appeared over the past few years, and also from improvements in analysis tools. these include profile methods for sequence searches, the use of three-dimensional structure information in sequence alignment and new homology modeling tools, specifically in the prediction of loop and side-chain conformations. there have also been important advances in understanding the physical chemical basis of protein stability and the corresponding use of physical chemical potential functions to identify correctly folded from incorrectly folded protein conformations.
1190	797020	article	science	\N	\N	american association for the advancement of science	3	312	5781	2006	jun	2008-01-25 17:50:20	school of physics and astronomy, university of st andrews, north haugh, st andrews ky16 9ss, scotland. ulf@st-andrews.ac.uk	optical conformal mapping	an invisibility device should guide light around an object as if nothing were there, regardless of where the light comes from. ideal invisibility devices are impossible, owing to the wave nature of light. this study develops a general recipe for the design of media that create perfect invisibility within the accuracy of geometrical optics. the imperfections of invisibility can be made arbitrarily small to hide objects that are much larger than the wavelength. with the use of modern metamaterials, practical demonstrations of such devices may be possible. the method developed here can also be applied to escape detection by other electromagnetic waves or sound.
1191	797379	article	genome research	\N	\N	\N	10	16	6	2006	jun	2006-08-11 20:02:27	department of human genetics, university of chicago, chicago, illinois 60637, usa. kteshima@uchicago.edu	how reliable are empirical genomic scans for selective sweeps?	10.1101/gr.5105206 the beneficial substitution of an allele shapes patterns of genetic variation at linked sites. thus, in principle, adaptations can be mapped by looking for the signature of directional selection in polymorphism data. in practice, such efforts are hampered by the need for an accurate characterization of the demographic history of the species and of the effects of positive selection. in an attempt to circumvent these difficulties, researchers are increasingly taking a purely empirical approach, in which a large number of genomic regions are ordered by summaries of the polymorphism data, and loci with extreme values are considered to be likely targets of positive selection. we evaluated the reliability of the  ” empirical” approach, focusing on applications to human data and to maize. to do so, we considered a coalescent model of directional selection in a sensible demographic setting, allowing for selection on standing variation as well as on a new mutation. our simulations suggest that while empirical approaches will identify several interesting candidates, they will also miss many—in some cases, most—loci of interest. the extent of the trade-off depends on the mode of positive selection and the demographic history of the population. specifically, the false-discovery rate is higher when directional selection involves a recessive rather than a co-dominant allele, when it acts on a previously neutral rather than a new allele, and when the population has experienced a population bottleneck rather than maintained a constant size. one implication of these results is that, insofar as attributes of the beneficial mutation (e.g., the dominance coefficient) affect the power to detect targets of selection, genomic scans will yield an unrepresentative subset of loci that contribute to adaptations.
1192	800904	inproceedings	\N	cvpr	\N	ieee computer society	9	1	\N	2006	jun	2006-08-14 15:34:39	washington, dc, usa	a comparison and evaluation of {multi-view} stereo reconstruction algorithms	this paper presents a quantitative comparison of several multi-view stereo reconstruction algorithms. until now, the lack of suitable calibrated multi-view image datasets with known ground truth ({3d} shape models) has prevented such direct comparisons. in this paper, we first survey multi-view stereo algorithms and compare them qualitatively using a taxonomy that differentiates their key properties. we then describe our process for acquiring and calibrating multiview image datasets with high-accuracy ground truth and introduce our evaluation methodology. finally, we present the results of our quantitative comparison of state-of-the-art multi-view stereo reconstruction algorithms on six benchmark datasets. the datasets, evaluation details, and instructions for submitting new models are available online at http://vision.middlebury.edu/mview.
1193	801335	inproceedings	\N	proceedings of 18th international conference on machine learning (icml-01)	\N	\N	7	\N	\N	2001	\N	2006-08-14 20:35:16	\N	constrained k-means clustering with background knowledge	clustering is traditionally viewed as an unsupervised method for data analysis. however, in some cases information about the problem domain is available in addition to the data instances themselves. in this paper, we demonstrate how the popular k-means clustering algorithm can be pro tably modi- ed to make use of this information. in experiments with arti cial constraints on six data sets, we observe improvements in clustering accuracy. we also apply this method to the real-world problem of...
1194	801860	article	nature	\N	\N	\N	4	431	7006	2004	sep	2006-08-15 11:48:14	\N	revealing the world of {rna} interference.	the recent discoveries of {rna} interference and related {rna} silencing pathways have revolutionized our understanding of gene regulation. {rna} interference has been used as a research tool to control the expression of specific genes in numerous experimental organisms and has potential as a therapeutic strategy to reduce the expression of problem genes. at the heart of {rna} interference lies a remarkable {rna} processing mechanism that is now known to underlie many distinct biological phenomena.
1195	802360	inproceedings	autonomic computing, 2006. icac '06. ieee international conference on	\N	\N	\N	9	\N	\N	2006	\N	2006-08-15 20:33:22	\N	autonomic live adaptation of virtual computational environments in a {multi-domain} infrastructure	a shared distributed infrastructure is formed by federating computation resources from multiple domains. such shared infrastructures are increasing in popularity and are providing massive amounts of aggregated computation resources to large numbers of users. meanwhile, virtualization technologies, at machine and network levels, are maturing and enabling mutually isolated virtual computation environments for executing arbitrary parallel/distributed applications on top of such a shared physical infrastructure. in this paper; we go one step further by supporting autonomic adaptation of virtual computation environments as active, integrated entities. more specifically, driven by both dynamic availability of infrastructure resources and dynamic application resource demand, a virtual computation environment is able to automatically relocate itself across the infrastructure and scale its share of infrastructural resources. such autonomic adaptation is transparent to both users of virtual environments and administrators of infrastructures, maintaining the look and feel of a stable, dedicated environment for the user as our proof-of-concept, we present the design, implementation and evaluation of a system called {violin}, which is composed of a virtual network of virtual machines capable of live migration across a multi-domain physical infrastructure.
1196	804369	article	ethnographic praxis in industry conference proceedings	\N	\N	\N	6	2005	1	2005	\N	2006-08-17 18:33:19	\N	accelerating collaboration with social tools	as more and more corporate ethnographic work is crossing international borders, we are increasingly collaborating with teams that are spread across the globe. as a result, we need tools that enable us to work across boundaries. since early 2004, the authors have been collaborating on a research project developed by an american company seeking to develop solutions specific to the indian market. one of us, an indian sociologist, led a team of ethnographers in india, while the other, an american anthropologist, managed research and analysis for concept development in the {us}. while all of the {us} -based team members spent time in the field in india during the project, integrating the teams into the same "brainspace" was a challenge. this paper describes how we used social tools to enable each set of team members to understand the work being done on the other side of the world.
1197	808558	article	cell	\N	\N	\N	7	124	4	2006	feb	2006-08-20 00:57:07	\N	the evolution of adaptive immune systems	a clonally diverse anticipatory repertoire in which each lymphocyte bears a unique antigen receptor is the central feature of the adaptive immune system that evolved in our vertebrate ancestors. the survival advantage gained through adding this type of adaptive immune system to a pre-existing innate immune system led to the evolution of alternative ways for lymphocytes to generate diverse antigen receptors for use in recognizing and repelling pathogen invaders. all jawed vertebrates assemble their antigen-receptor genes through recombinatorial rearrangement of different immunoglobulin or t cell receptor gene segments. the surviving jawless vertebrates, lampreys and hagfish, instead solved the receptor diversification problem by the recombinatorial assembly of leucine-rich-repeat genetic modules to encode variable lymphocyte receptors. the convergent evolution of these remarkably different adaptive immune systems involved innovative genetic modification of innate-immune-system components.
1198	808574	electronic	\N	\N	\N	\N	\N	\N	\N	2002	dec	2006-08-20 04:35:55	\N	discrete mathematics: methods and challenges	combinatorics is a fundamental mathematical discipline as well as an essential component of many mathematical areas, and its study has experienced an impressive growth in recent years. one of the main reasons for this growth is the tight connection between discrete mathematics and theoretical computer science, and the rapid development of the latter. while in the past many of the basic combinatorial results were obtained mainly by ingenuity and detailed reasoning, the modern theory has grown out of this early stage, and often relies on deep, well developed tools. this is a survey of two of the main general techniques that played a crucial role in the development of modern combinatorics; algebraic methods and probabilistic methods. both will be illustrated by examples, focusing on the basic ideas and the connection to other areas.
1199	812862	article	rna	\N	\N	\N	7	12	9	2006	sep	2006-08-22 18:26:26	1center for bioinformatics and department of genetics, school of medicine, university of pennsylvania, philadelphia, pennsylvania 19104, usa.	{microrna} promoter element discovery in arabidopsis	10.1261/rna.130506 in this study we present a method of identifying  {mirna} promoter elements using known transcription factor binding motifs. we provide a comparative analysis of the representation of these elements in {mirna} promoters, protein-coding gene promoters, and random genomic sequences. we report five transcription factor ({tf}) binding motifs that show evidence of overrepresentation in {mirna} promoter regions relative to the promoter regions of protein-coding genes. this investigation is based on the analysis of 800-nucleotide regions upstream of 63 experimentally verified transcription start sites ({tss}) for {mirna} primary transcripts in . while the {tata}-box binding motif was also previously reported by xie and colleagues, the transcription factors {atmyc2}, {arf}, {sorlrep3}, and {lfy} are identified for the first time as overrepresented binding motifs in {mirna} promoters.
1200	814224	article	computer	\N	\N	\N	\N	29	3	1996	\N	2006-08-23 21:51:26	\N	artificial neural networks: a tutorial	numerous advances have been made in developing intelligent programs, some inspired by biological neural networks. researchers from many scientific disciplines are designing artificial neural networks ({anns}) to solve a variety of problems in pattern recognition, prediction, optimization, associative memory; and control. although successful conventional applications can be found in certain well-constrained environments, none is flexible enough to perform well outside its domain. {anns} provide exciting alternatives, and many applications could benefit from using them. this article is for those readers with little or no knowledge of {anns} to help them understand the other articles in this issue of computer. it discusses the motivation behind the development of {anns}; describes the basic biological neuron and the artificial computation model; outlines network architectures and learning processes; and presents multilayer feed-forward networks, kohonen's self-organizing maps, carpenter and grossberg's adaptive resonance theory models, and the hopfield network. it concludes with character recognition, a successful {ann} application.
1201	816093	article	ieee trans med imaging	\N	\N	\N	10	21	9	2002	sep	2006-08-24 23:08:44	laboratory of biomedical engineering, helsinki university of technology, p.o. box 2200, fin-02015 hut, finland. timo.makela@hut.fi	a review of cardiac image registration methods.	in this paper, the current status of cardiac image registration methods is reviewed. the combination of information from multiple cardiac image modalities, such as magnetic resonance imaging, computed tomography, positron emission tomography, single-photon emission computed tomography, and ultrasound, is of increasing interest in the medical community for physiologic understanding and diagnostic purposes. registration of cardiac images is a more complex problem than brain image registration because the heart is a nonrigid moving organ inside a moving body. moreover, as compared to the registration of brain images, the heart exhibits much fewer accurate anatomical landmarks. in a clinical context, physicians often mentally integrate image information from different modalities. automatic registration, based on computer programs, might, however, offer better accuracy and repeatability and save time.
1202	816513	inproceedings	\N	proceedings of the 7th international conference on learning sciences	\N	\N	6	\N	\N	2006	\N	2006-08-25 12:11:11	\N	from wikipedia to the classroom: exploring online publication and learning	wikipedia represents an intriguing new publishing paradigm---can it be used to engage students in authentic collaborative writing activities? how can we design wiki publishing tools and curricula to support learning among student authors? we suggest that wiki publishing environments can create learning opportunities that address four dimensions of authenticity: personal, real world, disciplinary, and assessment. we have begun a series of design studies to investigate links between wiki publishing experiences and writing-to-learn. the results of an initial study in an undergraduate government course indicate that perceived audience plays an important role in helping students monitor the quality of writing; however, students' perception of audience on the internet is not straightforward. this preliminary iteration resulted in several guidelines that are shaping efforts to design and implement new wiki publishing tools and curricula for students and teachers.
1203	816965	article	bmc bioinformatics	\N	\N	\N	\N	3	\N	2002	oct	2006-08-25 22:13:10	center for studies in physics and biology, the rockefeller university, 1230 york avenue, new york, ny, usa. nr@edsb.rockefeller.edu	computational detection of genomic cis-regulatory modules applied to body patterning in the early drosophila embryo.	regulation of gene transcription is crucial for the function and development of all organisms. while gene prediction programs that identify protein coding sequence are used with remarkable success in the annotation of genomes, the development of computational methods to analyze noncoding regions and to delineate transcriptional control elements is still in its infancy. here we present novel algorithms to detect cis-regulatory modules through genome wide scans for clusters of transcription factor binding sites using three levels of prior information. when binding sites for the factors are known, our statistical segmentation algorithm, ahab, yields about 150 putative gap gene regulated modules, with no adjustable parameters other than a window size. if one or more related modules are known, but no binding sites, repeated motifs can be found by a customized gibbs sampler and input to ahab, to predict genes with similar regulation. finally using only the genome, we developed a third algorithm, argos, that counts and scores clusters of overrepresented motifs in a window of sequence. argos recovers many of the known modules, upstream of the segmentation genes, with no training data. we have demonstrated, in the case of body patterning in the drosophila embryo, that our algorithms allow the genome-wide identification of regulatory modules. we believe that ahab overcomes many problems of recent approaches and we estimated the false positive rate to be about 50\\%. argos is the first successful attempt to predict regulatory modules using only the genome without training data. complete results and module predictions across the drosophila genome are available at http://uqbar.rockefeller.edu/\\~{}siggia/.
1204	820132	inproceedings	\N	www	\N	acm press	6	\N	\N	2003	\N	2006-08-28 20:40:06	new york, ny, usa	mining newsgroups using networks arising from social behavior	recent advances in information retrieval over hyperlinked corpora have convincingly demonstrated that links carry less noisy information than text. we investigate the feasibility of applying link-based methods in new applications domains. the specific application we consider is to partition authors into opposite camps within a given topic in the context of newsgroups. a typical newsgroup posting consists of one or more quoted lines from another posting followed by the opinion of the author. this social behavior gives rise to a network in which the vertices are individuals and the links represent "responded-to" relationships. an interesting characteristic of many newsgroups is that people more frequently respond to a message when they disagree than when they agree. this behavior is in sharp contrast to the www link graph, where linkage is an indicator of agreement or common interest. by analyzing the graph structure of the responses, we are able to effectively classify people into opposite camps. in contrast, methods based on statistical analysis of text yield low accuracy on such datasets because the vocabulary used by the two sides tends to be largely identical, and many newsgroup postings consist of relatively few words of text.
1205	820297	article	pattern recognition letters	roc analysis in pattern recognition	\N	elsevier science inc.	13	27	8	2006	jun	2006-08-29 02:24:46	new york, ny, usa	an introduction to {roc} analysis	receiver operating characteristics ({roc}) graphs are useful for organizing classifiers and visualizing their performance. {roc} graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. although {roc} graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. the purpose of this article is to serve as an introduction to {roc} graphs and as a guide for using them in research.
1206	820473	electronic	\N	\N	\N	\N	\N	\N	\N	2006	aug	2009-01-26 12:16:41	\N	granger causality: basic theory and application to neuroscience	multi-electrode neurophysiological recordings produce massive quantities of data. multivariate time series analysis provides the basic framework for analyzing the patterns of neural interactions in these data. it has long been recognized that neural interactions are directional. being able to assess the directionality of neuronal interactions is thus a highly desired capability for understanding the cooperative nature of neural computation. research over the last few years has shown that granger causality is a key technique to furnish this capability. the main goal of this article is to provide an expository introduction to the concept of granger causality. mathematical frameworks for both bivariate granger causality and conditional granger causality are developed in detail with particular emphasis on their spectral representations. the technique is demonstrated in numerical examples where the exact answers of causal influences are known. it is then applied to analyze multichannel local field potentials recorded from monkeys performing a visuomotor task. our results are shown to be physiologically interpretable and yield new insights into the dynamical organization of large-scale oscillatory cortical networks.
1207	820512	article	pac {s}ymp {b}iocomput	\N	\N	\N	11	\N	\N	1999	\N	2006-08-29 10:15:30	\N	{{i}dentification of genetic networks from a small number of gene expression patterns under the {b}oolean network model}	{{l}iang, {f}uhrman and {s}omogyi ({psb}98, 18-29, 1998) have described an algorithm for inferring genetic network architectures from state transition tables which correspond to time series of gene expression patterns, using the {b}oolean network model. {t}heir results of computational experiments suggested that a small number of state transition ({input}/{output}) pairs are sufficient in order to infer the original {b}oolean network correctly. {t}his paper gives a mathematical proof for their observation. {p}recisely, this paper devises a much simpler algorithm for the same problem and proves that, if the indegree of each node (i.e., the number of input nodes to each node) is bounded by a constant, only {o}(log n) state transition pairs (from 2n pairs) are necessary and sufficient to identify the original {b}oolean network of n nodes correctly with high probability. {w}e made computational experiments in order to expose the constant factor involved in {o}(log n) notation. {t}he computational results show that the {b}oolean network of size 100,000 can be identified by our algorithm from about 100 {input}/{output} pairs if the maximum indegree is bounded by 2. {i}t is also a merit of our algorithm that the algorithm is conceptually so simple that it is extensible for more realistic network models.}
1208	820838	article	genome {r}es	\N	\N	\N	-687	10	6	2000	\N	2006-08-29 10:16:25	\N	conservation of {dna} regulatory motifs and discovery of new motifs in microbial genomes	regulatory motifs can be found by local multiple alignment of upstream regions from coregulated sets of genes, or regulons. {w}e searched for regulatory motifs using the program {a}lign{ace} together with a set of filters that helped us choose the motifs most likely to be biologically relevant in 17 complete microbial genomes. {w}e searched the upstream regions of potentially coregulated genes grouped by three methods: (1) genes that make up functional pathways; (2) genes homologous to regulons from a well-studied species ({e}scherichia coli); and (3) groups of genes derived from conserved operons. {t}his last group is based on the observation that genes making up homologous regulons in different species are often assorted into coregulated operons in different combinations. {t}his allows partial reconstruction of regulons by looking at operon structure across several species. {u}nlike other methods for predicting regulons, this method does not depend on the availability of experimental data other than the genome sequence and the locations of genes. {n}ew, statistically significant motifs were found in the genome sequence of each organism using each grouping method. {t}he most significant new motif was found upstream of genes in the methane-metabolism functional group in {m}ethanobacterium thermoautotrophicum. {w}e found that at least 27\\% of the known {e}. coli {dna}-regulatory motifs are conserved in one or more distantly related eubacteria. {w}e also observed significant motifs that differed from the {e}. coli motif in other organisms upstream of sets of genes homologous to known {e}. coli regulons, including {c}rp, {l}ex{a}, and {a}rc{a} in {b}acillus subtilis; four anaerobic regulons in {a}rchaeoglobus fulgidus ({n}ar{l}, {n}ar{p}, {f}nr, and {m}od{e}); and the {p}ho{b}, {p}ur{r}, {r}po{h}, and {f}hl{a} regulons in other archaebacterial species. {w}e also used motif conservation to aid in finding new motifs by grouping upstream regions from closely related bacteria, thus increasing the number of instances of the motif in the sequence to be aligned. {f}or example, by grouping upstream sequences from three archaebacterial species, we found a conserved motif that may regulate ferrous ion transport that was not found in individual genomes. {d}iscovery of conserved motifs becomes easier as the number of closely related genome sequences increases.
1209	821236	article	journal of computational biology : a journal of computational molecular cell biology	\N	\N	\N	17	13	2	2006	mar	2006-08-29 17:50:38	department of computer sciences, purdue university, west lafayette, in 47907, usa. koyuturk@cs.purdue.edu	pairwise alignment of protein interaction networks.	with an ever-increasing amount of available data on protein-protein interaction ({ppi}) networks and research revealing that these networks evolve at a modular level, discovery of conserved patterns in these networks becomes an important problem. although available data on protein-protein interactions is currently limited, recently developed algorithms have been shown to convey novel biological insights through employment of elegant mathematical models. the main challenge in aligning {ppi} networks is to define a graph theoretical measure of similarity between graph structures that captures underlying biological phenomena accurately. in this respect, modeling of conservation and divergence of interactions, as well as the interpretation of resulting alignments, are important design parameters. in this paper, we develop a framework for comprehensive alignment of {ppi} networks, which is inspired by duplication/divergence models that focus on understanding the evolution of protein interactions. we propose a mathematical model that extends the concepts of match, mismatch, and gap in sequence alignment to that of match, mismatch, and duplication in network alignment and evaluates similarity between graph structures through a scoring function that accounts for evolutionary events. by relying on evolutionary models, the proposed framework facilitates interpretation of resulting alignments in terms of not only conservation but also divergence of modularity in {ppi} networks. furthermore, as in the case of sequence alignment, our model allows flexibility in adjusting parameters to quantify underlying evolutionary relationships. based on the proposed model, we formulate {ppi} network alignment as an optimization problem and present fast algorithms to solve this problem. detailed experimental results from an implementation of the proposed framework show that our algorithm is able to discover conserved interaction patterns very effectively, in terms of both accuracies and computational cost.
1210	821916	article	plos comput biol	\N	\N	\N	\N	2	8	2006	aug	2006-08-30 11:49:45	\N	conservation of expression and sequence of metabolic genes is reflected by activity across metabolic states.	variation in gene expression levels on a genomic scale has been detected among different strains, among closely related species, and within populations of genetically identical cells. what are the driving forces that lead to expression divergence in some genes and conserved expression in others? here we employ flux balance analysis to address this question for metabolic genes. we consider the genome-scale metabolic model of saccharomyces cerevisiae, and its entire space of optimal and near-optimal flux distributions. we show that this space reveals underlying evolutionary constraints on expression regulation, as well as on the conservation of the underlying gene sequences. genes that have a high range of optimal flux levels tend to display divergent expression levels among different yeast strains and species. this suggests that gene regulation has diverged in those parts of the metabolic network that are less constrained. in addition, we show that genes that are active in a large fraction of the space of optimal solutions tend to have conserved sequences. this supports the possibility that there is less selective pressure to maintain genes that are relevant for only a small number of metabolic states.
1211	822067	book	\N	\N	\N	london mathematical society	\N	\N	\N	1976	\N	2006-08-30 14:42:18	\N	on numbers and games	{onag, as the book is known, is one of those rare publications that sprang to life in a  moment of creative energy and has remained influential for over a quarter of a century. still in high  demand, it is being republished with some adjustments and corrections. the original motivation for  writing the book was an attempt to understand the relation between the theories of transfinite numbers and  mathematical games. by defining numbers as the strengths of positions in certain games, the author  arrives at a new class, the surreal numbers (so named by donald knuth) that includes at the same time the  real numbers and the ordinal numbers.   <p>this new edition ends with an epilogue that sets the stage for further research on surreal numbers. the  book is a must-have for all readers with a serious interest in the mathematical foundations of game  strategies.}
1212	822218	article	nucleic acids research	\N	\N	\N	5	34	Web Server issue	2006	jul	2006-08-30 16:38:48	lawrence berkeley national laboratory, center for environmental biotechnology, berkeley, ca, usa.	{nast}: a multiple sequence alignment server for comparative analysis of {16s} {rrna} genes.	microbiologists conducting surveys of bacterial and archaeal diversity often require comparative alignments of thousands of 16s rrna genes collected from a sample. the computational resources and bioinformatics expertise required to construct such an alignment has inhibited high-throughput analysis. it was hypothesized that an online tool could be developed to efficiently align thousands of 16s rrna genes via the nast (nearest alignment space termination) algorithm for creating multiple sequence alignments (msa). the tool was implemented with a web-interface at http://greengenes.lbl.gov/nast. each user-submitted sequence is compared with greengenes' 'core set', comprising approximately 10,000 aligned non-chimeric sequences representative of the currently recognized diversity among bacteria and archaea. user sequences are oriented and paired with their closest match in the core set to serve as a template for inserting gap characters. non-16s data (sequence from vector or surrounding genomic regions) are conveniently removed in the returned alignment. from the resulting msa, distance matrices can be calculated for diversity estimates and organisms can be classified by taxonomy. the ability to align and categorize large sequence sets using a simple interface has enabled researchers with various experience levels to obtain bacterial and archaeal community profiles.
1213	828031	article	bmc bioinformatics	\N	\N	\N	\N	7	1	2006	sep	2006-09-07 20:44:24	\N	efficient pairwise {rna} structure prediction and alignment using sequence alignment constraints	{background}:we are interested in the problem of predicting secondary structure for small sets of homologous {rnas}, by incorporating limited comparative sequence information into an {rna} folding model. the sankoff algorithm for simultaneous {rna} folding and alignment is a basis for approaches to this problem. there are two open problems in applying a sankoff algorithm: development of a good unified scoring system for alignment and folding and development of practical heuristics for dealing with the computational complexity of the {algorithm.results}:we use probabilistic models (pair stochastic context-free grammars, {pairscfgs}) as a unifying framework for scoring pairwise alignment and folding. a constrained version of the {pairscfg} structural alignment algorithm was developed which assumes knowledge of a few confidently aligned positions (pins). these pins are selected based on the posterior probabilities of a probabilistic pairwise sequence {alignment.conclusion}:pairwise {rna} structural alignment improves on structure prediction accuracy relative to single sequence folding. constraining on alignment is a straightforward method of reducing the runtime and memory requirements of the algorithm. five practical implementations of the pairwise sankoff algorithm - this work (consan), david mathews' dynalign, ian holmes' stemloc, ivo hofacker's {pmcomp}, and jan gorodkin's {foldalign} - have comparable overall performance with different strengths and weaknesses.
1214	832827	inproceedings	\N	cikm	\N	acm press	7	\N	\N	2001	\N	2007-10-22 00:50:35	new york, ny, usa	evaluation of {item-based} {top-n} recommendation algorithms	the explosive growth of the world-wide-web and the emergence of e-commerce has led to the development of recommender systems---a personalized information filtering technology used to identify a set of n items that will be of interest to a certain user. user-based collaborative filtering is the most successful technology for building recommender systems to date, and is extensively used in many commercial recommender systems. unfortunately, the computational complexity of these methods grows...
1215	834982	article	online information review	\N	\N	emerald group publishing limited	6	30	4	2006	\N	2006-09-14 08:21:09	\N	public library 2.0: towards a new mission for public libraries as a ?network of community knowledge?	this article seeks to propose a new vision for public libraries in the digital age. this conceptual paper is based on an understanding of the recent developments in ict, internet and digital libraries; and also on the authors' personal experience in research and development in library and information science - especially in relation to public libraries - and digital libraries. the study argues that currently there are no proper mechanisms for capturing, preserving and disseminating community knowledge, and proposes that public libraries in the digital age should take a new role whereby they should act not only as a gateway to knowledge, but also as a platform facilitating the creation of, and access to, local community knowledge. the paper proposes a model for pl2.0 where public libraries can take on this new role to build a network of community knowledge. it also proposes a conceptual model for the second generation of public libraries, and further studies are required to test and implement the model. the paper proposes that the new role of public libraries will be to shift from solely providing access to knowledge to acting as a platform for the storage and dissemination of local community knowledge within the global context created by twenty-first century digital technologies.
1216	837815	article	jour. neurosci.	\N	\N	\N	24	58	\N	1987	\N	2006-09-09 23:26:54	\N	the {two-dimensional} spatial structure of simple receptive fields in cat striate cortex	1. a reverse correlation (6, 8, 25, 35) method is developed that allows quantitative determination of visual receptive-field structure in two spatial dimensions. this method is applied to simple cells in the cat striate cortex. 2. it is demonstrated that the reverse correlation method yields results with several desirable properties, including convergence and reproducibility independent of modest changes in stimulus parameters. 3. in contrast to results obtained with moving stimuli, we find that the bright and dark excitatory subregions in simple receptive fields do not overlap to any great extent. this difference in results may be attributed to confounding the independent variables space and time when using moving stimuli. 4. all simple receptive fields have subregions that vary smoothly in all directions in space. there are no sharp transitions either between excitatory subregions or between subregions and the area surrounding the receptive field. 5. simple receptive fields vary both in the number of subregions observed, in the elongation of each subregion, and in the overall elongation of the field. in contrast with results obtained using moving stimuli, we find that subregions within a given receptive field need not be the same length. 6. the hypothesis that simple receptive fields can be modeled as either even symmetric or odd symmetric about a central axis is evaluated. this hypothesis is found to be false in general. most simple receptive fields are neither even symmetric nor odd symmetric. 7. the hypothesis that simple receptive fields can be modeled as the product of a width response profile and an orthogonal length response profile (cartesian separability) is evaluated. this hypothesis is found to be true for only approximately 50% of the cells in our sample.
1217	841692	inproceedings	\N	proceedings of the 24th annual conference on computer graphics and interactive techniques	siggraph	acm press/addison-wesley publishing co.	9	\N	\N	1997	\N	2006-09-13 09:36:43	new york, ny, usa	recovering high dynamic range radiance maps from photographs	an abstract is not available.
1218	841946	article	nature	\N	\N	nature publishing group	3	411	6839	2001	jun	2006-09-13 16:26:35	\N	jamming phase diagram for attractive particles	a wide variety of systems, including granular media, colloidal suspensions and molecular systems, exhibit non-equilibrium transitions from a fluid-like to a solid-like state, characterized solely by the sudden arrest of their dynamics. crowding or jamming of the constituent particles traps them kinetically, precluding further exploration of the phase space1. the disordered fluid-like structure remains essentially unchanged at the transition. the jammed solid can be refluidized by thermalization, through temperature or vibration, or by an applied stress. the generality of the jamming transition led to the proposal2 of a unifying description, based on a jamming phase diagram. it was further postulated that attractive interactions might have the same effect in jamming the system as a confining pressure, and thus could be incorporated into the generalized description. here we study experimentally the fluid-to-solid transition of weakly attractive colloidal particles, which undergo markedly similar gelation behaviour with increasing concentration and decreasing thermalization or stress. our results support the concept of a jamming phase diagram for attractive colloidal particles, providing a unifying link between the glass transition3, gelation4, 5 and aggregation6, 7, 8.
1219	848440	article	ecosystems	\N	\N	\N	5	1	5	1998	\N	2006-09-18 06:52:29	\N	ecosystems and the biosphere as complex adaptive systems	ecosystems are prototypical examples of complex adaptive systems, in which patterns at higher levels emerge from localized interactions and selection processes acting at lower levels. an essential aspect of such systems is nonlinearity, leading to historical dependency and multiple possible outcomes of dynamics. given this, it is essential to determine the degree to which system features are determined by environmental conditions, and the degree to which they are the result of self-organization. furthermore, given the multiple levels at which dynamics become apparent and at which selection can act, central issues relate to how evolution shapes ecosystems properties, and whether ecosystems become buffered to changes (more resilient) over their ecological and evolutionary development or proceed to critical states and the edge of chaos.
1220	848639	article	plos genet	\N	\N	public library of science	\N	2	3	2006	mar	2006-09-18 14:53:49	max-planck-institut f\\"{u}r informatik, saarbr\\"{u}cken, germany. cbock@mpi-inf.mpg.de	{cpg} island methylation in human lymphocytes is highly correlated with {dna} sequence, repeats, and predicted {dna} structure	{cpg} island methylation plays an important role in epigenetic gene control during mammalian development and is frequently altered in disease situations such as cancer. the majority of {cpg} islands is normally unmethylated, but a sizeable fraction is prone to become methylated in various cell types and pathological situations. the goal of this study is to show that a computational epigenetics approach can discriminate between {cpg} islands that are prone to methylation from those that remain unmethylated. we develop a bioinformatics scoring and prediction method on the basis of a set of 1,184 {dna} attributes, which refer to sequence, repeats, predicted structure, {cpg} islands, genes, predicted binding sites, conservation, and single nucleotide polymorphisms. these attributes are scored on 132 {cpg} islands across the entire human chromosome 21, whose methylation status was previously established for normal human lymphocytes. our results show that three groups of {dna} attributes, namely certain sequence patterns, specific {dna} repeats, and a particular {dna} structure, are each highly correlated with {cpg} island methylation (correlation coefficients of 0.64, 0.66, and 0.49, respectively). we predicted, and subsequently experimentally examined 12 {cpg} islands from human chromosome 21 with unknown methylation patterns and found more than 90\\% of our predictions to be correct. in addition, we applied our prediction method to analyzing human epigenome project methylation data on human chromosome 6 and again observed high prediction accuracy. in summary, our results suggest that {dna} composition of {cpg} islands (sequence, repeats, and structure) plays a significant role in predisposing {cpg} islands for {dna} methylation. this finding may have a strong impact on our understanding of changes in {cpg} island methylation in development and disease. {dna} methylation is the only epigenetic mechanism in eukaryotes that is known to directly modify the {dna}. it plays an important role for gene control during development and cell differentiation, and it is a promising therapeutic target in cancer research. while a genome-wide picture of {dna} methylation patterns is currently emerging, we have only fragmentary knowledge about the linkage between {dna} methylation and other genomic attributes such as {dna} sequence and structure, repetitive elements, or sequence conservation. the authors fill this gap by reporting on a comprehensive bioinformatical analysis of {dna} methylation on human chromosome 21—and in part, extending to other regions of the human genome. they report new associations that will help elucidate the functions of {dna} methylation along the human genome. furthermore, the authors show that their findings can be applied to predicting {dna} methylation patterns from genome sequence. such predictions have the potential of speeding up genome-wide epigenetic profiling: it may be possible to focus experimental resources on a few selected areas while bioinformatics procedures are applied to the bulk of the genome.
1221	850491	article	journal of {m}emory and {l}anguage	\N	\N	\N	\N	\N	\N	2000	\N	2006-09-20 03:18:45	\N	symbol grounding and meaning: {a} comparison of high-dimensional and embodied theories of meaning.	latent semantic analysis (landauer & dumais, 1997) and hyperspace analogue to language (burgess & lund, 1997) model meaning as the relations among abstract symbols that are arbitrarily related to what they signify. these symbols are ungrounded in that they are not tied to perceptual experience or action. because the symbols are ungrounded, they cannot, in principle, capture the meaning of novel situations. in contrast, participants in three experiments found it trivially easy to discriminate between descriptions of sensible novel situations (e.g., using a newspaper to protect one's face from the wind) and nonsense novel situations (e.g., using a matchbook to protect one's face from the wind). these results support the indexical hypothesis that the meaning of a sentence is constructed by (a) indexing words and phrases to real objects or perceptual, analog symbols; (b) deriving affordances from the objects and symbols; and (c) meshing the affordances under the guidance of syntax. copyright 2000 academic press.
1222	867069	article	commun. acm	\N	\N	acm	7	45	9	2002	sep	2006-09-24 20:13:29	new york, ny, usa	finding the flow in web site search	designing a search system and interface may best be served (and executed) by scrutinizing usability studies.
1223	867481	article	journal of theoretical biology	\N	\N	\N	9	239	2	2006	mar	2006-09-25 07:12:30	atelier de bioinformatique, universit\\'{e} paris vi, 75005 paris, france.	comparisons of {dn}/{ds} are time dependent for closely related bacterial genomes	the ratio of non-synonymous ({dn}) to synonymous ({ds}) changes between taxa is frequently computed to assay the strength and direction of selection. here we note that for comparisons between closely related strains and/or species a second parameter needs to be considered, namely the time since divergence of the two sequences under scrutiny. we demonstrate that a simple time lag model provides a general, parsimonious explanation of the extensive variation in the {dn}/{ds} ratio seen when comparing closely related bacterial genomes. we explore this model through simulation and comparative genomics, and suggest a role for hitch-hiking in the accumulation of non-synonymous mutations. we also note taxon-specific differences in the change of {dn}/{ds} over time, which may indicate variation in selection, or in population genetics parameters such as population size or the rate of recombination. the effect of comparing intra-species polymorphism and inter-species substitution, and the problems associated with these concepts for asexual prokaryotes, are also discussed. we conclude that, because of the critical effect of time since divergence, inter-taxa comparisons are only possible by comparing trajectories of {dn}/{ds} over time and it is not valid to compare taxa on the basis of single time points.
1224	874803	article	trends in neurosciences	\N	\N	\N	6	21	5	1998	may	2006-09-27 06:39:46	\N	language within our grasp	in monkeys, the rostral part of ventral premotor cortex (area f5) contains neurons that discharge, both when the monkey grasps or manipulates objects and when it observes the experimenter making similar actions.  these neurons (mirror neurons) appear to represent a system that matches observed events to similar, internally generated actions, and in this way forms a link between the observer and the actor.  transcranial magnetic stimulation and positron emission tomography ({pet}) experiments suggest that a mirror system for gesture recognition also exists in humans and includes broca's area. we propose here that such an observation/execution matching system provides a necessary bridge from 'doing' to 'communicating', as the link between actor and observer becomes a link between the sender and the receiver of each message.
1225	875656	inproceedings	proceedings of the ieee	\N	\N	\N	12	93	3	2005	mar	2006-09-27 14:16:48	\N	the semantic grid: past, present and future.	grid computing offers significant enhancements to our capabilities for computation, information processing, and collaboration, and has exciting ambitions in many fields of endeavor. we argue that the full richness of the grid vision, with its application in e-science, e-research, or e-business, requires the "semantic grid." the semantic grid is an extension of the current grid in which information and services are given well-defined meaning, better enabling computers and people to work in cooperation. to this end, we outline the requirements of the semantic grid, discuss the state of the art in achieving them, and identify the key research challenges in realizing this vision.
1226	876030	article	nature	\N	\N	nature publishing group	6	428	6984	2004	apr	2006-09-28 07:20:20	department of developmental biology and hhmi, stanford university school of medicine, stanford, california 94305-5329, usa.	genetic and developmental basis of evolutionary pelvic reduction in threespine sticklebacks	hindlimb loss has evolved repeatedly in many different animals by means of molecular mechanisms that are still unknown. to determine the number and type of genetic changes underlying pelvic reduction in natural populations, we carried out genetic crosses between threespine stickleback fish with complete or missing pelvic structures. genome-wide linkage mapping shows that pelvic reduction is controlled by one major and four minor chromosome regions. pitx1 maps to the major chromosome region controlling most of the variation in pelvic size. pelvic-reduced fish show the same left–right asymmetry seen in pitx1 knockout mice, but do not show changes in pitx1 protein sequence. instead, pelvic-reduced sticklebacks show site-specific regulatory changes in pitx1 expression, with reduced or absent expression in pelvic and caudal fin precursors. regulatory mutations in major developmental control genes may provide a mechanism for generating rapid skeletal changes in natural populations, while preserving the essential roles of these genes in other processes.
1227	876469	article	nucleic acids research	\N	\N	oxford university press	4	34	suppl 2	2006	jul	2006-09-28 19:17:46	institute of molecular bioscience, the university of queensland, st lucia, qld 4072, australia. t.bailey@imb.uq.edu.au	{meme}: discovering and analyzing {dna} and protein sequence motifs	{meme} (multiple {em} for motif elicitation) is one of the most widely used tools for searching for novel 'signals' in sets of biological sequences. applications include the discovery of new transcription factor binding sites and protein domains. {meme} works by searching for repeated, ungapped sequence patterns that occur in the {dna} or protein sequences provided by the user. users can perform {meme} searches via the web server hosted by the national biomedical computation resource (http://meme.nbcr.net) and several mirror sites. through the same web server, users can also access the motif alignment and search tool to search sequence databases for matches to motifs encoded in several popular formats. by clicking on buttons in the {meme} output, users can compare the motifs discovered in their input sequences with databases of known motifs, search sequence databases for matches to the motifs and display the motifs in various formats. this article describes the freely accessible web server and its architecture, and discusses ways to use {meme} effectively to find new sequence patterns in biological sequences and analyze their significance.
1228	876610	inproceedings	pac symp biocomp	\N	\N	\N	11	\N	\N	2001	\N	2006-09-28 20:41:48	\N	bioprospector: discovering conserved {dna} motifs in upstream regulatory regions of co-expressed genes.	the development of genome sequencing and dna microarray analysis of gene expression gives rise to the demand for data-mining tools. bioprospector, a c program using a gibbs sampling strategy, examines the upstream region of genes in the same gene expression pattern group and looks for regulatory sequence motifs. bioprospector uses zero to third-order markov background models whose parameters are either given by the user or estimated from a specified sequence file. the significance of each motif found is judged based on a motif score distribution estimated by a monte carlo method. in addition, bioprospector modifies the motif model used in the earlier gibbs samplers to allow for the modeling of gapped motifs and motifs with palindromic patterns. all these modifications greatly improve the performance of the program. although testing and development are still in progress, the program has shown preliminary success in finding the binding motifs for saccharomyces cerevisiae rap1, bacillus subtilis rna polymerase, and escherichia coli crp. we are currently working on combining bioprospector with a clustering program to explore gene expression networks and regulatory mechanisms. for a copy of the program and documentation for unix systems, please contact xliu@smi.stanford.edu.
1229	876801	article	plos genetics	\N	\N	\N	\N	2	7	2006	jul	2007-11-08 15:00:12	\N	possible ancestral structure in human populations	determining the evolutionary relationships between fossil hominid groups such as neanderthals and modern humans has been a question of enduring interest in human evolutionary genetics. here we present a new method for addressing whether archaic human groups contributed to the modern gene pool (called ancient admixture), using the patterns of variation in contemporary human populations. our method improves on previous work by explicitly accounting for recent population history before performing the analyses. using sequence data from the environmental genome project, we find strong evidence for ancient admixture in both a european and a west african population (p ? 10?7), with contributions to the modern gene pool of at least 5\\%. while neanderthals form an obvious archaic source population candidate in europe, there is not yet a clear source population candidate in west africa.
1230	878218	article	visualization and computer graphics, ieee transactions on	\N	\N	\N	22	5	2	1999	\N	2006-09-29 19:11:47	\N	large datasets at a glance: combining textures and colors in scientific visualization	we present a new method for using texture and color to visualize multivariate data elements arranged on an underlying height field. we combine simple texture patterns with perceptually uniform colors to increase the number of attribute values we can display simultaneously. our technique builds multicolored perceptual texture elements (or pexels) to represent each data element. attribute values encoded in an element are used to vary the appearance of its pexel. texture and color patterns that form when the pexels are displayed can be used to rapidly and accurately explore the dataset. our pexels are built by varying three separate texture dimensions: height, density, and regularity. results from computer graphics, computer vision, and human visual psychophysics have identified these dimensions as important for the formation of perceptual texture patterns. the pexels are colored using a selection technique that controls color distance, linear separation, and color category. proper use of these criteria guarantees colors that are equally distinguishable from one another. we describe a set of controlled experiments that demonstrate the effectiveness of our texture dimensions and color selection criteria. we then discuss new work that studies how texture and color can be used simultaneously in a single display
1231	886600	article	plos computational biology	\N	\N	\N	\N	preprint	2006	2006	oct	2006-10-06 04:40:57	\N	{3d} complex: a structural classification of protein complexes	most of the proteins in a cell assemble into complexes to carry out their function. it is therefore crucial to understand the physico-chemical properties as well as the evolution of interactions between proteins. the protein data bank represents an important source of information for such studies, because more than half of the structures are homo- or heteromeric protein complexes. here we propose the first hierarchical classification of whole protein complexes of known three-dimensional structure, based on representing their fundamental structural features as a graph. this classification provides the first overview of all the complexes in the protein data bank and allows non-redundant sets to be derived at different levels of detail. this reveals that between one half and two thirds of known structures are multimeric, depending on the level of redundancy accepted. we also analyse the structures in terms of the topological arrangement of their subunits, and find that they form a small number of arrangements compared to all theoretically possible ones. this is because most complexes contain four subunits or less, and the large majority are homomeric. in addition, there is a strong tendency for symmetry in complexes, even for heteromeric complexes. finally, through comparison of biological units in the protein data bank with the protein quaternary structure database, we identified many possible errors in quaternary structure assignments. our classification, available as a database and web server at {www.3dcomplex}.org, will be a starting point for future work aimed at understanding the structure and evolution of protein complexes.
1232	888790	article	science	\N	\N	\N	\N	\N	\N	2006	\N	2006-10-08 01:33:30	\N	from plant traits to plant communities: a statistical mechanistic approach to biodiversity	we develop a quantitative method, analogous to those used in statistical mechanics, to predict how biodiversity will vary across environments, which plant species from a species pool will be found in which relative abundances in a given environment, and which plant traits determine community assembly. this provides a scaling from plant traits to ecological communities while bypassing the complications of population dynamics. community development is treated as a sorting process involving species that are ecologically equivalent except with respect to particular functional traits, leading to a constrained random assembly of species, the relative abundance of each following a general exponential distribution as a function of its traits. using data on 8 functional traits of 30 herbaceous species, and community-aggregated values of these traits in 12 sites along a 42-year chronosequence of secondary succession, we predict 94\\% of the variance in the relative abundances.
1233	891480	inproceedings	\N	proceedings of the 12th international conference on world wide web	www	acm	8	\N	\N	2003	\N	2006-10-10 16:35:12	new york, ny, usa	scaling personalized web search	recent web search techniques augment traditional text matching with a global notion of "importance" based on the linkage structure of the web, such as in google's {pagerank} algorithm. for more refined searches, this global notion of importance can be specialized to create personalized views of importance--for example, importance scores can be biased according to a user-specified set of initially-interesting pages. computing and storing all possible personalized views in advance is impractical, as is computing personalized views at query time, since the computation of each view requires an iterative computation over the web graph. we present new graph-theoretical results, and a new technique based on these results, that encode personalized views as partial vectors. partial vectors are shared across multiple personalized views, and their computation and storage costs scale well with the number of views. our approach enables incremental computation, so that the construction of personalized views from partial vectors is practical at query time. we present efficient dynamic programming algorithms for computing partial vectors, an algorithm for constructing personalized views from partial vectors, and experimental results demonstrating the effectiveness and scalability of our techniques.
1234	892193	article	bioinformatics	\N	\N	\N	\N	\N	\N	2006	aug	2006-10-11 03:38:28	department of chemistry \\& biochemistry, university of california at san diego, la jolla, ca 92093, usa.	bio3d: an r package for the comparative analysis of protein structures.	{summary}: an automated procedure for the analysis of homologous protein structures has been developed. the method facilitates the characterization of internal conformational differences and inter-conformer relationships and provides a framework for the analysis of protein structural evolution. the method is implemented in bio3d, an r package for the exploratory analysis of structure and sequence data. {availability}: the bio3d package is distributed with full source code as a platform independent r package under a {gpl2} license from: http://mccammon.ucsd.edu/\\~{}bgrant/bio3d/.
1235	898525	book	\N	\N	\N	island press	\N	\N	\N	2001	dec	2006-10-15 22:24:38	\N	panarchy: understanding transformations in human and natural systems	creating institutions to meet the challenge of sustainability is arguably the most important task confronting society; it is also dauntingly complex. ecological, economic, and social elements all play a role, but despite ongoing efforts, researchers have yet to succeed in integrating the various disciplines in a way that gives adequate representation to the insights of {each.panarchy}, a term devised to describe evolving hierarchical systems with multiple interrelated elements, offers an important new framework for understanding and resolving this dilemma. panarchy is the structure in which systems, including those of nature (e.g., forests) and of humans (e.g., capitalism), as well as combined human-natural systems (e.g., institutions that govern natural resource use such as the forest service), are interlinked in continual adaptive cycles of growth, accumulation, restructuring, and renewal. these transformational cycles take place at scales ranging from a drop of water to the biosphere, over periods from days to geologic epochs. by understanding these cycles and their scales, researchers can identify the points at which a system is capable of accepting positive change, and can use those leverage points to foster resilience and sustainability within the {system.this} volume brings together leading thinkers on the subject -- including fikret berkes, buz brock, steve carpenter, carl folke, lance gunderson, {c.s}. holling, don ludwig, {karl-goran} maler, charles perrings, marten scheffer, brian walker, and frances westley -- to develop and examine the concept of panarchy and to consider how it can be applied to human, natural, and human-natural systems. throughout, contributors seek to identify adaptive approaches to management that recognize uncertainty and encourage innovation while fostering {resilience.the} book is a fundamental new development in a widely acclaimed line of inquiry. it represents the first step in integrating disciplinary knowledge for the adaptive management of human-natural systems across widely divergent scales, and offers an important base of knowledge from which institutions for adaptive management can be developed. it will be an invaluable source of ideas and understanding for students, researchers, and professionals involved with ecology, conservation biology, ecological economics, environmental policy, or related fields.
1236	907397	article	trends in biotechnology	\N	\N	\N	8	24	12	2006	dec	2006-10-20 04:39:09	school of computer science, national centre for text mining, the manchester interdisciplinary biocentre, the university of manchester, 131 princess st, manchester m1 7nd, uk; national centre for text mining, the manchester interdisciplinary biocentre, the university of manchester, 131 princess st, manchester m1 7nd, uk.	text mining and its potential applications in systems biology.	with biomedical literature increasing at a rate of several thousand papers per week, it is impossible to keep abreast of all developments; therefore, automated means to manage the information overload are required. text mining techniques, which involve the processes of information retrieval, information extraction and data mining, provide a means of solving this. by adding meaning to text, these techniques produce a more structured analysis of textual knowledge than simple word searches, and can provide powerful tools for the production and analysis of systems biology models.
1237	909969	article	journal of environmental psychology  (j. environ. psychol.)	\N	\N	\N	2	15	3	1995	\N	2006-10-22 20:04:44	\N	the role of uncertainty in resource dilemmas	three models of the sources of overprotection in stroke patients were tested in a study of the behavioral and attitudinal concomitants of overprotective caregiving. stroke patients and their family member caregivers were interviewed to assess feelings of overprotection, physical and mental functioning, and caregiving-related attitudes. the couples were then videotaped as they interacted on four tasks and the tapes were coded for specific behaviors and affect directed toward the patient. strong support was found for the resentment model that overprotection will be associated with an overcontrolling caregiving style, negative affect, and resentment toward the stroke patient. no support was found for the model that overprotection is related to overhelping. in addition, feeling overprotected was associated with patient dependency. two conclusions are discussed: the variety of potential sources of overprotection found in the study and the importance of the emotional tone with which caregivers provide assistance.
1238	910675	article	genome research	genome research	\N	cold spring harbor laboratory press	9	16	12	2006	dec	2006-10-24 01:34:33	genomic functional analysis section, national human genome research institute, national institutes of health, rockville, maryland 20878, usa;	locating mammalian transcription factor binding sites: a survey of computational and experimental techniques	an international, peer-reviewed genome sciences journal featuring outstanding original research that offers novel insights into the biology of all organisms
1239	913189	article	nature neuroscience	\N	\N	nature publishing group	6	9	11	2006	oct	2006-10-26 20:33:24	\N	bayesian inference with probabilistic population codes	recent psychophysical experiments indicate that humans perform near-optimal bayesian inference in a wide variety of tasks, ranging from cue integration to decision making to motor control. this implies that neurons both represent probability distributions and combine those distributions according to a close approximation to bayes' rule. at first sight, it would seem that the high variability in the responses of cortical neurons would make it difficult to implement such optimal statistical inference in cortical circuits. we argue that, in fact, this variability implies that populations of neurons automatically represent probability distributions over the stimulus, a type of code we call probabilistic population codes. moreover, we demonstrate that the poisson-like variability observed in cortex reduces a broad class of bayesian inference to simple linear combinations of populations of neural activity. these results hold for arbitrary probability distributions over the stimulus, for tuning curves of arbitrary shape and for realistic neuronal variability.
1240	913652	book	\N	\N	\N	{houghton mifflin}	\N	\N	\N	2006	sep	2006-10-26 16:00:31	\N	the god delusion	{discover magazine recently called richard dawkins "darwin's rottweiler" for his fierce and effective defense of evolution. prospect magazine voted him among the top three public intellectuals in the world (along with umberto eco and noam chomsky). now dawkins turns his considerable intellect on religion, denouncing its faulty logic and the suffering it causes.  he critiques god in all his forms, from the sex-obsessed tyrant of the old testament to the more benign (but still illogical) celestial watchmaker favored by some enlightenment thinkers. he eviscerates the major arguments for religion and demonstrates the supreme improbability of a supreme being. he shows how religion fuels war, foments bigotry, and abuses children, buttressing his points with historical and contemporary evidence. in so doing, he makes a compelling case that belief in god is not just irrational, but potentially deadly.  dawkins has fashioned an impassioned, rigorous rebuttal to religion, to be embraced by anyone who sputters at the inconsistencies and cruelties that riddle the bible, bristles at the inanity of "intelligent design," or agonizes over fundamentalism in the middle east\\&\\#8212;or middle america.}
1241	913782	article	j. med. chem.	journal of medicinal chemistry	\N	american chemical society	12	49	23	2006	oct	2006-10-26 18:11:12	\N	benchmarking sets for molecular docking	ligand enrichment among top-ranking hits is a key metric of molecular docking. to avoid bias, decoys should resemble ligands physically, so that enrichment is not simply a separation of gross features, yet be chemically distinct from them, so that they are unlikely to be binders. we have assembled a directory of useful decoys ({dud}), with 2950 ligands for 40 different targets. every ligand has 36 decoy molecules that are physically similar but topologically distinct, leading to a database of 98?266 compounds. for most targets, enrichment was at least half a log better with uncorrected databases such as the {mddr} than with {dud}, evidence of bias in the former. these calculations also allowed 40 ? 40 cross-docking, where the enrichments of each ligand set could be compared for all 40 targets, enabling a specificity metric for the docking screens. {dud} is freely available online as a benchmarking set for docking at http://blaster.docking.org/dud/.
1242	916031	misc	\N	\N	\N	\N	\N	\N	\N	2002	\N	2006-10-29 04:30:11	\N	the emergence of linguistic structure: an overview of the iterated learning model	introduction  as language users humans possess a culturally transmitted system of unparalleled complexity in the natural world. linguistics has revealed over the past 40 years the degree to which the syntactic structure of language in particular is strikingly complex. furthermore, as pinker and bloom point out in their agenda-setting paper natural language and natural selection \\grammar is a complex mechanism tailored to the transmission of propositional structures through a serial interface...
1243	916577	article	ecological modelling	\N	\N	\N	19	176	3-4	2004	sep	2008-01-16 19:20:57	\N	multi-agent simulations and ecosystem management: a review	this paper proposes a review of the development and use of multi-agent simulations ({mas}) for ecosystem management. the use of this methodology and the associated tools accompanies the shifts in various paradigms on the study of ecological complexity. behavior and interactions are now key issues for understanding and modeling ecosystem organization, and models are used in a constructivist way. {mas} are introduced conceptually and are compared with individual-based modeling approaches. various architectures of agents are presented, the role of the environment is emphasized and some computer tools are presented. a discussion follows on the use of {mas} for ecosystem management. the strength of {mas} has been discussed for social sciences and for spatial issues such as land-use change. we argue here that {mas} are useful for problems integrating social and spatial aspects. then we discuss how {mas} can be used for several purposes, from theorization to collective decision-making support. we propose some research perspectives on individual decision making processes, institutions, scales, the credibility of models and the use of {mas}. in conclusion we argue that researchers in the field of ecosystem management can use multi-agent systems to go beyond the role of the individual and to study more deeply and more effectively the different forms of organization (spatial, networks, hierarchies) and interactions among different organizational levels. for that objective there is considerably more fruit to be had on the tree of collaboration between social, ecological, and computer scientists than has so far been harvested.
1244	921343	article	nature genetics	\N	\N	nature publishing group	2	38	12	2006	oct	2006-11-01 08:27:44	hubrecht laboratory–knaw, uppsalalaan 8, 3584 ct utrecht, the netherlands.	diversity of {micrornas} in human and chimpanzee brain	we used massively parallel sequencing to compare the {microrna} ({mirna}) content of human and chimpanzee brains, and we identified 447 new {mirna} genes. many of the new {mirnas} are not conserved beyond primates, indicating their recent origin, and some {mirnas} seem species specific, whereas others are expanded in one species through duplication events. these data suggest that evolution of {mirnas} is an ongoing process and that along with ancient, highly conserved {mirnas}, there are a number of emerging {mirnas}.
1245	921768	article	nature	\N	\N	nature publishing group	3	444	7115	2006	nov	2008-10-21 14:25:18	\N	self-cooling of a micromirror by radiation pressure	cooling of mechanical resonators is currently a popular topic in many fields of physics including ultra-high precision measurements1, detection of gravitational waves2, 3 and the study of the transition between classical and quantum behaviour of a mechanical system4, 5, 6. here we report the observation of self-cooling of a micromirror by radiation pressure inside a high-finesse optical cavity. in essence, changes in intensity in a detuned cavity, as caused by the thermal vibration of the mirror, provide the mechanism for entropy flow from the mirror's oscillatory motion to the low-entropy cavity field2. the crucial coupling between radiation and mechanical motion was made possible by producing free-standing micromirrors of low mass (m  400 ng), high reflectance (more than 99.6\\%) and high mechanical quality (q  10,000). we observe cooling of the mechanical oscillator by a factor of more than 30; that is, from room temperature to below 10 k. in addition to purely photothermal effects7 we identify radiation pressure as a relevant mechanism responsible for the cooling. in contrast with earlier experiments, our technique does not need any active feedback8, 9, 10. we expect that improvements of our method will permit cooling ratios beyond 1,000 and will thus possibly enable cooling all the way down to the quantum mechanical ground state of the micromirror.
1246	922550	article	bioinformatics	\N	\N	\N	10	19 Suppl 2	\N	2003	oct	2006-11-02 10:06:39	institut fur mikrobiologie und genetik, abteilung bioinformatik, universitat gottingen, goldschmidtstrasse 1, gottingen, 37077, germany institut fur numerische und angewandte mathematik, universitat gottingen, lotzestrasse 16-18, gottingen, 37083, germany.	gene prediction with a hidden markov model and a new intron submodel	motivation: the problem of finding the genes in eukaryotic {dna} sequences by computational methods is still not satisfactorily solved. gene finding programs have achieved relatively high accuracy on short genomic sequences but do not perform well on longer sequences with an unknown number of genes in them. here existing programs tend to predict many false exons. results: we have developed a new program, {augustus}, for the ab initio prediction of protein coding genes in eukaryotic genomes. the program is based on a hidden markov model and integrates a number of known methods and submodels. it employs a new way of modeling intron lengths. we use a new donor splice site model, a new model for a short region directly upstream of the donor splice site model that takes the reading frame into account and apply a method that allows better {gc}-content dependent parameter estimation. {augustus} predicts on longer sequences far more human and drosophila genes accurately than the ab initio gene prediction programs we compared it with, while at the same time being more specific. availability: a web interface for {augustus} and the executable program are located at http://augustus.gobics.de. supplementary information: the datasets used for testing and training are available at http://augustus.gobics.de/datasets/ contact: mstanke@gwdg.de
1247	922563	article	nucleic acids res	\N	\N	\N	11	29	12	2001	jun	2006-11-02 10:06:46	school of biology and school of mathematics, georgia institute of technology, atlanta, ga 30332-0230, usa.	{genemarks}: a self-training method for prediction of gene starts in microbial genomes. implications for finding sequence motifs in regulatory regions	improving the accuracy of prediction of gene starts is one of a few remaining open problems in computer prediction of prokaryotic genes. its difficulty is caused by the absence of relatively strong sequence patterns identifying true translation initiation sites. in the current paper we show that the accuracy of gene start prediction can be improved by combining models of protein-coding and non-coding regions and models of regulatory sites near gene start within an iterative hidden markov model based algorithm. the new gene prediction method, called {genemarks}, utilizes a non-supervised training procedure and can be used for a newly sequenced prokaryotic genome with no prior knowledge of any protein or {rrna} genes. the {genemarks} implementation uses an improved version of the gene finding program {genemark}.hmm, heuristic markov models of coding and non-coding regions and the gibbs sampling multiple alignment program. {genemarks} predicted precisely 83.2\\% of the translation starts of {genbank} annotated bacillus subtilis genes and 94.4\\% of translation starts in an experimentally validated set of escherichia coli genes. we have also observed that {genemarks} detects prokaryotic genes, in terms of identifying open reading frames containing real genes, with an accuracy matching the level of the best currently used gene detection methods. accurate translation start prediction, in addition to the refinement of protein sequence n-terminal data, provides the benefit of precise positioning of the sequence region situated upstream to a gene start. therefore, sequence motifs related to transcription and translation regulatory sites can be revealed and analyzed with higher precision. these motifs were shown to possess a significant variability, the functional and evolutionary connections of which are discussed.
1248	922585	article	bioinformatics	\N	\N	\N	1	20	16	2004	nov	2006-11-02 10:06:56	bioinformatics department, the institute for genomic research, rockville, md 20850, usa. bmajoros@tigr.org	{tigrscan} and {glimmerhmm}: two open source ab initio eukaryotic gene-finders	we describe two new generalized hidden markov model implementations for ab initio eukaryotic gene prediction. the {c/c}++ source code for both is available as open source and is highly reusable due to their modular and extensible architectures. unlike most of the currently available gene-finders, the programs are re-trainable by the end user. they are also re-configurable and include several types of probabilistic submodels which can be independently combined, such as maximal dependence decomposition trees and interpolated markov models. both programs have been used at {tigr} for the annotation of the aspergillus fumigatus and toxoplasma gondii genomes. {availability}: source code and documentation are available under the open source artistic license from http://www.tigr.org/software/pirate
1249	922588	article	bioinformatics	\N	\N	\N	9	18	10	2002	oct	2006-11-02 10:06:59	the wellcome trust sanger institute, wellcome trust genome campus, hinxton, cambridge cb10 1sa, uk. im1@sanger.ac.uk	comparative ab initio prediction of gene structures using pair {hmms}	we present a novel comparative method for the ab initio prediction of protein coding genes in eukaryotic genomes. the method simultaneously predicts the gene structures of two un-annotated input {dna} sequences which are homologous to each other and retrieves the subsequences which are conserved between the two {dna} sequences. it is capable of predicting partial, complete and multiple genes and can align pairs of genes which differ by events of exon-fusion or exon-splitting. the method employs a probabilistic pair hidden markov model. we generate annotations using our model with two different algorithms: the viterbi algorithm in its linear memory implementation and a new heuristic algorithm, called the stepping stone, for which both memory and time requirements scale linearly with the sequence length. we have implemented the model in a computer program called {doublescan}. in this article, we introduce the method and confirm the validity of the approach on a test set of 80 pairs of orthologous {dna} sequences from mouse and human. more information can be found at: {http://www.sanger.ac.uk/software}/analysis/doublescan/
1250	925514	article	information retrieval	\N	\N	kluwer academic publishers	14	2	1	2000	\N	2006-11-02 12:41:29	\N	a {task-oriented} {non-interactive} evaluation methodology for information retrieval systems	past research has identified many different types of relevance in information retrieval ({ir}). so far, however, most evaluation of {ir} systems has been through batch experiments conducted with test collections containing only expert, topical relevance judgements. recently, there has been some movement away from this traditional approach towards interactive, more user-centred methods of evaluation. however, these are expensive for evaluators in terms both of time and of resources. this paper describes a new evaluation methodology, using a task-oriented test collection, which combines the advantages of traditional non-interactive testing with a more user-centred emphasis. the main features of a task-oriented test collection are the adoption of the task, rather than the query, as the primary unit of evaluation and the naturalistic character of the relevance judgements.
1251	930113	inproceedings	\N	proceedings of the third acl applied nlp	\N	\N	3	\N	\N	1992	\N	2006-11-06 08:06:16	trento, italy	a simple rule-based part-of-speech tagger	automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rule-based methods. in this paper, we present a simple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers. the rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease...
1252	930125	inproceedings	\N	proceedings of coling	\N	\N	\N	\N	\N	1988	\N	2006-11-06 08:06:17	\N	a statistical approach to language translation	an approach to automatic translation is outlined that utilizes techniques of statistical information extraction from large data bases. the method is based on the availability of pairs of large corresponding texts that are translations of each other. in our case, the texts are in english and french.fundamental to the technique is a complex glossary of correspondence of fixed locutions. the steps of the proposed translation process are: (1) partition the source text into a set of fixed locutions. (2) use the glossary plus contextual information to select the corresponding set of fixed locutions into a sequence forming the target sentence. (3) arrange the words of the target fixed locutions into a sequence forming the target sentence.we have developed statistical techniques facilitating both the automatic creation of the glossary, and the performance of the three translation steps, all on the basis of an alignment of corresponding sentences in the two texts.while we are not yet able to provide examples of french / english translation, we present some encouraging intermediate results concerning glossary creation and the arrangement of target word sequences.
1253	930236	inproceedings	\N	proceedings of the 12th international conference on machine learning	\N	morgan kaufmann	8	\N	\N	1995	\N	2006-11-06 08:06:22	\N	fast effective rule induction	many existing rule learning systems are computationally expensive on large noisy datasets. in this paper we evaluate the recently-proposed rule learning algorithm irep on a large and diverse collection of benchmark problems. we show that while irep is extremely efficient, it frequently gives error rates higher than those of c4.5 and c4.5rules. we then propose a number of modifications resulting in an algorithm ripperk that is very competitive with c4.5rules with respect to error rates, but much more efficient on large samples. ripperk obtains error rates lower than or equivalent to c4.5rules on 22 of 37 benchmark problems, scales nearly linearly with the number of training examples, and can efficiently process noisy datasets containing hundreds of thousands of examples. 1 introduction systems that learn sets of rules have a number of desirable properties. rule sets are relatively easy for people to understand [ catlett, 1991 ] , and rule learning systems outperform decision tree lear...
1254	931018	book	\N	\N	\N	oxford university press	\N	\N	\N	2003	\N	2006-11-06 08:07:17	oxford	the {o}xford {h}andbook of {c}omputational {l}inguistics	thirty-seven chapters, commissioned from experts all over the world, describe major concepts, methods, and applications in computational linguistics. part i, linguistic fundamentals, provides an overview of the field suitable for senior undergraduates and non-specialists from other fields of linguistics and related disciplines. part ii describes current tasks, techniques, and tools in natural language processing and aims to meet the needs of post-doctoral workers and others embarking on computational language research. part iii surveys current applications. the book is a state-of-the-art reference to one of the most active and productive fields in linguistics. it will be of interest and practical use to a wide range of linguists, as well as to researchers in such fields as informatics, artificial intelligence, language engineering, and cognitive science.
1255	931189	article	journal of machine learning research	\N	\N	\N	\N	\N	\N	2002	\N	2006-11-06 08:07:21	\N	shallow parsing using noisy and {non-stationary} training material	shallow parsers are usually assumed to be trained on noise-free material, drawn from the same distribution as the testing material. however, when either the training set is noisy or else drawn from a different distributions, performance may be degraded. using the parsed wall street journal, we investigate the performance of four shallow parsers (maximum entropy, memory-based learning, n-grams and ensemble learning) trained using various types of artificially noisy material. our first set of results show that shallow parsers are surprisingly robust to synthetic noise, with performance gradually decreasing as the rate of noise increases. further results show that no single shallow parser performs best in all noise situations. final results show that simple, parser-specific extensions can improve noise-tolerance. our second set of results addresses the question of whether naturally occurring disfluencies undermines performance more than does a change in distribution. results using the parsed switchboard corpus suggest that, although naturally occurring disfluencies might harm performance, differences in distribution between the training set and the testing set are more significant.
1256	931732	techreport	\N	\N	\N	\N	\N	\N	\N	1991	\N	2006-11-06 08:07:53	{pittsburgh}	parsing english with a link grammar	we define a new type of formal grammatical system called a _link grammar_. a sequence of words is in the language of a link grammar if there is a way to draw _links_ between words in such a way that (1) the local requirements of each word are satisfied, (2) the links do not cross, and (3) the words form a connected graph. we have encoded english grammar into such a system, and written a program (based on new algorithms) for efficiently parsing with a link grammar. the breadth of english phenomena that our system handles is quite large -- it handles: noun-verb agreement, questions, imperatives, complex and irregular verbs, many types of nouns, past- or present-participles in noun phrases, commas, a variety of adjective types, prepositions, adverbs, relative clauses, possessives, coordinating conjunctions, and many other things. a number of sophisticated and new techniques were used to allow efficient parsing of this very complex grammar. our program is written in c, and the entire system may be obtained via anonymous ftp.
1257	933268	article	pattern analysis and machine intelligence, ieee transactions on	pattern analysis and machine intelligence, ieee transactions on	\N	ieee	16	23	10	2001	oct	2006-11-06 11:48:00	los alamitos, ca, usa	toward machine emotional intelligence analysis of affective physiological state	â€”the ability to recognize emotion is one of the hallmarks of emotional intelligence, an aspect of human intelligence that has been argued to be even more important than mathematical and verbal intelligences. this paper proposes that machine intelligence needs to include emotional intelligence and demonstrates results toward this goal: developing a machine's ability to recognize human affective state given four physiological signals. we describe difficult issues unique to obtaining reliable affective data and collect a large set of data from a subject trying to elicit and experience each of eight emotional states, daily, over multiple weeks. this paper presents and compares multiple algorithms for feature-based recognition of emotional state from this data. we analyze four physiological signals that exhibit problematic day-to-day variations: the features of different emotions on the same day tend to cluster more tightly than do the features of the same emotion on different days. to handle the daily variations, we propose new features and algorithms and compare their performance. we find that the technique of seeding a fisher projection with the results of sequential floating forward search improves the performance of the fisher projection and provides the highest recognition rates reported to date for classification of affect from physiology: 81 percent recognition accuracy on eight classes of emotion, including neutral.
1258	933410	inproceedings	\N	proceedings of the 38th hawaii international conference on system sciences	\N	\N	\N	\N	\N	-1	\N	2006-11-06 14:40:15	\N	beyond personal webpublishing: an exploratory study of conversational blogging practices	although initially developed as low-threshold tools to publish on-line, weblogs increasingly appear to facilitate conversations. the objective of this study is to identify practices of conversational blogging. this paper presents results of an exploratory qualitative analysis of a weblog-mediated conversation case, focusing on participation rhythm, media choices and specific linking practices. based on our findings we propose attributes of conversational blogging: linking as conversational glue, tangential conversations and interplays between conversation with self and conversations with others. finally, future research directions are discussed.
1259	934384	article	nature biotechnology	\N	\N	nature publishing group	6	24	11	2006	nov	2006-11-07 13:57:48	[1] division of genetics, department of medicine, brigham and women's hospital and harvard medical school, boston, massachusetts 02115, usa. [2] harvard university graduate biophysics program, cambridge, massachusetts 02138, usa. [3] these authors contributed equally to this work.	compact, universal {dna} microarrays to comprehensively determine transcription-factor binding site specificities.	transcription factors ({tfs}) interact with specific {dna} regulatory sequences to control gene expression throughout myriad cellular processes. however, the {dna} binding specificities of only a small fraction of {tfs} are sufficiently characterized to predict the sequences that they can and cannot bind. we present a maximally compact, synthetic {dna} sequence design for protein binding microarray ({pbm}) experiments that represents all possible {dna} sequence variants of a given length k (that is, all 'k-mers') on a single, universal microarray. we constructed such all k-mer microarrays covering all 10-base pair (bp) binding sites by converting high-density single-stranded oligonucleotide arrays to double-stranded (ds) {dna} arrays. using these microarrays we comprehensively determined the binding specificities over a full range of affinities for five {tfs} of different structural classes from yeast, worm, mouse and human. the unbiased coverage of all k-mers permits high-throughput interrogation of binding site preferences, including nucleotide interdependencies, at unprecedented resolution.
1260	942734	article	decision support systems	\N	\N	\N	22	27	\N	1999	\N	2006-11-14 10:05:12	\N	supporting collaborative process knowledge management in new product development teams	knowledge centric activities of developing new products and services are becoming the primary source of sustainable competitive advantage in an era characterized by short product life cycles, dynamic markets and complex processes. we view new product development (npd) as a knowledge-intensive activity. based on a case study in the consumer electronics industry, we identify problems associated with knowledge management (km) in the context of npd by cross-functional collaborative teams. we map these problems to broad information technology enabled solutions and subsequently translate these into specific system characteristics and requirements. a prototype system that meets these requirements developed to capture and manage tacit and explicit process knowledge is further discussed. the functionalities of the system include functions for representing context with informal components, easy access to process knowledge, assumption surfacing, review of past knowledge, and management of dependencies. we demonstrate the validity our proposed solutions using scenarios drawn from our case study.
1261	943475	article	acta psychologica	\N	\N	\N	28	107	1-3	2001	apr	2006-11-14 21:41:18	\N	{fmr}-adaptation: a tool for studying the functional properties of human cortical neurons	the invariant properties of human cortical neurons cannot be studied directly by {fmri} due to its limited spatial resolution. one voxel obtained from a {fmri} scan contains several hundred thousands neurons. therefore, the {fmri} signal may average out a heterogeneous group of highly selective neurons. here, we present a novel experimental paradigm for {fmri}, functional magnetic resonance-adaptation ({fmr}-a), that enables to tag specific neuronal populations within an area and investigate their functional properties. this approach contrasts with conventional mapping methods that measure the averaged activity of a region. the application of {fmr}-a to study the functional properties of cortical neurons proceeds in two stages: first, the neuronal population is adapted by repeated presentation of a single stimulus. second, some property of the stimulus is varied and the recovery from adaptation is assessed. if the signal remains adapted, it will indicate that the neurons are invariant to that attribute. however, if the {fmri} signal will recover from the adapted state it would imply that the neurons are sensitive to the property that was varied. here, an application of {fmr}-a for studying the invariant properties of high-order object areas (lateral occipital complex - {loc}) to changes in object size, position, illumination and rotation is presented. the results show that {loc} is less sensitive to changes in object size and position compared to changes of illumination and viewpoint. {fmr}-a can be extended to other neuronal systems in which adaptation is manifested and can be used with event-related paradigms as well. by manipulating experimental parameters and testing recovery from adaptation it should be possible to gain insight into the functional properties of cortical neurons which are beyond the spatial resolution limits imposed by conventional {fmri}.
1262	943570	article	science	\N	\N	american association for the advancement of science	3	291	5506	2001	feb	2006-11-14 23:51:29	fred hutchinson cancer research center, 1100 fairview avenue north, seattle, wa 98109-1024, usa.	principles for the buffering of genetic variation	most genetic research has used inbred organisms and has not explored the complexity of natural genetic variation present in outbred populations. the translation of genotype to phenotype is complicated by gene interactions observed as epistasis, canalization, robustness, or buffering. analysis of double mutations in inbred experimental organisms suggests some principles for gene interaction that may apply to natural variation as well. the buffering of variation in one gene is most often due to a small number of other genes that function in the same biochemical process. however, buffering can also result from genes functioning in processes extrinsic to that of the primary gene.
1263	944934	article	journal of physiology, paris	\N	\N	\N	17	100	1-3	2006	jul	2006-11-15 16:43:52	the wellcome department of imaging neuroscience, institute of neurology, university college london, 12 queen square, london wc1n 3b, united kingdom.	a free energy principle for the brain.	by formulating helmholtz's ideas about perception, in terms of modern-day theories, one arrives at a model of perceptual inference and learning that can explain a remarkable range of neurobiological facts: using constructs from statistical physics, the problems of inferring the causes of sensory input and learning the causal structure of their generation can be resolved using exactly the same principles. furthermore, inference and learning can proceed in a biologically plausible fashion. the ensuing scheme rests on empirical bayes and hierarchical models of how sensory input is caused. the use of hierarchical models enables the brain to construct prior expectations in a dynamic and context-sensitive fashion. this scheme provides a principled way to understand many aspects of cortical organisation and responses. in this paper, we show these perceptual processes are just one aspect of emergent behaviours of systems that conform to a free energy principle. the free energy considered here measures the difference between the probability distribution of environmental quantities that act on the system and an arbitrary distribution encoded by its configuration. the system can minimise free energy by changing its configuration to affect the way it samples the environment or change the distribution it encodes. these changes correspond to action and perception respectively and lead to an adaptive exchange with the environment that is characteristic of biological systems. this treatment assumes that the system's state and structure encode an implicit and probabilistic model of the environment. we will look at the models entailed by the brain and how minimisation of its free energy can explain its dynamics and structure.
1264	948687	inproceedings	\N	proceedings of the 1998 ieee international conference on computer vision, bombay, india	\N	\N	7	\N	\N	1998	\N	2006-11-16 13:39:31	\N	{bilateral filtering for gray and color images}	bilateral filtering smooths images while preserving edges, by means of a nonlinear combination of nearby image values. the method is noniterative, local, and simple. it combines gray levels or colors based on both their geometric closeness and their photometric similarity, and prefers near values to distant values in both domain and range. in contrast with filters that operate on the three bands of a color image separately, a bilateral filter can enforce the perceptual metric underlying the cie-lab color space, and smooth colors and preserve edges in a way that is tuned to human perception. also, in contrast with standard filtering, bilateral filtering produces no phantom colors along edges in color images, and reduces phantom colors where they appear in the original image.
1265	948882	inproceedings	\N	proceedings of the european conference on computer vision (eccv)	\N	\N	\N	\N	\N	2004	\N	2006-11-16 14:33:59	\N	a statistical model for general contextual object recognition	we consider object recognition as the process of attaching meaningful labels to specific regions of an image, and propose a model that learns spatial relationships between objects. given a set of images and their associated text (e.g. keywords, captions, descriptions), the objective is to segment an image, in either a crude or sophisticated fashion, then to find the proper associations between words and regions. previous models are limited by the scope of the representation. in particular, they fail to exploit spatial context in the images and words. we develop a more expressive model that takes this into account. we formulate a spatially consistent probabilistic mapping between continuous image feature vectors and the supplied word tokens. by learning both word-to-region associations and object relations, the proposed model augments scene segmentations due to smoothing implicit in spatial consistency. context introduces cycles to the undirected graph, so we cannot rely on a straightforward implementation of the em algorithm for estimating the model parameters and densities of the unknown alignment variables. instead, we develop an approximate em algorithm that uses loopy belief propagation in the inference step and iterative scaling on the pseudo-likelihood approximation in the parameter update step. the experiments indicate that our approximate inference and learning algorithm converges to good local solutions. experiments on a diverse array of images show that spatial context considerably improves the accuracy of object recognition. most significantly, spatial context combined with a nonlinear discrete object representation allows our models to cope well with over-segmented scenes.
1266	948905	inproceedings	\N	proceedings of the ieee conference on computer vision and pattern recognition (cvpr)	\N	\N	\N	\N	\N	2006	\N	2007-03-29 13:31:31	\N	using multiple segmentations to discover objects and their extent in image collections	given a large dataset of images, we seek to automatically determine the visually similar object and scene classes together with their image segmentation. to achieve this we combine two ideas: (i) that a set of segmented objects can be partitioned into visual object classes using topic discovery models from statistical text analysis; and (ii) that visual object classes can be used to assess the accuracy of a segmentation. to tie these ideas together we compute multiple segmentations of each image and then: (i) learn the object classes; and (ii) choose the correct segmentations. we demonstrate that such an algorithm succeeds in automatically discovering many familiar objects in a variety of image datasets, including those from caltech, msrc and labelme.
1267	948907	inproceedings	\N	proceedings of european conference computer vision (eccv)	\N	\N	\N	\N	\N	2006	\N	2006-11-16 14:34:05	\N	{textonboost}: joint appearance, shape and context modeling for {multi-class} object recognition and segmentation	this paper proposes a new approach to learning a discriminative model of object classes, incorporating appearance, shape and context information efficiently. the learned model is used for automatic visual recognition and semantic segmentation of photographs. our discriminative model exploits novel features, based on textons, which jointly model shape and texture. unary classification and feature selection is achieved using shared boosting to give an efficient classifier which can be applied to a large number of classes. accurate image segmentation is achieved by incorporating these classifiers in a conditional random field. efficient training of the model on very large datasets is achieved by exploiting both random feature selection and piecewise training methods. high classification and segmentation accuracy are demonstrated on three different databases: i) our own 21-object class database of photographs of real objects viewed under general lighting conditions, poses and viewpoints, ii) the 7-class corel subset and iii) the 7-class sowerby database used in [1]. the proposed algorithm gives competitive results both for highly textured (e.g. grass, trees), highly structured (e.g. cars, faces, bikes, aeroplanes) and articulated objects (e.g. body, cow). Â© springer-verlag berlin heidelberg 2006.
1268	948910	article	international journal of computer vision	\N	\N	\N	\N	53(2)	\N	2003	\N	2006-11-16 14:34:06	\N	contextual priming for object detection	there is general consensus that context can be a rich source of information about an object&#039;s identity, location and scale. in fact, the structure of many real-world scenes is governed by strong configurational rules akin to those that apply to a single object. here we introduce a simple framework for modeling the relationship between context and object properties based on the correlation between the statistics of low-level features across the entire scene and the objects that it contains. the resulting scheme serves as an effective procedure for object priming, context driven focus of attention and automatic scale-selection on real-world scenes.
1269	950849	article	neuroimage	\N	\N	\N	12	9	\N	1999	\N	2006-11-18 15:29:35	\N	cortical surface-based analysis {ii}: inflation, flattening, and a surface-based coordinate system	the surface of the human cerebral cortex is a highly folded sheet with the majority of its surface area buried within folds. as such, it is a difficult domain for computational as well as visualization purposes. we have therefore designed a set of procedures for modifying the representation of the cortical surface to (i) inflate it so that activity buried inside sulci may be visualized, (ii) cut and flatten an entire hemisphere, and (iii) transform a hemisphere into a simple parameterizable surface such as a sphere for the purpose of establishing a surface-based coordinate system.
1270	952015	article	neural comput	\N	\N	\N	28	13	6	2001	jun	2006-11-19 17:49:23	centre de recherche cerveau et cognition, facult\\'{e} de m\\'{e}decine rangueil, 31062 toulouse cedex, france.	rate coding versus temporal order coding: what the retinal ganglion cells tell the visual cortex.	it is often supposed that the messages sent to the visual cortex by the retinal ganglion cells are encoded by the mean firing rates observed on spike trains generated with a poisson process. using an information transmission approach, we evaluate the performances of two such codes, one based on the spike count and the other on the mean interspike interval, and compare the results with a rank order code, where the first ganglion cells to emit a spike are given a maximal weight. our results show that the rate codes are far from optimal for fast information transmission and that the temporal structure of the spike train can be efficiently used to maximize the information transfer rate under conditions where each cell needs to fire only one spike.
1271	952543	book	\N	\N	\N	cambridge university press	\N	\N	\N	2002	nov	2006-11-20 04:27:54	\N	navigating {social-ecological} systems: building resilience for complexity and change	drawing on complex systems theory, this book investigates how human societies deal with change in linked social-ecological systems, and build capacity to adapt to change. the concept of resilience is central in this context. resilient social-ecological systems have the potential to sustain development in a manner that does not lead to loss of future options. resilient systems provide capacity for renewal and innovation in the face of rapid transformation and crisis. case studies and examples from several geographic areas, cultures and resource types are included; merging forefront research from natural sciences, social sciences and the humanities into an innovative framework for sustainable systems.
1272	954839	book	\N	\N	\N	\N	\N	\N	\N	2006	\N	2006-11-21 08:20:49	\N	wikis und blogs. planen. einrichten. verwalten	im internet sind sie schon als unkompliziertes dokumentations- und marketinginstrument etabliert: wikis und blogs. in firmen befinden sie sich auf dem vormarsch. beide medien eignen sich besonders dann, wenn die anwender keine computerprofis sind. was auf anwenderseite sehr einfach ist, verlangt vom administrator besondere kenntnisse. dieses buch gibt entscheidern und administratoren einen Ãœberblick Ã¼ber die bestehenden implementierungen im internet und liefert ideen fÃ¼r den einsatz im eigenen netzwerk, wobei sogar die den gesetzlichen anforderungen entsprechende revisionssicherheit von wikis als dokumentationsmittel in der arbeitsgruppe herausgearbeitet wird. es werden ausfÃ¼hrlich die verschiedenen techniken vorgestellt und es wird detailliert beschrieben, wie die engines eingerichtet, konfiguriert und administriert werden. die erweiterung der jeweiligen systeme mit c-sharp, java, perl, php und basic wird ausfÃ¼hrlich erklÃ¤rt. selbstverstÃ¤ndlich kommt auch das backoffice nicht zu kurz: der einrichtung einer webserver- und datenbank-umgebung ist ein extrakapitel gewidmet. die themen: - die wiki- und blogoshÃ¤re:ideen und anregungen - arbeiten mit wikis und blogs: die techniken - blogs im detail: wordpress, blog:cms, lifetype, pivot, loudblog und snipsnap - wikis installieren, konfigurieren und verwalten: mediawiki, pmwiki, flexwiki, twiki, jspwiki, usemodwiki und viele andere - zukunftsperspektive semantic wiki - das backoffice: server und datenbanken
1273	955090	inproceedings	\N	proceedings of the 4th decennial conference on critical computing: between sense and sensibility	cc	acm	9	\N	\N	2005	\N	2006-11-21 13:08:21	new york, ny, usa	reflective design	as computing moves into every aspect of our daily lives, the values and assumptions that underlie our technical practices may unwittingly be propagated throughout our culture. drawing on existing critical approaches in computing, we argue that reflection on unconscious values embedded in computing and the practices that it supports can and should be a core principle of technology design. building on a growing body of work in critical computing, reflective design combines analysis of the ways in which technologies reflect and perpetuate unconscious cultural assumptions, with design, building, and evaluation of new computing devices that reflect alternative possibilities. we illustrate this approach through two design case studies.
1274	955467	article	econometrica	\N	\N	the econometric society	20	50	6	1982	\N	2006-11-21 17:06:02	\N	strategic information transmission	this paper develops a model of strategic communication, in which a better-informed sender (s) sends a possibly noisy signal to a receiver (r), who then takes an action that determines the welfare of both. we characterize the set of bayesian nash equilibria under standard assumptions, and show that equilibrium signaling always takes a strikingly simple form, in which s partitions the support of the (scalar) variable that represents his private information and introduces noise into his signal by reporting, in effect, only which element of the partition his observation actually lies in. we show under further assumptions that before s observes his private information, the equilibrium whose partition has the greatest number of elements is pareto-superior to all other equilibria, and that if agents coordinate on this equilibrium, r's equilibrium expected utility rises when agents' preferences become more similar. since r bases his choice of action on rational expectations, this establishes a sense in which equilibrium signaling is more informative when agents' preferences are more similar.
1275	957758	inproceedings	\N	pacific symposium on biocomputing	\N	\N	11	\N	12	2007	\N	2006-11-22 17:52:25	\N	extracting semantic predications from medline citations for pharmacogenomics	we describe a natural language processing system (enhanced semrep) to identify core assertions on pharmacogenomics in medline citations. extracted information is represented as semantic predications covering a range of relations relevant to this domain. the specific relations addressed by the system provide greater precision than that achievable with methods that rely on entity co-occurrence. the development of enhanced semrep is based on the adaptation of an existing system and crucially depends on domain knowledge in the unified medical language system. we provide a preliminary evaluation (55\\\\ recall and 73% precision) and discuss the potential of this system in assisting both clinical practice and scientific investigation.
1276	963455	article	statistical applications in genetics and molecular biology	\N	\N	\N	\N	4	1	2005	\N	2006-11-27 15:02:30	\N	a shrinkage approach to {large-scale} covariance matrix estimation and implications for functional genomics	inferring large-scale covariance matrices from sparse genomic data is an ubiquitous problem in bioinformatics. clearly, the widely used standard covariance and correlation estimators are ill-suited for this purpose. as statistically efficient and computationally fast alternative we propose a novel shrinkage covariance estimator that exploits the ledoit-wolf (2003) lemma for analytic calculation of the optimal shrinkage intensity. subsequently, we apply this improved covariance estimator (which has guaranteed minimum mean squared error, is well-conditioned, and is always positive definite even for small sample sizes) to the problem of inferring large-scale gene association networks. we show that it performs very favorably compared to competing approaches both in simulations as well as in application to real expression data.
1277	964843	inproceedings	\N	iccv	\N	ieee	7	\N	\N	2005	\N	2006-11-28 06:59:35	washington, dc, usa	local features for object class recognition	in this paper, we compare the performance of local detectors and descriptors in the context of object class recognition. recently, many detectors/descriptors have been evaluated in the context of matching as well as invariance to viewpoint changes (mikolajczyk and schmid, 2004). however, it is unclear if these results can be generalized to categorization problems, which require different properties of features. we evaluate 5 state-of-the-art scale invariant region detectors and 5 descriptors. local features are computed for 20 object classes and clustered using hierarchical agglomerative clustering. we measure the quality of appearance clusters and location distributions using entropy as well as precision. we also measure how the clusters generalize from training set to novel test data. our results indicate that attended {sift} descriptors (mikolajczyk and schmid, 2005) computed on {hessian-laplace} regions perform best. second score is obtained by salient regions (kadir and brady, 2001). the results also show that these two detectors provide complementary features. the new detectors/descriptors significantly improve the performance of a state-of-the art recognition approach (leibe, et al., 2005) in pedestrian detection task
1278	965068	proceedings	computer vision and pattern recognition, 2006 ieee computer society conference on	\N	\N	\N	7	1	\N	2006	\N	2006-11-28 09:34:15	\N	multiclass object recognition with sparse, localized features	we apply a biologically inspired model of visual object recognition to the multiclass object categorization problem. our model modifies that of serre, wolf, and poggio. as in that work, we first apply gabor filters at all positions and scales; feature complexity and position/scale invariance are then built up by alternating template matching and max pooling operations. we refine the approach in several biologically plausible ways, using simple versions of sparsification and lateral inhibition. we demonstrate the value of retaining some position and scale information above the intermediate feature level. using feature selection we arrive at a model that performs better with fewer features. our final model is tested on the caltech 101 object categories and the {uiuc} car localization task, in both cases achieving state-of-the-art performance. the results strengthen the case for using this class of model in computer vision.
1279	968552	book	\N	\N	\N	the mit press	\N	\N	\N	2006	sep	2006-11-30 11:16:38	\N	{semi-supervised} learning (adaptive computation and machine learning series)	in the field of machine learning, semi-supervised learning ({ssl}) occupies the middle ground, between supervised learning (in which all training examples are labeled) and unsupervised learning (in which no label data are given). interest in {ssl} has increased in recent years, particularly because of application domains in which unlabeled data are plentiful, such as images, text, and bioinformatics. this first comprehensive overview of {ssl} presents state-of-the-art algorithms, a taxonomy of the field, selected applications, benchmark experiments, and perspectives on ongoing and future research. <br /> <br /> <{i>semi}-supervised learning</i> first presents the key assumptions and ideas underlying the field: smoothness, cluster or low-density separation, manifold structure, and transduction. the core of the book is the presentation of {ssl} methods, organized according to algorithmic strategies. after an examination of generative models, the book describes algorithms that implement the low-density separation assumption, graph-based methods, and algorithms that perform two-step learning. the book then discusses {ssl} applications and offers guidelines for {ssl} practitioners by analyzing the results of extensive benchmark experiments. finally, the book looks at interesting directions for {ssl} research. the book closes with a discussion of the relationship between semi-supervised learning and transduction.<br /> <br /> olivier chapelle and alexander zien are research scientists and bernhard sch\\"{o}lkopf is professor and director at the max planck institute for biological cybernetics in t\\"{u}bingen. sch\\"{o}lkopf is coauthor of <{i>learning} with kernels</i> ({mit} press, 2002) and is a coeditor of <{i>advances} in kernel methods: support vector learning</i> (1998), <{i>advances} in {large-margin} classifiers</i> (2000), and <{i>kernel} methods in computational biology</i> (2004), all published by the {mit} press.
1280	970694	article	cell	\N	\N	\N	18	124	4	2006	feb	2006-12-01 18:45:16	department of host defense, research institute for microbial diseases, osaka university, 3-1 yamada-oka, suita, osaka 565-0871, japan. sakira@biken.osaka-u.ac.jp	pathogen recognition and innate immunity.	microorganisms that invade a vertebrate host are initially recognized by the innate immune system through germline-encoded pattern-recognition receptors ({prrs}). several classes of {prrs}, including toll-like receptors and cytoplasmic receptors, recognize distinct microbial components and directly activate immune cells. exposure of immune cells to the ligands of these receptors activates intracellular signaling cascades that rapidly induce the expression of a variety of overlapping and unique genes involved in the inflammatory and immune responses. new insights into innate immunity are changing the way we think about pathogenesis and the treatment of infectious diseases, allergy, and autoimmunity.
1281	975240	article	ecosystems	\N	\N	\N	9	V9	7	2006	nov	2006-12-05 12:16:13	\N	reconciling carbon-cycle concepts, terminology, and methods	abstract&nbsp;&nbsp;recent projections of climatic change have focused a great deal of scientific and public attention on patterns of carbon (c) cycling as well as its controls, particularly the factors that determine whether an ecosystem is a net source or sink of atmospheric carbon dioxide (co2). net ecosystem production (nep), a central concept in c-cycling research, has been used by scientists to represent two different concepts. we propose that nep be restricted to just one of its two original definitionsâ€”the imbalance between gross primary production (gpp) and ecosystem respiration (er). we further propose that a new termâ€”net ecosystem carbon balance (necb)â€”be applied to the net rate of c accumulation in (or loss from [negative sign]) ecosystems. net ecosystem carbon balance differs from nep when c fluxes other than c fixation and respiration occur, or when inorganic c enters or leaves in dissolved form. these fluxes include the leaching loss or lateral transfer of c from the ecosystem; the emission of volatile organic c, methane, and carbon monoxide; and the release of soot and co2 from fire. carbon fluxes in addition to nep are particularly important determinants of necb over long time scales. however, even over short time scales, they are important in ecosystems such as streams, estuaries, wetlands, and cities. recent technological advances have led to a diversity of approaches to the measurement of c fluxes at different temporal and spatial scales. these approaches frequently capture different components of nep or necb and can therefore be compared across scales only by carefully specifying the fluxes included in the measurements. by explicitly identifying the fluxes that comprise necb and other components of the c cycle, such as net ecosystem exchange (nee) and net biome production (nbp), we can provide a less ambiguous framework for understanding and communicating recent changes in the global c cycle.
1282	988558	inproceedings	\N	sigir	\N	acm press	1	\N	\N	2006	\N	2006-12-11 13:12:58	new york, ny, usa	social networks, incentives, and search	the role of network structure has grown in significance over the past ten years in the field of information retrieval, stimulated to a great extent by the importance of link analysis in the development of web search techniques [4]. this body of work has focused primarily on the network that is most clearly visible on the web: the network of hyperlinks connecting documents to documents. but the web has always contained a second network, less explicit but equally important, and this is the social network on its users, with latent person-to-person links encoding a variety of relationships including friendship, information exchange, and influence. developments over the past few years --- including the emergence of social networking systems and rich social media, as well as the availability of large-scale e-mail and instant messenging datasets --- have highlighted the crucial role played by on-line social networks, and at the same time have made them much easier to uncover and analyze. there is now a considerable opportunity to exploit the information content inherent in these networks, and this prospect raises a number of interesting research challenge.within this context, we focus on some recent efforts to formalize the problem of searching a social network. the goal is to capture the issues underlying a variety of related scenarios: a member of a social networking system such as myspace seeks a piece of information that may be held by a friend of a friend [27, 28]; an employee in a large company searches his or her network of colleagues for expertise in a particular subject [9]; a node in a decentralized peer-to-peer file-sharing system queries for a file that is likely to be a small number of hops away [2, 6, 16, 17]; or a user in a distributed ir or federated search setting traverses a network of distributed resources connected by links that may not just be informational but also economic or contractual [3, 5, 7, 8, 13, 18, 21]. in their most basic forms, these scenarios have some essential features in common: a node in a network, without global knowledge, must find a short path to a desired "target" node (or to one of several possible target nodes).to frame the underlying problem, we go back to one of the most well-known pieces of empirical social network analysis --- stanley milgram's research into the small-world phenomenon, also known as the "six degrees of separation" [19, 24, 25]. the form of milgram's experiments, in which randomly chosen starters had to forward a letter to a designated target individual, established not just that short chains connecting far-flung pairs of people are abundant in large social networks, but also that the individuals in these networks, operating with purely local information about their own friends and acquaintances, are able to actually find these chains [10]. the milgram experiments thus constituted perhaps the earliest indication that large-scale social networks are structured to support this type of decentralized search. within a family of random-graph models proposed by watts and strogatz [26], we have shown that the ability of a network to support this type of decentralized search depends in subtle ways on how its "long-range" connections are correlated with the underlying spatial or organizational structure in which it is embedded [10, 11]. recent studies using data on communication within organizations [1] and the friendships within large on-line communities [15] have established the striking fact that real social networks closely match some of the structural features predicted by these mathematical models.if one looks further at the on-line settings that provide the initial motivation for these issues, there is clearly interest from many directions in their long-term economic implications --- essentially, the consequences that follow from viewing distributed information retrieval applications, peer-to-peer systems, or social-networking sites as providing marketplaces for information and services. how does the problem of decentralized search in a network change when the participants are not simply agents following a fixed algorithm, but strategic actors who make decisions in their own self-interest, and may demand compensation for taking part in a protocol? such considerations bring us into the realm of algorithmic game theory, an active area of current research that uses game-theoretic notions to quantify the performance of systems in which the participants follow their own self-interest [20, 23] in a simple model for decentralized search in the presence of incentives, we find that performance depends crucially on both the rarity of the information and the richness of the network topology [12] --- if the network is too structurally impoverished, an enormous investment may be required to produce a path from a query to an answer.
1283	990151	article	nat neurosci	\N	\N	\N	6	5	12	2002	dec	2006-12-12 20:47:48	center for learning and memory and department of brain and cognitive sciences, massachusetts institute of technology, cambridge, massachusetts 02139, usa.	population coding of shape in area v4.	shape is represented in the visual system by patterns of activity across populations of neurons. we studied the population code for shape in area v4 of macaque monkeys, which is part of the ventral (object-related) pathway in primate visual cortex. we have previously found that many macaque v4 neurons are tuned for the curvature and object-centered position of boundary fragments (such as 'concavity on the right'). here we tested the hypothesis that populations of such cells represent complete shapes as aggregates of boundary fragments. to estimate the population representation of a given shape, we scaled each cell's tuning peak by its response to that shape, summed across cells and smoothed. the resulting population response surface contained 3-8 peaks that represented major boundary features and could be used to reconstruct (approximately) the original shape. this exemplifies how a multi-peaked neural population response can represent a complex stimulus in terms of its constituent elements.
1284	997130	article	neuroimage	\N	\N	\N	\N	\N	\N	-1	\N	2007-08-06 20:49:35	\N	dynamic causal modeling: a generative model of slice timing in {fmri}	dynamic causal modeling ({dcm}) of functional magnetic resonance imaging ({fmri}) data allows one to make inferences about the architecture of distributed networks in the brain, in terms of effective connectivity. {fmri} data are usually acquired using echo planar imaging ({epi}). {epi} sequences typically acquire slices at different times over a few seconds. {dcm}, in its original inception, was not informed about these slice timings and assumed that all slices were acquired simultaneously. it has been shown that {dcm} can cope with slice timing differences of up to 1 s. however, many {fmri} studies employ a repetition time ({tr}) of 3 to 5[no-break space]s, which precludes a straightforward {dcm} of these {data.we} show that this limitation can be overcome easily by including slice timing in the {dcm}. using synthetic data we show that the extended {dcm} furnishes veridical posterior means, even if there are large slice-timing differences. model comparisons show that, in general, the extended {dcm} out-performs the original model. we contrast the modeling of slice timing, in the context of {dcm}, with the less effective approach of `slice-timing correction', prior to modeling. we apply our procedure to real data and show that slice timings are important parameters. we conclude that, generally, one should use {dcm} with slice timing.
1285	997264	article	science	\N	\N	\N	5	295	\N	2002	feb	2006-12-15 16:48:28	\N	capturing chromosome conformation	we describe an approach to detect the frequency of interaction between any two genomic loci. generation of a matrix of interaction frequencies between sites on the same or different chromosomes reveals their relative spatial disposition and provides information about the physical properties of the chromatin fiber. this methodology can be applied to the spatial organization of entire genomes in organisms from bacteria to human. using the yeast saccharomyces cerevisiae, we could confirm known qualitative features of chromosome organization within the nucleus and dynamic changes in that organization during meiosis. we also analyzed yeast chromosome iii at the g1 stage of the cell cycle. we found that chromatin is highly flexible throughout. furthermore, functionally distinct at- and gc-rich domains were found to exhibit different conformations, and a population-average 3d model of chromosome iii could be determined. chromosome iii emerges as a contorted ring.
1286	997319	article	bmc bioinformatics	\N	\N	\N	\N	7	1	2006	\N	2006-12-15 17:04:45	department of plant sciences, university of tennessee, knoxville, tn 37996, usa. syuan@utk.edu	statistical analysis of real-time {pcr} data.	even though real-time {pcr} has been broadly applied in biomedical sciences, data processing procedures for the analysis of quantitative real-time {pcr} are still lacking; specifically in the realm of appropriate statistical treatment. confidence interval and statistical significance considerations are not explicit in many of the current data analysis approaches. based on the standard curve method and other useful data analysis methods, we present and compare four statistical approaches and models for the analysis of real-time {pcr} data. in the first approach, a multiple regression analysis model was developed to derive {deltadeltact} from estimation of interaction of gene and treatment effects. in the second approach, an {ancova} (analysis of covariance) model was proposed, and the {deltadeltact} can be derived from analysis of effects of variables. the other two models involve calculation {deltact} followed by a two group t-test and non-parametric analogous wilcoxon test. {sas} programs were developed for all four models and data output for analysis of a sample set are presented. in addition, a data quality control model was developed and implemented using {sas}. practical statistical solutions with {sas} programs were developed for real-time {pcr} data and a sample dataset was analyzed with the {sas} programs. the analysis using the various models and programs yielded similar results. data quality control and analysis procedures presented here provide statistical elements for the estimation of the relative expression of genes using real-time {pcr}.
1287	997773	article	plos comput biol	\N	\N	public library of science	\N	2	12	2006	dec	2006-12-16 15:40:19	\N	transcriptional regulation by competing transcription factor modules	gene regulatory networks lie at the heart of cellular computation. in these networks, intracellular and extracellular signals are integrated by transcription factors, which control the expression of transcription units by binding to cis-regulatory regions on the {dna}. the designs of both eukaryotic and prokaryotic cis-regulatory regions are usually highly complex. they frequently consist of both repetitive and overlapping transcription factor binding sites. to unravel the design principles of these promoter architectures, we have designed in silico prokaryotic transcriptional logic gates with predefined input–output relations using an evolutionary algorithm. the resulting cis-regulatory designs are often composed of modules that consist of tandem arrays of binding sites to which the transcription factors bind cooperatively. moreover, these modules often overlap with each other, leading to competition between them. our analysis thus identifies a new signal integration motif that is based upon the interplay between intramodular cooperativity and intermodular competition. we show that this signal integration mechanism drastically enhances the capacity of cis-regulatory domains to integrate signals. our results provide a possible explanation for the complexity of promoter architectures and could be used for the rational design of synthetic gene circuits. transcription regulatory networks are the central processing units of living cells. they allow cells to integrate different intracellular and extracellular signals to recognize patterns in, for instance, the food supply of the organism. the elementary calculations are performed at the cis-regulatory domains of genes, where transcription factors bind to the {dna} to regulate the expression level of the genes. the logic of the computations that are performed depends upon the design of the cis-regulatory region. not only in eukaryotic cells, but also in prokaryotic cells, the architectures of the cis-regulatory regions are often highly complex. they often contain long arrays of transcription factor binding sites. moreover, the binding sites often overlap with one another. hermsen, tans, and ten wolde discuss whether such complex architectures can be explained from the basic function of cis-regulatory regions to integrate signals. the authors combine a physicochemical model of prokaryotic transcription regulation with an evolutionary algorithm to design cis-regulatory constructs with predefined elementary functions. the resulting architectures make extensive use of repeating binding sites that are organized into cooperative modules. more surprisingly, these modules often overlap with each other, leading to competition between them. this interplay between intramodular cooperativity and intermodular competition is a powerful mechanism to achieve complex functionality, which may explain the daunting complexity of promoter architectures found in nature.
1288	1007150	article	physical review letters	\N	\N	\N	\N	96	\N	2006	\N	2006-12-22 10:30:47	\N	k-core organization of complex networks	we analytically describe the architecture of randomly damaged uncorrelated networks as a set of successively enclosed substructuresâ€” k -cores. the  k -core is the largest subgraph where vertices have at least  k  interconnections. we find the structure of  k -cores, their sizes, and their birthpointsâ€”the bootstrap percolation thresholds. we show that in networks with a finite mean number  z 2  of the second-nearest neighbors, the emergence of a  k -core is a hybrid phase transition. in contrast, if  z 2  diverges, the networks contain an infinite sequence of  k -cores which are ultrarobust against random damage.
1289	1008038	article	nucleic acids research	\N	\N	oxford university press	17	34	22	2006	dec	2007-01-03 13:00:44	\N	the interaction networks of structured {rnas}.	all pairwise interactions occurring between bases which could be detected in three-dimensional structures of crystallized {rna} molecules are annotated on new planar diagrams. the diagrams attempt to map the underlying complex networks of base-base interactions and, especially, they aim at conveying key relationships between helical domains: co-axial stacking, bending and all {watson-crick} as well as {non-watson}-crick base pairs. although such wiring diagrams cannot replace full stereographic images for correct spatial understanding and representation, they reveal structural similarities as well as the conserved patterns and distances between motifs which are present within the interaction networks of folded {rnas} of similar or unrelated functions. finally, the diagrams could help devising methods for meaningfully transforming {rna} structures into graphs amenable to network analysis.
1290	1009267	incollection	\N	user centered system design: new perspectives on human-computer interaction	\N	lawrence erlbaum associates	\N	\N	\N	1986	\N	2006-12-22 19:52:13	hillsdale, {nj}, {usa}	direct manipulation interfaces	direct manipulation has been lauded as a good form of interface design, and some interfaces that have this property have been well received by users. in this article we seek a cognitive account of both the advantages and disadvantages of direct manipulation interfaces. we identify two underlying phenomena that give rise to the feeling of directness. one deals with the information processing distance between the user's intentions and the facilities provided by the machine. reduction of this distance makes the interface feel direct by reducing the effort required of the user to accomplish goals. the second phenomenon concerns the relation between the input and output vocabularies of the interface language. in particular, direct manipulation requires that the system provide representations of objects that behave as if they are the objects themselves. this provides the feeling of directness of manipulation.
1291	1014999	book	\N	\N	\N	{duke university press}	\N	\N	\N	2006	jun	2006-12-26 15:50:30	\N	biocapital: the constitution of postgenomic life	{<i>biocapital</i> is a major theoretical contribution to science studies and political economy. grounding his analysis in a multi-sited ethnography of genomic research and drug development marketplaces in the united states and india, kaushik sunder rajan argues that contemporary biotechnologies such as genomics can only be understood in relation to the economic markets within which they emerge. sunder rajan conducted fieldwork in biotechnology labs and in small start-up companies in the united states (mostly in the san francisco bay area) and india (mainly in new delhi, hyderabad, and bombay) over a five-year period spanning 1999 to 2004. he draws on his research with scientists, entrepreneurs, venture capitalists, and policymakers to compare drug development in the two countries, examining the practices and goals of research, the financing mechanisms, the relevant government regulations, and the hype and marketing surrounding promising new technologies. in the process, he illuminates the global flow of ideas, information, capital, and people connected to biotech initiatives.<br><br>sunder rajan\\&rsquo;s ethnography informs his theoretically sophisticated inquiry into how the contemporary world is shaped by the marriage of biotechnology and market forces, by what he calls technoscientific capitalism. bringing marxian theories of value into conversation with foucaultian notions of biopolitics, he traces how the life sciences came to be significant producers of both economic and epistemic value in the late twentieth century and early twenty-first.}
1292	1018763	article	\N	\N	\N	\N	\N	\N	\N	2006	dec	2006-12-29 06:41:46	\N	a new era in citation and bibliometric analyses: web of science, scopus, and google scholar	academic institutions, federal agencies, publishers, editors, authors, and librarians increasingly rely on citation analysis for making hiring, promotion, tenure, funding, and/or reviewer and journal evaluation and selection decisions. the institute for scientific information's (isi) citation databases have been used for decades as a starting point and often as the only tools for locating citations and/or conducting citation analyses. isi databases (or web of science), however, may no longer be adequate as the only or even the main sources of citations because new databases and tools that allow citation searching are now available. whether these new databases and tools complement or represent alternatives to web of science (wos) is important to explore. using a group of 15 library and information science faculty members as a case study, this paper examines the effects of using scopus and google scholar (gs) on the citation counts and rankings of scholars as measured by wos. the paper discusses the strengths and weaknesses of wos, scopus, and gs, their overlap and uniqueness, quality and language of the citations, and the implications of the findings for citation analysis. the project involved citation searching for approximately 1,100 scholarly works published by the study group and over 200 works by a test group (an additional 10 faculty members). overall, more than 10,000 citing and purportedly citing documents were examined. wos data took about 100 hours of collecting and processing time, scopus consumed 200 hours, and gs a grueling 3,000 hours.
1293	1018856	article	physical review e	\N	\N	american physical society	\N	74	1	2006	jul	2006-12-29 09:32:05	\N	wikipedias: collaborative web-based encyclopedias as complex networks	wikipedia is a popular web-based encyclopedia edited freely and collaboratively by its users. in this paper we present an analysis of wikipedias in several languages as complex networks. the hyperlinks pointing from one wikipedia article to another are treated as directed links while the articles represent the nodes of the network. we show that many network characteristics are common to different language versions of wikipedia, such as their degree distributions, growth, topology, reciprocity, clustering, assortativity, path lengths, and triad significance profiles. these regularities, found in the ensemble of wikipedias in different languages and of different sizes, point to the existence of a unique growth process. we also compare wikipedias to other previously studied networks.
1294	1019645	article	lit linguist computing	\N	\N	\N	11	17	4	2002	nov	2006-12-30 08:52:53	\N	automatically categorizing written texts by author gender	the problem of automatically determining the gender of a document's author would appear to be a more subtle problem than those of categorization by topic or authorship attribution. nevertheless, it is shown that automated text categorization techniques can exploit combinations of simple lexical and syntactic features to infer the gender of the author of an unseen formal written document with approximately 80 per cent accuracy. the same techniques can be used to determine if a document is fiction or non-fiction with approximately 98 per cent accuracy. 10.1093/llc/17.4.401
1295	1021904	article	journal of theoretical biology	\N	\N	\N	17	223	1	2003	jul	2007-01-02 01:31:15	school of mathematics, university of minnesota, 127 vincent hall, minneapolis, mn 55455, usa. ralbert@math.umn.edu	the topology of the regulatory interactions predicts the expression pattern of the segment polarity genes in drosophila melanogaster.	expression of the drosophila segment polarity genes is initiated by a pre-pattern of pair-rule gene products and maintained by a network of regulatory interactions throughout several stages of embryonic development. analysis of a model of gene interactions based on differential equations showed that wild-type expression patterns of these genes can be obtained for a wide range of kinetic parameters, which suggests that the steady states are determined by the topology of the network and the type of regulatory interactions between components, not the detailed form of the rate laws. to investigate this, we propose and analyse a boolean model of this network which is based on a binary {on}/{off} representation of {mrna} and protein levels, and in which the interactions are formulated as logical functions. in this model the spatial and temporal patterns of gene expression are determined by the topology of the network and whether components are present or absent, rather than the absolute levels of the {mrnas} and proteins and the functional details of their interactions. the model is able to reproduce the wild-type gene expression patterns, as well as the ectopic expression patterns observed in overexpression experiments and various mutants. furthermore, we compute explicitly all steady states of the network and identify the basin of attraction of each steady state. the model gives important insights into the functioning of the segment polarity gene network, such as the crucial role of the wingless and sloppy paired genes, and the network's ability to correct errors in the pre-pattern.
1296	1024536	article	nucleic acids res	\N	\N	\N	\N	\N	\N	2006	dec	2007-01-04 08:12:20	department of molecular sciences memphis, tn 38163, usa.	{polymirts} database: linking polymorphisms in {microrna} target sites with complex traits.	polymorphism in {microrna} target site ({polymirts}) database is a collection of naturally occurring {dna} variations in putative {microrna} target sites. {polymirtss} may affect gene expression and cause variations in complex phenotypes. the database integrates sequence polymorphism, phenotype and expression microarray data, and characterizes {polymirtss} as potential candidates responsible for the quantitative trait locus ({qtl}) effects. it is a resource for studying {polymirtss} and their implications in phenotypic variations. {polymirts} database can be accessed at {http://compbio.utmem.edu/mirsnp}/.
1297	1026394	article	phys. rev. e	\N	\N	american physical society	\N	73	1	2006	jan	2009-07-24 20:54:12	\N	statistical properties of sampled networks	we study the statistical properties of the sampled scale-free networks, deeply related to the proper identification of various real-world networks. we exploit three methods of sampling and investigate the topological properties such as degree and betweenness centrality distribution, average path length, assortativity, and clustering coefficient of sampled networks compared with those of original networks. it is found that the quantities related to those properties in sampled networks appear to be estimated quite differently for each sampling method. we explain why such a biased estimation of quantities would emerge from the sampling procedure and give appropriate criteria for each sampling method to prevent the quantities from being overestimated or underestimated.
1298	1030375	article	nature biotechnology	nat biotech	\N	nature publishing group	7	25	1	2007	jan	2007-01-08 20:51:53	\N	absolute protein expression profiling estimates the relative contributions of transcriptional and translational regulation.	we report a method for large-scale absolute protein expression measurements (apex) and apply it to estimate the relative contributions of transcriptional- and translational-level gene regulation in the yeast and escherichia coli proteomes. apex relies upon correcting each protein's mass spectrometry sampling depth (observed peptide count) by learned probabilities for identifying the peptides. apex abundances agree with measurements from controls, western blotting, flow cytometry and two-dimensional gels, as well as known correlations with mrna abundances and codon bias, providing absolute protein concentrations across approximately three to four orders of magnitude. using apex, we demonstrate that 73\\% of the variance in yeast protein abundance (47\\% in e. coli) is explained by mrna abundance, with the number of proteins per mrna log-normally distributed about approximately 5,600 ( approximately 540 in e. coli) protein molecules/mrna. therefore, levels of both eukaryotic and prokaryotic proteins are set per mrna molecule and independently of overall protein concentration, with >70\\% of yeast gene expression regulation occurring through mrna-directed mechanisms.
1299	1031386	article	nature reviews. genetics	\N	\N	nature publishing group	9	5	7	2004	jul	2007-01-09 11:12:55	cold spring harbor laboratory, watson school of biological sciences, 1 bungtown road, cold spring harbor, new york 11724, usa.	{micrornas}: small {rnas} with a big role in gene regulation.	{micrornas} are a family of small, non-coding {rnas} that regulate gene expression in a sequence-specific manner. the two founding members of the {microrna} family were originally identified in caenorhabditis elegans as genes that were required for the timed regulation of developmental events. since then, hundreds of {micrornas} have been identified in almost all metazoan genomes, including worms, flies, plants and mammals. {micrornas} have diverse expression patterns and might regulate various developmental and physiological processes. their discovery adds a new dimension to our understanding of complex gene regulatory networks.
1300	1031537	article	journal of the american statistical association	\N	\N	american statistical association	9	96	456	2001	\N	2007-01-09 13:37:17	\N	empirical bayes analysis of a microarray experiment	microarrays are a novel technology that facilitates the simultaneous measurement of thousands of gene expression levels. a typical microarray experiment can produce millions of data points, raising serious problems of data reduction, and simultaneous inference. we consider one such experiment in which oligonucleotide arrays were employed to assess the genetic effects of ionizing radiation on seven thousand human genes. a simple nonparametric empirical bayes model is introduced, which is used to guide the efficient reduction of the data to a single summary statistic per gene, and also to make simultaneous inferences concerning which genes were affected by the radiation. although our focus is on one specific experiment, the proposed methods can be applied quite generally. the empirical bayes inferences are closely related to the frequentist false discovery rate ({fdr}) criterion.
1301	1031598	article	computer	computer	\N	ieee	2	39	6	2006	jun	2007-01-09 15:44:42	los alamitos, ca, usa	games with a purpose	through online games, people can collectively solve large-scale computational problems. such games constitute a general mechanism for using brain power to solve open problems. in fact, designing such a game is much like designing an algorithm - it must be proven correct, its efficiency can be analyzed, a more efficient version can supersede a less efficient one, and so on. "games with a purpose" have a vast range of applications in areas as diverse as security, computer vision, internet accessibility, adult content filtering, and internet search. any game designed to address these and other problems must ensure that game play results in a correct solution and, at the same time, is enjoyable. people will play such games to be entertained, not to solve a problem - no matter how laudable the objective
1302	1032933	article	plos biology	\N	\N	public library of science	\N	5	1	2007	jan	2007-01-10 08:55:03	\N	large-scale mapping and validation of escherichia coli transcriptional regulation from a compendium of expression profiles.	machine learning approaches offer the potential to systematically identify transcriptional regulatory interactions from a compendium of microarray expression profiles. however, experimental validation of the performance of these methods at the genome scale has remained elusive. here we assess the global performance of four existing classes of inference algorithms using 445 escherichia coli affymetrix arrays and 3,216 known e. coli regulatory interactions from {regulondb}. we also developed and applied the context likelihood of relatedness ({clr}) algorithm, a novel extension of the relevance networks class of algorithms. {clr} demonstrates an average precision gain of 36\\% relative to the next-best performing algorithm. at a 60\\% true positive rate, {clr} identifies 1,079 regulatory interactions, of which 338 were in the previously known network and 741 were novel predictions. we tested the predicted interactions for three transcription factors with chromatin immunoprecipitation, confirming 21 novel interactions and verifying our {regulondb}-based performance estimates. {clr} also identified a regulatory link providing central metabolic control of iron transport, which we confirmed with real-time quantitative {pcr}. the compendium of expression data compiled in this study, coupled with {regulondb}, provides a valuable model system for further improvement of network inference algorithms using experimental data.
1303	1036116	article	nucleic acids research	\N	\N	oxford university press	4	35	Database issue	2007	jan	2007-05-30 15:39:32	\N	{intact}--open source resource for molecular interaction data.	intact is an open source database and software suite for modeling, storing and analyzing molecular interaction data. the data available in the database originates entirely from published literature and is manually annotated by expert biologists to a high level of detail, including experimental methods, conditions and interacting domains. the database features over 126,000 binary interactions extracted from over 2100 scientific publications and makes extensive use of controlled vocabularies. the web site provides tools allowing users to search, visualize and download data from the repository. intact supports and encourages local installations as well as direct data submission and curation collaborations. intact source code and data are freely available from http://www.ebi.ac.uk/intact.
1304	1036807	article	nat phys	\N	\N	nature publishing group	6	3	1	2007	jan	2007-01-11 14:29:43	\N	classes of complex networks defined by role-to-role connectivity profiles	in physical, biological, technological and social systems, interactions between units give rise to intricate networks. these—typically non-trivial—structures, in turn, critically affect the dynamics and properties of the system. the focus of most current research on complex networks is, still, on global network properties. a caveat of this approach is that the relevance of global properties hinges on the premise that networks are homogeneous, whereas most real-world networks have a markedly modular structure. here, we report that networks with different functions, including the internet, metabolic, air transportation and protein interaction networks, have distinct patterns of connections among nodes with different roles, and that, as a consequence, complex networks can be classified into two distinct functional classes on the basis of their link type frequency. importantly, we demonstrate that these structural features cannot be captured by means of often studied global properties.
1305	1037601	inproceedings	\N	in: proc. of iswc-2006 international semantic web conference	\N	springer, lncs	\N	\N	\N	2006	nov	2007-01-12 10:36:37	athens, ga, usa	on how to perform a gold standard based evaluation of ontology learning	in recent years several measures for the gold standard based evaluation of ontology learning were proposed. they can be distinguished by the layers of an ontology (e.g. lexical term layer and concept hierarchy) they evaluate. judging those measures with a list of criteria we show that there exist some measures sufficient for evaluating the lexical term layer. however, existing measures for the evaluation of concept hierarchies fail to meet basic criteria. this paper presents a new taxonomic measure which overcomes the problems of current approaches.
1306	1041298	inproceedings	\N	proceedings of the sigchi conference on human factors in computing systems	chi	acm	9	\N	\N	2006	\N	2007-01-14 17:44:53	new york, ny, usa	{fathumb}: a facet-based interface for mobile search	in this paper we describe a novel approach for searching large data sets from a mobile phone. existing interfaces for mobile search require keyword text entry and are not suited for browsing. our alternative uses a hybrid model to de-emphasize tedious keyword entry in favor of iterative data filtering. we propose navigation and selection of hierarchical metadata (facet navigation), with incremental text entry to further narrow the results. we conducted a formative evaluation to understand the relative advantages of keyword entry versus facet navigation for both browse and search tasks on the phone. we found keyword entry to be more powerful when the name of the search target is known, while facet navigation is otherwise more effective and strongly preferred.
1307	1044715	article	biostatistics (oxford, england)	\N	\N	oxford university press	14	8	2	2007	apr	2007-01-16 18:06:34	department of biostatistics, johns hopkins university, baltimore md 21205.	exploration, normalization, and genotype calls of high-density oligonucleotide {snp} array data.	in most microarray technologies, a number of critical steps are required to convert raw intensity measurements into the data relied upon by data analysts, biologists, and clinicians. these data manipulations, referred to as preprocessing, can influence the quality of the ultimate measurements. in the last few years, the high-throughput measurement of gene expression is the most popular application of microarray technology. for this application, various groups have demonstrated that the use of modern statistical methodology can substantially improve accuracy and precision of the gene expression measurements, relative to ad hoc procedures introduced by designers and manufacturers of the technology. currently, other applications of microarrays are becoming more and more popular. in this paper, we describe a preprocessing methodology for a technology designed for the identification of {dna} sequence variants in specific genes or regions of the human genome that are associated with phenotypes of interest such as disease. in particular, we describe a methodology useful for preprocessing affymetrix single-nucleotide polymorphism chips and obtaining genotype calls with the preprocessed data. we demonstrate how our procedure improves existing approaches using data from 3 relatively large studies including the one in which large numbers of independent calls are available. the proposed methods are implemented in the package oligo available from bioconductor.
1308	1049337	article	proceedings of the national academy of sciences	\N	\N	\N	4	103	35	2006	aug	2007-01-18 23:15:48	\N	a climate-change risk analysis for world ecosystems	we quantify the risks of climate-induced changes in key ecosystem processes during the 21st century by forcing a dynamic global vegetation model with multiple scenarios from 16 climate models and mapping the proportions of model runs showing forest/nonforest shifts or exceedance of natural variability in wildfire frequency and freshwater supply. our analysis does not assign probabilities to scenarios or weights to models. instead, we consider distribution of outcomes within three sets of model runs grouped by the amount of global warming they simulate: <{2°c} (including simulations in which atmospheric composition is held constant, i.e., in which the only climate change is due to greenhouse gases already emitted), {2–3°c}, and >{3°c}. high risk of forest loss is shown for eurasia, eastern china, canada, central america, and amazonia, with forest extensions into the arctic and semiarid savannas; more frequent wildfire in amazonia, the far north, and many semiarid regions; more runoff north of {50°n} and in tropical africa and northwestern south america; and less runoff in west africa, central america, southern europe, and the eastern {u.s}. substantially larger areas are affected for global warming >{3°c} than for <{2°c}; some features appear only at higher warming levels. a land carbon sink of ?1 pg of c per yr is simulated for the late 20th century, but for >{3°c} this sink converts to a carbon source during the 21st century (implying a positive climate feedback) in 44\\% of cases. the risks continue increasing over the following 200 years, even with atmospheric composition held constant.
1309	1050114	article	molecular systems biology	\N	\N	nature publishing group	\N	3	\N	2007	jan	2007-01-19 08:27:43	\N	identification of tightly regulated groups of genes during drosophila melanogaster embryogenesis	time-series analysis of whole-genome expression data during drosophila melanogaster development indicates that up to 86\\% of its genes change their relative transcript level during embryogenesis. by applying conservative filtering criteria and requiring 'sharp' transcript changes, we identified 1534 maternal genes, 792 transient zygotic genes, and 1053 genes whose transcript levels increase during embryogenesis. each of these three categories is dominated by groups of genes where all transcript levels increase and/or decrease at similar times, suggesting a common mode of regulation. for example, 34\\% of the transiently expressed genes fall into three groups, with increased transcript levels between 2.5–12, 11–20, and 15–20 h of development, respectively. we highlight common and distinctive functional features of these expression groups and identify a coupling between downregulation of transcript levels and targeted protein degradation. by mapping the groups to the protein network, we also predict and experimentally confirm new functional associations.
1310	1051429	article	annual review of genetics	\N	\N	\N	48	39	1	2005	\N	2007-01-19 11:44:27	center for molecular and mitochondrial medicine and genetics, department of ecology and evolutionary biology, university of california, irvine, california 92697-3940, usa. dwallace@uci.edu	a mitochondrial paradigm of metabolic and degenerative diseases, aging, and cancer: a dawn for evolutionary medicine.	abstract life is the interplay between structure and energy, yet the role of energy deficiency in human disease has been poorly explored by modern medicine. since the mitochondria use oxidative phosphorylation (oxphos) to convert dietary calories into usable energy, generating reactive oxygen species (ros) as a toxic by-product, i hypothesize that mitochondrial dysfunction plays a central role in a wide range of age-related disorders and various forms of cancer. because mitochondrial dna (mtdna) is present in thousands of copies per cell and encodes essential genes for energy production, i propose that the delayed-onset and progressive course of the age-related diseases results from the accumulation of somatic mutations in the mtdnas of post-mitotic tissues. the tissue-specific manifestations of these diseases may result from the varying energetic roles and needs of the different tissues. the variation in the individual and regional predisposition to degenerative diseases and cancer may result from the interaction of modern dietary caloric intake and ancient mitochondrial genetic polymorphisms. therefore the mitochondria provide a direct link between our environment and our genes and the mtdna variants that permitted our forbears to energetically adapt to their ancestral homes are influencing our health today.
1311	1056288	article	bioinformatics	\N	\N	oxford university press	6	23	4	2007	feb	2007-01-20 19:51:24	\\'{e}quipe de statistique appliqu\\'{e}e, \\'{e}cole sup\\'{e}rieure de physique et de chimie industrielles (espci), 10 rue vauquelin, 75005 paris, france.	enrichment or depletion of a {go} category within a class of genes: which test?	motivation: a number of available program packages determine the significant enrichments and/or depletions of {go} categories among a class of genes of interest. whereas a correct formulation of the problem leads to a single exact null distribution, these {go} tools use a large variety of statistical tests whose denominations often do not clarify the underlying p-value computations.
1312	1059893	misc	\N	iswc	\N	\N	\N	\N	\N	2006	\N	2007-01-22 14:32:33	\N	extending faceted navigation for {rdf} data	data on the semantic web is semi-structured and does not follow one fixed schema. faceted browsing [23] is a natural technique for navigating such data, partitioning the information space into orthogonal conceptual dimensions. current faceted interfaces are manually constructed and have limited query expressiveness. we develop an expressive faceted interface for semi-structured data and formally show the improvement over existing interfaces. secondly, we develop metrics for automatic ranking of ...
1313	1060049	article	nature	\N	\N	nature publishing group	4	445	7128	2007	feb	2007-02-08 10:53:48	\N	repression of the human dihydrofolate reductase gene by a non-coding interfering transcript	alternative promoters within the same gene are a general phenomenon in gene expression1, 2. mechanisms of their selective regulation vary from one gene to another and are still poorly understood. here we show that in quiescent cells the mechanism of transcriptional repression of the major promoter of the gene encoding dihydrofolate reductase depends on a non-coding transcript initiated from the upstream minor promoter and involves both the direct interaction of the {rna} and promoter-specific interference. the specificity and efficiency of repression is ensured by the formation of a stable complex between non-coding {rna} and the major promoter, direct interaction of the non-coding {rna} with the general transcription factor {iib} and dissociation of the preinitiation complex from the major promoter. by using in vivo and in vitro assays such as inducible and reconstituted transcription, {rna} bandshifts, {rna} interference, chromatin immunoprecipitation and {rna} immunoprecipitation, we show that the regulatory transcript produced from the minor promoter has a critical function in an epigenetic mechanism of promoter-specific transcriptional repression.
1314	1062025	article	plos comput biol	\N	\N	public library of science	\N	3	1	2007	jan	2007-01-23 14:08:16	\N	{large-scale} discovery of promoter motifs in drosophila melanogaster	a key step in understanding gene regulation is to identify the repertoire of transcription factor binding motifs ({tfbms}) that form the building blocks of promoters and other regulatory elements. identifying these experimentally is very laborious, and the number of {tfbms} discovered remains relatively small, especially when compared with the hundreds of transcription factor genes predicted in metazoan genomes. we have used a recently developed statistical motif discovery approach, {nestedmica}, to detect candidate {tfbms} from a large set of drosophila melanogaster promoter regions. of the 120 motifs inferred in our initial analysis, 25 were statistically significant matches to previously reported motifs, while 87 appeared to be novel. analysis of sequence conservation and motif positioning suggested that the great majority of these discovered motifs are predictive of functional elements in the genome. many motifs showed associations with specific patterns of gene expression in the d. melanogaster embryo, and we were able to obtain confident annotation of expression patterns for 25 of our motifs, including eight of the novel motifs. the motifs are available through tiffin, a new database of {dna} sequence motifs. we have discovered many new motifs that are overrepresented in d. melanogaster promoter regions, and offer several independent lines of evidence that these are novel {tfbms}. our motif dictionary provides a solid foundation for further investigation of regulatory elements in drosophila, and demonstrates techniques that should be applicable in other species. we suggest that further improvements in computational motif discovery should narrow the gap between the set of known motifs and the total number of transcription factors in metazoan genomes. in contrast to the genomic sequences that encode proteins, little is known about the regulatory elements that instruct the cell as to when and where a given gene should be active. regulatory elements are thought to consist of clusters of short {dna} words (motifs), each of which acts as a binding site for sequence-specific {dna} binding protein. thus, building a comprehensive dictionary of such motifs is an important step towards a broader understanding of gene regulation. using the recently published {nestedmica} method for detecting overrepresented motifs in a set of sequences, we build a dictionary of 120 motifs from regulatory sequences in the fruitfly genome, 87 of which are novel. analysis of positional biases, conservation across species, and association with specific patterns of gene expression in fruitfly embryos suggest that the great majority of these newly discovered motifs represent functional regulatory elements. in addition to providing an initial motif dictionary for one of the most intensively studied model organisms, this work provides an analytical framework for the comprehensive discovery of regulatory motifs in complex animal genomes.
1315	1067406	article	proceedings of the national academy of sciences of the united states of america	\N	\N	\N	5	103	8	2006	feb	2007-01-25 19:16:22	department of genome sciences, university of washington, wa 98195, usa.	automated cell lineage tracing in caenorhabditis elegans.	the invariant cell lineage and cell fate of caenorhabditis elegans provide a unique opportunity to decode the molecular mechanisms of animal development. to exploit this opportunity, we have developed a system for automated cell lineage tracing during c. elegans embryogenesis, based on {3d}, time-lapse imaging and automated image analysis. using ubiquitously expressed {histone-gfp} fusion protein to label cells/nuclei and a confocal microscope, the imaging protocol captures embryogenesis at high spatial (31 planes at 1 microm apart) and temporal (every minute) resolution without apparent effects on development. a set of image analysis algorithms then automatically recognizes cells at each time point, tracks cell movements, divisions and deaths over time and assigns cell identities based on the canonical naming scheme. starting from the four-cell stage (or earlier), our software, named starrynite, can trace the lineage up to the 350-cell stage in 25 min on a desktop computer. the few errors of automated lineaging can then be corrected in a few hours with a graphic interface that allows easy navigation of the images and the reported lineage tree. the system can be used to characterize lineage phenotypes of genes and/or extended to determine gene expression patterns in a living embryo at the single-cell level. we envision that this automation will make it practical to systematically decipher the developmental genes and pathways encoded in the genome of c. elegans.
1316	1068847	book	\N	\N	\N	cambridge university press	\N	\N	\N	2006	dec	2007-01-26 08:11:15	\N	data analysis using regression and {multilevel/hierarchical} models	{data analysis using regression and multilevel/hierarchical models is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. the book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. the book illustrates the concepts by working through scores of real data examples that have arisen from the authors' own applied research, with programming codes provided for each one. topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. practical tips regarding building, fitting, and understanding are provided throughout.}
1317	1069345	article	science	\N	\N	american association for the advancement of science	2	315	5811	2007	jan	2007-01-26 15:35:50	\N	a virus in a fungus in a plant: {three-way} symbiosis required for thermal tolerance	a mutualistic association between a fungal endophyte and a tropical panic grass allows both organisms to grow at high soil temperatures. we characterized a virus from this fungus that is involved in the mutualistic interaction. fungal isolates cured of the virus are unable to confer heat tolerance, but heat tolerance is restored after the virus is reintroduced. the virus-infected fungus confers heat tolerance not only to its native monocot host but also to a eudicot host, which suggests that the underlying mechanism involves pathways conserved between these two groups of plants.
1318	1069750	article	\N	proceedings of the 2007 international conference on intelliget user interfaces	\N	\N	\N	\N	\N	2007	\N	2007-01-26 19:32:19	\N	unsupervised and supervised machine learning in user modeling for intelligent learning environments.	in this research, we outline a user modeling framework that uses both unsupervised and supervised machine learning in order to reduce development costs of building user models, and facilitate transferability. we apply the framework to model student learning during interaction with the adaptive coach for exploration (ace) learning environment (using both interface and eye-tracking data). in addition to demonstrating framework effectiveness, we also compare results from previous research on applying the framework to a different learning environment and data type. our results also confirm previous research on the value of using eye-tracking data to assess student learning.
1319	1082136	book	\N	\N	\N	emerald group publishing limited	\N	\N	\N	2006	dec	2007-02-01 15:14:01	\N	looking for information: a survey of research on information seeking, needs, and behavior (library and information science)	{<i>looking for information</i> explores human information seeking and use. it provides examples of methods, models and theories used in information behavior research, and reviews more than four decades of research on the topic. the book should prove useful for scholars in related fields, but also for students at the graduate and advanced undergraduate levels. it is intended for use not only in information studies and communication, but also in the disciplines of education, management, business, medicine, nursing, public health, and social work. <br><br>this second editon of <i>looking for information</i> reflects a vastly increased literature on the topic of information behavior. among the additions are over 400 new citations to relevant works, most of which appeared between march, 2002, and january, 2006. many new studies are described in the section reviewing research findings (chapters eleven and twelve), chapter nines examples of methods, and a widely expanded discussion of theories applied in information behavior research (chapter seven).<br><br>*reviews over 1,100 works -- 60\\% more than the first edition<br>*adds many new studies conducted from 2002 to 2006<br>*expanded coverage of models and theories of information behavior<br>*many new examples of occupations and roles -- the contexts of information seeking}
1320	1086538	article	neuroimage	\N	\N	\N	9	35	1	2007	mar	2007-02-04 05:31:40	department of neurology, washington university school of medicine, campus box 8111, 660 s. euclid, st. louis, mo 63110, usa.	a method for using blocked and event-related {fmri} data to study "resting state" functional connectivity.	resting state functional connectivity mri (fcmri) has become a particularly useful tool for studying regional relationships in typical and atypical populations. because many investigators have already obtained large data sets of task-related fmri, the ability to use this existing task data for resting state fcmri is of considerable interest. two classes of data sets could potentially be modified to emulate resting state data. these data sets include: (1) "interleaved" resting blocks from blocked or mixed blocked/event-related sets, and (2) residual timecourses from event-related sets that lack rest blocks. using correlation analysis, we compared the functional connectivity of resting epochs taken from a mixed blocked/event-related design fmri data set and the residuals derived from event-related data with standard continuous resting state data to determine which class of data can best emulate resting state data. we show that, despite some differences, the functional connectivity for the interleaved resting periods taken from blocked designs is both qualitatively and quantitatively very similar to that of "continuous" resting state data. in contrast, despite being qualitatively similar to "continuous" resting state data, residuals derived from event-related design data had several distinct quantitative differences. these results suggest that the interleaved resting state data such as those taken from blocked or mixed blocked/event-related fmri designs are well-suited for resting state functional connectivity analyses. although using event-related data residuals for resting state functional connectivity may still be useful, results should be interpreted with care.
1321	1088311	article	magnetic resonance imaging	\N	\N	\N	10	22	9	2004	nov	2007-03-07 03:22:00	\N	a method for comparing group {fmri} data using independent component analysis: application to visual, motor and visuomotor tasks	independent component analysis ({ica}) is an approach for decomposing {fmri} data into spatially independent maps and time courses. we have recently proposed a method for {ica} of multisubject data; in the current paper, an extension is proposed for allowing {ica} group comparisons. this method is applied to data from experiments designed to stimulate visual cortex, motor cortex or both visual and motor cortices. several intergroup and intragroup metrics are proposed for assessing the utility of the components for comparisons of group {ica} data. the proposed method may prove to be useful in answering questions requiring multigroup comparisons when a flexible modeling approach is desired.
1322	1088398	article	molecular \\& cellular proteomics	\N	\N	american society for biochemistry and molecular biology	11	6	3	2007	mar	2007-02-05 14:08:13	department of cellular and molecular pharmacology, university of california, san francisco, san francisco, ca 94158.	toward a comprehensive atlas of the physical interactome of saccharomyces cerevisiae	defining protein complexes is critical to virtually all aspects of cell biology. two recent affinity purification/mass spectrometry studies in saccharomyces cerevisiae have vastly increased the available protein interaction data. the practical utility of such high throughput interaction sets, however, is substantially decreased by the presence of false positives. here we created a novel probabilistic metric that takes advantage of the high density of these data, including both the presence and absence of individual associations, to provide a measure of the relative confidence of each potential protein-protein interaction. this analysis largely overcomes the noise inherent in high throughput immunoprecipitation experiments. for example, of the 12,122 binary interactions in the general repository of interaction data ({biogrid}) derived from these two studies, we marked 7504 as being of substantially lower confidence. additionally, applying our metric and a stringent cutoff we identified a set of 9074 interactions (including 4456 that were not among the 12,122 interactions) with accuracy comparable to that of conventional small scale methodologies. finally we organized proteins into coherent multisubunit complexes using hierarchical clustering. this work thus provides a highly accurate physical interaction map of yeast in a format that is readily accessible to the biological community.
1323	1090474	article	science (new york, n.y.)	\N	\N	american association for the advancement of science	5	314	5796	2006	oct	2008-02-18 05:02:03	\N	modeling single-neuron dynamics and computations: a balance of detail and abstraction.	the fundamental building block of every nervous system is the single neuron. understanding how these exquisitely structured elements operate is an integral part of the quest to solve the mysteries of the brain. quantitative mathematical models have proved to be an indispensable tool in pursuing this goal. we review recent advances and examine how single-cell models on five levels of complexity, from black-box approaches to detailed compartmental simulations, address key questions about neural dynamics and signal processing.
1324	1091142	article	california management review	\N	\N	\N	14	40	3	1998	\N	2007-02-06 21:31:36	\N	the concept of "ba": building a foundation for knowledge creation	a paper introduces the japanese concept of "ba" to organizational theory. ba (equivalent to "place" in english) is a shared space for emerging relationships. it can be a physical, virtual, or mental space. knowledge, in contrast to information, cannot be separated from the context - it is embedded in ba. to support the process of knowledge creation, a foundation in ba is required. the paper develops and explains 4 specific platforms and their relationships to knowledge creation. each of the knowledge conversion modes is promoted by a specific ba. a self-transcending process of knowledge creation can be supported by providing ba on different organizational levels. this article presents case studies of 3 companies that employ ba on the team, division, and corporate level to enhance knowledge creation.
1325	1094357	book	\N	\N	\N	igi global	\N	\N	\N	2006	sep	2007-02-08 07:32:16	\N	games and simulations in online learning: research and development frameworks	{nearly all early learning happens during play, and new technology has added video games to the list of ways children learn interaction and new concepts. although video games are everywhere, on web sites, in stores, streamed to the desktop, on television, they are absent from the classroom. computer-based simulations, a form of computer games, have begun to appear, but they are not as wide-spread as email, discussion threads, and blogs.    games and simulations in online learning: research and development frameworks examines the potential of games and simulations in online learning, and how the future could look as developers learn to use the emerging capabilities of the semantic web. it presents a general understanding of how the semantic web will impact education and how games and simulations can evolve to become robust teaching resources.}
1326	1096254	article	bmc bioinformatics	\N	\N	\N	\N	8	1	2007	feb	2007-02-11 07:27:33	\N	improved precision and accuracy for microarrays using updated probe set definitions.	background: microarrays enable high throughput detection of transcript expression levels. different investigators have recently introduced updated probe set definitions to more accurately map probes to our current knowledge of genes and transcripts. results: we demonstrate that updated probe set definitions provide both better precision and accuracy in probe set estimates compared to the original affymetrix definitions. we show that the improved precision mainly depends on the increased number of probes that are integrated into each probe set, but we also demonstrate an improvement when the same number of probes is used. conclusion: updated probe set definitions does not only offer expression levels that are more accurately associated to genes and transcripts but also improvements in the estimated transcript expression levels. these results give support for the use of updated probe set definitions for analysis and meta-analysis of microarray data.
1327	1097083	article	bmc bioinformatics	\N	\N	\N	\N	8	1	2007	\N	2007-02-09 15:05:02	\N	{bioinfer}: a corpus for information extraction in the biomedical domain	{background}:lately, there has been a great interest in the application of information extraction methods to the biomedical domain, in particular, to the extraction of relationships of genes, proteins, and {rna} from scientific publications. the development and evaluation of such methods requires annotated domain {corpora.results}:we present {bioinfer} (bio information extraction resource), a new public resource providing an annotated corpus of biomedical english. we describe an annotation scheme capturing named entities and their relationships along with a dependency analysis of sentence syntax. we further present ontologies defining the types of entities and relationships annotated in the corpus. currently, the corpus contains 1100 sentences from abstracts of biomedical research articles annotated for relationships, named entities, as well as syntactic dependencies. supporting software is provided with the corpus. the corpus is unique in the domain in combining these annotation types for a single set of sentences, and in the level of detail of the relationship {annotation.conclusion}:we introduce a corpus targeted at protein, gene, and {rna} relationships which serves as a resource for the development of information extraction systems and their components such as parsers and domain analyzers. the corpus will be maintained and further developed with a current version being available at {http://www.it.utu.fi/bioinfer} webcite.
1328	1103162	article	plos comput biol	\N	\N	public library of science	\N	2	11	2006	nov	2007-02-12 12:18:24	\N	identification of the {proliferation/differentiation} switch in the cellular network of multicellular organisms	the protein\\^{a}€ ” protein interaction networks, or interactome networks, have been shown to have dynamic modular structures, yet the functional connections between and among the modules are less well understood. here, using a new pipeline to integrate the interactome and the transcriptome, we identified a pair of transcriptionally anticorrelated modules, each consisting of hundreds of genes in multicellular interactome networks across different individuals and populations. the two modules are associated with cellular proliferation and differentiation, respectively. the proliferation module is conserved among eukaryotic organisms, whereas the differentiation module is specific to multicellular organisms. upon differentiation of various tissues and cell lines from different organisms, the expression of the proliferation module is more uniformly suppressed, while the differentiation module is upregulated in a tissue- and species-specific manner. our results indicate that even at the tissue and organism levels, proliferation and differentiation modules may correspond to two alternative states of the molecular network and may reflect a universal symbiotic relationship in a multicellular organism. our analyses further predict that the proteins mediating the interactions between these modules may serve as modulators at the proliferation/differentiation switch.
1329	1105908	article	journal of theoretical biology	\N	\N	\N	10	208	1	2001	jan	2007-02-13 21:14:48	university of michigan, 701 tappan road, d3276, ann arbor, mi 48109-1234, usa. henrich@umich.edu	why people punish defectors	in this paper, we present a cultural evolutionary model in which norms for cooperation and punishment are acquired via two cognitive mechanisms: (1) payoff-biased transmission—a tendency to copy the most successful individual; and (2) conformist transmission—a tendency to copy the most frequent behavior in the population. we first show that if a finite number of punishment stages is permitted (e.g. two stages of punishment occur if some individuals punish people who fail to punish non-cooperators), then an arbitrarily small amount of conformist transmission will stabilize cooperative behavior by stabilizing punishment at some n -th stage. we then explain how, once cooperation is stabilized in one group, it may spread through a multi-group population via cultural group selection. finally, once cooperation is prevalent, we show how prosocial genes favoring cooperation and punishment may invade in the wake of cultural group selection.
1330	1106067	inproceedings	\N	proc. of the international joint conference on artificial intelligence (ijcai)	\N	\N	\N	\N	\N	2007	jan	2007-02-13 23:23:47	\N	open information extraction from the web	traditionally, information extraction (ie) has fo- cused on satisfying precise, narrow, pre-speciï¬ed requests from small homogeneous corpora (e.g., extract the location and time of seminars from a set of announcements). shifting to a new domain requires the user to name the target relations and to manually create new extraction rules or hand-tag new training examples. this manual labor scales linearly with the number of target relations. this paper introduces open ie (oie), a new ex- traction paradigm where the system makes a single data-driven pass over its corpus and extracts a large set of relational tuples without requiring any human input. the paper also introduces t ext runner, a fully implemented, highly scalable oie system where the tuples are assigned a probability and indexed to support efï¬cient extraction and explo- ration via user queries. we report on experiments over a 9,000,000 web page corpus that compare t ext runner with k now i ta ll, a state-of-the-art web ie system. t ext runner achieves an error reduction of 33% on a comparable set of extractions. furthermore, in the amount of time it takes k now i ta ll to per- form extraction for a handful of pre-speciï¬ed re- lations, t ext runner extracts a far broader set of facts reï¬‚ecting orders of magnitude more rela- tions, discovered on the ï¬‚y. we report statistics on t ext runnerâ€™s 11,000,000 highest probability tuples, and show that they contain over 1,000,000 concrete facts and over 6,500,000 more abstract as- sertions.
1331	1106569	article	acm comput. surv.	\N	\N	acm press	46	30	2	1998	jun	2007-02-14 09:16:01	new york, ny, usa	models and languages for parallel computation	we survey parallel programming models and languages using six criteria to assess their suitability for realistic portable parallel programming. we argue that an ideal model should by easy to program, should have a software development methodology, should be architecture-independent, should be easy to understand, should guarantee performance, and should provide accurate information about the cost of programs. these criteria reflect our belief that developments in parallelism must be driven by a parallel software industry based on portability and efficiency. we consider programming models in six categories, depending on the level of abstraction they provide. those that are very abstract conceal even the presence of parallelism at the software level. such models make software easy to  build and port, but efficient and predictable performance is usually hard to achieve. at the other end of the spectrum, low-level models make all of the messy issues of parallel programming explicit (how many threads, how to place them, how to express communication, and how to schedule communication), so that software is hard to build and not very portable, but is usually efficient. most recent models are near the center of this spectrum, exploring the best tradeoffs between expressiveness and performance. a few models have achieved both abstractness and efficiency. both kinds of models raise the possibility of parallelism as part of the mainstream of computing.
1332	1107595	article	reviews of modern physics	\N	\N	american physical society	49	78	4	2006	oct	2007-02-15 00:39:18	\N	supercontinuum generation in photonic crystal fiber	a topical review of numerical and experimental studies of supercontinuum generation in photonic crystal fiber is presented over the full range of experimentally reported parameters, from the femtosecond to the continuous-wave regime. results from numerical simulations are used to discuss the temporal and spectral characteristics of the supercontinuum, and to interpret the physics of the underlying spectral broadening processes. particular attention is given to the case of supercontinuum generation seeded by femtosecond pulses in the anomalous group velocity dispersion regime of photonic crystal fiber, where the processes of soliton fission, stimulated raman scattering, and dispersive wave generation are reviewed in detail. the corresponding intensity and phase stability properties of the supercontinuum spectra generated under different conditions are also discussed.
1333	1109408	proceedings	web intelligence, 2005. proceedings. the 2005 ieee/wic/acm international conference on	\N	\N	\N	6	\N	\N	2005	\N	2007-02-16 07:22:38	\N	personalized search based on user search histories	user profiles, descriptions of user interests, can be used by search engines to provide personalized search results. many approaches to creating user profiles collect user information through proxy servers (to capture browsing histories) or desktop bots (to capture activities on a personal computer). both these techniques require participation of the user to install the proxy server or the bot. in this study, we explore the use of a less-invasive means of gathering user information for personalized search. in particular, we build user profiles based on activity at the search site itself and study the use of these profiles to provide personalized search results. by implementing a wrapper around the google search engine, we were able to collect information about individual user search activities. in particular, we collected the queries for which at least one search result was examined, and the snippets (titles and summaries) for each examined result. user profiles were created by classifying the collected information (queries or snippets) into concepts in a reference concept hierarchy. these profiles were then used to re-rank the search results and the rank-order of the user-examined results before and after re-ranking were compared. our study found that user profiles based on queries were as effective as those based on snippets. we also found that our personalized re-ranking resulted in a 34\\% improvement in the rankorder of the user-selected results.
1334	1110638	article	nat rev genet	\N	\N	nature publishing group	10	8	3	2007	mar	2007-02-20 12:35:13	\N	the evolutionary significance of cis-regulatory mutations	for decades, evolutionary biologists have argued that changes in cis-regulatory sequences constitute an important part of the genetic basis for adaptation. although originally based on first principles, this claim is now empirically well supported: numerous studies have identified cis-regulatory mutations with functionally significant consequences for morphology, physiology and behaviour. the focus has now shifted to considering whether cis-regulatory and coding mutations make qualitatively different contributions to phenotypic evolution. cases in which parallel mutations have produced parallel trait modifications in particular suggest that some phenotypic changes are more likely to result from cis-regulatory mutations than from coding mutations.
1335	1111375	article	bioinformatics (oxford, england)	\N	\N	oxford university press	7	23	8	2007	apr	2007-02-18 10:20:36	leiden university medical center, dept. of medical statistics and bioinformatics, postzone s5-p, p.o. box 9600, 2300 rc leiden, the netherlands. seminar f\\"{u}r statistik, eth zurich, ch-8092 z\\"{u}rich, switzerland.	analyzing gene expression data in terms of gene sets: methodological issues.	many statistical tests have been proposed in recent years for analyzing gene expression data in terms of gene sets, usually from gene ontology. these methods are based on widely different methodological assumptions. some approaches test differential expression of each gene set against differential expression of the rest of the genes, whereas others test each gene set on its own. also, some methods are based on a model in which the genes are the sampling units, whereas others treat the subjects as the sampling units. this article aims to clarify the assumptions behind different approaches and to indicate a preferential methodology of gene set testing. we identify some crucial assumptions which are needed by the majority of methods. p-values derived from methods that use a model which takes the genes as the sampling unit are easily misinterpreted, as they are based on a statistical model that does not resemble the biological experiment actually performed. furthermore, because these models are based on a crucial and unrealistic independence assumption between genes, the p-values derived from such methods can be wildly anti-conservative, as a simulation experiment shows. we also argue that methods that competitively test each gene set against the rest of the genes create an unnecessary rift between single gene testing and gene set testing.
1336	1116783	inproceedings	\N	proceedings of chi	\N	\N	\N	\N	\N	2007	\N	2007-02-21 18:25:15	\N	{over-exposed}? privacy patterns and considerations in online and mobile photo sharing	as sharing personal media online becomes easier and widely spread, new privacy concerns emerge – especially when the persistent nature of the media and associated context reveals details about the physical and social context in which the media items were created. in a first-of-its-kind study, we use context-aware camerephone devices to examine privacy decisions in mobile and online photo sharing. through data analysis on a corpus of privacy decisions and associated context data from a real-world system, we identify relationships between location of photo capture and photo privacy settings. our data analysis leads to further questions which we investigate through a set of interviews with 15 users. the interviews reveal common themes in privacy considerations: security, social disclosure, identity and convenience. finally, we highlight several implications and opportunities for design of media sharing applications, including using past privacy patterns to prevent oversights and errors.
1337	1116982	article	ieee transactions on systems, man and cybernetics	\N	\N	\N	4	9	1	1979	jan	2007-02-21 21:15:56	\N	a threshold selection method from gray-level histograms	a nonparametric and unsupervised method of automatic threshold selection for picture segmentation is presented. an optimal threshold is selected by the discriminant criterion, namely, so as to maximize the separability of the resultant classes in gray levels. the procedure is very simple, utilizing only the zeroth- and the first-order cumulative moments of the gray-level histogram. it is straightforward to extend the method to multithreshold problems. several experimental results are also presented to support the validity of the method.
1338	1117170	article	acm comput. surv.	\N	\N	acm	53	22	4	1990	dec	2007-02-22 03:44:44	new york, ny, usa	distributed file systems: concepts and examples	the purpose of a distributed file system ({dfs}) is to allow users of physically distributed computers to share data and storage resources by using a common file system. a typical configuration for a {dfs} is a collection of workstations and mainframes connected by a local area network ({lan}). a {dfs} is implemented as part of the operating system of each of the connected computers. this paper establishes a viewpoint that emphasizes the dispersed structure and decentralization of both data and control in the design of such systems. it defines the concepts of transparency, fault tolerance, and scalability and discusses them in the context of {dfss}. the paper claims that the principle of distributed operation is fundamental for a fault tolerant and scalable {dfs} design. it also presents   alternatives for the semantics of sharing and methods for providing access to remote files. a survey of contemporary {unix}-based systems, namely, {unix} united, locus, sprite, sun's network file system, and {itc}'s andrew, illustrates the concepts and demonstrates various implementations and design alternatives. based on the assessment of these systems, the paper makes the point that a departure from the extending centralized file systems over a communication network is necessary to accomplish sound distributed file system design.
1339	1118438	article	the quarterly journal of economics	\N	\N	\N	24	108	3	1993	\N	2007-02-23 02:20:53	\N	the {o-ring} theory of economic development	this paper proposes a production function describing processes subject to mistakes in any of several tasks. it shows that high-skill workers--those who make few mistakes--will be matched together in equilibrium, and that wages and output will rise steeply in skill. the model is consistent with large income differences between countries, the predominance of small firms in poor countries, and the positive correlation between the wages of workers in different occupations within enterprises. imperfect observability of skill leads to imperfect matching and thus to spillovers, strategic complementarity, and multiple equilibria in education.
1340	1121438	inproceedings	\N	proc. roy. soc. a (in submission)	\N	\N	\N	\N	\N	2006	\N	2007-02-25 20:58:57	\N	eigenbehaviors: identifying structure in routine	abstract&nbsp;&nbsp;longitudinal behavioral data generally contains a significant amount of structure. in this work, we identify the structure inherent in daily behavior with models that can accurately analyze, predict, and cluster multimodal data from individuals and communities within the social network of a population. we represent this behavioral structure by the principal components of the complete behavioral dataset, a set of characteristic vectors we have termed eigenbehaviors. in our model, an individualâ€™s behavior over a specific day can be approximated by a weighted sum of his or her primary eigenbehaviors. when these weights are calculated halfway through a day, they can be used to predict the dayâ€™s remaining behaviors with 79% accuracy for our test subjects. additionally, we demonstrate the potential for this dimensionality reduction technique to infer community affiliations within the subjectsâ€™ social network by clustering individuals into a â€œbehavior spaceâ€ spanned by a set of their aggregate eigenbehaviors. these behavior spaces make it possible to determine the behavioral similarity between both individuals and groups, enabling 96% classification accuracy of community affiliations within the population-level social network. additionally, the distance between individuals in the behavior space can be used as an estimate for relational ties such as friendship, suggesting strong behavioral homophily amongst the subjects. this approach capitalizes on the large amount of rich data previously captured during the reality mining study from mobile phones continuously logging location, proximate phones, and communication of 100 subjects at mit over the course of 9&nbsp;months. as wearable sensors continue to generate these types of rich, longitudinal datasets, dimensionality reduction techniques such as eigenbehaviors will play an increasingly important role in behavioral research.
1341	1122346	article	\N	acl 2005	\N	\N	\N	\N	\N	2005	\N	2007-02-26 09:09:04	\N	a {high-performance} {semi-supervised} learning method for text chunking	in machine learning, whether one can build a more accurate classiï¬er by using unlabeled data (semi-supervised learning) is an important issue. although a num- ber of semi-supervised methods have been proposed, their effectiveness on nlp tasks is not always clear. this paper presents a novel semi-supervised method that em- ploys a learning paradigm which we call structural learning. the idea is to ï¬nd â€œwhat good classiï¬ers are likeâ€ by learn- ing from thousands of automatically gen- erated auxiliary classiï¬cation problems on unlabeled data. by doing so, the common predictive structure shared by the multiple classiï¬cation problems can be discovered, which can then be used to improve perfor- mance on the target problem. the method produces performance higher than the pre- vious best results on conllâ€™00 syntac- tic chunking and conllâ€™03 named entity chunking (english and german).
1342	1123052	inproceedings	\N	wi-iat 2006 workshops)(wi-iatw	\N	ieee computer society	3	\N	\N	2006	\N	2007-02-26 14:41:32	washington, dc, usa	{remarkables}: a {web-based} research collaboration support system using social bookmarking tools	the paper describes design and implementation of a web-based research collaboration support system (rcss) using social bookmarking tools for scientific research communities. a key feature of the developed system remarkables is that it provides topic bookmarks function to build research communities in which the members can share bookmarks with specific tags and communicate with each other using mailing list and wiki pages associated with a topic bookmark list. also, the system provides topic recommender function to promote the research collaboration using the topic bookmark list. the system is implemented as an extension of connotea social bookmarking tools as open source software.
1343	1125833	article	library review	\N	\N	emerald group publishing limited	11	56	1	2007	\N	2007-03-18 11:41:25	\N	digital libraries and the future of the library profession	abstract: purpose â€“ to argue that unique contemporary cultural shifts are leading to a new form of librarianship that can be characterised as â€œpostmodernâ€ in nature, and that this form of professional specialism will be increasingly influential in the decades to come. design/methodology/approach â€“ a theoretical piece based on ideas from cultural history. findings â€“ that postmodern library and information science {(lis)} concepts will be a vital new strand to professional practice, but they will most likely subsist alongside more familiar concepts of practice which have proved readily applicable in the early years of â€œfirst waveâ€ web technologies. research limitations/implications â€“ these are purely conceptual approaches to {lis} and need to be investigated evidentially. practical implications â€“ the change from â€œfirst waveâ€ web technologies to web 2.0 information technologies may have a greater impact on future techniques in digital librarianship than the change from print to the first electronic libraries in the 1990s. originality/value â€“ this {lis} paper is distinctive in that it borrows original ideas from the humanities to offer an understanding of {lis} practice in the context of broad â€œcultural theoryâ€, rather than in the narrower context of change in mechanical and technological processes
1344	1126745	inproceedings	\N	\N	\N	\N	\N	\N	\N	-1	\N	2007-02-27 10:09:50	\N	building quality assurance into metadata creation: an analysis based on the learning objects and {e-prints} communities of practice	this paper challenges some of the assumptions underlying the metadata creation process in the context of two communities of practice, based around learning object repositories and open e- print archives. the importance of quality assurance for metadata creation is discussed and evidence from the literature, from the practical experiences of repositories and archives, and from related research and practices within other communities is presented. issues for debate and further investigation are identified, formulated as a series of key research questions. although there is much work to be done in the area of quality assurance for metadata creation, this paper represents an important first step towards a fuller understanding of the subject.
1345	1126765	article	bmc bioinformatics	\N	\N	\N	\N	8	1	2007	feb	2007-03-01 11:28:55	\N	minimus: a fast, lightweight genome assembler	{background}:genome assemblers have grown very large and complex in response to the need for algorithms to handle the challenges of large whole-genome sequencing projects. many of the most common uses of assemblers, however, are best served by a simpler type of assembler that requires fewer software components, uses less memory, and is far easier to install and {run.results}:we have developed the minimus assembler to address these issues, and tested it on a range of assembly problems. we show that minimus performs well on several small assembly tasks, including the assembly of viral genomes, individual genes, and {bac} clones. in addition, we evaluate minimus' performance in assembling bacterial genomes in order to assess its suitability as a component of a larger assembly pipeline. we show that, unlike other software currently used for these tasks, minimus produces significantly fewer assembly errors, at the cost of generating a more fragmented {assembly.conclusion}:we find that for small genomes and other small assembly tasks, minimus is faster and far more flexible than existing tools. due to its small size and modular design minimus is perfectly suited to be a component of complex assembly pipelines. minimus is released as an open-source software project and the code is available as part of the {amos} project at sourceforge.
1346	1130486	article	proceedings of the national academy of sciences	\N	\N	national academy of sciences	5	104	9	2007	feb	2007-02-28 17:09:58	\N	human polymorphism at {micrornas} and {microrna} target sites	{micrornas} ({mirnas}) function as endogenous translational repressors of protein-coding genes in animals by binding to target sites in the 3? {utrs} of {mrnas}. because a single nucleotide change in the sequence of a target site can affect {mirna} regulation, naturally occurring {snps} in target sites are candidates for functional variation that may be of interest for biomedical applications and evolutionary studies. however, little is known to date about variation among humans at {mirnas} and their target sites. in this study, we analyzed publicly available {snp} data in context with {mirnas} and their target sites throughout the human genome, and we found a relatively low level of variation in functional regions of {mirnas}, but an appreciable level of variation at target sites. approximately 400 {snps} were found at experimentally verified target sites or predicted target sites that are otherwise evolutionarily conserved across mammals. moreover, ?250 {snps} potentially create novel target sites for {mirnas} in humans. if some variants have functional effects, they might confer phenotypic differences among humans. although the majority of these {snps} appear to be evolving under neutrality, interestingly, some of these {snps} are found at relatively high population frequencies even in experimentally verified targets, and a few variants are associated with atypically long-range haplotypes that may have been subject to recent positive selection.
1347	1137074	article	reviews of modern physics	\N	\N	american physical society	39	79	1	2007	jan	2007-03-02 17:40:58	\N	linear optical quantum computing with photonic qubits	linear optics with photon counting is a prominent candidate for practical quantum computing. the protocol by  knill, laflamme and milburn  nature (london) 409 46 (2001)] explicitly demonstrates that efficient scalable quantum computing with single photons, linear optical elements, and projective measurements is possible. subsequently, several improvements on this protocol have started to bridge the gap between theoretical scalability and practical implementation. the original theory and its improvements are reviewed, and a few examples of experimental two-qubit gates are given. the use of realistic components, the errors they induce in the computation, and how these errors can be corrected is discussed.
1348	1142586	article	international journal of data warehousing and mining	\N	\N	idea group publishing	\N	3	3	2007	\N	2007-03-06 02:02:43	\N	multi label classification: an overview	nowadays, multi-label classification methods are increasingly required by modern applications, such as protein function classification, music categorization and semantic scene classification. this paper introduces the task of multi-label classification, organizes the sparse related literature into a structured presentation and performs comparative experimental results of certain multi-label classification methods. it also contributes the definition of concepts for the quantification of the multi-label nature of a data set.
1349	1144984	article	j. phys. chem. b	the journal of physical chemistry b	\N	american chemical society	9	107	3	2003	jan	2007-03-07 02:04:36	department of chemistry, northwestern university, evanston, il 60208-3113, united states	the optical properties of metal nanoparticles:? the influence of size, shape, and dielectric environment	the optical properties of metal nanoparticles have long been of interest in physical chemistry, starting with faraday's investigations of colloidal gold in the middle 1800s. more recently, new lithographic techniques as well as improvements to classical wet chemistry methods have made it possible to synthesize noble metal nanoparticles with a wide range of sizes, shapes, and dielectric environments. in this feature article, we describe recent progress in the theory of nanoparticle optical properties, particularly methods for solving maxwell's equations for light scattering from particles of arbitrary shape in a complex environment. included is a description of the qualitative features of dipole and quadrupole plasmon resonances for spherical particles; a discussion of analytical and numerical methods for calculating extinction and scattering cross-sections, local fields, and other optical properties for nonspherical particles; and a survey of applications to problems of recent interest involving triangular silver particles and related shapes.
1350	1145123	article	briefings in bioinformatics	\N	\N	oxford university press	11	5	3	2004	sep	2007-03-07 04:40:42	the institute for genomic research, 9712 medical center drive, rockville, md 20850, usa. mpop@tigr.org	comparative genome assembly	one of the most complex and computationally intensive tasks of genome sequence analysis is genome assembly. even today, few centres have the resources, in both software and hardware, to assemble a genome from the thousands or millions of individual sequences generated in a whole-genome shotgun sequencing project. with the rapid growth in the number of sequenced genomes has come an increase in the number of organisms for which two or more closely related species have been sequenced. this has created the possibility of building a comparative genome assembly algorithm, which can assemble a newly sequenced genome by mapping it onto a reference {genome.we} describe here a novel algorithm for comparative genome assembly that can accurately assemble a typical bacterial genome in less than four minutes on a standard desktop computer. the software is available as part of the open-source {amos} project.
1351	1145683	inproceedings	\N	proceedings of the sigchi conference on human factors in computing systems	\N	acm press	7	\N	\N	2003	\N	2007-03-07 14:18:26	\N	semi-public displays for small, co-located groups	the majority of systems using public displays to foster awareness have focused on providing information across remote locations or among people who are loosely connected and lack awareness of each other's activities or interests. we have, however, identified many potential benefits for an awareness system that displays information within a small, co-located group in which the members already possess some awareness of each other's activities. by using "semi-public displays," public displays scoped for small groups, we can make certain types of information visible in the environment, promoting collaboration and providing lightweight information about group activity. compared to designing for large, loosely connected groups, designing for semi-public displays mitigates typically problematic issues in sustaining relevant content for the display and minimizing privacy concerns. we are using these applications to support and enhance the interactions and information that group members utilize to maintain awareness and collaborate.
1352	1146027	article	methods (san diego, calif.)	\N	\N	\N	8	25	1	2001	sep	2007-03-07 21:02:46	department of physics, center for biophysics and computational biology, urbana 61801, usa. tjha@uiuc.edu	single-molecule fluorescence resonance energy transfer.	fluorescent resonance energy transfer ({fret}) is a powerful technique for studying conformational distribution and dynamics of biological molecules. some conformational changes are difficult to synchronize or too rare to detect using ensemble {fret}. {fret}, detected at the single-molecule level, opens up new opportunities to probe the detailed kinetics of structural changes without the need for synchronization. here, we discuss practical considerations for its implementation including experimental apparatus, fluorescent probe selection, surface immobilization, single-molecule {fret} analysis schemes, and interpretation.
1353	1146124	article	artificial intelligence	\N	\N	\N	60	92	\N	1997	\N	2007-03-07 23:39:00	\N	map learning with uninterpreted sensors and effectors	this paper presents a set of methods by which a learning agent can learn a sequence of increasingly abstract and powerful interfaces to control a robot whose sensorimotor apparatus and environment are initially unknown. the result of the learning is a rich hierarchical model of the robot's world (its sensimotor apparatus and environment). the learning methods rely on generic properties of the robot's world such as almost-everywhere smooth effects of motor control signals on sensory features. at the lowest level of the hierarchy, the learning agent analyzes the effects of its motor control signals in order to define a new set of control signals, one of each of the robot's degrees of freedom. it uses a generate-and-test approach to define sensory features that capture important aspects of the environment. it uses linear regression to learn models that characterize context-dependent effects of the control laws for finding and following paths defined using constraints on the learned features. the agent abstracts these control laws, which interact with the continuous environment, to a finite set of actions that implement discrete state transitions. at this point, the agent has abstracted the robot's continuous world to a finite-state world and can use existing methods to learn its structure. the learning agent's methods are evaluated on several simulated robots with different sensorimotor systems and environments.
1354	1146927	article	nature biotechnology	\N	\N	nature publishing group	7	25	3	2007	mar	2007-03-08 13:03:54	\N	a human phenome-interactome network of protein complexes implicated in genetic disorders.	we performed a systematic, large-scale analysis of human protein complexes comprising gene products implicated in many different categories of human disease to create a phenome-interactome network. this was done by integrating quality-controlled interactions of human proteins with a validated, computationally derived phenotype similarity score, permitting identification of previously unknown complexes likely to be associated with disease. using a phenomic ranking of protein complexes linked to human disease, we developed a bayesian predictor that in 298 of 669 linkage intervals correctly ranks the known disease-causing protein as the top candidate, and in 870 intervals with no identified disease-causing gene, provides novel candidates implicated in disorders such as retinitis pigmentosa, epithelial ovarian cancer, inflammatory bowel disease, amyotrophic lateral sclerosis, alzheimer disease, type 2 diabetes and coronary heart disease. our publicly available draft of protein complexes associated with pathology comprises 506 complexes, which reveal functional relationships between disease-promoting genes that will inform future experimentation.
1355	1152368	article	biomedical digital libraries	\N	\N	\N	\N	4	1	2007	mar	2007-07-04 19:07:41	\N	factors influencing publication choice: why faculty choose open access	{background}: in an attempt to identify motivating factors involved in decisions to publish in open access and open archives ({oa}) journals, individual interviews with biomedical faculty members at the university of north carolina at chapel hill ({unc}-chapel hill) and duke university, two major research universities, were conducted. the interviews focused on faculty identified as early adopters of {oa}/free full-text publishing. {methods}: searches conducted in {pubmed} and {pubmed} central identified faculty from the two institutions who have published works in {oa}/free full-text journals. the searches targeted authors with multiple {oa} citations during a specified 18 month period. semi-structured interviews were conducted with the most prolific {oa} authors at each university. individual interviews attempted to determine whether the authors were aware they published in {oa} journals, why they chose to publish in {oa} journals, what factors influenced their publishing decisions, and their general attitude towards {oa} publishing models. {results} \\& {discussion}: fourteen interviews were granted and completed. respondents included a fairly even mix of assistant, associate and full professors. results indicate that when targeting biomedical faculty at {unc}-chapel hill and duke, speed of publication and copyright retention are unlikely motivating factors or incentives for the promotion of {oa} publishing. in addition, author fees required by some open access journals are unlikely barriers or disincentives. {conclusion}: it appears that publication quality is of utmost importance when choosing publication venues in general, while free access and visibility are specifically noted incentives for selection of {oa} journals. therefore, free public availability and increased exposure may not be strong enough incentives for authors to choose open access over more traditional and respected subscription based publications, unless the quality issue is also addressed.
1356	1153312	inproceedings	\N	proceedings of the 8th international conference on intelligent user interfaces	iui	acm	7	\N	\N	2003	jan	2007-03-11 00:35:40	new york, ny, usa	learning implicit user interest hierarchy for context in personalization	to provide a more robust context for personalization, we desire to extract a continuum of general (long-term) to specific (short-term) interests of a user. our proposed approach is to learn a user interest hierarchy ({uih}) from a set of web pages visited by a user. we devise a divisive hierarchical clustering ({dhc}) algorithm to group words (topics) into a hierarchy where more general interests are represented by a larger set of words. each web page can then be assigned to nodes in the hierarchy for further processing in learning and predicting interests. this approach is analogous to building a subject taxonomy for a library catalog system and assigning books to the taxonomy. our approach does not need user involvement and learns the {uih} "implicitly." furthermore, it allows the original objects, web pages, to be assigned to multiple topics (nodes in the hierarchy). in this paper, we focus on learning the {uih} from a set of visited pages. we propose a few similarity functions and dynamic threshold-finding methods, and evaluate the resulting hierarchies according to their meaningfulness and shape
1357	1155153	inproceedings	\N	proceedings of the ieee	\N	\N	30	63	9	1975	\N	2007-03-12 08:28:44	\N	the protection of information in computer systems	this tutorial paper explores the mechanics of protecting computer-stored information from unauthorized use or modification. it concentrates on those architectural structures-whether hardware or software-that are necessary to support information protection. the paper develops in three main sections. section i describes desired functions, design principles, and examples of elementary protection and authentication mechanisms. any reader familiar with computers should find the first section to be reasonably accessible. section {ii} requires some familiarity with descriptor-based computer architecture. it examines in depth the principles of modern protection architectures and the relation between capability systems and access control list systems, and ends with a brief analysts of protected subsystems and protected objects. the reader who is dismayed by either the prerequisites or the level of detail in the second section may wish to skip to section {iii}, which reviews the state of the art and current research projects and provides suggestions for further reading.
1358	1155801	article	physical review lett	\N	\N	\N	\N	88	\N	2002	\N	2007-03-12 17:56:42	\N	random packings of frictionless particles	we conduct numerical simulations of random packings of frictionless particles at tÂ =Â 0. the packing fraction where the pressure becomes nonzero is the same as the jamming threshold, where the static shear modulus becomes nonzero. the distribution of threshold packing fractions narrows, and its peak approaches random close packing as the system size increases. for packing fractions within the peak, there is no self-averaging, leading to exponential decay of the interparticle force distribution.
1359	1158212	article	american journal of human genetics	\N	\N	\N	11	79	1	2006	jul	2007-03-13 12:14:31	division of public health sciences, fred hutchinson cancer research center, seattle, wa 98109, usa. huatang@fhcrc.org	reconstructing genetic ancestry blocks in admixed individuals.	a chromosome in an individual of recently admixed ancestry resembles a mosaic of chromosomal segments, or ancestry blocks, each derived from a particular ancestral population. we consider the problem of inferring ancestry along the chromosomes in an admixed individual and thereby delineating the ancestry blocks. using a simple population model, we infer gene-flow history in each individual. compared with existing methods, which are based on a hidden markov model, the markov-hidden markov model ({mhmm}) we propose has the advantage of accounting for the background linkage disequilibrium ({ld}) that exists in ancestral populations. when there are more than two ancestral groups, we allow each ancestral population to admix at a different time in history. we use simulations to illustrate the accuracy of the inferred ancestry as well as the importance of modeling the background {ld}; not accounting for background {ld} between markers may mislead us to false inferences about mixed ancestry in an indigenous population. the {mhmm} makes it possible to identify genomic blocks of a particular ancestry by use of any high-density single-nucleotide-polymorphism panel. one application of our method is to perform admixture mapping without genotyping special ancestry-informative-marker panels.
1360	1158241	article	international journal on digital libraries	\N	\N	springer berlin / heidelberg	15	3	2	2000	aug	2007-03-13 13:15:38	\N	automatic recognition of multi-word terms:. the {c-value/nc}-value method	abstract. &nbsp;&nbsp;technical terms (henceforth called terms ), are important elements for digital libraries. in this paper we present a domain-independent method for the automatic extraction of multi-word terms, from machine-readable special language corpora. the method, (c-value/nc-value ), combines linguistic and statistical information. the first part, c-value, enhances the common statistical measure of frequency of occurrence for term extraction, making it sensitive to a particular type of multi-word terms, the nested terms. the second part, nc-value, gives: 1) a method for the extraction of term context words (words that tend to appear with terms); 2) the incorporation of information from term context words to the extraction of terms.
1361	1158393	article	computer vision and image understanding	\N	\N	elsevier science inc.	21	61	1	1995	jan	2007-03-13 16:24:07	new york, ny, usa	active shape {models-their} training and application	model-based vision is firmly established as a robust approach to recognizing and locating known rigid objects in the presence of noise, clutter, and occlusion. it is more problematic to apply model-based methods to images of objects whose appearance can vary, though a number of approaches based on the use of flexible templates have been proposed. the problem with existing methods is that they sacrifice model specificity in order to accommodate variability, thereby compromising robustness during image interpretation. we argue that a model should only be able to deform in ways characteristic of the class of objects it represents. we describe a method for building models by learning patterns of variability from a training set of correctly annotated images. these models can be used for image search in an iterative refinement algorithm analogous to that employed by active contour models (snakes). the key difference is that our active shape models can only deform to fit the data in ways consistent with the training set. we show several practical examples where we have built such models and used them to locate partially occluded objects in noisy, cluttered images.
1362	1159111	article	cell	\N	\N	\N	9	128	4	2007	feb	2007-03-14 07:10:56	department of urology, biochemistry, and molecular biology, usc/norris comprehensive cancer center, keck school of medicine, university of southern california, los angeles, ca 90089, usa. jones\\_p@ccnt.usc.edu	the epigenomics of cancer	aberrant gene function and altered patterns of gene expression are key features of cancer. growing evidence shows that acquired epigenetic abnormalities participate with genetic alterations to cause this dysregulation. here, we review recent advances in understanding how epigenetic alterations participate in the earliest stages of neoplasia, including stem/precursor cell contributions, and discuss the growing implications of these advances for strategies to control cancer.
1363	1159921	inproceedings	\N	p{ads}	\N	acm press	7	\N	\N	2004	\N	2007-03-14 13:41:47	new york, ny, usa	a component-based simulation layer for {james}	if a model shall be executed in a parallel, distributed instead of a sequential manner, typically the entire simulation engine has to be exchanged. to adapt the simulation layer more easily to the requirements of a concrete model to be run in a specific environment a component based simulation layer has been developed for james. a set of different simulator components demonstrates that a component-based design facilitates the exchange of simulators and their combination.
1364	1159939	article	bioinformatics	\N	\N	\N	1	17	6	2001	jun	2007-03-14 13:41:47	\N	s{{tochsim}}: modelling of stochastic biomolecular processes.	s{ummary}: {stochsim} is a stochastic simulator for chemical reactions. {m}olecules are represented as individual software objects that react according to probabilities derived from concentrations and rate constants. {v}ersion 1.2 of {stochsim} provides a novel cross-platform graphical interface written in {p}erl/{t}k. {a} simple two-dimensional spatial structure has also been implemented, in which nearest-neighbour interactions of molecules in a 2-{d} lattice can be simulated.
1365	1161647	article	genes dev	\N	\N	\N	5	21	4	2007	feb	2007-03-14 21:10:21	whitehead institute for biomedical research, nine cambridge center, cambridge, massachusetts 02142, usa.	whole-genome {chip}-chip analysis of dorsal, twist, and snail suggests integration of diverse patterning processes in the drosophila embryo.	genetic studies have identified numerous sequence-specific transcription factors that control development, yet little is known about their in vivo distribution across animal genomes. we determined the genome-wide occupancy of the dorsoventral ({dv}) determinants dorsal, twist, and snail in the drosophila embryo using chromatin immunoprecipitation coupled with microarray analysis ({chip}-chip). the in vivo binding of these proteins correlate tightly with the limits of known enhancers. our analysis predicts substantially more target genes than previous estimates, and includes dpp signaling components and anteroposterior ({ap}) segmentation determinants. thus, the {chip}-chip data uncover a much larger than expected regulatory network, which integrates diverse patterning processes during development.
1366	1165718	book	\N	\N	\N	princeton university press	\N	\N	\N	2007	jan	2007-03-15 16:20:25	\N	the difference: how the power of diversity creates better groups, firms, schools, and societies	{<p>in this landmark book, scott page redefines the way we understand ourselves in relation to one another. <i>the difference</i> is about how we think in groups--and how our collective wisdom exceeds the sum of its parts. why can teams of people find better solutions than brilliant individuals working alone? and why are the best group decisions and predictions those that draw upon the very qualities that make each of us unique? the answers lie in diversity--not what we look like outside, but what we look like within, our distinct tools and abilities.</p><p><i>the difference</i> reveals that progress and innovation may depend less on lone thinkers with enormous iqs than on diverse people working together and capitalizing on their individuality. page shows how groups that display a range of perspectives outperform groups of like-minded experts. diversity yields superior outcomes, and page proves it using his own cutting-edge research. moving beyond the politics that cloud standard debates about diversity, he explains why difference beats out homogeneity, whether you're talking about citizens in a democracy or scientists in the laboratory. he examines practical ways to apply diversity's logic to a host of problems, and along the way offers fascinating and surprising examples, from the redesign of the chicago "el" to the truth about where we store our ketchup.</p><p>page changes the way we understand diversity--how to harness its untapped potential, how to understand and avoid its traps, and how we can leverage our differences for the benefit of all.</p>}
1367	1167552	techreport	\N	\N	\N	\N	\N	\N	\N	2007	feb	2007-03-16 12:01:08	\N	trading structure for randomness in wireless opportunistic routing	opportunistic routing is a recent technique that achieves high throughput in the face of lossy wireless links. the current opportunistic routing protocol, exor, ties the mac with routing, imposing a strict schedule on routersâ€™ access to the medium. although the scheduler delivers opportunistic gains, it misses some of the inherent features of the 802.11 mac. for example, it prevents spatial reuse and thus may underutilize the wireless medium. it also eliminates the layering abstraction, making the protocol less amenable to extensions to alternate traffic types such as multicast. this paper presents more, a mac-independent opportunistic routing protocol. more randomly mixes packets before forwarding them. this randomness ensures that routers that hear the same transmission do not forward the same packets. thus, more needs no special scheduler to coordinate routers and can run directly on top of 802.11. experimental results from a 20-node wireless testbed show that moreâ€™s median unicast throughput is 22% higher than exor, and the gains rise to 45% over exor when there is a chance of spatial reuse. for multicast, moreâ€™s gains increase with the number of destinations, and are 35-200% greater than exor.
1368	1169116	article	genome biology	\N	\N	\N	\N	8	3	2007	mar	2007-05-21 03:25:13	\N	reactome: a knowledge base of biologic pathways and processes	reactome http://www.reactome.org, an online curated resource for human pathway data, provides infrastructure for computation across the biologic reaction network. we use reactome to infer equivalent reactions in multiple nonhuman species, and present data on the reliability of these inferred reactions for the distantly related eukaryote saccharomyces cerevisiae. finally, we describe the use of reactome both as a learning resource and as a computational tool to aid in the interpretation of microarrays and similar large-scale datasets.
1369	1169701	electronic	\N	\N	\N	\N	\N	\N	\N	2006	jun	2007-03-17 19:29:12	\N	synonym search in wikipedia: synarcher	the program synarcher for synonym (and related terms) search in the text corpus of special structure (wikipedia) was developed. the results of the search are presented in the form of graph. it is possible to explore the graph and search for graph elements interactively. adapted hits algorithm for synonym search, program architecture, and program work evaluation with test examples are presented in the paper. the proposed algorithm can be applied to a query expansion by synonyms (in a search engine) and a synonym dictionary forming.
1370	1169733	article	pnas	\N	\N	\N	5	93	\N	1996	\N	2007-03-17 19:40:09	\N	multiple {dna} and protein sequence alignment based on segment-to-segment comparison	in this paper, a new way to think about, and to construct, pairwise as well as multiple alignments of dna and protein sequences is proposed. rather than forcing alignments to either align single residues or to introduce gaps by defining an alignment as a path running right from the source up to the sink in the associated dot-matrix diagram, we propose to consider alignments as consistent equivalence relations defined on the set of all positions occurring in all sequences under consideration. we also propose constructing alignments from whole segments exhibiting highly significant overall similarity rather than by aligning individual residues. consequently, we present an alignment algorithm that (i) is based on segment-to-segment comparison instead of the commonly used residue-to-residue comparison and which (ii) avoids the well-known difficulties concerning the choice of appropriate gap penalties: gaps are not treated explicity, but remain as those parts of the sequences that do not belong to any of the aligned segments. finally, we discuss the application of our algorithm to two test examples and compare it with commonly used alignment methods. as a first example, we aligned a set of 11 dna sequences coding for functional helix-loop-helix proteins. though the sequences show only low overall similarity, our program correctly aligned all of the 11 functional sites, which was a unique result among the methods tested. as a by-product, the reading frames of the sequences were identified. next, we aligned a set of ribonuclease h proteins and compared our results with alignments produced by other programs as reported by mcclure et al. [mcclure, m. a., vasi, t. k. & fitch, w. m. (1994) mol. biol. evol. 11, 571-592]. our program was one of the best scoring programs. however, in contrast to other methods, our protein alignments are independent of user-defined parameters.
1371	1176143	article	nature neuroscience	\N	\N	\N	\N	\N	\N	2007	mar	2007-03-19 17:27:53	center for neurobiology and behavior, kolb research annex, columbia university college of physicians and surgeons, 1051 riverside drive, new york, new york 10032-2695, usa.	limits on the memory storage capacity of bounded synapses	memories maintained in patterns of synaptic connectivity are rapidly overwritten and destroyed by ongoing plasticity related to the storage of new memories. short memory lifetimes arise from the bounds that must be imposed on synaptic efficacy in any realistic model. we explored whether memory performance can be improved by allowing synapses to traverse a large number of states before reaching their bounds, or by changing the way these bounds are imposed. in the case of hard bounds, memory lifetimes grow proportional to the square of the number of synaptic states, but only if potentiation and depression are precisely balanced. improved performance can be obtained without fine tuning by imposing soft bounds, but this improvement is only linear with respect to the number of synaptic states. we explored several other possibilities and conclude that improving memory performance requires a more radical modification of the standard model of memory storage.
1372	1181456	article	wowmom	\N	\N	ieee computer society	11	0	\N	2006	\N	2007-03-23 09:20:16	los alamitos, ca, usa	network coding for wireless mesh networks: a case study	network coding is a new transmission paradigm that proved its strength in optimizing the usage of network resources. in this paper, we evaluate the gain from using network coding for file sharing applications running on top of wireless mesh networks. with extensive simulations carried out on a simulator we developed specifically for this study, we confirm that network coding can improve the performance of the file sharing application, but not as in wired networks. the main reason is that nodes over wireless cannot listen to different neighbors simultaneously. nevertheless, one can get more from network coding if the information transmission is made more diverse inside the network. we support this argument by varying the loss rate over wireless links and adding more sources. 1
1373	1181833	article	nature reviews. molecular cell biology	\N	\N	nature publishing group	11	8	4	2007	apr	2007-03-23 16:05:48	\N	the folding and evolution of multidomain proteins.	analyses of genomes show that more than 70\\% of eukaryotic proteins are composed of multiple domains. however, most studies of protein folding focus on individual domains and do not consider how interactions between domains might affect folding. here, we address this by analysing the three-dimensional structures of multidomain proteins that have been characterized experimentally and observe that where the interface is small and loosely packed, or unstructured, the folding of the domains is independent. furthermore, recent studies indicate that multidomain proteins have evolved mechanisms to minimize the problems of interdomain misfolding.
1374	1181873	article	science	\N	\N	american association for the advancement of science	3	315	5819	2007	mar	2007-03-23 17:01:34	\N	{crispr} provides acquired resistance against viruses in prokaryotes	clustered regularly interspaced short palindromic repeats ({crispr}) are a distinctive feature of the genomes of most bacteria and archaea and are thought to be involved in resistance to bacteriophages. we found that, after viral challenge, bacteria integrated new spacers derived from phage genomic sequences. removal or addition of particular spacers modified the phage-resistance phenotype of the cell. thus, {crispr}, together with associated cas genes, provided resistance against phages, and resistance specificity is determined by spacer-phage sequence similarity.
1375	1188027	article	science	\N	\N	\N	3	315	5819	2007	mar	2007-03-26 11:55:59	\N	tunability and noise dependence in differentiation dynamics	the dynamic process of differentiation depends on the architecture, quantitative parameters, and noise of underlying genetic circuits. however, it remains unclear how these elements combine to control cellular behavior. we analyzed the probabilistic and transient differentiation of bacillus subtilis cells into the state of competence. a few key parameters independently tuned the frequency of initiation and the duration of competence episodes and allowed the circuit to access different dynamic regimes, including oscillation. altering circuit architecture showed that the duration of competence events can be made more precise. we used an experimental method to reduce global cellular noise and showed that noise levels are correlated with frequency of differentiation events. together, the data reveal a noise-dependent circuit that is remarkably resilient and tunable in terms of its dynamic behavior.
1376	1193410	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2007-03-28 21:57:40	\N	collaborative filtering via gaussian probabilistic latent semantic analysis	collaborative filtering aims at learning predictive models of user preferences, interests or behavior from community data, i.e. a database of available user preferences. in this paper, we describe a new model-based algorithm designed for this task, which is based on a generalization of probabilistic latent semantic analysis to continuous-valued response variables. more specifically, we assume that the observed user ratings can be modeled as a mixture of user communities or interest groups, where users may participate probabilistically in one or more groups. each community is characterized by a gaussian distribution on the normalized ratings for each item. the normalization of ratings is performed in a user-specific manner to account for variations in absolute shift and variance of ratings. experiments on the eachmovie data set show that the proposed approach compares favorably with other collaborative filtering techniques.
1377	1198575	article	plos comput biol	\N	\N	public library of science	\N	3	3	2007	mar	2007-03-30 17:53:07	\N	ten simple rules for a successful collaboration	excerpt: scientific research has always been a collaborative undertaking, and this is particularly true today. for example, between 1981 and 2001, the average number of coauthors on a paper for the proceedings of the national academy of sciences u s a rose from 3.9 to 8.4 [1]. why the increase? biology has always been considered the study of living systems; many of us now think of it as the study of complex systems. understanding this complexity requires experts in many different domains. in short, these days success in being a biologist depends more on one's ability to collaborate than ever before. the medical research centers in the united kingdom figured this out long ago, and the new janelia farm research campus of the howard hughes medical institute in the united states has got the idea, as it strongly promotes intra- and inter-institutional collaborations [2]. given that collaboration is crucial, how do you go about picking the right collaborators, and how can you best make the collaboration work? here are ten simple rules based on our experience that we hope will help. additional suggestions can be found in the references [3,4]. above all, keep in mind that these rules are for both you and your collaborators. always remember to treat your collaborators as you would want to be treated yourselfâ€”empathy is key
1378	1201474	article	emotion	\N	\N	\N	19	7	1	2007	feb	2007-04-01 06:55:47	\N	the process of emotion inference	three experiments investigated the process of inferring emotions from brief descriptions of typical eliciting situations, using response time methodology. the initial hypothesis was that emotion inferences are mediated by inferred cognitive appraisals of the eliciting event (concerning e.g., its valence or the responsible agent). this hypothesis was contradicted by the finding of experiment 1 that emotion judgments are typically made faster than appraisal judgments. to explain this finding, it was hypothesized that emotion judgments are based on automatized (proceduralized) appraisal inferences. this hypothesis was tested in experiments 2 and 3 using a judgment facilitation paradigm. the results supported the proceduralization hypothesis by demonstrating that appraisal judgments are facilitated by prior emotion judgments.
1379	1201711	article	expert systems with applications	\N	\N	\N	11	33	1	2007	jul	2007-09-04 11:32:38	\N	educational data mining: a survey from 1995 to 2005	currently there is an increasing interest in data mining and educational systems, making educational data mining as a new growing research community. this paper surveys the application of data mining to traditional educational systems, particular web-based courses, well-known learning content management systems, and adaptive and intelligent web-based educational systems. each of these systems has different data source and objectives for knowledge discovering. after preprocessing the available data in each case, data mining techniques can be applied: statistics and visualization; clustering, classification and outlier detection; association rule mining and pattern mining; and text mining. the success of the plentiful work needs much more specialized work in order for educational data mining to become a mature area.
1380	1204337	inproceedings	\N	chi	\N	acm	1	\N	\N	2000	\N	2007-04-03 15:05:59	new york, ny, usa	timely reminders: a case study of temporal guidance in {pim} and email tools usage	we describe our research in progress that explores the use of  personal information management  ({pim}) tools in time and attempts to establish temporal attributes of information. we report on a short field study undertaken to examine relations between tools and information life-cycle. we propose four information types: prospective, ephemeral, working and retrospective. we outline relationships between {pim} tools, email and different types of information. we use this framework to explain problems observed with handling information.
1381	1204515	article	econometrica	\N	\N	\N	21	53	6	1985	\N	2007-04-03 15:29:28	\N	continuous auctions and insider trading	a dynamic model of insider trading with sequential auctions, structured to resemble a sequential equilibrium, is used to examine the informational content of prices, the liquidity characteristics of a speculative market, and the value of private information to an insider. the model has three kinds of traders: a single risk neutral insider, random noise traders, and competitive risk neutral market makers. the insider makes positive profits by exploiting his monopoly power optimally in a dynamic context, where noise trading provides camouflage which conceals his trading from market makers. as the time interval between auctions goes to zero, a limiting model of continuous trading is obtained. in this equilibrium, prices follow brownian motion, the depth of the market is constant over time, and all private information is incorporated into prices by the end of trading.
1382	1205857	article	innovate journal of online education	\N	\N	\N	\N	3	4	2007	\N	2007-04-04 13:36:10	\N	teaching and learning with the net generation	a decade ago, the first wave of the net generation began to enter college, forcing educational institutions to deal with a new population of learners with unique characteristics. with the net generation representing nearly 7\\% of the population today (bartlett 2005) and with nearly 49.5 million students enrolled in schools in 2003 (enrollment management report 2005), responding to the specific needs of this generation of learners is becoming increasingly important. the challenge of evolving pedagogy to meet the needs of net-savvy students is daunting, but educators are assisted by the fact that this generation values education. these students learn in a different way than their predecessors did, but they do want to learn. in this article we will define the characteristics of net geners' learning styles and discuss how educators can make the most of these particular traits.
1383	1205895	article	environmental microbiology	\N	\N	blackwell science ltd	9	6	9	2004	sep	2007-04-04 13:50:00	department of molecular ecology, genomics group, max planck institute for marine microbiology, d-28359 bremen, germany. ;  department of molecular ecology, max planck institute for marine microbiology, d-28359 bremen, germany.	application of tetranucleotide frequencies for the assignment of genomic fragments	a basic problem of the metagenomic approach in microbial ecology is the assignment of genomic fragments to a certain species or taxonomic group, when suitable marker genes are absent. currently, the (g + ?c)-content together with phylogenetic information and codon adaptation for functional genes is mostly used to assess the relationship of different fragments. these methods, however, can produce ambiguous results. in order to evaluate sequence-based methods for fragment identification, we extensively compared (g + c)-contents and tetranucleotide usage patterns of 9054 fosmid-sized genomic fragments generated in silico from 118 completely sequenced bacterial genomes (40 982 931 fragment pairs were compared in total). the results of this systematic study show that the discriminatory power of correlations of tetranucleotide-derived z-scores is by far superior to that of differences in (g + c)-content and provides reasonable assignment probabilities when applied to metagenome libraries of small diversity. using six fully sequenced fosmid inserts from a metagenomic analysis of microbial consortia mediating the anaerobic oxidation of methane ({aom}), we demonstrate that discrimination based on tetranucleotide-derived z-score correlations was consistent with corresponding data from {16s} ribosomal {rna} sequence analysis and allowed us to discriminate between fosmid inserts that were indistinguishable with respect to their (g + c)-contents.
1384	1208448	article	nature	\N	\N	nature publishing group	6	446	7136	2007	apr	2007-04-10 16:52:31	\N	multimodal fast optical interrogation of neural circuitry.	our understanding of the cellular implementation of systems-level neural processes like action, thought and emotion has been limited by the availability of tools to interrogate specific classes of neural cells within intact, living brain tissue. here we identify and develop an archaeal light-driven chloride pump ({nphr}) from natronomonas pharaonis for temporally precise optical inhibition of neural activity. {nphr} allows either knockout of single action potentials, or sustained blockade of spiking. {nphr} is compatible with {chr2}, the previous optical excitation technology we have described, in that the two opposing probes operate at similar light powers but with well-separated action spectra. {nphr}, like {chr2}, functions in mammals without exogenous cofactors, and the two probes can be integrated with calcium imaging in mammalian brain tissue for bidirectional optical modulation and readout of neural activity. likewise, {nphr} and {chr2} can be targeted together to caenorhabditis elegans muscle and cholinergic motor neurons to control locomotion bidirectionally. {nphr} and {chr2} form a complete system for multimodal, high-speed, genetically targeted, all-optical interrogation of living neural circuits.
1385	1217461	article	plos genet	\N	\N	\N	\N	3	4	2007	apr	2007-04-08 23:05:22	\N	lifespan regulation by evolutionarily conserved genes essential for viability.	evolutionarily conserved mechanisms that control aging are predicted to have prereproductive functions in order to be subject to natural selection. genes that are essential for growth and development are highly conserved in evolution, but their role in longevity has not previously been assessed. we screened 2,700 genes essential for caenorhabditis elegans development and identified 64 genes that extend lifespan when inactivated postdevelopmentally. these candidate lifespan regulators are highly conserved from yeast to humans. classification of the candidate lifespan regulators into functional groups identified the expected insulin and metabolic pathways but also revealed enrichment for translation, {rna}, and chromatin factors. many of these essential gene inactivations extend lifespan as much as the strongest known regulators of aging. early gene inactivations of these essential genes caused growth arrest at larval stages, and some of these arrested animals live much longer than wild-type adults. daf-16 is required for the enhanced survival of arrested larvae, suggesting that the increased longevity is a physiological response to the essential gene inactivation. these results suggest that insulin-signaling pathways play a role in regulation of aging at any stage in life.
1386	1217723	proceedings	\N	\N	\N	\N	\N	\N	\N	-1	\N	2007-04-09 09:21:07	\N	{icwsm} || full paper || {tagassist}: automatic tag suggestion for blog posts	in this paper, we describe a system called tagassist that provides tag suggestions for new blog posts by utilizing existing tagged posts. the system is able to increase the quality of suggested tags by performing lossless compression over existing tag data. in addition, the system employs a set of metrics to evaluate the quality of a potential tag suggestion. coupled with the ability for users to manually add tags, tagassist can ease the burden of tagging and increase the utility of retrieval and browsing systems built on top of tagging data.
1387	1219929	article	plos computational biology	\N	\N	\N	\N	preprint	2007	2007	apr	2007-04-11 07:24:42	\N	binding site graphs: a new graph theoretical framework for prediction of transcription factor binding sites	computational prediction of nucleotide binding specificity for transcription factors remains a fundamental and largely unsolved problem. determination of binding positions is a prerequisite for research in gene regulation, a major mechanism controlling phenotypic diversity. furthermore, an accurate determination of binding specificities from high-throughput data sources is necessary to realize the full potential of systems biology. unfortunately, recently preformed independent evaluation showed that more than half of the predictions from most widely used algorithms are false. we introduce a graph-theoretical framework to describe local sequence similarity as the pair-wise distances between nucleotides in promoter sequences, and hypothesize that densely connected subgraphs are indicative of transcription factor binding sites. using a well-established sampling algorithm coupled with simple clustering and scoring schemes, we identify sets of closely related nucleotides and test those for known {tf} binding activity. using an independent benchmark, we find our algorithm predicts yeast binding motifs considerably better than currently available techniques and without manual curation. importantly, we reduce the number of false positive predictions in yeast to less than 30\\%. we also develop a framework to evaluate the statistical significance of our motif predictions. we show that our approach is robust to the choice of input promoters, and thus can be used in the context of predicting binding positions from noisy experimental data. we apply our method to identify binding sites using data from genome scale {chip}-chip experiments. results from these experiments are publicly available at {http://cagt10.bu.edu/bsg}/. the graphical framework developed here may be useful when combining predictions from numerous computational and experimental measures. finally, we discuss how our algorithm can be used to improve the sensitivity of computational predictions of transcription factor binding specificities.
1388	1222463	inproceedings	\N	cikm	\N	acm	7	\N	\N	2006	nov	2007-04-12 17:04:38	new york, ny, usa	bayesian adaptive user profiling with explicit \\& implicit feedback	research in information retrieval is now moving into a personalized scenario where a retrieval or filtering system maintains a separate user profile for each user. in this framework, information delivered to the user can be automatically personalized and catered to individual user's information needs. however, a practical concern for such a personalized system is the "cold start problem": any user new to the system must endure poor initial performance until sufficient feedback from that user is {provided.to} solve this problem, we use both explicit and implicit feedback to build a user's profile and use bayesian hierarchical methods to borrow information from existing users. we analyze the usefulness of implicit feedback and the adaptive performance of the model on two data sets gathered from user studies where users' interaction with a document, or  implicit feedback , were recorded along with explicit feedback. our results are two-fold: first, we demonstrate that the bayesian modeling approach effectively trades off between shared and user-specific information, alleviating poor initial performance for each user. second, we find that implicit feedback has very limited unstable predictive value by itself and only marginal value when combined with explicit feedback.
1389	1223530	article	science (new york, n.y.)	\N	\N	american association for the advancement of science	12	316	5822	2007	apr	2007-04-13 11:06:47	\N	evolutionary and biomedical insights from the rhesus macaque genome.	the rhesus macaque (macaca mulatta) is an abundant primate species that diverged from the ancestors of homo sapiens about 25 million years ago. because they are genetically and physiologically similar to humans, rhesus monkeys are the most widely used nonhuman primate in basic and applied biomedical research. we determined the genome sequence of an indian-origin macaca mulatta female and compared the data with chimpanzees and humans to reveal the structure of ancestral primate genomes and to identify evidence for positive selection and lineage-specific expansions and contractions of gene families. a comparison of sequences from individual animals was used to investigate their underlying genetic diversity. the complete description of the macaque genome blueprint enhances the utility of this animal model for biomedical research and improves our understanding of the basic biology of the species. 10.1126/science.1139247
1390	1226694	article	bmc evolutionary biology	\N	\N	\N	\N	7	1	2007	apr	2007-05-02 13:13:34	\N	analysis of epistatic interactions and fitness landscapes using a new geometric approach	{background}:understanding interactions between mutations and how they affect fitness is a central problem in evolutionary biology that bears on such fundamental issues as the structure of fitness landscapes and the evolution of sex. to date, analyses of fitness landscapes have focused either on the overall directional curvature of the fitness landscape or on the distribution of pairwise interactions. in this paper, we propose and employ a new mathematical approach that allows a more complete description of multi-way interactions and provides new insights into the structure of fitness {landscapes.results}:we apply the mathematical theory of gene interactions developed by beerenwinkel et al. to a fitness landscape for escherichia coli obtained by elena and lenski. the genotypes were constructed by introducing nine mutations into a wild-type strain and constructing a restricted set of 27 double mutants. despite the absence of mutants higher than second order, our analysis of this genotypic space points to previously unappreciated gene interactions, in addition to the standard pairwise epistasis. our analysis confirms elena and lenski's inference that the fitness landscape is complex, so that an overall measure of curvature obscures a diversity of interaction types. we also demonstrate that some mutations contribute disproportionately to this complexity. in particular, some mutations are systematically better than others at mixing with other mutations. we also find a strong correlation between epistasis and the average fitness loss caused by deleterious mutations. in particular, the epistatic deviations from multiplicative expectations tend toward more positive values in the context of more deleterious mutations, emphasizing that pairwise epistasis is a local property of the fitness landscape. finally, we determine the geometry of the fitness landscape, which reflects many of these biologically interesting {features.conclusion}:a full description of complex fitness landscapes requires more information than the average curvature or the distribution of independent pairwise interactions. we have proposed a mathematical approach that, in principle, allows a complete description and, in practice, can suggest new insights into the structure of real fitness landscapes. our analysis emphasizes the value of non-independent genotypes for these inferences.
1391	1229624	article	nature reviews. genetics	\N	\N	\N	8	8	5	2007	may	2007-04-16 11:59:14	christian de duve is at the christian de duve institute of cellular pathology (icp), 75 avenue hippocrate, b–1200 brussels, belgium, and the rockefeller university, 1234 york avenue, new york, ny 10021, usa. deduve@icp.ucl.ac.be or cdeduve@rockefeller.edu.	the origin of eukaryotes: a reappraisal.	ever since the elucidation of the main structural and functional features of eukaryotic cells and subsequent discovery of the endosymbiotic origin of mitochondria and plastids, two opposing hypotheses have been proposed to account for the origin of eukaryotic cells. one hypothesis postulates that the main features of these cells, including their ability to capture food by endocytosis and to digest it intracellularly, were developed first, and later had a key role in the adoption of endosymbionts; the other proposes that the transformation was triggered by an interaction between two typical prokaryotic cells, one of which became the host and the other the endosymbiont. re-examination of this question in the light of cell-biological and phylogenetic data leads to the conclusion that the first model is more likely to be the correct one.
1392	1233025	article	ieee internet computing	\N	\N	\N	5	8	1	2004	\N	2007-04-18 07:58:22	\N	understanding link quality in 802.11 mobile ad hoc networks	mobile ad hoc wireless networks will extend the internet into new territory, making web services available "anytime, anywhere." this creates new markets in such areas as pervasive computing and traffic management. we show that the communication quality of current 802.11 ad hoc networks is low, and that users can experience strong fluctuations in link quality as a result. they identify key factors that cause these fluctuations and derive implications for application development. in particular, applications must tolerate frequent disconnections, network partitioning, and latency variations that are far more severe than in conventional networks.
1393	1233290	inproceedings	\N	proceedings of the 1st international workshop on semantic web services and web process composition, swswpc2004, lncs	\N	\N	\N	\N	\N	2004	\N	2007-04-18 09:55:54	san diego, usa	a survey of automated web service composition methods	in todayâ€™s web, web services are created and updated on the fly. itâ€™s already beyond the human ability to analysis them and generate the composition plan manually. a number of approaches have been proposed to tackle that problem. most of them are inspired by the researches in cross-enterprise workflow and ai planning. this paper gives an overview of recent research efforts of automatic web service composition both from the workflow and ai planning research community.
1394	1247838	article	physical review letters	\N	\N	american physical society	\N	99	9	2007	aug	2007-04-24 14:17:11	\N	theory of ground state cooling of a mechanical oscillator using dynamical backaction	a quantum theory of cooling of a mechanical oscillator by radiation pressure-induced dynamical backaction is developed, which is analogous to sideband cooling of trapped ions. we find that final occupancies well below unity can be attained when the mechanical oscillation frequency is larger than the optical cavity linewidth. it is shown that the final average occupancy can be retrieved directly from the optical output spectrum.
1395	1259291	incollection	artificial life	mit press	\N	\N	\N	13	\N	2007	\N	2007-04-27 09:16:07	\N	evolving virtual creatures and catapults	doi: 10.1162/artl.2007.13.2.139 abstract we present a system that can evolve the morphology and the controller of virtual walking and block-throwing creatures (catapults) using a genetic algorithm. the system is based on sims' work, implemented as a flexible platform with an off-the-shelf dynamics engine. experiments aimed at evolving sims-type walkers resulted in the emergence of various realistic gaits while using fairly simple objective functions. due to the flexibility of the system, drastically different morphologies and functions evolved with only minor modifications to the system and objective function. for example, various throwing techniques evolved when selecting for catapults that propel a block as far as possible. among the strategies and morphologies evolved, we find the drop-kick strategy, as well as the systematic invention of the principle behind the wheel, when allowing mutations to the projectile.
1396	1260763	article	journal of personality and social psychology	\N	\N	\N	25	65	4	1993	oct	2007-04-27 17:47:09	\N	social loafing: a {meta-analytic} review and theoretical integration	social loafing is the tendency for individuals to expend less effort when working collectively than when working individually. a meta-analysis of 78 studies demonstrates that social loafing is robust and generalizes across tasks and s populations. a large number of variables were found to moderate social loafing. evaluation potential, expectations of co-worker performance, task meaningfulness, and culture had espeically strong influence. these findings are interpreted in the light of a collective effort model that integrates elements of expectancy-value, social identity, and self-validation theories.
1397	1261882	article	physical review letters	\N	\N	american physical society	\N	96	16	2006	apr	2007-04-28 02:23:10	\N	nonlinear theory for relativistic plasma wakefields in the blowout regime	we present a theory for nonlinear, multidimensional plasma waves with phase velocities near the speed of light. it is appropriate for describing plasma waves excited when all electrons are expelled out from a finite region by either the space charge of a short electron beam or the radiation pressure of a short intense laser. it works very well for the first bucket before phase mixing occurs. we separate the plasma response into a cavity or blowout region void of all electrons and a sheath of electrons just beyond the cavity. this simple model permits the derivation of a single equation for the boundary of the cavity. it works particularly well for narrow electron bunches and for short lasers with spot sizes matched to the radius of the cavity. it is also used to describe the structure of both the accelerating and focusing fields in the wake.
1398	1269699	article	circuits and systems for video technology, ieee transactions on	circuits and systems for video technology, ieee transactions on	\N	ieee	16	13	7	2003	jul	2007-05-01 06:31:04	\N	overview of the {h.264/avc} video coding standard	{h.264/avc} is newest video coding standard of the {itu}-t video coding experts group and the {iso}/{iec} moving picture experts group. the main goals of the {h.264/avc} standardization effort have been enhanced compression performance and provision of a "network-friendly" video representation addressing "conversational" (video telephony) and "nonconversational" (storage, broadcast, or streaming) applications. {h.264/avc} has achieved a significant improvement in rate-distortion efficiency relative to existing standards. this article provides an overview of the technical features of {h.264/avc}, describes profiles and applications for the standard, and outlines the history of the standardization process.
1399	1271262	article	bmc bioinformatics	\N	\N	\N	\N	8	1	2007	\N	2007-05-02 10:41:24	\N	pre-processing agilent microarray data	{background}:pre-processing methods for two-sample long oligonucleotide arrays, specifically the agilent technology, have not been extensively studied. the goal of this study is to quantify some of the sources of error that affect measurement of expression using agilent arrays and to compare agilent's feature extraction software with pre-processing methods that have become the standard for normalization of {cdna} arrays. these include log transformation followed by loess normalization with or without background subtraction and often a between array scale normalization procedure. the larger goal is to define best study design and pre-processing practices for agilent arrays, and we offer some {suggestions.results}:simple loess normalization without background subtraction produced the lowest variability. however, without background subtraction, fold changes were biased towards zero, particularly at low intensities. {roc} analysis of a spike-in experiment showed that differentially expressed genes are most reliably detected when background is not subtracted. loess normalization and no background subtraction yielded an {auc} of 99.7\\% compared with 88.8\\% for agilent processed fold changes. all methods performed well when error was taken into account by t- or z-statistics, {aucs} [greater than or equal to] 99.8\\%. a substantial proportion of genes showed dye effects, 43 ({99\\%ci} : 39\\%, 47\\%). however, these effects were generally small regardless of the pre-processing {method.conclusion}:simple loess normalization without background subtraction resulted in low variance fold changes that more reliably ranked gene expression than the other methods. while t-statistics and other measures that take variation into account, including agilent's z-statistic, can also be used to reliably select differentially expressed genes, fold changes are a standard measure of differential expression for exploratory work, cross platform comparison, and biological interpretation and can not be entirely replaced. although dye effects are small for most genes, many array features are affected. therefore, an experimental design that incorporates dye swaps or a common reference could be valuable.
1400	1279064	article	nature biotechnology	\N	\N	nature publishing group	7	25	5	2007	may	2007-05-09 12:14:35	\N	towards zoomable multidimensional maps of the cell	the detailed structure of molecular networks, including their dependence on conditions and time, are now routinely assayed by various experimental techniques. visualization is a vital aid in integrating and interpreting such data. we describe emerging approaches for representing and visualizing systems data and for achieving semantic zooming, or changes in information density concordant with scale. a central challenge is to move beyond the display of a static network to visualizations of networks as a function of time, space and cell state, which capture the adaptability of the cell. we consider approaches for representing the role of protein complexes in the cell cycle, displaying modules of metabolism in a hierarchical format, integrating experimental interaction data with structured vocabularies such as gene ontology categories and representing conserved interactions among orthologous groups of genes.
1401	1279899	inproceedings	\N	proceedings of the sigchi conference on human factors in computing systems	chi	acm	9	\N	\N	2007	\N	2007-05-05 20:34:44	new york, ny, usa	a familiar face(book): profile elements as signals in an online social network	using data from a popular online social network site, this paper explores the relationship between profile structure (namely, which fields are completed) and number of friends, giving designers insight into the importance of the profile and how it works to encourage connections and articulated relationships between users. we describe a theoretical framework that draws on aspects of signaling theory, common ground theory, and transaction costs theory to generate an understanding of why certain profile fields may be more predictive of friendship articulation on the site. using a dataset consisting of 30,773 facebook profiles, we determine which profile elements are most likely to predict friendship links and discuss the theoretical and design implications of our findings.
1402	1281087	article	nature	\N	\N	nature publishing group	4	403	6765	2000	jan	2007-05-06 22:30:39	\N	the language of covalent histone modifications	histone proteins and the nucleosomes they form with {dna} are the fundamental building blocks of eukaryotic chromatin. a diverse array of post-translational modifications that often occur on tail domains of these proteins has been well documented. although the function of these highly conserved modifications has remained elusive, converging biochemical and genetic evidence suggests functions in several chromatin-based processes. we propose that distinct histone modifications, on one or more tails, act sequentially or in combination to form a 'histone code' that is, read by other proteins to bring about distinct downstream events.
1403	1282453	article	trends in genetics : tig	\N	\N	elsevier trends journals	8	23	4	2007	apr	2007-05-07 23:55:19	department of biochemistry, emory university school of medicine, atlanta, ga 30322, usa; center for bioinformatics, emory university school of medicine, atlanta, ga 30322, usa.	which transposable elements are active in the human genome?	although a large proportion (44\\%) of the human genome is occupied by transposons and transposon-like repetitive elements, only a small proportion (<0.05\\%) of these elements remain active today. recent evidence indicates that approximately 35-40 subfamilies of alu, l1 and {sva} elements (and possibly {herv}-k elements) remain actively mobile in the human genome. these active transposons are of great interest because they continue to produce genetic diversity in human populations and also cause human diseases by integrating into genes. in this review, we examine these active human transposons and explore mechanistic factors that influence their mobilization.
1404	1284347	inproceedings	\N	proc. 7th usenix symposium on operating systems design and implementation (osdi)	\N	\N	\N	\N	\N	2006	nov	2007-05-09 00:25:22	\N	fidelity and yield in a volcano monitoring sensor network	we present a science-centric evaluation of a 19-day sensor network deployment at reventador, an active volcano in ecuador. each of the 16 sensors continuously sampled seismic and acoustic data at 100 hz. nodes used an event-detection algorithm to trigger on interesting volcanic activity and initiate reliable data transfer to the base station. during the deployment, the network recorded 229 earthquakes, eruptions, and other seismoacoustic events.
1405	1286356	inproceedings	\N	www	\N	acm	9	\N	\N	2007	\N	2007-05-09 21:19:35	new york, ny, usa	internet-scale collection of human-reviewed data	enterprise and web data processing and content aggregation systems often require extensive use of human-reviewed data (e.g. for training and monitoring machine learning-based applications). today these needs are often met by in-house efforts or out-sourced offshore contracting. emerging applications attempt to provide automated collection of human-reviewed data at internet-scale. we conduct extensive experiments to study the effectiveness of one such application. we also study the feasibility of using yahoo! answers, a general question-answering forum, for human-reviewed data collection.
1406	1287783	article	science (new york, n.y.)	\N	\N	\N	4	297	5589	2002	sep	2007-05-10 09:53:09	department of biochemistry and molecular pharmacology, university of massachusetts medical school, lazare research building, room 825, 364 plantation street, worcester, ma 01605, usa.	a {microrna} in a multiple-turnover {rnai} enzyme complex.	in animals, the double-stranded rna-specific endonuclease dicer produces two classes of functionally distinct, tiny rnas: micrornas (mirnas) and small interfering rnas (sirnas). mirnas regulate mrna translation, whereas sirnas direct rna destruction via the rna interference (rnai) pathway. here we show that, in human cell extracts, the mirna let-7 naturally enters the rnai pathway, which suggests that only the degree of complementarity between a mirna and its rna target determines its function. human let-7 is a component of a previously identified, mirna-containing ribonucleoprotein particle, which we show is an rnai enzyme complex. each let-7-containing complex directs multiple rounds of rna cleavage, which explains the remarkable efficiency of the rnai pathway in human cells.
1407	1288127	inproceedings	\N	infoscale	\N	acm	\N	\N	\N	2006	\N	2007-05-10 15:12:48	new york, ny, usa	a picture of search	we survey many of the measures used to describe and evaluate the efficiency and effectiveness of large-scale search services. these measures, herein visualized versus verbalized, reveal a domain rich in complexity and scale. we cover six principle facets of search: the query space, users' query sessions, user behavior, operational requirements, the content space, and user demographics. while this paper focuses on measures, the measurements themselves raise questions and suggest avenues of further investigation.
1408	1288910	inproceedings	\N	proceedings of the 16th international conference on world wide web	www	acm	9	\N	\N	2007	\N	2007-05-10 22:53:53	new york, ny, usa	scaling up all pairs similarity search	given a large collection of sparse vector data in a high dimensional space, we investigate the problem of finding all pairs of vectors whose similarity score (as determined by a function such as cosine distance) is above a given threshold. we propose a simple algorithm based on novel indexing and optimization strategies that solves this problem without relying on approximation methods or extensive parameter tuning. we show the approach efficiently handles a variety of datasets across a wide setting of similarity thresholds, with large speedups over previous state-of-the-art approaches.
1409	1289316	article	proceedings of the national academy of sciences	\N	\N	\N	5	104	17	2007	apr	2007-05-11 09:54:09	broad institute of mit and harvard, massachusetts institute of technology and harvard medical school, cambridge, ma 02142.	systematic discovery of regulatory motifs in conserved regions of the human genome, including thousands of {ctcf} insulator sites	conserved noncoding elements ({cnes}) constitute the majority of sequences under purifying selection in the human genome, yet their function remains largely unknown. experimental evidence suggests that many of these elements play regulatory roles, but little is known about regulatory motifs contained within them. here we describe a systematic approach to discover and characterize regulatory motifs within mammalian {cnes} by searching for long motifs (12–22 nt) with significant enrichment in {cnes} and studying their biochemical and genomic properties. our analysis identifies 233 long motifs ({lms}), matching a total of ?60,000 conserved instances across the human genome. these motifs include 16 previously known regulatory elements, such as the histone {3?-utr} motif and the neuron-restrictive silencer element, as well as striking examples of novel functional elements. the most highly enriched motif ({lm1}) corresponds to the x-box motif known from yeast and nematode. we show that it is bound by the {rfx1} protein and identify thousands of conserved motif instances, suggesting a broad role for the {rfx} family in gene regulation. a second group of motifs ({lm2}*) does not match any previously known motif. we demonstrate by biochemical and computational methods that it defines a binding site for the {ctcf} protein, which is involved in insulator function to limit the spread of gene activation. we identify nearly 15,000 conserved sites that likely serve as insulators, and we show that nearby genes separated by predicted {ctcf} sites show markedly reduced correlation in gene expression. these sites may thus partition the human genome into domains of expression.
1410	1290889	article	acoustics, speech and signal processing, ieee transactions on	\N	\N	ieee	11	37	3	1989	mar	2007-05-11 23:08:36	\N	phoneme recognition using time-delay neural networks	the authors present a time-delay neural network ({tdnn}) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the {tdnn} learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. as a recognition task, the speaker-dependent recognition of the phonemes b, d, and g in varying phonetic contexts was chosen. for comparison, several discrete hidden markov models ({hmm}) were trained to perform the same task. performance evaluation over 1946 testing tokens from three speakers showed that the {tdnn} achieves a recognition rate of 98.5\\% correct while the rate obtained by the best of the {hmms} was only 93.7\\%
1411	1291533	inproceedings	\N	proceedings of the 16th international conference on world wide web	www	acm	9	\N	\N	2007	\N	2007-05-12 18:30:01	new york, ny, usa	netprobe: a fast and scalable system for fraud detection in online auction networks	given a large online network of online auction users and their histories of transactions, how can we spot anomalies and auction fraud? this paper describes the design and implementation of {netprobe}, a system that we propose for solving this problem. {netprobe} models auction users and transactions as a markov random field tuned to detect the suspicious patterns that fraudsters create, and employs a belief propagation mechanism to detect likely fraudsters. our experiments show that {netprobe} is both efficient and effective for fraud detection. we report experiments on synthetic graphs with as many as 7,000 nodes and 30,000 edges, where {netprobe} was able to spot fraudulent nodes with over 90\\% precision and recall, within a matter of seconds. we also report experiments on a real dataset crawled from {ebay}, with nearly 700,000 transactions between more than 66,000users, where {netprobe} was highly effective at unearthing hidden networks of fraudsters, within a realistic response time of about 6 minutes. for scenarios where the underlying data is dynamic in nature, we propose {incrementalnetprobe}, which is an approximate, but fast, variant of {netprobe}. our experiments prove that incremental {netprobe} executes nearly doubly fast as compared to {netprobe}, while retaining over 99\\% of its accuracy.
1412	1297459	article	developmental review	\N	\N	\N	51	27	2	2007	jun	2007-05-15 17:07:14	\N	the development of scientific thinking skills in elementary and middle school	the goal of this article is to provide an integrative review of research that has been conducted on the development of children's scientific reasoning. broadly defined, scientific thinking includes the skills involved in inquiry, experimentation, evidence evaluation, and inference that are done in the service of conceptual change or scientific understanding. therefore, the focus is on the thinking and reasoning skills that support the formation and modification of concepts and theories about the natural and social world. recent trends include a focus on definitional, methodological and conceptual issues regarding what is normative and authentic in the context of the science lab and the science classroom, an increased focus on metacognitive and metastrategic skills, and explorations of different types of instructional and practice opportunities that are required for the development, consolidation and subsequent transfer of such skills.
1413	1300254	inproceedings	computer vision and pattern recognition, 2004. cvpr 2004. proceedings of the 2004 ieee computer society conference on	2004 ieee conference on computer vision and pattern recognition	\N	ieee	\N	2	\N	2004	jun	2007-05-16 15:32:47	\N	multiscale conditional random fields for image labeling	we propose an approach to include contextual features for labeling images, in which each pixel is assigned to one of a finite set of labels. the features are incorporated into a probabilistic framework, which combines the outputs of several components. components differ in the information they encode. some focus on the image-label mapping, while others focus solely on patterns within the label field. components also differ in their scale, as some focus on fine-resolution patterns while others on coarser, more global structure. a supervised version of the contrastive divergence algorithm is applied to learn these features from labeled image data. we demonstrate performance on two real-world image databases and compare it to a classifier and a markov random field.
1414	1302073	article	nature	\N	\N	nature publishing group	4	447	7142	2007	may	2007-05-17 13:42:59	\N	origins of major human infectious diseases	many of the major human infectious diseases, including some now confined to humans and absent from animals, are 'new' ones that arose only after the origins of agriculture. where did they come from? why are they overwhelmingly of old world origins? here we show that answers to these questions are different for tropical and temperate diseases; for instance, in the relative importance of domestic animals and wild primates as sources. we identify five intermediate stages through which a pathogen exclusively infecting animals may become transformed into a pathogen exclusively infecting humans. we propose an initiative to resolve disputed origins of major diseases, and a global early warning system to monitor pathogens infecting individuals exposed to wild animals.
1415	1302950	article	bioinformatics	\N	\N	\N	2	22	12	2006	jun	2007-05-17 17:18:12	banting and best department of medical research, donnelly ccbr, university of toronto 160 college street, toronto, on m5s 3e1, canada.	{pseudopipe}: an automated pseudogene identification pipeline	motivation: mammalian genomes contain many 'genomic fossils' i.e. pseudogenes. these are disabled copies of functional genes that have been retained in the genome by gene duplication or retrotransposition events. pseudogenes are important resources in understanding the evolutionary history of genes and {genomes.results}: we have developed a homology-based computational pipeline ('{pseudopipe}') that can search a mammalian genome and identify pseudogene sequences in a comprehensive and consistent manner. the key steps in the pipeline involve using {blast} to rapidly cross-reference potential  ” parent” proteins against the intergenic regions of the genome and then processing the resulting  ” raw hits” -- i.e. eliminating redundant ones, clustering together neighbors, and associating and aligning clusters with a unique parent. finally, pseudogenes are classified based on a combination of criteria including homology, intron-exon structure, and existence of stop codons and {frameshifts.availability}: the {pseudopipe} program is implemented in python and can be downloaded at {http://pseudogene.org/contact}:{mark.gerstein}@yale.edu or zhaolei.zhang@utoronto.ca
1416	1304918	article	the journal of economic perspectives	\N	\N	\N	23	16	2	2002	\N	2007-05-18 08:30:03	\N	evolutionary theorizing in economics	the article focuses on economic analysis oriented towards understanding the workings of economies. attempts have been made to promote an evolutionary approach towards economics. basic questions related to the coordination of economic activity depend upon a central authority guiding and commanding action. the questions concerned with the processes driving economic progress, pulls the theorizing of economics, towards an evolutionary orientation. the question of economic development has waxed and waned in centrality to the discipline, as has the importance of evolutionary theorizing. it became the standard view that microeconomic theory was about equilibrium conditions. many neoclassical economists seem to hold the view that an evolutionary theory of firm and industry behavior and a neoclassical one really amount to the same thing. the problem of variety is that for the selection process to arrive at a neoclassical destination, the existing firms must represent a wide enough variety of s
1417	1307464	article	nat rev genet	nat rev genet	\N	nature publishing group	11	8	6	2007	jun	2007-05-20 11:38:23	\N	network motifs: theory and experimental approaches	transcription regulation networks control the expression of genes. the transcription networks of well-studied microorganisms appear to be made up of a small set of recurring regulation patterns, called network motifs. the same network motifs have recently been found in diverse organisms from bacteria to humans, suggesting that they serve as basic building blocks of transcription networks. here i review network motifs and their functions, with an emphasis on experimental studies. network motifs in other biological networks are also mentioned, including signalling and neuronal networks.
1418	1314047	article	neuron	\N	\N	cell press,	11	53	3	2007	feb	2007-05-20 23:55:38	university of california, helen wills neuroscience institute, 230 barker hall \\#3190, berkeley, ca 94720, usa.	timing in the absence of clocks: encoding time in neural network states.	decisions based on the timing of sensory events are fundamental to sensory processing. however, the mechanisms by which the brain measures time over ranges of milliseconds to seconds remain unclear. the dominant model of temporal processing proposes that an oscillator emits events that are integrated to provide a linear metric of time. we examine an alternate model in which cortical networks are inherently able to tell time as a result of time-dependent changes in network state. using computer simulations we show that within this framework, there is no linear metric of time, and that a given interval is encoded in the context of preceding events. human psychophysical studies were used to examine the predictions of the model. our results provide theoretical and experimental evidence that, for short intervals, there is no linear metric of time, and that time may be encoded in the high-dimensional state of local neural networks.
1419	1314313	article	annual review of biochemistry	\N	\N	\N	44	70	1	2001	\N	2007-05-21 05:07:32	department of microbiology, school of medicine, university of washington, seattle, washington 98195-7242, usa. champoux@u.washington.edu	{dna} topoisomerases: structure, function, and mechanism.	Ã¢â€“Âª abstractÃ¢â‚¬â€š dna topoisomerases solve the topological problems associated with dna replication, transcription, recombination, and chromatin remodeling by introducing temporary single- or double-strand breaks in the dna. in addition, these enzymes fine-tune the steady-state level of dna supercoiling both to facilitate protein interactions with the dna and to prevent excessive supercoiling that is deleterious. in recent years, the crystal structures of a number of topoisomerase fragments, representing nearly all the known classes of enzymes, have been solved. these structures provide remarkable insights into the mechanisms of these enzymes and complement previous conclusions based on biochemical analyses. surprisingly, despite little or no sequence homology, both type ia and type iia topoisomerases from prokaryotes and the type iia enzymes from eukaryotes share structural folds that appear to reflect functional motifs within critical regions of the enzymes. the type ib enzymes are structurally distinct from all other known topoisomerases but are similar to a class of enzymes referred to as tyrosine recombinases. the structural themes common to all topoisomerases include hinged clamps that open and close to bind dna, the presence of dna binding cavities for temporary storage of dna segments, and the coupling of protein conformational changes to dna rotation or dna movement. for the type ii topoisomerases, the binding and hydrolysis of atp further modulate conformational changes in the enzymes to effect changes in dna topology.
1420	1318673	article	trends in genetics	\N	\N	\N	\N	\N	\N	-1	\N	2007-05-22 06:27:40	\N	coevolution of genomic intron number and splice sites	spliceosomal intron numbers and boundary sequences vary dramatically in eukaryotes. we found a striking correspondence between low intron number and strong sequence conservation of 5' splice sites (5'ss) across eukaryotic genomes. the phylogenetic pattern suggests that ancestral 5'ss were relatively weakly conserved, but that some lineages independently underwent both major intron loss and 5'ss strengthening. it seems that eukaryotic ancestors had relatively large intron numbers and `weak' 5'ss, a pattern associated with frequent alternative splicing in modern organisms.
1421	1324984	article	user modeling and user-adapted interaction	\N	\N	springer	\N	17	3	2007	\N	2007-05-24 13:22:46	\N	adaptive, intelligent presentation of information for the museum visitor in {peach}	the study of intelligent user interfaces and user modeling and adaptation is well suited for augmenting educational visits to museums. we have defined a novel integrated framework for museum visits and claim that such a framework is essential in such a vast domain that inherently implies complex interactivity. we found that it requires a significant investment in software and hardware infrastructure, design and implementation of intelligent interfaces, and a systematic and iterative evaluation of the design and functionality of user interfaces, involving actual visitors at every stage. we defined and built a suite of interactive and user-adaptive technologies for museum visitors, which was then evaluated at the buonconsiglio castle in trento, italy: (1) animated agents that help motivate visitors and focus their attention when necessary, (2) automatically generated, adaptive video documentaries on mobile devices, and (3) automatically generated post-visit summaries that reflect the individual interests of visitors as determined by their behavior and choices during their visit. these components are supported by underlying user modeling and inference mechanisms that allow for adaptivity and personalization. novel software infrastructure allows for agent connectivity and fusion of multiple positioning data streams in the museum space. we conducted several experiments, focusing on various aspects of peach. in one, conducted with 110 visitors, we found evidence that even older users are comfortable interacting with a major component of the system.
1422	1324988	article	current opinion in structural biology	\N	\N	\N	6	17	2	2007	apr	2007-05-24 13:24:10	molecular and cellular modeling group, eml research, schloss-wolfsbrunnenweg 33, d-69118 heidelberg, germany.	bridging from molecular simulation to biochemical networks	how can we make the connection between the three-dimensional structures of individual proteins and understanding how complex biological systems involving many proteins work? the modelling and simulation of protein structures can help to answer this question for systems ranging from multimacromolecular complexes to organelles and cells. on one hand, multiscale modelling and simulation techniques are advancing to permit the spatial and temporal properties of large systems to be simulated using atomic-detail structures. on the other hand, the estimation of kinetic parameters for the mathematical modelling of biochemical pathways using protein structure information provides a basis for iterative manipulation of biochemical pathways guided by protein structure. recent advances include the structural modelling of protein complexes on the genomic level, novel coarse-graining strategies to increase the size of the system and the time span that can be simulated, and comparative molecular field analyses to estimate enzyme kinetic parameters.
1423	1325178	article	distance education	\N	\N	\N	12	28	1	2007	may	2007-05-24 15:05:29	\N	new learning design in distance education: the impact on student perception and motivation	many forms of e-learning (such as online courses with authentic tasks and computer-supported collaborative learning) have become important in distance education. very often, such e-learning courses or tasks are set up following constructivist design principles. often, this leads to learning environments with authentic problems in ill-structured tasks that are supposed to motivate students. however, constructivist design principles are difficult to implement because developers must be able to predict how students perceive the tasks and whether the tasks motivate the students. the research in this article queries some of the assumed effects. it presents a study that provides increased insight into the actual perception of electronic authentic learning tasks. the main questions are how students learn in such e-learning environments with "virtual" reality and authentic problems and how they perceive them. to answer these questions, in two e-learning programs developed at the open university of the netherlands ({ounl}) designers' expectations were contrasted with student perceptions. the results show a gap between the two, for students experience much less authenticity than developers assume.
1424	1326435	inproceedings	\N	proceedings of the 11th conference of the european chapter of the association for computational linguistics (eacl-06), trento, italy	\N	\N	7	\N	\N	2006	\N	2007-05-25 04:09:54	\N	using encyclopedic knowledge for named entity disambiguation	we present a new method for detecting and disambiguating named entities in open domain text. a disambiguation {svm} kernel is trained to exploit the high coverage and rich structure of the knowledge encoded in an online encyclopedia. the resulting model significantly outperforms a less informed baseline.
1425	1341447	article	the quarterly journal of economics	\N	\N	oxford university press	20	107	3	1992	aug	2007-05-29 17:35:12	\N	a simple model of herd behavior	we analyze a sequential decision model in which each decision maker looks at the decisions made by previous decision makers in taking her own decision. this is rational for her because these other decision makers may have some information that is important for her. we then show that the decision rules that are chosen by optimizing individuals will be characterized by herd behavior; i.e., people will be doing what others are doing rather than using their information. we then show that the resulting equilibrium is inefficient.
1426	1352715	article	{ieee} transactions on pattern analysis and machine intelligence	\N	\N	\N	11	25	9	2003	\N	2007-05-31 21:12:30	\N	face recognition based on fitting a {3d} morphable model	this paper presents a method for face recognition across variations in pose, ranging from frontal to profile views, and across a wide range of illuminations, including cast shadows and specular reflections. to account for these variations, the algorithm simulates the process of image formation in 3d space, using computer graphics, and it estimates 3d shape and texture of faces from single images. the estimate is achieved by fitting a statistical, morphable model of 3d faces to images. the model is learned from a set of textured 3d scans of heads. we describe the construction of the morphable model, an algorithm to fit the model to images, and a framework for face identification. in this framework, faces are represented by model parameters for 3d shape and texture. we present results obtained with 4,488 images from the publicly available cmu-pie database and 1,940 images from the feret database.
1427	1355785	techreport	\N	\N	\N	\N	\N	\N	\N	2003	may	2007-06-01 16:22:56	\N	a note on platt's probabilistic outputs for support vector machines	abstract&nbsp;&nbsp; plattâ€™s probabilistic outputs for support vector machines (platt, j. in smola, a., et al. (eds.) advances in large margin classifiers. cambridge, 2000) has been popular for applications that require posterior class probabilities. in this note, we propose an improved algorithm that theoretically converges and avoids numerical difficulties. a simple and ready-to-use pseudo code is included.
1428	1356652	article	ieee/acm trans. comput. biol. bioinformatics	\N	\N	ieee computer society press	13	4	2	2007	apr	2007-06-02 05:00:07	los alamitos, ca, usa	multiobjective optimization in bioinformatics and computational biology	this paper reviews the application of multiobjective optimization in the fields of bioinformatics and computational biology. a survey of existing work, organized by application area, forms the main body of the review, following an introduction to the key concepts in multiobjective optimization. an original contribution of the review is the identification of five distinct "contexts,” giving rise to multiple objectives: these are used to explain the reasons behind the use of multiobjective optimization in each application area and also to point the way to potential future uses of the technique.
1429	1366162	article	neuroscientist	\N	\N	\N	14	13	3	2007	jun	2007-06-05 21:20:53	\N	the neural basis of inhibition in cognitive control	the concept of "inhibition" is widely used in synaptic, circuit, and systems neuroscience, where it has a clear meaning because it is clearly observable. the concept is also ubiquitous in psychology. one common use is to connote an active/willed process underlying cognitive control. many authors claim that subjects execute cognitive control over unwanted stimuli, task sets, responses, memories, and emotions by inhibiting them, and that frontal lobe damage induces distractibility, impulsivity, and perseveration because of damage to an inhibitory mechanism. however, with the exception of the motor domain, the notion of an active inhibitory process underlying cognitive control has been heavily challenged. alternative explanations have been provided that explain cognitive control without recourse to inhibition as concept, mechanism, or theory. this article examines the role that neuroscience can play when examining whether the psychological concept of active inhibition can be meaningfully applied in cognitive control research. {neuroscientist} 13(3):214--228, 2007. 10.1177/1073858407299288
1430	1373854	article	bmc bioinformatics	\N	\N	\N	\N	3	1	2002	may	2007-06-09 04:52:33	howard hughes medical institute and department of genetics, washington university school of medicine, st louis, mo 63110, usa. zmasek@genetics.wustl.edu	{rio}: analyzing proteomes by automated phylogenomics using resampled inference of orthologs.	{background}: when analyzing protein sequences using sequence similarity searches, orthologous sequences (that diverged by speciation) are more reliable predictors of a new protein's function than paralogous sequences (that diverged by gene duplication). the utility of phylogenetic information in high-throughput genome annotation ("phylogenomics") is widely recognized, but existing approaches are either manual or not explicitly based on phylogenetic trees. {results}: here we present {rio} (resampled inference of orthologs), a procedure for automated phylogenomics using explicit phylogenetic inference. {rio} analyses are performed over bootstrap resampled phylogenetic trees to estimate the reliability of orthology assignments. we also introduce supplementary concepts that are helpful for functional inference. {rio} has been implemented as perl pipeline connecting several c and java programs. it is available at http://www.genetics.wustl.edu/eddy/forester/. a web server is at http://www.rio.wustl.edu/. {rio} was tested on the arabidopsis thaliana and caenorhabditis elegans proteomes. {conclusion}: the {rio} procedure is particularly useful for the automated detection of first representatives of novel protein subfamilies. we also describe how some orthologies can be misleading for functional inference.
1431	1387480	article	journal of computational chemistry	\N	\N	\N	20	16	11	1995	\N	2007-06-13 15:04:48	\N	application of the multimolecule and multiconformational {resp} methodology to biopolymers: charge derivation for {dna}, {rna}, and proteins	we present the derivation of charges of ribo- and deoxynucleosides, nucleotides, and peptide fragments using electrostatic potentials obtained from ab initio calculations with the 6-31g* basis set. for the nucleic acid fragments, we used electrostatic potentials of the four deoxyribonucleosides (a, g, c, t) and four ribonucleosides (a, g, c, u) and dimethylphosphate. the charges for the deoxyribose nucleosides and nucleotides are derived using multiple-molecule fitting and restrained electrostatic potential (resp) fits,1,2 with lagrangian multipliers ensuring a net charge of 0 or Â± 1. we suggest that the preferred approach for deriving charges for nucleosides and nucleotides involves allowing only c1? and h1? of the sugar to vary as the nucleic acid base, with the remainder of sugar and backbone atoms forced to be equivalent. for peptide fragments, we have combined multiple conformation fitting, previously employed by williams3 and reynolds et al.,4 with the resp approach1,2 to derive charges for blocked dipeptides appropriate for each of the 20 naturally occuring amino acids. based on our results for propyl amine,1,2 we suggest that two conformations for each peptide suffice to give charges that represent well the conformationally dependent electrostatic properties of molecules, provided that these two conformations contain different values of the dihedral angles that terminate in heteroatoms or hydrogens attached to heteroatoms. in these blocked dipeptide models, it is useful to require equivalent nÂ -Â h and c&dbond;o charges for all amino acids with a given net charge (except proline), and this is accomplished in a straightforward fashion with multiple-molecule fitting. finally, the application of multiple lagrangian constraints allows for the derivation of monomeric residues with the appropriate net charge from a chemically blocked version of the residue. the multiple lagrange constraints also enable charges from two or more molecules to be spliced together in a well-defined fashion. thus, the combined use of multiple molecules, multiple conformations, multiple lagrangian constraints, and resp fitting is shown to be a powerful approach to deriving electrostatic charges for biopolymers. {\\\\\\\\copyright} 1995 john wiley & sons, inc.
1432	1389669	article	transactions in gis	\N	\N	\N	21	11	3	2007	\N	2007-06-14 10:22:26	\N	a {task-based} ontology approach to automate geospatial data retrieval	abstract this paper presents a task-based and semantic web approach to find geospatial data. the purpose of the project is to improve data discovery and facilitate automatic retrieval of data sources. the work presented here helps create the beginnings of a geospatial semantic web. the intent is to create a system that provides appropriate results to application users who search for data when facing tasks such as emergency response or planning activities. in our task-based system, we formalize the relationships between types of tasks, including emergency response, and types of data sources needed for those tasks. domain knowledge, including criteria describing data sources, is recorded in an ontology language. with the ontology, reasoning can be done to infer various types of information including which data sources meet specific criteria for use in particular tasks. the vision presented here is that in an emergency, for example, a user accesses a web-based application and selects the type of emergency and the geographic area. the application then returns the types and locations ({urls}) of the specific geospatial data needed. we explore the abilities and limitations of the {owl} web ontology language along with other semantic web technologies for this purpose.
1433	1390187	article	genome research	\N	\N	\N	10	17	6	2007	jun	2007-06-14 16:33:43	\N	statistical analysis of the genomic distribution and correlation of regulatory elements in the {encode} regions	the comprehensive inventory of functional elements in 44 human genomic regions carried out by the {encode} project consortium enables for the first time a global analysis of the genomic distribution of transcriptional regulatory elements. in this study we developed an intuitive and yet powerful approach to analyze the distribution of regulatory elements found in many different {chip}–chip experiments on a 10?100-kb scale. first, we focus on the overall chromosomal distribution of regulatory elements in the {encode} regions and show that it is highly nonuniform. we demonstrate, in fact, that regulatory elements are associated with the location of known genes. further examination on a local, single-gene scale shows an enrichment of regulatory elements near both transcription start and end sites. our results indicate that overall these elements are clustered into regulatory rich  ” islands” and poor  ” deserts.” next, we examine how consistent the nonuniform distribution is between different transcription factors. we perform on all the factors a multivariate analysis in the framework of a biplot, which enhances biological signals in the experiments. this groups transcription factors into sequence-specific and sequence-nonspecific clusters. moreover, with experimental variation carefully controlled, detailed correlations show that the distribution of sites was generally reproducible for a specific factor between different laboratories and microarray platforms. data sets associated with histone modifications have particularly strong correlations. finally, we show how the correlations between factors change when only regulatory elements far from the transcription start sites are considered.
1434	1395097	article	\N	\N	\N	\N	\N	\N	\N	1996	\N	2007-06-17 14:31:18	\N	calculating scattering amplitudes efficiently	we review techniques for more efficient computation of perturbative scattering amplitudes in gauge theory, in particular tree and one-loop multi-parton amplitudes in qcd. we emphasize the advantages of (1) using color and helicity information to decompose amplitudes into smaller gauge-invariant pieces, and (2) exploiting the analytic properties of these pieces, namely their cuts and poles. other useful tools include recursion relations, special gauges and supersymmetric rearrangements.
1435	1396810	book	\N	\N	\N	oxford university press, usa	\N	\N	\N	2005	dec	2007-06-18 12:57:26	\N	23 problems in systems neuroscience (computational neuroscience series)	{the complexity of the brain and the protean nature of behavior remain the most elusive area of science, but also the most important. van hemmen and sejnowski invited 23 experts from the many areas--from evolution to qualia--of systems neuroscience to formulate one problem each. although each chapter was written independently and can be read separately, together they provide a useful roadmap to the field of systems neuroscience and will serve as a source of inspirations for future explorers of the brain.}
1436	1401901	inproceedings	infocom 2004. twenty-third annualjoint conference of the ieee computer and communications societies	infocom 2004. twenty-third annualjoint conference of the ieee computer and communications societies	\N	ieee	11	1	\N	2004	mar	2007-06-21 06:22:31	\N	peer-to-peer support for massively multiplayer games	we present an approach to support massively multiplayer games on peer-to-peer overlays. our approach exploits the fact that players in {mmgs} display locality of interest, and therefore can form self-organizing groups based on their locations in the virtual world. to this end, we have designed scalable mechanisms to distribute the game state to the participating players and to maintain consistency in the face of node failures. the resulting system dynamically scales with the number of online players. it is more flexible and has a lower deployment cost than centralized games servers. we have implemented a simple game we call {simmud}, and experimented with up to 4000 players to demonstrate the applicability of this approach.
1437	1405800	book	\N	\N	\N	yale university press	\N	\N	\N	1996	\N	2007-06-23 04:41:26	new haven, ct	pathologies of rational choice theory: a critique of applications in political science	this is the first comprehensive critical evaluation of the use of rational choice explanations in political science. writing in an accessible and nontechnical style, donald p. green and ian shapiro assess rational choice theory where it is reputed to be most successful: the study of collective action, the behavior of political parties and politicians, and such phenomena as voting cycles and prisoner's dilemmas. in their hard-hitting critique, green and shapiro demonstrate that the much-heralded achievements of rational choice theory are in fact deeply suspect and that fundamental rethinking is needed if rational choice theorists are to contribute to the understanding of politics. green and shapiro show that empirical tests of rational choice theories are marred by a series of methodological defects. these defects flow from the characteristic rational choice impulse to defend universal theories of politics. as a result, many tests are so poorly conducted as to be irrelevant to evaluating rational choice models. tests that are properly conducted either tend to undermine rational choice theories or to lend support for propositions that are banal. green and shapiro offer numerous suggestions as to how rational choice propositions might be reformulated as parts of testable hypotheses for the study of politics. in a final chapter they anticipate and respond to a variety of rational choice counterarguments, thereby initiating a dialogue that is bound to continue for some time
1438	1409286	article	briefings in bioinformatics	\N	\N	oxford university press	10	7	2	2006	jun	2007-06-24 16:21:12	\N	flux balance analysis in the era of metabolomics	flux balance analysis ({fba}) has emerged as an effective means to analyse biological networks in a quantitative manner. much progress has been made on the extension of {fba} to incorporate a priori biological knowledge, provide more practical descriptions of observed cell behaviours, and predict the outcome of network perturbations. metabolomics is independently advancing as a set of high-throughput data acquisition tools providing dynamic profiles of metabolites in an unbiased manner. these data sets are neither yet sufficiently comprehensive nor accurate enough for generating large-scale kinetic models. thus, there is a pressing need to develop quantitative techniques that can make use of the emerging data and embrace the associated uncertainties. this article reviews recent advances in {fba} to meet this need and discusses the utility of {fba} as a complement to metabolomics and the expected synergy as a result of combining these two techniques.
1439	1413697	article	proceedings of the royal society b: biological sciences	\N	\N	the royal society	5	272	1561	2005	\N	2007-06-26 14:18:50	\N	{discrete hierarchical organization of social group sizes}	the 'social brain hypothesis' for the evolution of large brains in primates has led to evidence for the coevolution of neocortical size and social group sizes, suggesting that there is a cognitive constraint on group size that depends, in some way, on the volume of neural material available for processing and synthesizing information on social relationships. more recently, work on both human and non-human primates has suggested that social groups are often hierarchically structured. we combine data on human grouping patterns in a comprehensive and systematic study. using fractal analysis, we identify, with high statistical confidence, a discrete hierarchy of group sizes with a preferred scaling ratio close to three: rather than a single or a continuous spectrum of group sizes, humans spontaneously form groups of preferred sizes organized in a geometrical series approximating 3-5, 9-15, 30-45, etc. such discrete scale invariance could be related to that identified in signatures of herding behaviour in financial markets and might reflect a hierarchical processing of social nearness by human brains. Â© 2005 the royal society.
1440	1424521	article	archives of biochemistry and biophysics	\N	\N	\N	15	469	1	2008	jan	2007-07-02 01:12:30	department of biochemistry and biophysics, the university of north carolina at chapel hill, school of medicine, chapel hill, nc 27599, usa.	protein folding: then and now	over the past three decades the protein folding field has undergone monumental changes. originally a purely academic question, how a protein folds has now become vital in understanding diseases and our abilities to rationally manipulate cellular life by engineering protein folding pathways. we review and contrast past and recent developments in the protein folding field. specifically, we discuss the progress in our understanding of protein folding thermodynamics and kinetics, the properties of evasive intermediates, and unfolded states. we also discuss how some abnormalities in protein folding lead to protein aggregation and human diseases.
1441	1427756	article	nature	\N	\N	nature publishing group	4	448	7155	2007	jul	2007-08-17 18:13:43	\N	structure-based activity prediction for an enzyme of unknown function	with many genomes sequenced, a pressing challenge in biology is predicting the function of the proteins that the genes encode. when proteins are unrelated to others of known activity, bioinformatics inference for function becomes problematic. it would thus be useful to interrogate protein structures for function directly. here, we predict the function of an enzyme of unknown activity, tm0936 from thermotoga maritima, by docking high-energy intermediate forms of thousands of candidate metabolites. the docking hit list was dominated by adenine analogues, which appeared to undergo c6-deamination. four of these, including 5-methylthioadenosine and s-adenosylhomocysteine ({sah}), were tested as substrates, and three had substantial catalytic rate constants (105?m-1?s-1). the x-ray crystal structure of the complex between tm0936 and the product resulting from the deamination of {sah}, s-inosylhomocysteine, was determined, and it corresponded closely to the predicted structure. the deaminated products can be further metabolized by t. maritima in a previously uncharacterized {sah} degradation pathway. structure-based docking with high-energy forms of potential substrates may be a useful tool to annotate enzymes for function.
1442	1436865	inproceedings	\N	acm sigcomm	\N	\N	\N	\N	\N	2007	aug	2007-07-05 14:28:08	kyoto, japan	can internet {video-on-demand} be profitable?	video-on-demand in the internet has become an immensely popular service in recent years. but due to its high bandwidth requirements and popularity, it is also a costly service to provide. we consider the design and potential benefits of peer-assisted video-on-demand, in which participating peers assist the server in delivering vod content. the assistance is done in such a way that it provides the same user quality experience as pure client-server distribution. we focus on the single-video approach, whereby a peer only redistributes a video that it is currently watching.
1443	1439241	article	molecular cell	\N	\N	\N	14	27	1	2007	jul	2007-07-06 11:03:39	\N	{microrna} targeting specificity in mammals: determinants beyond seed pairing	summary mammalian {micrornas} ({mirnas}) pair to {3'utrs} of {mrnas} to direct their posttranscriptional repression. important for target recognition are \\~{}7 nt sites that match the seed region of the {mirna}. however, these seed matches are not always sufficient for repression, indicating that other characteristics help specify targeting. by combining computational and experimental approaches, we uncovered five general features of site context that boost site efficacy: {au}-rich nucleotide composition near the site, proximity to sites for coexpressed {mirnas} (which leads to cooperative action), proximity to residues pairing to {mirna} nucleotides 13-16, positioning within the {3'utr} at least 15 nt from the stop codon, and positioning away from the center of long {utrs}. a model combining these context determinants quantitatively predicts site performance both for exogenously added {mirnas} and for endogenous {mirna}-message interactions. because it predicts site efficacy without recourse to evolutionary conservation, the model also identifies effective nonconserved sites and {sirna} off-targets.
1444	1439666	article	science (new york, n.y.)	\N	\N	\N	4	317	5839	2007	aug	2007-07-06 16:30:47	\N	high-speed imaging reveals neurophysiological links to behavior in an animal model of depression.	the hippocampus is one of several brain areas thought to play a central role in affective behaviors, but the underlying local network dynamics are not understood. we used quantitative voltage-sensitive dye imaging to probe hippocampal dynamics with millisecond resolution in brain slices after bidirectional modulation of affective state in rat models of depression. we found that a simple measure of real-time activity-stimulus-evoked percolation of activity through the dentate gyrus relative to the hippocampal output subfield-accounted for induced changes in animal behavior independent of the underlying mechanism of action of the treatments. our results define a circuit-level neurophysiological endophenotype for affective behavior and suggest an approach to understanding circuit-level substrates underlying psychiatric disease symptoms.
1445	1446281	inproceedings	\N	icse	\N	\N	\N	\N	\N	2007	\N	2007-07-10 11:52:13	minneapolis, mn, usa	feedback-directed random test generation	we present a technique that improves random test generation by incorporating feedback obtained from executing test inputs as they are created. our technique builds inputs incrementally by randomly selecting a method call to apply and finding arguments from among previously-constructed inputs. as soon as an input is built, it is executed and checked against a set of contracts and filters. the result of the execution determines whether the input is redundant, illegal, contract-violating, or useful for generating more inputs. the technique outputs a test suite consisting of unit tests for the classes under test. passing tests can be used to ensure that code contracts are preserved across program changes; failing tests (that violate one or more contract) point to potential errors that should be corrected. our experimental results indicate that feedback-directed random test generation can outperform systematic and undirected random test generation, in terms of coverage and error detection. on four small but nontrivial data structures (used previously in the literature), our technique achieves higher or equal block and predicate coverage than model checking (with and without abstraction) and undirected random generation. on 14 large, widely-used libraries (comprising 780kloc), feedback-directed random test generation finds many previously-unknown errors, not found by either model checking or undirected random generation.
1446	1464882	article	\N	\N	\N	\N	3	290	5496	2000	\N	2007-07-18 13:46:17	\N	{nanoelectromechanical systems}	nanoelectromechanical systems (nems) are drawing interest from both technical and scientific communities. these are electromechanical systems, much like microelectromechanical systems, mostly operated in their resonant modes with dimensions in the deep submicron. in this size regime, they come with extremely high fundamental resonance frequencies, diminished active masses,and tolerable force constants; the quality (q) factors of resonance are in the range q âˆ¼ 103â€“105â€”significantly higher than those of electrical resonant circuits. these attributes collectively make nems suitable for a multitude of technological applications such as ultrafast sensors, actuators, and signal processing components. experimentally, nems are expected to open up investigations of phonon mediated mechanical processes and of the quantum behavior of mesoscopic mechanical systems. however, there still exist fundamental and technological challenges to nems optimization. in this review we shall provide a balanced introduction to nems by discussing the prospects and challenges in this rapidly developing field and outline an exciting emerging application, nanoelectromechanical mass detection.
1447	1465977	article	nature reviews. genetics	\N	\N	nature publishing group	8	8	8	2007	aug	2007-07-20 17:26:50	\N	the distribution of fitness effects of new mutations.	the distribution of fitness effects ({dfe}) of new mutations is a fundamental entity in genetics that has implications ranging from the genetic basis of complex disease to the stability of the molecular clock. it has been studied by two different approaches: mutation accumulation and mutagenesis experiments, and the analysis of {dna} sequence data. the proportion of mutations that are advantageous, effectively neutral and deleterious varies between species, and the {dfe} differs between coding and non-coding {dna}. despite these differences between species and genomic regions, some general principles have emerged: advantageous mutations are rare, and those that are strongly selected are exponentially distributed; and the {dfe} of deleterious mutations is complex and multi-modal.
1448	1467510	proceedings	computer vision and pattern recognition, 2006 ieee computer society conference on	cvpr	\N	ieee computer society	7	1	\N	2006	jul	2007-07-19 16:19:58	washington, dc, usa	principled hybrids of generative and discriminative models	when labelled training data is plentiful, discriminative techniques are widely used since they give excellent generalization performance. however, for large-scale applications such as object recognition, hand labelling of data is expensive, and there is much interest in semi-supervised techniques based on generative models in which the majority of the training data is unlabelled. although the generalization performance of generative models can often be improved by \\&\\#145;training them discriminatively\\&\\#146;, they can then no longer make use of unlabelled data. in an attempt to gain the benefit of both generative and discriminative approaches, heuristic procedure have been proposed [2, 3] which interpolate between these two extremes by taking a convex combination of the generative and discriminative objective functions. in this paper we adopt a new perspective which says that there is only one correct way to train a given model, and that a \\&\\#145;discriminatively trained\\&\\#146; generative model is fundamentally a new model [7]. from this viewpoint, generative and discriminative models correspond to specific choices for the prior over parameters. as well as giving a principled interpretation of \\&\\#145;discriminative training\\&\\#146;, this approach opens door to very general ways of interpolating between generative and discriminative extremes through alternative choices of prior. we illustrate this framework using both synthetic data and a practical example in the domain of multi-class object recognition. our results show that, when the supply of labelled training data is limited, the optimum performance corresponds to a balance between the purely generative and the purely discriminative.
1449	1470445	article	genes \\& development	\N	\N	\N	6	21	14	2007	jul	2007-07-21 02:25:24	gene center of the ludwig-maximilians-university, d-81377 m\\"{u}nchen, germany;	{pirnas}—the ancient hunters of genome invaders	in addition to {mirnas} and {sirnas}, a third small {rna} silencing system has been uncovered that prevents the spreading of selfish genetic elements. production of the piwi-associated {rnas} ({pirnas}), which mediate the silencing activity in this pathway, is initiated at a few master control regions within the genome. the nature of the primary {pirna}-generating transcript is still unknown, but {rna} interference ({rnai})-like cleavage events are likely defining the 5?-ends of mature {pirnas}. we summarize the recent literature on {pirna} biogenesis and function with an emphasis on work in drosophila, where genetics and biochemistry have met very successfully.
1450	1471614	article	proceedings of the national academy of sciences	\N	\N	\N	5	104	30	2007	jul	2007-07-21 16:43:45	\N	widely distributed noncoding purifying selection in the human genome	10.1073/pnas.0705140104 it is widely assumed that human noncoding sequences comprise a substantial reservoir for functional variants impacting gene regulation and other chromosomal processes. evolutionarily conserved noncoding sequences ({cnss}) in the human genome have attracted considerable attention for their potential to simplify the search for functional elements and phenotypically important human alleles. a major outstanding question is whether functionally significant human noncoding variation is concentrated in {cnss} or distributed more broadly across the genome. here, we combine wholegenome sequence data from four nonhuman species (chimp, dog, mouse, and rat) with recently available comprehensive human polymorphism data to analyze selection at single-nucleotide resolution. we show that a substantial fraction of active purifying selection in human noncoding sequences occurs outside of {cnss} and is diffusely distributed across the genome. this finding suggests the existence of a large complement of human noncoding variants that may impact gene expression and phenotypic traits, the majority of which will escape detection with current approaches to genome analysis.
1451	1485773	article	information systems research	\N	\N	\N	30	2	3	1991	\N	2007-07-25 13:49:36	\N	development of an instrument to measure the perceptions of adopting an information technology innovation	this paper reports on the development of an instrument designed to measure the various perceptions that an individual may have of adopting an information technology (it) innovation. this instrument is intended to be a tool for the study of the initial adoption and eventual diffusion of it innovations within organizations. while the adoption of information technologies by individuals and organizations has been an area of substantial research interest since the early days of computerization, research efforts to date have led to mixed and inconclusive outcomes. the lack of a theoretical foundation for such research and inadequate definition and measurement of constructs have been identified as major causes for such outcomes. in a recent study examining the diffusion of new end-user it, we decided to focus on measuring the potential adopters' perceptions of the technology. measuring such perceptions has been termed a "classic issue" in the innovation diffusion literature, and a key to integrating the various findings of diffusion research. the perceptions of adopting were initially based on the five characteristics of innovations derived by rogers (1983) from the diffusion of innovations literature, plus two developed specifically within this study. of the existing scales for measuring these characteristics, very few had the requisite levels of validity and reliability. for this study, both newly created and existing items were placed in a common pool and subjected to four rounds of sorting by judges to establish which items should be in the various scales. the objective was to verify the convergent and discriminant validity of the scales by examining how the items were sorted into various construct categories. analysis of interjudge agreement about item placement identified both bad items as well as weaknesses in some of the constructs' original definitions. these were subsequently redefined. scales for the resulting constructs were subjected to three separate... abstract from author copyright of information systems research is the property of informs: institute for operations research and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. however, users may print, download, or email articles for individual use. this abstract may be abridged. no warranty is given about the accuracy of the copy. users should refer to the original published version of the material for the full abstract. (copyright applies to all abstracts)
1452	1505738	misc	\N	\N	\N	\N	\N	\N	\N	-1	\N	2007-07-26 20:53:08	\N	computational genetics, physiology, metabolism, neural systems, learning, vision, and behavior or {polyworld}: life in a new context	this paper discusses a computer model of living organisms and the ecology they exist in called polyworld. polyworld attempts to bring together all the principle components of real living systems into a single artificial (man-made) living system. polyworld brings together biologically motivated genetics, simple simulated physiologies and metabolisms, hebbian learning in arbitrary neural network architectures, a visual perceptive mechanism, and a suite of primitive behaviors in artificial organisms grounded in an ecology just complex enough to foster speciation and inter-species competition. predation, mimicry, sexual reproduction, and even communication are all supported in a straightforward fashion. the resulting survival strategies, both individual and group, are purely emergent, as are the functionalities embodied in their neural network &#034;brains&#034;. complex behaviors resulting from the simulated neural activity are unpredictable, and change as natural selection acts over multiple generations. in many ways, polyworld may be thought of as a sort of electronic primordial soup experiment, in the vein of urey and miller&#039;s [33] classic experiment, only commencing at a much higher level of organization. while one could claim that urey and miller really just threw a bunch of ingredients in a pot and watched to see what happened, the reason these men made a contribution to science rather than ratatouille is that they put the right ingredients in the right pot ... and watched to see what happened. here we start with software-coded genetics and various simple nerve cells (lightsensitive, motor, and unspecified neuronal) as the ingredients, and place them in a competitive ecological crucible which subjects them to an internally consistent physics and the process of natural selectio...
1453	1506799	article	bioinformatics (oxford, england)	\N	\N	oxford university press	1	22	19	2006	oct	2007-07-27 11:55:18	\N	{alibaba}: {pubmed} as a graph.	the biomedical literature contains a wealth of information on associations between many different types of objects, such as protein-protein interactions, gene-disease associations and subcellular locations of proteins. when searching such information using conventional search engines, e.g. {pubmed}, users see the data only one-abstract at a time and 'hidden' in natural language text. {alibaba} is an interactive tool for graphical summarization of search results. it parses the set of abstracts that fit a {pubmed} query and presents extracted information on biomedical objects and their relationships as a graphical network. {alibaba} extracts associations between cells, diseases, drugs, proteins, species and tissues. several filter options allow for a more focused search. thus, researchers can grasp complex networks described in various articles at a glance. http://alibaba.informatik.hu-berlin.de/
1454	1507853	article	nature genetics	\N	\N	nature publishing group	4	39	8	2007	aug	2007-08-01 19:39:31	\N	evolution of chromosome organization driven by selection for reduced gene expression noise.	the distribution of genes on eukaryotic chromosomes is nonrandom, but the reasons behind this are not well understood. the commonly observed clustering of essential genes is a case in point. here we model and test a new hypothesis. essential proteins are unusual in that random fluctuations in abundance (noise) can be highly deleterious. we hypothesize that persistently open chromatin domains are sinks for essential genes, as they enable reduced noise by avoidance of transcriptional bursting associated with chromatin remodeling. simulation of the model captures clustering and correctly predicts that (i) essential gene clusters are associated with low nucleosome occupancy (ii) noise-sensitive nonessential genes cluster with essential genes (iii) nonessential genes of similar knockout fitness are physically linked (iv) genes in domains rich in essential genes have low noise (v) essential genes are rare subtelomerically and (vi) essential gene clusters are preferentially conserved. we conclude that different noise characteristics of different genomic domains favors nonrandom gene positioning. this has implications for gene therapy and understanding transgenic phenotypes.
1455	1510686	inproceedings	\N	proceedings of the 30th annual international acm sigir conference on research and development in information retrieval	sigir	acm	7	\N	\N	2007	\N	2007-07-29 01:00:06	new york, ny, usa	towards automatic extraction of event and place semantics from flickr tags	we describe an approach for extracting semantics of tags, unstructured text-labels assigned to resources on the web, based on each tag's usage patterns. in particular, we focus on the problem of extracting place and event semantics for tags that are assigned to photos on flickr, a popular photo sharing website that supports time and location (latitude/longitude) metadata. we analyze two methods inspired by well-known burst-analysis techniques and one novel method: scale-structure identification. we evaluate the methods on a subset of flickr data, and show that our scale-structure identification method outperforms the existing techniques. the approach and methods described in this work can be used in other domains such as geo-annotated web pages, where text terms can be extracted and associated with usage pattern
1456	1512273	article	annual review of physical chemistry	\N	\N	\N	20	58	1	2007	\N	2007-07-30 08:29:59	dan t gillespie consulting, castaic, ca 91384, usa. gillespiedt@mailaps.org	stochastic simulation of chemical kinetics	{abstractstochastic} chemical kinetics describes the time evolution of a well-stirred chemically reacting system in a way that takes into account the fact that molecules come in whole numbers and exhibit some degree of randomness in their dynamical behavior. researchers are increasingly using this approach to chemical kinetics in the analysis of cellular systems in biology, where the small molecular populations of only a few reactant species can lead to deviations from the predictions of the deterministic differential equations of classical chemical kinetics. after reviewing the supporting theory of stochastic chemical kinetics, i discuss some recent advances in methods for using that theory to make numerical simulations. these include improvements to the exact stochastic simulation algorithm ({ssa}) and the approximate explicit tau-leaping procedure, as well as the development of two approximate strategies for simulating systems that are dynamically stiff: implicit tau-leaping and the slow-scale {ssa}.
1457	1514425	article	journal of biomedical informatics	\N	\N	\N	17	41	1	2008	feb	2007-07-30 16:49:27	national cancer institute center for bioinformatics (ncicb), 2115 e. jefferson st., suite 5000, rockville, md 20852, usa.	{cacore} version 3: implementation of a model driven, service-oriented architecture for semantic interoperability	one of the requirements for a federated information system is interoperability, the ability of one computer system to access and use the resources of another system. this feature is particularly important in biomedical research systems, which need to coordinate a variety of disparate types of data. in order to meet this need, the national cancer institute center for bioinformatics ({ncicb}) has created the cancer common ontologic representation environment ({cacore}), an interoperability infrastructure based on model driven architecture. the {cacore} infrastructure provides a mechanism to create interoperable biomedical information systems. systems built using the {cacore} paradigm address both aspects of interoperability: the ability to access data (syntactic interoperability) and understand the data once retrieved (semantic interoperability). this infrastructure consists of an integrated set of three major components: a controlled terminology service (enterprise vocabulary services), a standards-based metadata repository (the cancer data standards repository) and an information system with an application programming interface ({api}) based on domain model driven architecture. this infrastructure is being leveraged to create a semantic {service-oriented} architecture ({ssoa}) for cancer research by the national cancer institute's cancer biomedical informatics grid ({cabig}™).
1458	1524073	article	molecular systems biology	\N	\N	nature publishing group	\N	3	1	2007	jun	2007-07-31 07:08:42	\N	a genome-scale metabolic reconstruction for escherichia coli k-12 {mg1655} that accounts for 1260 {orfs} and thermodynamic information.	an updated genome-scale reconstruction of the metabolic network in escherichia coli k-12 {mg1655} is presented. this updated metabolic reconstruction includes: (1) an alignment with the latest genome annotation and the metabolic content of {ecocyc} leading to the inclusion of the activities of 1260 {orfs}, (2) characterization and quantification of the biomass components and maintenance requirements associated with growth of e. coli and (3) thermodynamic information for the included chemical reactions. the conversion of this metabolic network reconstruction into an in silico model is detailed. a new step in the metabolic reconstruction process, termed thermodynamic consistency analysis, is introduced, in which reactions were checked for consistency with thermodynamic reversibility estimates. applications demonstrating the capabilities of the genome-scale metabolic model to predict high-throughput experimental growth and gene deletion phenotypic screens are presented. the increased scope and computational capability using this new reconstruction is expected to broaden the spectrum of both basic biology and applied systems biology studies of e. coli metabolism.
1459	1526396	article	mol syst biol	\N	\N	\N	\N	3	\N	2007	jul	2007-08-01 07:24:07	\N	a synthetic gene network for tuning protein degradation in saccharomyces cerevisiae	protein decay rates are regulated by degradation machinery that clears unnecessary housekeeping proteins and maintains appropriate dynamic resolution for transcriptional regulators. turnover rates are also crucial for fluorescence reporters that must strike a balance between sufficient fluorescence for signal detection and temporal resolution for tracking dynamic responses. here, we use components of the escherichia coli degradation machinery to construct a saccharomyces cerevisiae strain that allows for tunable degradation of a tagged protein. using a microfluidic platform tailored for single-cell fluorescence measurements, we monitor protein decay rates after repression using an ssra-tagged fluorescent reporter. we observe a half-life ranging from 91 to 22 min, depending on the level of activation of the degradation genes. computational modeling of the underlying set of enzymatic reactions leads to gfp decay curves that are in excellent agreement with the observations, implying that degradation is governed by michaelis?menten-type interactions. in addition to providing a reporter with tunable dynamic resolution, our findings set the stage for explorations of the effect of protein degradation on gene regulatory and signalling pathways.keywords: gene regulation, temporal dynamics
1460	1528517	techreport	\N	\N	\N	\N	\N	\N	\N	-1	\N	2007-08-01 18:29:44	\N	swoosh: a generic approach to entity resolution	we consider the entity resolution (er) problem (also known as deduplication, or merge---purge), in which records determined to represent the same real-world entity are successively located and merged. we formalize the generic er problem, treating the functions for comparing and merging records as black-boxes, which permits expressive and extensible er solutions. we identify four important properties that, if satisfied by the match and merge functions, enable much more efficient er algorithms. we develop three efficient er algorithms: g-swoosh for the case where the four properties do not hold, and r-swoosh and f-swoosh that exploit the four properties. f-swoosh in addition assumes knowledge of the "features" (e.g., attributes) used by the match function. we experimentally evaluate the algorithms using comparison shopping data from yahoo! shopping and hotel information data from yahoo! travel. we also show that r-swoosh (and f-swoosh) can be used even when the four match and merge properties do not hold, if an "approximate" result is acceptable.
1461	1528747	article	plos biology	\N	\N	public library of science	\N	5	7	2007	jul	2007-08-01 19:58:52	\N	development of the human infant intestinal microbiota.	almost immediately after a human being is born, so too is a new microbial ecosystem, one that resides in that person's gastrointestinal tract. although it is a universal and integral part of human biology, the temporal progression of this process, the sources of the microbes that make up the ecosystem, how and why it varies from one infant to another, and how the composition of this ecosystem influences human physiology, development, and disease are still poorly understood. as a step toward systematically investigating these questions, we designed a microarray to detect and quantitate the small subunit ribosomal {rna} ({ssu} {rrna}) gene sequences of most currently recognized species and taxonomic groups of bacteria. we used this microarray, along with sequencing of cloned libraries of {pcr}-amplified {ssu} {rdna}, to profile the microbial communities in an average of 26 stool samples each from 14 healthy, full-term human infants, including a pair of dizygotic twins, beginning with the first stool after birth and continuing at defined intervals throughout the first year of life. to investigate possible origins of the infant microbiota, we also profiled vaginal and milk samples from most of the mothers, and stool samples from all of the mothers, most of the fathers, and two siblings. the composition and temporal patterns of the microbial communities varied widely from baby to baby. despite considerable temporal variation, the distinct features of each baby's microbial community were recognizable for intervals of weeks to months. the strikingly parallel temporal patterns of the twins suggested that incidental environmental exposures play a major role in determining the distinctive characteristics of the microbial community in each baby. by the end of the first year of life, the idiosyncratic microbial ecosystems in each baby, although still distinct, had converged toward a profile characteristic of the adult gastrointestinal tract.
1462	1530733	inproceedings	\N	proceedings of the 18th annual acm symposium on user interface software and technology	uist	acm	3	\N	\N	2005	\N	2007-08-02 15:07:24	new york, ny, usa	low-cost multi-touch sensing through frustrated total internal reflection	this paper describes a simple, inexpensive, and scalable technique for enabling high-resolution multi-touch sensing on rear-projected interactive surfaces based on frustrated total internal reflection. we review previous applications of this phenomenon to sensing, provide implementation details, discuss results from our initial prototype, and outline future directions.
1463	1539479	book	\N	\N	\N	vdm verlag dr. m\\"{u}ller	\N	\N	\N	2007	\N	2007-08-07 09:45:44	\N	social software im unternehmen. wikis und weblogs f\\"{u}r wissensmanagement und kommunikation	"in den letzten jahren hat sich im internet eine neue art von software, sogenannte social software, zu der auch wikis und weblogs geh\\"{o}ren, entwickelt. gleichzeitig besteht bei vielen unternehmen der bedarf, das im unternehmen vorhandene wissen besser zu nutzen und die kommunikation effizienter zu gestalten. das buch f\\"{u}hrt in die thematiken wissensmanagement und unternehmenskommunikation ein, um anschlie{\\ss}end detailliert social software und deren auspr\\"{a}gungen zu er\\"{o}rtern. darauf folgend werden konkrete einsatzm\\"{o}glichkeiten insbesondere von wikis und weblogs in einem unternehmerischen umfeld dargestellt. die nutzung und die entstehenden l\\"{o}sungen werden dabei nicht als rein technologie getrieben angesehen; daher werden f\\"{u}r die umsetzung relevante aspekte ber\\"{u}cksichtigt wie erfolgsfaktoren im wissensmanagement, die f\\"{u}r einen erfolg entscheidende mitarbeitermotivation, best practices, {usability-tests} sowie wiki- und {blog-policies}. abschlie{\\ss}end wird ein umfassendes rahmenwerk vorgestellt, das alle wichtigen t\\"{a}tigkeiten, die f\\"{u}r eine einf\\"{u}hrung von social {software-anwendungen} im unternehmen notwendig sind, zusammenfassend darstellt. das buch richtet sich an studenten, wissenschaftler, f\\"{u}hrungskr\\"{a}fte, manager sowie alle am thema interessierten."
1464	1542309	article	nucleic acids research	\N	\N	\N	3	35	Web Server issue	2007	jul	2007-08-08 05:37:38	mcdermott center for human growth and development and the department for translational research, the university of texas southwestern medical center, 5323 harry hines blvd, dallas, tx 75390-9185, usa.	{etblast}: a web server to identify expert reviewers, appropriate journals and similar publications.	authors, editors and reviewers alike use the biomedical literature to identify appropriate journals in which to publish, potential reviewers for papers or grants, and collaborators (or competitors) with similar interests. traditionally, this process has either relied upon personal expertise and knowledge or upon a somewhat unsystematic and laborious process of manually searching through the literature for trends. to help with these tasks, we report three utilities that parse and summarize the results of an abstract similarity search to find appropriate journals for publication, authors with expertise in a given field, and documents similar to a submitted query. the utilities are based upon a program, etblast, designed to identify similar documents within literature databases such as (but not limited to) medline. these services are freely accessible through the internet at http://invention.swmed.edu/etblast/etblast.shtml, where users can upload a file or paste text such as an abstract into the browser interface.
1465	1544270	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2007-08-08 23:55:14	\N	three methods for optimization of cross-laboratory and cross-platform microarray expression data	microarray gene expression data becomes more valuable as our confidence in the results grows. guaranteeing data quality becomes increasingly important as microarrays are being used to diagnose and treat patients (1-4). the maqc quality control consortium, the fda's critical path initiative, nci's cabig and others are implementing procedures that will broadly enhance data quality. as geo continues to grow, its usefulness is constrained by the level of correlation across experiments and general applicability. although rna preparation and array platform play important roles in data accuracy, pre-processing is a user-selected factor that has an enormous effect. normalization of expression data is necessary, but the methods have specific and pronounced effects on precision, accuracy and historical correlation. as a case study, we present a microarray calibration process using normalization as the adjustable parameter. we examine the impact of eight normalizations across both agilent and affymetrix expression platforms on three expression readouts: (1) sensitivity and power, (2) functional/biological interpretation and (3) feature selection and classification error. the reader is encouraged to measure their own discordant data, whether cross-laboratory, cross-platform or across any other variance source, and to use their results to tune the adjustable parameters of their laboratory to ensure increased correlation.
1466	1550730	inproceedings	\N	www	\N	acm	9	\N	\N	2007	\N	2007-08-10 01:01:33	new york, ny, usa	subspace: secure cross-domain communication for web mashups	combining data and code from third-party sources has enabled a new wave of  web mashups  that add creativity and functionality to web applications. however, browsers are poorly designed to pass data between domains, often forcing web developers to abandon security in the name of functionality. to address this deficiency, we developed  subspace , a cross-domain communication mechanism that allows efficient communication across domains without sacrificing security. our prototype requires only a small {javascript} library, and works across all major browsers. we believe subspace can serve as a new secure communication primitive for web mashups.
1467	1551015	article	brief bioinform	\N	\N	\N	10	8	3	2007	may	2007-08-10 09:02:27	ibm, san jose, ca 95141, usa. dennisq@us.ibm.com	improving life sciences information retrieval using semantic web technology.	the ability to retrieve relevant information is at the heart of every aspect of research and development in the life sciences industry. information is often distributed across multiple systems and recorded in a way that makes it difficult to piece together the complete picture. differences in data formats, naming schemes and network protocols amongst information sources, both public and private, must be overcome, and user interfaces not only need to be able to tap into these diverse information sources but must also assist users in filtering out extraneous information and highlighting the key relationships hidden within an aggregated set of information. the semantic web community has made great strides in proposing solutions to these problems, and many efforts are underway to apply semantic web techniques to the problem of information retrieval in the life sciences space. this article gives an overview of the principles underlying a semantic web-enabled information retrieval system: creating a unified abstraction for knowledge using the {rdf} semantic network model; designing semantic lenses that extract contextually relevant subsets of information; and assembling semantic lenses into powerful information displays. furthermore, concrete examples of how these principles can be applied to life science problems including a scenario involving a drug discovery dashboard prototype called {biodash} are provided.
1468	1560459	article	neural comp.	\N	\N	\N	23	19	4	2007	apr	2007-08-14 15:33:44	\N	distinguishing causal interactions in neural populations	we describe a theoretical network analysis that can distinguish statistically causal interactions in population neural activity leading to a specific output. we introduce the concept of a causal core to refer to the set of neuronal interactions that are causally significant for the output, as assessed by granger causality. because our approach requires extensive knowledge of neuronal connectivity and dynamics, an illustrative example is provided by analysis of darwin x, a brain-based device that allows precise recording of the activity of neuronal units during behavior. in darwin x, a simulated neuronal model of the hippocampus and surrounding cortical areas supports learning of a spatial navigation task in a real environment. analysis of darwin x reveals that large repertoires of neuronal interactions contain comparatively small causal cores and that these causal cores become smaller during learning, a finding that may reflect the selection of specific causal pathways from diverse neuronal repertoires.
1469	1572970	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2007-08-18 00:01:07	\N	identification and characterization of cell {type–specific} and ubiquitous chromatin regulatory structures in the human genome	the identification of regulatory elements from different cell types is necessary for understanding the mechanisms controlling cell typeÃ¢â‚¬â€œspecific and housekeeping gene expression. mapping dnasei hypersensitive (hs) sites is an accurate method for identifying the location of functional regulatory elements. we used a high throughput method called dnase-chip to identify 3,904 dnasei hs sites from six cell types across 1% of the human genome. a significant number (22%) of dnasei hs sites from each cell type are ubiquitously present among all cell types studied. surprisingly, nearly all of these ubiquitous dnasei hs sites correspond to either promoters or insulator elements: 86% of them are located near annotated transcription start sites and 10% are bound by ctcf, a protein with known enhancer-blocking insulator activity. we also identified a large number of dnasei hs sites that are cell type specific (only present in one cell type); these regions are enriched for enhancer elements and correlate with cell typeÃ¢â‚¬â€œspecific gene expression as well as cell typeÃ¢â‚¬â€œspecific histone modifications. finally, we found that approximately 8% of the genome overlaps a dnasei hs site in at least one the six cell lines studied, indicating that a significant percentage of the genome is potentially functional.
1470	1573176	article	nature reviews genetics	nat rev genet	\N	nature publishing group	13	8	9	2007	sep	2007-08-21 15:48:45	\N	mechanistic approaches to the study of evolution: the functional synthesis	an emerging synthesis of evolutionary biology and experimental molecular biology is providing much stronger and deeper inferences about the dynamics and mechanisms of evolution than were possible in the past. the new approach combines statistical analyses of gene sequences with manipulative molecular experiments to reveal how ancient mutations altered biochemical processes and produced novel phenotypes. this functional synthesis has set the stage for major advances in our understanding of fundamental questions in evolutionary biology. here we describe this emerging approach, highlight important new insights that it has made possible, and suggest future directions for the field.
1471	1576654	article	nature methods	\N	\N	nature publishing group	5	4	9	2007	sep	2007-08-20 11:53:26	center for cancer research, e17-529b massachusetts institute of technology, cambridge, massachusetts 02139, usa.	{microrna} sponges: competitive inhibitors of small {rnas} in mammalian cells.	micrornas are predicted to regulate thousands of mammalian genes, but relatively few targets have been experimentally validated and few microrna loss-of-function phenotypes have been assigned. as an alternative to chemically modified antisense oligonucleotides, we developed microrna inhibitors that can be expressed in cells, as rnas produced from transgenes. termed 'microrna sponges', these competitive inhibitors are transcripts expressed from strong promoters, containing multiple, tandem binding sites to a microrna of interest. when vectors encoding these sponges are transiently transfected into cultured cells, sponges derepress microrna targets at least as strongly as chemically modified antisense oligonucleotides. they specifically inhibit micrornas with a complementary heptameric seed, such that a single sponge can be used to block an entire microrna seed family. rna polymerase ii promoter (pol ii)-driven sponges contain a fluorescence reporter gene for identification and sorting of sponge-treated cells. we envision the use of stably expressed sponges in animal models of disease and development.
1472	1578346	article	biology direct	\N	\N	\N	\N	2	1	2007	aug	2007-10-28 15:29:50	\N	the biological big bang model for the major transitions in evolution	{background}:major transitions in biological evolution show the same pattern of sudden emergence of diverse forms at a new level of complexity. the relationships between major groups within an emergent new class of biological entities are hard to decipher and do not seem to fit the tree pattern that, following darwin's original proposal, remains the dominant description of biological evolution. the cases in point include the origin of complex {rna} molecules and protein folds; major groups of viruses; archaea and bacteria, and the principal lineages within each of these prokaryotic domains; eukaryotic supergroups; and animal phyla. in each of these pivotal nexuses in life's history, the principal "types" seem to appear rapidly and fully equipped with the signature features of the respective new level of biological organization. no intermediate "grades" or intermediate forms between different types are detectable. usually, this pattern is attributed to cladogenesis compressed in time, combined with the inevitable erosion of the phylogenetic {signal.hypothesis}:i propose that most or all major evolutionary transitions that show the "explosive" pattern of emergence of new types of biological entities correspond to a boundary between two qualitatively distinct evolutionary phases. the first, inflationary phase is characterized by extremely rapid evolution driven by various processes of genetic information exchange, such as horizontal gene transfer, recombination, fusion, fission, and spread of mobile elements. these processes give rise to a vast diversity of forms from which the main classes of entities at the new level of complexity emerge independently, through a sampling process. in the second phase, evolution dramatically slows down, the respective process of genetic information exchange tapers off, and multiple lineages of the new type of entities emerge, each of them evolving in a tree-like fashion from that point on. this biphasic model of evolution incorporates the previously developed concepts of the emergence of protein folds by recombination of small structural units and origin of viruses and cells from a pre-cellular compartmentalized pool of recombining genetic elements. the model is extended to encompass other major transitions. it is proposed that bacterial and archaeal phyla emerged independently from two distinct populations of primordial cells that, originally, possessed leaky membranes, which made the cells prone to rampant gene exchange; and that the eukaryotic supergroups emerged through distinct, secondary endosymbiotic events (as opposed to the primary, mitochondrial endosymbiosis). this biphasic model of evolution is substantially analogous to the scenario of the origin of universes in the eternal inflation version of modern cosmology. under this model, universes like ours emerge in the infinite multiverse when the eternal process of exponential expansion, known as inflation, ceases in a particular region as a result of false vacuum decay, a first order phase transition process. the result is the nucleation of a new universe, which is traditionally denoted big bang, although this scenario is radically different from the big bang of the traditional model of an expanding universe. hence i denote the phase transitions at the end of each inflationary epoch in the history of life biological big bangs ({bbb}).{conclusion}:a biological big bang ({bbb}) model is proposed for the major transitions in life's evolution. according to this model, each transition is a {bbb} such that new classes of biological entities emerge at the end of a rapid phase of evolution (inflation) that is characterized by extensive exchange of genetic information which takes distinct forms for different {bbbs}. the major types of new forms emerge independently, via a sampling process, from the pool of recombining entities of the preceding generation. this process is envisaged as being qualitatively different from tree-pattern {cladogenesis.reviewers}:this article was reviewed by william martin, sergei maslov, and leonid mirny.
1473	1603668	article	bioinformatics	\N	\N	oxford university press	7	23	20	2007	oct	2007-08-29 06:57:20	department of oncology, university of cambridge, cruk cambridge research institute, li ka shing centre, robinson way, cambridge cb2 0re, united kingdom. bioinformatics division, immunology division, the walter and eliza hall institute of medical research, 1g royal parade, parkville, victoria 3050, australia, the peter maccallum cancer centre, st andrews place, east melbourne, victoria 3002, australia.	a comparison of background correction methods for two-colour microarrays	motivation: microarray data must be background corrected to remove the effects of non-specific binding or spatial heterogeneity across the array, but this practice typically causes other problems such as negative corrected intensities and high variability of low intensity log-ratios. different estimators of background, and various model-based processing methods, are compared in this study in search of the best option for differential expression analyses of small microarray experiments.
1474	1604373	article	genome research	\N	\N	\N	9	13	1	2003	jan	2007-08-29 10:26:56	informatics department, the wellcome trust sanger institute, wellcome trust genome campus, hinxton, cambridge cb10 1sa, uk.	the phusion assembler	the phusion assembler has assembled the mouse genome from the whole-genome shotgun ({wgs}) dataset collected by the mouse genome sequencing consortium, at ?7.5× sequence coverage, producing a high-quality draft assembly 2.6 gigabases in size, of which 90\\% of these bases are in 479 scaffolds. for the mouse genome, which is a large and repeat-rich genome, the input dataset was designed to include a high proportion of paired end sequences of various size selected inserts, from 2–200 kbp lengths, into various host vector templates. phusion uses sequence data, called reads, and information about reads that share common templates, called read pairs, to drive the assembly of this large genome to highly accurate results. the preassembly stage, which clusters the reads into sensible groups, is a key element of the entire assembler, because it permits a simple approach to parallelization of the assembly stage, as each cluster can be treated independent of the others. in addition to the application of phusion to the mouse genome, we will also present results from the {wgs} assembly {ofcaenorhabditis} briggsae sequenced to about 11× coverage. {thec}. briggsae assembly was accessioned through {embl},http://www.ebi.ac.uk/services/index.html, using the series {caac01000001}–{caac01000578}, however, the phusion mouse assembly described here was not accessioned. the mouse data was generated by the mouse genome sequencing consortium. the c. briggsae sequence was generated at the wellcome trust sanger institute and the genome sequencing center, washington university school of medicine.
1475	1616758	article	nature	\N	\N	\N	3	407	6804	2000	\N	2007-09-03 22:05:44	\N	{minimization of boolean complexity in human concept learning}	one of the unsolved problems in the field of human concept learning concerns the factors that determine the subjective difficulty of concepts: why are some concepts psychologically simple and easy to learn, while others seem difficult, complex or incoherent? this question was much studied in the 1960s1 but was never answered, and more recent characterizations of concepts as prototypes rather than logical rules [2, 3] leave it unsolved [4, 5, 6]. here i investigate this question in the domain of boolean concepts (categories defined by logical rules). a series of experiments measured the subjective difficulty of a wide range of logical varieties of concepts (41 mathematically distinct types in six familiesâ€”a far wider range than has been tested previously). the data reveal a surprisingly simple empirical 'law': the subjective difficulty of a concept is directly proportional to its boolean complexity (the length of the shortest logically equivalent propositional formula)â€”that is, to its logical incompressibility.
1476	1618886	article	journal of neurophysiology	\N	\N	american physiological society	27	79	2	1998	feb	2007-09-04 12:00:50	computational neurobiology laboratory, howard hughes medical institute, the salk institute for biological studies, la jolla, ca 92037, usa.	interpreting neuronal population activity by reconstruction: unified framework with application to hippocampal place cells.	physical variables such as the orientation of a line in the visual field or the location of the body in space are coded as activity levels in populations of neurons. reconstruction or decoding is an inverse problem in which the physical variables are estimated from observed neural activity. reconstruction is useful first in quantifying how much information about the physical variables is present in the population and, second, in providing insight into how the brain might use distributed representations in solving related computational problems such as visual object recognition and spatial navigation. two classes of reconstruction methods, namely, probabilistic or bayesian methods and basis function methods, are discussed. they include important existing methods as special cases, such as population vector coding, optimal linear estimation, and template matching. as a representative example for the reconstruction problem, different methods were applied to multi-electrode spike train data from hippocampal place cells in freely moving rats. the reconstruction accuracy of the trajectories of the rats was compared for the different methods. bayesian methods were especially accurate when a continuity constraint was enforced, and the best errors were within a factor of two of the information-theoretic limit on how accurate any reconstruction can be and were comparable with the intrinsic experimental errors in position tracking. in addition, the reconstruction analysis uncovered some interesting aspects of place cell activity, such as the tendency for erratic jumps of the reconstructed trajectory when the animal stopped running. in general, the theoretical values of the minimal achievable reconstruction errors quantify how accurately a physical variable is encoded in the neuronal population in the sense of mean square error, regardless of the method used for reading out the information. one related result is that the theoretical accuracy is independent of the width of the gaussian tuning function only in two dimensions. finally, all the reconstruction methods considered in this paper can be implemented by a unified neural network architecture, which the brain feasibly could use to solve related problems.
1477	1621877	article	bioinformatics	bioinformatics	\N	\N	8	23	20	2007	oct	2007-09-05 07:30:58	department of electrical eng. and computer science, case western reserve university, cleveland, oh 44106, usa.	mining biological networks for unknown pathways	motivation: biological pathways provide significant insights on the interaction mechanisms of molecules. presently, many essential pathways still remain unknown or incomplete for newly sequenced organisms. moreover, experimental validation of enormous numbers of possible pathway candidates in a wet-lab environment is time- and effort-extensive. thus, there is a need for comparative genomics tools that help scientists predict pathways in an organism's biological {network.results}: in this article, we propose a technique to discover unknown pathways in organisms. our approach makes in-depth use of gene ontology ({go})-based functionalities of enzymes involved in metabolic pathways as follows: model each pathway as a biological functionality graph of enzyme {go} functions, which we call pathway functionality {template.locate} frequent pathway functionality patterns so as to infer previously unknown pathways through pattern matching in metabolic networks of {organisms.we} have experimentally evaluated the accuracy of the presented technique for 30 bacterial organisms to predict around 1500 organism-specific versions of 50 reference pathways. using cross-validation strategy on known pathways, we have been able to infer pathways with 86\\% precision and 72\\% recall for enzymes (i.e. nodes). the accuracy of the predicted enzyme relationships has been measured at 85\\% precision with 64\\% {recall.availability}: code upon {request.contact}: {ali.cakmak@case.edusupplementary} information: supplementary data are available at bioinformatics online.
1478	1624269	electronic	\N	\N	\N	\N	\N	\N	\N	2007	sep	2007-09-05 15:26:49	\N	navigability of complex networks	targeted or quasi-targeted propagation of information is a fundamental process running in complex networked systems. optimal communication in a network is easy to achieve if all its nodes have a full view of the global topological structure of the network. however many complex networks manifest communication efficiency without nodes having a full view of the network, and yet there is no generally applicable explanation of what mechanisms may render efficient such communication in the dark. in this work we model this communication as an oblivious routing process greedily operating on top of a network and making its decisions based only on distances within a hidden metric space lying underneath. abstracting intrinsic similarities among networked elements, this hidden metric space is not easily reconstructible from the visible network topology. yet we find that the structure of complex networks observed in reality, characterized by strong clustering and specific values of exponents of power-law degree distributions, maximizes their navigability, i.e., the efficiency of the greedy path-finding strategy in this hidden framework. we explain this observation by showing that more navigable networks have more prominent hierarchical structures which are congruent with the optimal layout of routing paths through a network. this finding has potentially profound implications for constructing efficient routing and searching strategies in communication and social networks, such as the internet, web, etc., and merits further research that would explain whether navigability of complex networks does indeed follow naturally from specifics of their evolution.
1479	1624654	article	nature	\N	\N	nature publishing group	3	449	7158	2007	sep	2007-09-05 18:17:01	\N	temporal precision in the neural code and the timescales of natural vision	the timing of action potentials relative to sensory stimuli can be precise down to milliseconds in the visual system1, 2, 3, 4, 5, 6, 7, even though the relevant timescales of natural vision are much slower. the existence of such precision contributes to a fundamental debate over the basis of the neural code and, specifically, what timescales are important for neural computation8, 9, 10. using recordings in the lateral geniculate nucleus, here we demonstrate that the relevant timescale of neuronal spike trains depends on the frequency content of the visual stimulus, and that 'relative', not absolute, precision is maintained both during spatially uniform white-noise visual stimuli and naturalistic movies. using information-theoretic techniques, we demonstrate a clear role of relative precision, and show that the experimentally observed temporal structure in the neuronal response is necessary to represent accurately the more slowly changing visual world. by establishing a functional role of precision, we link visual neuron function on slow timescales to temporal structure in the response at faster timescales, and uncover a straightforward purpose of fine-timescale features of neuronal spike trains.
1480	1629604	article	nature genetics	\N	\N	nature publishing group	7	39	7 Suppl	2007	jun	2007-09-09 20:32:19	department of genome sciences, university of washington, seattle, washington 98195, usa. coopergm@u.washington.edu	mutational and selective effects on copy-number variants in the human genome	comprehensive descriptions of large insertion/deletion or segmental duplication polymorphisms ({sds}) in the human genome have recently been generated. these annotations, known collectively as structural or copy-number variants ({cnvs}), include thousands of discrete genomic regions and span hundreds of millions of nucleotides. here we review the genomic distribution of {cnvs}, which is strongly correlated with gene, repeat and segmental duplication content. we explore the evolutionary mechanisms giving rise to this nonrandom distribution, considering the available data on both human polymorphisms and the fixed changes that differentiate humans from other species. it is likely that mutational biases, selective effects and interactions between these forces all contribute substantially to the spectrum of human copy-number variation. although defining these variants with nucleotide-level precision remains a largely unmet but critical challenge, our understanding of their potential medical impact and evolutionary importance is rapidly emerging.
1481	1631086	article	the academy of management review	\N	\N	\N	19	24	4	1999	oct	2007-09-07 12:43:31	\N	strategies for theorizing from process data	in this article i describe and compare ct number of alternative generic strategies for the analysis of process data, looking at the consequences of these strategies for emerging theories. i evaluate the strengths and weaknesses of the strategies in terms of their capacity to generate theory that is accurate, parsimonious, general, and useful and suggest that method and theory are inextricably intertwined, that multiple strategies are often advisable, and that no analysis strategy will produce theory without an uncodifiable creative leap, however small. finally, i argue that there is room in the organizational research literature for more openness within the academic community toward a variety of forms of coupling between theory and data.
1482	1631927	article	science	\N	\N	american association for the advancement of science	3	317	5843	2007	sep	2007-09-07 16:15:37	\N	evolution in the social brain	the evolution of unusually large brains in some groups of animals, notably primates, has long been a puzzle. although early explanations tended to emphasize the brain's role in sensory or technical competence (foraging skills, innovations, and way-finding), the balance of evidence now clearly favors the suggestion that it was the computational demands of living in large, complex societies that selected for large brains. however, recent analyses suggest that it may have been the particular demands of the more intense forms of pairbonding that was the critical factor that triggered this evolutionary development. this may explain why primate sociality seems to be so different from that found in most other birds and mammals: primate sociality is based on bonded relationships of a kind that are found only in pairbonds in other taxa.
1483	1634025	article	science	\N	\N	\N	6	317	5843	2007	sep	2007-09-08 17:41:42	\N	humans have evolved specialized skills of social cognition: the cultural intelligence hypothesis	humans have many cognitive skills not possessed by their nearest primate relatives. the cultural intelligence hypothesis argues that this is mainly due to a species-specific set of social-cognitive skills, emerging early in ontogeny, for participating and exchanging knowledge in cultural groups. we tested this hypothesis by giving a comprehensive battery of cognitive tests to large numbers of two of humans' closest primate relatives, chimpanzees and orangutans, as well as to 2.5-year-old human children before literacy and schooling. supporting the cultural intelligence hypothesis and contradicting the hypothesis that humans simply have more  ” general intelligence,” we found that the children and chimpanzees had very similar cognitive skills for dealing with the physical world but that the children had more sophisticated cognitive skills than either of the ape species for dealing with the social world.
1484	1638951	article	science	\N	\N	american association for the advancement of science	4	318	5848	2007	oct	2007-09-28 15:24:11	department of entomology, the pennsylvania state university, 501 asi building, university park, pa 16802, usa.	a metagenomic survey of microbes in honey bee colony collapse disorder	in colony collapse disorder ({ccd}), honey bee colonies inexplicably lose their workers. {ccd} has resulted in a loss of 50 to 90\\% of colonies in beekeeping operations across the united states. the observation that irradiated combs from affected colonies can be repopulated with naive bees suggests that infection may contribute to {ccd}. we used an unbiased metagenomic approach to survey microflora in {ccd} hives, normal hives, and imported royal jelly. candidate pathogens were screened for significance of association with {ccd} by the examination of samples collected from several sites over a period of 3 years. one organism, israeli acute paralysis virus of bees, was strongly correlated with {ccd}.
1485	1655248	article	science	\N	\N	\N	4	317	5844	2007	sep	2007-09-14 10:10:10	\N	global pattern formation and {ethnic/cultural} violence	we identify a process of global pattern formation that causes regions to differentiate by culture. violence arises at boundaries between regions that are not sufficiently well defined. we model cultural differentiation as a separation of groups whose members prefer similar neighbors, with a characteristic group size at which violence occurs. application of this model to the area of the former yugoslavia and to india accurately predicts the locations of reported conflict. this model also points to imposed mixing or boundary clarification as mechanisms for promoting peace.
1486	1657521	article	lingvisticae investigationes	\N	\N	john benjamins publishing company	23	30	1	2007	jan	2007-12-18 14:50:06	\N	a survey of named entity recognition and classification	this survey covers fifteen years of research in the named entity recognition and classification ({nerc}) field, from 1991 to 2006. we report observations about languages, named entity types, domains and textual genres studied in the literature. from the start, {nerc} systems have been developed using hand-made rules, but now machine learning techniques are widely used. these techniques are surveyed along with other critical aspects of {nerc} such as features and evaluation methods. features are word-level, dictionary-level and corpus-level representations of words in a document. evaluation techniques, ranging from intuitive exact match to very complex matching techniques with adjustable cost of errors, are an indisputable key to progress.
1487	1668327	article	psychological review	\N	\N	\N	27	99	1	1992	jan	2008-03-17 01:01:42	department of psychology, carnegie mellon university, pittsburgh, pennsylvania 15213.	a capacity theory of comprehension: individual differences in working memory.	a theory of the way working memory capacity constrains comprehension is proposed. the theory proposes that both processing and storage are mediated by activation and that the total amount of activation available in working memory varies among individuals. individual differences in working memory capacity for language can account for qualitative and quantitative differences among college-age adults in several aspects of language comprehension. one aspect is syntactic modularity: the larger capacity of some individuals permits interaction among syntactic and pragmatic information, so that their syntactic processes are not informationally encapsulated. another aspect is syntactic ambiguity: the larger capacity of some individuals permits them to maintain multiple interpretations. the theory is instantiated as a production system model in which the amount of activation available to the model affects how it adapts to the transient computational and storage demands that occur in comprehension.
1488	1668986	incollection	the adaptive web	the adaptive web	lecture notes in computer science	springer berlin / heidelberg	22	4321	\N	2007	\N	2007-09-18 13:03:59	berlin, heidelberg	semantic web technologies for the adaptive web the adaptive web	ontologies and reasoning are the key terms brought into focus by the semantic web community. formal representation of ontologies in a common data model on the web can be taken as a foundation for adaptive web technologies as well. this chapter describes how ontologies shared on the semantic web provide conceptualization for the links which are a main vehicle to access information on the web. the subject domain ontologies serve as constraints for generating only those links which are relevant for the domain a user is currently interested in. furthermore, user model ontologies provide additional means for deciding which links to show, annotate, hide, generate, and reorder. the semantic web technologies provide means to formalize the domain ontologies and metadata created from them. the formalization enables reasoning for personalization decisions. this chapter describes which components are crucial to be formalized by the semantic web ontologies for adaptive web. we use examples from an {elearning} domain to illustrate the principles which are broadly applicable to any information domain on the web.
1489	1673083	article	nature reviews. genetics	\N	\N	nature publishing group	13	8	10	2007	oct	2007-09-19 10:05:52	\N	gene conversion: mechanisms, evolution and human disease.	gene conversion, one of the two mechanisms of homologous recombination, involves the unidirectional transfer of genetic material from a 'donor' sequence to a highly homologous 'acceptor'. considerable progress has been made in understanding the molecular mechanisms that underlie gene conversion, its formative role in human genome evolution and its implications for human inherited disease. here we assess current thinking about how gene conversion occurs, explore the key part it has played in fashioning extant human genes, and carry out a meta-analysis of gene-conversion events that are known to have caused human genetic disease.
1490	1673084	article	nat rev genet	nat rev genet	\N	nature publishing group	10	8	10	2007	oct	2007-09-19 10:09:45	\N	the evolution of genetic networks by non-adaptive processes	although numerous investigators assume that the global features of genetic networks are moulded by natural selection, there has been no formal demonstration of the adaptive origin of any genetic network. this analysis shows that many of the qualitative features of known transcriptional networks can arise readily through the non-adaptive processes of genetic drift, mutation and recombination, raising questions about whether natural selection is necessary or even sufficient for the origin of many aspects of gene-network topologies. the widespread reliance on computational procedures that are devoid of population-genetic details to generate hypotheses for the evolution of network configurations seems to be unjustified.
1491	1674923	article	nat genet	\N	\N	nature publishing group	4	39	10	2007	oct	2007-09-19 09:10:12	[1] school of human evolution and social change, arizona state university, tempe, arizona 85287, usa. [2] department of pathology, brigham and women's hospital, boston, massachusetts 02115, usa.	diet and the evolution of human amylase gene copy number variation	starch consumption is a prominent characteristic of agricultural societies and hunter-gatherers in arid environments. in contrast, rainforest and circum-arctic hunter-gatherers and some pastoralists consume much less starch1, 2, 3. this behavioral variation raises the possibility that different selective pressures have acted on amylase, the enzyme responsible for starch hydrolysis4. we found that copy number of the salivary amylase gene (amy1) is correlated positively with salivary amylase protein level and that individuals from populations with high-starch diets have, on average, more amy1 copies than those with traditionally low-starch diets. comparisons with other loci in a subset of these populations suggest that the extent of amy1 copy number differentiation is highly unusual. this example of positive selection on a copy numberâ€“variable gene is, to our knowledge, one of the first discovered in the human genome. higher amy1 copy numbers and protein levels probably improve the digestion of starchy foods and may buffer against the fitness-reducing effects of intestinal disease.
1492	1678574	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2007-09-20 10:02:24	\N	classical electrostatics in biology and chemistry	a major revival in the use of classical electrostatics as an approach to the study of charged and polar molecules in aqueous solution has been made possible through the development of fast numerical and computational methods to solve the {poisson-boltzmann} equation for solute molecules that have complex shapes and charge distributions. graphical visualization of the calculated electrostatic potentials generated by proteins and nucleic acids has revealed insights into the role of electrostatic interactions in a wide range of biological phenomena. classical electrostatics has also proved to be a successful quantitative tool yielding accurate descriptions of electrical potentials, diffusion limited processes, {ph}-dependent properties of proteins, ionic strength-dependent phenomena, and the solvation free energies of organic molecules.
1493	1681472	article	physical review e	\N	\N	american physical society	\N	68	1	2003	jul	2007-09-21 05:04:17	\N	jamming at zero temperature and zero applied stress: the epitome of disorder	we have studied how two- and three-dimensional systems made up of particles interacting with finite range; repulsive potentials jam (i.e.; develop a yield stress in a disordered state) at zero temperature and zero applied stress. at low packing fractions ?; the system is not jammed and each particle can move without impediment from its neighbors. for each configuration; there is a unique jamming threshold ? c at which particles can no longer avoid each other; and the bulk and shear moduli simultaneously become nonzero. the distribution of ? c values becomes narrower as the system size increases; so that essentially all configurations jam at the same packing fraction in the thermodynamic limit. this packing fraction corresponds to the previously measured value for random close packing. in fact; our results provide a well-defined meaning for  ” random close packing” in terms of the fraction of all phase space with inherent structures that jam. the jamming threshold; point j ; occurring at zero temperature and applied stress and at the random-close-packing density; has properties reminiscent of an ordinary critical point. as point j is approached from higher packing fractions; power-law scaling is found for the divergence of the first peak in the pair correlation function and in the vanishing of the pressure; shear modulus; and excess number of overlapping neighbors. moreover; near point j ; certain quantities no longer self-average; suggesting the existence of a length scale that diverges at j . however; point j also differs from an ordinary critical point: the scaling exponents do not depend on dimension but do depend on the interparticle potential. finally; as point j is approached from high packing fractions; the density of vibrational states develops a large excess of low-frequency modes. indeed; at point j ; the density of states is a constant all the way down to zero frequency. all of these results suggest that point j is a point of maximal disorder and may control behavior in its vicinity—perhaps even at the glass transition.
1494	1693077	article	physical review	\N	\N	american physical society	7	136	3B	1964	nov	2007-09-25 15:16:52	\N	inhomogeneous electron gas	this paper deals with the ground state of an interacting electron gas in an external potential v(r). it is proved that there exists a universal functional of the density, f[n(r)], independent of v(r), such that the expression {e??v(r)n(r)dr+f}[n(r)] has as its minimum value the correct ground-state energy associated with v(r). the functional f[n(r)] is then discussed for two situations: (1) n(r)=n0+n?(r), n?/n0?1, and (2) n(r)=?(r/r0) with ? arbitrary and r0??. in both cases f can be expressed entirely in terms of the correlation energy and linear and higher order electronic polarizabilities of a uniform electron gas. this approach also sheds some light on generalized {thomas-fermi} methods and their limitations. some new extensions of these methods are presented.
1495	1697751	article	information processing \\& management	\N	\N	\N	6	38	5	2002	sep	2007-09-26 17:49:49	\N	issues of context in information retrieval ({ir}): an introduction to the special issue	the subject of context has received a great deal of attention in the information retrieval ({ir}) literature over the past decade, primarily in studies of information seeking and {ir} interactions. recently, attention to context in {ir} has expanded to address new problems in new environments. in this paper we outline five overlapping dimensions of context which we believe to be important constituent elements and we discuss how they are related to different issues in {ir} research. the papers in this special issue are summarized with respect to how they represent work that is being conducted within these dimensions of context. we conclude with future areas of research which are needed in order to fully understand the multidimensional nature of context in {ir}.
1496	1713592	article	journal of biological chemistry	\N	\N	\N	8	282	39	2007	sep	2007-10-01 05:32:28	department of bioengineering, university of california at san diego, la jolla, california 92093-0412 and genomatica, inc., san diego, california 92121.	genome-scale reconstruction of metabolic network in bacillus subtilis based on high-throughput phenotyping and gene essentiality data	in this report, a genome-scale reconstruction of bacillus subtilis metabolism and its iterative development based on the combination of genomic, biochemical, and physiological information and high-throughput phenotyping experiments is presented. the initial reconstruction was converted into an in silico model and expanded in a four-step iterative fashion. first, network gap analysis was used to identify 48 missing reactions that are needed for growth but were not found in the genome annotation. second, the computed growth rates under aerobic conditions were compared with high-throughput phenotypic screen data, and the initial in silico model could predict the outcomes qualitatively in 140 of 271 cases considered. detailed analysis of the incorrect predictions resulted in the addition of 75 reactions to the initial reconstruction, and 200 of 271 cases were correctly computed. third, in silico computations of the growth phenotypes of knock-out strains were found to be consistent with experimental observations in 720 of 766 cases evaluated. fourth, the integrated analysis of the large-scale substrate utilization and gene essentiality data with the genome-scale metabolic model revealed the requirement of 80 specific enzymes (transport, 53; intracellular reactions, 27) that were not in the genome annotation. subsequent sequence analysis resulted in the identification of genes that could be putatively assigned to 13 intracellular enzymes. the final reconstruction accounted for 844 open reading frames and consisted of 1020 metabolic reactions and 988 metabolites. hence, the in silico model can be used to obtain experimentally verifiable hypothesis on the metabolic functions of various genes.
1497	1713694	article	nat genet	\N	\N	nature publishing group	9	39	10	2007	oct	2007-10-22 09:19:20	\N	a high-resolution atlas of nucleosome occupancy in yeast	we present the first complete high-resolution map of nucleosome occupancy across the whole saccharomyces cerevisiae genome, identifying over 70,000 positioned nucleosomes occupying 81\\% of the genome. on a genome-wide scale, the persistent nucleosome-depleted region identified previously in a subset of genes demarcates the transcription start site. both nucleosome occupancy signatures and overall occupancy correlate with transcript abundance and transcription rate. in addition, functionally related genes can be clustered on the basis of the nucleosome occupancy patterns observed at their promoters. a quantitative model of nucleosome occupancy indicates that {dna} structural features may account for much of the global nucleosome occupancy.
1498	1715029	article	bmc bioinformatics	\N	\N	\N	\N	8	Suppl 6	2007	\N	2007-10-01 13:13:16	\N	inferring cellular networks – a review	in this review we give an overview of computational and statistical methods to reconstruct cellular networks. although this area of research is vast and fast developing, we show that most currently used methods can be organized by a few key concepts. the first part of the review deals with conditional independence models including gaussian graphical models and bayesian networks. the second part discusses probabilistic and graph-based methods for data from experimental interventions and perturbations.
1499	1716134	article	nature genetics	\N	\N	nature publishing group	5	39	10	2007	oct	2007-10-01 17:01:36	\N	the {ncbi} {dbgap} database of genotypes and phenotypes.	the national center for biotechnology information has created the dbgap public repository for individual-level phenotype, exposure, genotype and sequence data and the associations between them. dbgap assigns stable, unique identifiers to studies and subsets of information from those studies, including documents, individual phenotypic variables, tables of trait data, sets of genotype data, computed phenotype-genotype associations, and groups of study subjects who have given similar consents for use of their data.
1500	1724599	article	nucleic acids res	\N	\N	\N	3	31	13	2003	jul	2007-10-03 17:18:25	center for information biology and dna data bank of japan, national institute of genetics, 1111 yata, mishima, shizuoka 411-8540, japan. hsugawar@genes.nig.uc.jp	biological {soap} servers and web services provided by the public sequence data bank.	a number of biological data resources (i.e. databases and data analytical tools) are searchable and usable on-line thanks to the internet and the world wide web ({www}) servers. the output from the web server is easy for us to browse. however, it is laborious and sometimes impossible for us to write a computer program that finds a useful data resource, sends a proper query and processes the output. it is a serious obstacle to the integration of distributed heterogeneous data resources. to solve the issue, we have implemented a {soap} (simple object access protocol) server and web services that provide a program-friendly interface. the web services are accessible at http://www.xml.nig.ac.jp/.
1501	1727461	inproceedings	\N	proceedings of iswc 2007 (to appear)	\N	\N	\N	\N	\N	2007	\N	2007-10-04 14:48:00	\N	{dbpedia}: a nucleus for a web of open data	abstract dbpedia is a community effort to extract structured information from wikipedia and to make this information available on the web. dbpedia allows you to ask sophisticated queries against datasets derived from wikipedia and to link other datasets on the web to wikipedia data. we describe the extraction of the dbpedia datasets, and how the resulting information is published on the web for human- and machineconsumption. we describe some emerging applications from the dbpedia community and show how website authors can facilitate dbpedia content within their sites. finally, we present the current status of interlinking dbpedia with other open datasets on the web and outline how dbpedia could serve as a nucleus for an emerging web of open data. 1
1502	1744606	inproceedings	\N	proceedings of the 21st acm symposium on operating systems principles	\N	\N	\N	\N	\N	2007	oct	2007-10-09 07:21:20	stevenson, wa	dynamo: amazon's highly available {key-value} store	reliability at massive scale is one of the biggest challenges we face at amazon.com, one of the largest e-commerce operations in the world; even the slightest outage has significant financial consequences and impacts customer trust. the amazon.com platform, which provides services for many web sites worldwide, is implemented on top of an infrastructure of tens of thousands of servers and network components located in many datacenters around the world. at this scale, small and large components fail continuously and the way persistent state is managed in the face of these failures drives the reliability and scalability of the software systems.  this paper presents the design and implementation of dynamo, a highly available key-value storage system that some of amazonâ€™s core services use to provide an â€œalways-onâ€ experience. to achieve this level of availability, dynamo sacrifices consistency under certain failure scenarios. it makes extensive use of object versioning and application-assisted conflict resolution in a manner that provides a novel interface for developers to use.
1503	1756933	article	reviews of modern physics	\N	\N	american physical society	23	79	4	2007	oct	2007-12-17 11:25:17	\N	\\textit{colloquium} : light scattering by particle and hole arrays	this colloquium analyzes the interaction of light with two-dimensional periodic arrays of particles and holes. the enhanced optical transmission observed in the latter and the presence of surface modes in patterned metal surfaces is thoroughly discussed. a review of the most significant discoveries in this area is presented first. a simple tutorial model is then formulated to capture the essential physics involved in these phenomena, while allowing analytical derivations that provide deeper insight. comparison with more elaborated calculations is offered as well. finally, hole arrays in plasmon-supporting metals are compared to perforated perfect conductors, thus assessing the role of plasmons in these types of structures through analytical considerations. the developments that have been made in nanophotonics areas related to plasmons in nanostructures, extraordinary optical transmission in hole arrays, complete resonant absorption and emission of light, and invisibility in structured metals are illustrated in this colloquium in a comprehensive, tutorial fashion.
1504	1783204	article	annual review of biophysics and biomolecular structure	\N	\N	\N	21	36	1	2007	\N	2007-10-18 08:30:49	department of chemical engineering, massachusetts institute of technology, cambridge, massachusetts 02139, usa. narendra@mit.edu	living with noisy genes: how cells function reliably with inherent variability in gene expression	abstract within a population of genetically identical cells there can be significant variation, or noise, in gene expression. yet even with this inherent variability, cells function reliably. this review focuses on our understanding of noise at the level of both single genes and genetic regulatory networks, emphasizing comparisons between theoretical models and experimental results whenever possible. to highlight the importance of noise, we particularly emphasize examples in which a stochastic description of gene expression leads to a qualitatively different outcome than a deterministic one.
1505	1785135	article	reviews of modern physics	\N	\N	american physical society	55	81	2	2009	may	2007-10-18 17:29:35	\N	statistical physics of social dynamics	statistical physics has proven to be a fruitful framework to describe phenomena outside the realm of traditional physics. recent years have witnessed an attempt by physicists to study collective phenomena emerging from the interactions of individuals as elementary units in social structures. a wide list of topics are reviewed ranging from opinion and cultural and language dynamics to crowd behavior, hierarchy formation, human dynamics, and social spreading. the connections between these problems and other, more traditional, topics of statistical physics are highlighted. comparison of model results with empirical data from social systems are also emphasized.
1506	1797414	article	journal of multivariate analysis	\N	\N	academic press, inc.	22	98	5	2007	may	2007-10-20 20:23:49	orlando, fl, usa	comparing clusterings—an information based distance	this paper proposes an information theoretic criterion for comparing two partitions, or clusterings  , of the same data set. the criterion, called variation of information ({vi}), measures the amount of information lost and gained in changing from clustering {cc} to clustering {c?c}?. the basic properties of {vi} are presented and discussed. we focus on two kinds of properties: (1) those that help one build intuition about the new criterion (in particular, it is shown the {vi} is a true metric on the space of clusterings), and (2) those that pertain to the comparability of {vi} values over different experimental conditions. as the latter properties have rarely been discussed explicitly before, other existing comparison criteria are also examined in their light. finally we present the {vi} from an axiomatic point of view, showing that it is the only  ” sensible” criterion for comparing partitions that is both aligned to the lattice and convexely additive. as a consequence, we prove an impossibility result for comparing partitions: there is no criterion for comparing partitions that simultaneously satisfies the above two desirable properties and is bounded.
1507	1806502	article	information services \\& use	\N	\N	\N	\N	27	3	2007	\N	2007-10-22 14:55:15	\N	folksonomies and science communication. a mash-up of professional science databases and web 2.0 services	folksonomies complete the methods of indexing scientific documents. now scientists in their function as readers may play an active role in science communication as well, since they can tag documents with terms taken from their professional or personal environment. folksonomies allow the indexing of documents by everyone without following any rules. besides the benefits of folksonomies there are severe problems, e.g. the tags' lack of precision. in order to overcome the shortcomings of this collaborative indexing method we introduce natural language processing of tags and a relevance ranking algorithm which is based on specific tag distributions, on aspects of collaboration and on the actions of the  ” prosumers”. this article is a plea for the combination of the  ” old” science databases and the benefits of the folksonomies.
1508	1817569	misc	\N	\N	\N	\N	\N	\N	\N	-1	\N	2007-10-24 23:47:15	\N	exploiting availability prediction in distributed systems	loosely-coupled distributed systems have significant scale and cost advantages over more traditional architectures, but the availability of the nodes in these systems varies widely. availability modeling is crucial for predicting per-machine resource burdens and understanding emergent, system-wide phenomena. we present new techniques for predicting availability and test them using traces taken from three distributed systems. we then describe three applications of availability prediction. the first, availability-guided replica placement, reduces object copying in a distributed data store while increasing data availability. the second shows how availability prediction can improve routing in delay-tolerant networks. the third combines availability prediction with virus modeling to improve forecasts of global infection dynamics.
1509	1826173	article	science	\N	\N	american association for the advancement of science	4	318	5850	2007	oct	2007-10-26 19:29:35	\N	decision theory: what "should" the nervous system do?	the purpose of our nervous system is to allow us to successfully interact with our environment. this normative idea is formalized by decision theory that defines which choices would be most beneficial. we live in an uncertain world, and each decision may have many possible outcomes; choosing the best decision is thus complicated. bayesian decision theory formalizes these problems in the presence of uncertainty and often provides compact models that predict observed behavior. with its elegant formalization of the problems faced by the nervous system, it promises to become a major inspiration for studies in neuroscience. 10.1126/science.1142998
1510	1840029	inproceedings	\N	proceedings of the 2007 international symposium on wikis	wikisym	acm	7	\N	\N	2007	\N	2007-10-30 11:20:15	new york, ny, usa	cooperation and quality in wikipedia	the rise of the internet has enabled collaboration and cooperation on anunprecedentedly large scale. the online encyclopedia wikipedia, which presently comprises 7.2 million articles created by 7.04 million distinct editors, provides a consummate example. we examined all 50 million edits made tothe 1.5 million english-language wikipedia articles and found that the high-quality articles are distinguished by a marked increase in number of edits, number of editors, and intensity of cooperative behavior, as compared to other articles of similar visibility and age. this is significant because in other domains, fruitful cooperation has proven to be difficult to sustain as the size of the collaboration increases. furthermore, in spite of the vagaries of human behavior, we show that wikipedia articles accrete edits according to a simple stochastic mechanism in which edits beget edits. topics of high interest or relevance are thus naturally brought to the forefront of quality.
1511	1840109	inproceedings	\N	proceedings of the eighteenth conference on hypertext and hypermedia	ht	acm	3	\N	\N	2007	\N	2007-10-30 11:53:57	new york, ny, usa	does it matter who contributes: a study on featured articles in the german wikipedia	the considerable high quality of wikipedia articles is often accredited to the large number of users who contribute to wikipedia's encyclopedia articles, who watch articles and correct errors immediately. in this paper, we are in particular interested in a certain type of wikipedia articles, namely, the featured articles - articles marked by a community's vote as being of outstanding quality. the german wikipedia has the nice property that it has two types of featured articles: excellent and worth reading. we explore on the german wikipedia whether only the mere number of contributors makes the difference or whether the high quality of featured articles results from having experienced authors contributing with a reputation for high quality contributions. our results indicate that it does matter who contributes.
1512	1865961	electronic	\N	\N	\N	\N	\N	\N	\N	-1	\N	2007-11-04 23:16:36	\N	information literacy	[autoren-abstract] this paper aims to provide an overview of some of the most recent developments in concepts and practices associated with information literacy worldwide, revealing the paradox that, while information literacy is a key discipline of the information society and knowledge economy and is well-understood in its broader sense, it has made little progress educationally, save for a few exceptions in countries such as australia, the usa, canada and the uk. deriving from the authors' background as university professors, the paper concentrates on approaches to promote information literacy in higher education. the paper concludes by pointing to the need to expand the debate on information literacy and how to raise ethical and moral concerns in the use of the internet and the new technologies. it also explores the potential role that the european commission esafe (2003-2004) programme can play to encourage research and practice on information literacy in its widest sense, as an intrinsic competency in the fight against the effects of disseminating illegal and harmful content through online and other new technologies.
1513	1869121	article	neuron	\N	\N	\N	14	56	2	2007	oct	2007-11-05 15:46:14	inserm, cognitive neuro-imaging unit, ifr 49, gif sur yvette, france; cea, neurospin center, ifr 49, gif sur yvette, france; coll\\`{e}ge de france, paris, france; univ paris-sud, ifr 49, f-91191 gif/yvette, france.	cultural recycling of cortical maps.	part of human cortex is specialized for cultural domains such as reading and arithmetic, whose invention is too recent to have influenced the evolution of our species. representations of letter strings and of numbers occupy reproducible locations within large-scale macromaps, respectively in the left occipito-temporal and bilateral intraparietal cortex. furthermore, recent {fmri} studies reveal a systematic architecture within these areas. to explain this paradoxical cerebral invariance of cultural maps, we propose a neuronal recycling hypothesis, according to which cultural inventions invade evolutionarily older brain circuits and inherit many of their structural constraints.
1514	1871475	article	journal of computer-mediated communication	\N	\N	\N	\N	13	1	2007	\N	2007-11-06 03:06:26	\N	publicly private and privately public social networking on youtube	youtube is a public video-sharing website where people can experience varying degrees of engagement with videos, ranging from casual viewing to sharing videos in order to maintain social relationships. based on a one-year ethnographic project, this article analyzes how youtube participants developed and maintained social networks by manipulating physical and interpretive access to their videos. the analysis reveals how circulating and sharing videos reflects different social relationships among youth. it also identifies varying degrees of "publicness" in video sharing. some participants exhibited "publicly private" behavior, in which video makers' identities were revealed, but content was relatively private because it was not widely accessed. in contrast, "privately public" behavior involved sharing widely accessible content with many viewers, while limiting access to detailed information about video producers' identities.
1515	1873083	article	genome research	\N	\N	cold spring harbor laboratory press	11	17	12	2007	dec	2007-11-08 08:03:40	\N	{28-way} vertebrate alignment and conservation track in the {ucsc} genome browser	an international, peer-reviewed genome sciences journal featuring outstanding original research that offers novel insights into the biology of all organisms
1516	1873815	article	journal of computer-mediated communication	\N	\N	\N	\N	12	2	2007	\N	2007-11-06 12:42:35	\N	exploring {e-science}: an introduction	a number of terms are in vogue that describe the transformation of science through utilization of grid computing, internet-based instrumentation, and global collaboration. for the purposes of this special theme section of the journal of {computer-mediated} communication, the term e-science serves as an umbrella for these initiatives. this article introduces the contributions to the collection and includes a number of suggestions for extending the exploratory work performed to date, including attention to disciplinary and contextual diversity and the importance of longitudinal research designs and historical awareness and of the social shaping of technology as a theoretical concept to understanding the changes currently underway in the scientific enterprise.
1517	1875092	article	organization studies	\N	\N	\N	19	28	3	2007	mar	2007-11-06 19:59:04	\N	contributing to public document repositories: a critical mass theory perspective	public document repositories ({pdrs}) are valuable resources available on the internet and are a component of the broader information commons freely accessible to the public. instances of {pdrs} include the repository of reviews at amazon.com and the online encyclopedia at wikipedia. these repositories are created and sustained by the voluntary contributions of individuals who are not compensated for their inputs. this paper draws on and extends critical mass theory in the context of {pdrs}. using data on the reviews written by prolific reviewers at amazon.com and the text of their personal profiles, we find the critical mass of contributors at the {pdr} not only to be prolific and contributing high-quality reviews, but also to be among the earliest contributors of reviews on products. reviewer profiles revealed the presence of multiple self-oriented motives (self expression, personal development, utilitarian motives, and enjoyment) and other-oriented motives (social affiliation, altruism, and reciprocity) for contribution. we find that the quality and quantity of contributions are inversely related and the motives for quantity of contribution are different from those related to the quality of contribution. the study highlights that {pdrs} are viewed by contributors as social contexts even though making contributions is an individual act that does not involve social interaction. 10.1177/0170840607076002
1518	1877609	article	bmc systems biology	\N	\N	\N	\N	1	1	2007	\N	2007-11-07 13:15:08	\N	models for synthetic biology	synthetic biological engineering is emerging from biology as a distinct discipline based on quantification. the technologies propelling synthetic biology are not new, nor is the concept of designing novel biological molecules. what is new is the emphasis on system {behavior.the} objective is the design and construction of new biological devices and systems to deliver useful applications. numerous synthetic gene circuits have been created in the past decade, including bistable switches, oscillators, and logic gates, and possible applications abound, including biofuels, detectors for biochemical and chemical weapons, disease diagnosis, and gene {therapies.more} than fifty years after the discovery of the molecular structure of {dna}, molecular biology is mature enough for real quantification that is useful for biological engineering applications, similar to the revolution in modeling in chemistry in the 1950s. with the excitement that synthetic biology is generating, the engineering and biological science communities appear remarkably willing to cross disciplinary boundaries toward a common goal.
1519	1884115	article	proceedings of the national academy of sciences of the united states of america	\N	\N	\N	4	104	46	2007	nov	2007-11-08 09:35:51	\N	distribution of node characteristics in complex networks.	our enhanced ability to map the structure of various complex networks is increasingly accompanied by the possibility of independently identifying the functional characteristics of each node. although this led to the observation that nodes with similar characteristics have a tendency to link to each other, in general we lack the tools to quantify the interplay between node properties and the structure of the underlying network. here we show that when nodes in a network belong to two distinct classes, two independent parameters are needed to capture the detailed interplay between the network structure and node properties. we find that the network structure significantly limits the values of these parameters, requiring a phase diagram to uniquely characterize the configurations available to the system. the phase diagram shows a remarkable independence from the network size, a finding that, together with a proposed heuristic algorithm, allows us to determine its shape even for large networks. to test the usefulness of the developed methods, we apply them to biological and socioeconomic systems, finding that protein functions and mobile phone usage occupy distinct regions of the phase diagram, indicating that the proposed parameters have a strong discriminating power.
1520	1899371	article	the american statistician	\N	\N	\N	6	55	3	2001	\N	2007-11-11 18:20:33	\N	some practical guidelines for effective sample size determination	sample size determination is often an important step in planning a statistical study-and it is usually a difficult one. among the important hurdles to be surpassed, one must obtain an estimate of one or more error variances and specify an effect size of importance. there is the temptation to take some shortcuts. this article offers some suggestions for successful and meaningful sample size determination. also discussed is the possibility that sample size may not be the main issue, that the real goal is to design a high-quality study. finally, criticism is made of some ill-advised shortcuts relating to power and sample size.
1521	1906054	book	\N	\N	\N	springer	\N	\N	\N	2008	\N	2007-11-13 07:27:12	\N	web 2.0. neue perspektiven f\\"{u}r marketing und medien	nach dem platzen der  ? {internet-blase}" und einigen spektakul\\"{a}ren {dot-com}-pleiten im jahre 2001 setzte bei vielen unternehmen, kapitalgebern und usern ern\\"{u}chterung hinsichtlich der eignung des internet als handels- und kommunikationsplattform ein. mittlerweile wird ein neuer weltweiter {internet-boom} verzeichnet, in dessen windschatten zahlreiche faszinierende – und h\\"{a}ufig hoch-profitable – {internet-angebote} entstehen. diese neue phase des internet wird als web 2.0 bezeichnet. die herausgeber pr\\"{a}sentieren eine praxisorientierte einf\\"{u}hrung und einen systematischen einblick in aktuelle web {2.0-fragestellungen}. erstmalig thematisieren renommierte autoren aus wissenschaft und praxis die wichtigsten trends und die verflechtung privater internetnutzung und kommerzieller gesch\\"{a}ftsmodelle, die so typisch f\\"{u}r das web 2.0 ist. das buch wendet sich an f\\"{u}hrungskr\\"{a}fte aus den bereichen marketing, {e-commerce} und neue medien sowie an wissenschaftler und studierende auf diesen gebieten.
1522	1916533	article	plos computational biology	\N	\N	\N	\N	preprint	2007	2007	nov	2007-11-14 23:16:31	\N	analysis of sequence conservation at nucleotide resolution	one of the major goals of comparative genomics is to understand the evolutionary history of each nucleotide in the human genome sequence, and the degree to which it is under selective pressure. ascertainment of selective constraint at nucleotide resolution is particularly important for predicting the functional significance of human genetic variation and for analyzing the sequence substructure of cis-regulatory sequences and other functional elements. current methods for analysis of sequence conservation are focused on delineation of conserved regions comprising tens or even hundreds of consecutive nucleotides. we therefore developed a novel computational approach designed specifically for scoring evolutionary conservation at individual base pair resolution. our approach estimates the rate at which each nucleotide position is evolving, computes the probability of neutrality given this rate estimate, and summarizes the result in a sequence {conservation} evaluation ({scone}) score. we computed {scone} scores in a continuous fashion across 1\\% of the human genome for which high-quality sequence information from up to 23 genomes are available. we show that {scone} scores are clearly correlated with the allele frequency of human polymorphisms in both coding and non-coding regions. we find that the majority of non-coding conserved nucleotides lie outside of longer conserved elements predicted by other conservation analyses, and are experiencing ongoing selection in modern humans as evident from the allele frequency spectrum of human polymorphism. we also applied {scone} to analyze the distribution of conserved nucleotides within functional regions. these regions are markedly enriched in individually conserved positions and short (\\&\\#60;15bp) conserved \\&\\#34;chunks\\&\\#34;. our results collectively suggest that the majority of functionally important non-coding conserved positions are highly fragmented and reside outside of canonically-defined long conserved non-coding sequences. a small subset of these fragmented positions may be identified with high confidence.
1523	1922175	article	plos computational biology	\N	\N	\N	\N	preprint	2007	2007	oct	2007-11-15 12:50:59	\N	the modular organization of domain structures: insights into protein-protein binding	domains are the building blocks of proteins and play a crucial role in protein-protein interactions. here, we propose a new approach for the analysis and prediction of domain-domain interfaces. our method, which relies on the representation of domains as residue interacting networks, finds an optimal decomposition of domain structures into modules. the resulting modules are comprised of highly cooperative residues, which exhibit few connections with other modules. we found that binding sites in a domain, which are involved in different domain-domain interactions, are generally contained in different modules. this observation indicates that our modular decomposition is able to separate protein domains into regions with specialized functions. our results show that modules with high modularity values identify binding site regions, demonstrating the predictive character of modularity. furthermore, the combination of modularity with other characteristics, such as sequence conservation or surface patches, was found to improve our predictions. in an attempt to give a physical interpretation to the modular architecture of domains, we analyzed in detail six examples of protein domains with available experimental binding data. the modular configuration of the {tem1}-\\&\\#946;-lactamase binding site illustrates the energetic independence of hot spots located in different modules and cooperativity of those sited within the same modules. the energetic and structural cooperativity between intra-modular residues is also clearly shown in the example of the chymotrypsin inhibitor, where non-binding site residues have a synergistic effect on binding. interestingly, the binding site of the t cell receptor \\&\\#946; chain variable domain 2.1 is contained in one module, which includes structurally distant hot regions displaying positive cooperativity. these findings support the idea that modules possess certain functional and energetic independence. a modular organization of binding sites confers robustness and flexibility to the performance of the functional activity, and facilitates the evolution of protein interactions.
1524	1934660	article	quant-ph/0703044	\N	\N	\N	\N	\N	\N	2008	may	2007-11-18 23:46:57	\N	entanglement in {many-body} systems	the recent interest in aspects common to quantum information and condensed matter has prompted a prosperous activity at the border of these disciplines that were far distant until few years ago. numerous interesting questions have been addressed so far. here we review an important part of this field, the properties of the entanglement in many-body systems. we discuss the zero and finite temperature properties of entanglement in interacting spin, fermionic and bosonic model systems. both bipartite and multipartite entanglement will be considered. at equilibrium we emphasize on how entanglement is connected to the phase diagram of the underlying model. the behavior of entanglement can be related, via certain witnesses, to thermodynamic quantities thus offering interesting possibilities for an experimental test. out of equilibrium we discuss how to generate and manipulate entangled states by means of many-body hamiltonians.
1525	1963184	article	proceedings of the national academy of sciences	\N	\N	national academy of sciences	7	103	43	2006	oct	2007-11-23 03:43:01	department of physics and chemical engineering, university of california, santa barbara, ca 93106, usa.	recent progress in understanding hydrophobic interactions	we present here a brief review of direct force measurements between hydrophobic surfaces in aqueous solutions. for almost 70 years, researchers have attempted to understand the hydrophobic effect (the low solubility of hydrophobic solutes in water) and the hydrophobic interaction or force (the unusually strong attraction of hydrophobic surfaces and groups in water). after many years of research into how hydrophobic interactions affect the thermodynamic properties of processes such as micelle formation (self-assembly) and protein folding, the results of direct force measurements between macroscopic surfaces began to appear in the 1980s. reported ranges of the attraction between variously prepared hydrophobic surfaces in water grew from the initially reported value of 80–100 \\aa{} to values as large as 3,000 \\aa{}. recent improved surface preparation techniques and the combination of surface force apparatus measurements with atomic force microscopy imaging have made it possible to explain the long-range part of this interaction (at separations >200 \\aa{}) that is observed between certain surfaces. we tentatively conclude that only the short-range part of the attraction (<100 \\aa{}) represents the true hydrophobic interaction, although a quantitative explanation for this interaction will require additional research. although our force-measuring technique did not allow collection of reliable data at separations <10 \\aa{}, it is clear that some stronger force must act in this regime if the measured interaction energy curve is to extrapolate to the measured adhesion energy as the surface separation approaches zero (i.e., as the surfaces come into molecular contact).
1526	2007189	article	knowledge engineering review	\N	\N	cambridge university press	\N	23	1	2008	\N	2007-11-28 20:18:46	\N	personalized information retrieval based on context and ontological knowledge	context modeling has been long acknowledged as a key aspect in a wide variety of problem domains. in this paper we focus on the combination of contextualization and personalization methods to improve the performance of personalized information retrieval. the key aspects in our proposed approach are a) the explicit distinction between historic user context and live user context, b) the use of ontology-driven representations of the domain of discourse, as a common, enriched representational ground for content meaning, user interests, and contextual conditions, enabling the definition of effective means to relate the three of them, and c) the introduction of fuzzy representations as an instrument to properly handle the uncertainty and imprecision involved in the automatic interpretation of meanings, user attention, and user wishes. based on a formal grounding at the representational level, we propose methods for the automatic extraction of persistent semantic user preferences, and live, ad-hoc user interests, which are combined in order to improve the accuracy and reliability of personalization for retrieval.
1527	2010561	article	nature	\N	\N	nature publishing group	5	451	7175	2007	nov	2008-01-02 09:05:00	\N	{rna}-mediated epigenetic programming of a genome-rearrangement pathway	genome-wide {dna} rearrangements occur in many eukaryotes but are most exaggerated in ciliates, making them ideal model systems for epigenetic phenomena. during development of the somatic macronucleus, oxytricha trifallax destroys 95\\% of its germ line, severely fragmenting its chromosomes, and then unscrambles hundreds of thousands of remaining fragments by permutation or inversion. here we demonstrate that {dna} or {rna} templates can orchestrate these genome rearrangements in oxytricha, supporting an epigenetic model for sequence-dependent comparison between germline and somatic genomes. a complete {rna} cache of the maternal somatic genome may be available at a specific stage during development to provide a template for correct and precise {dna} rearrangement. we show the existence of maternal {rna} templates that could guide {dna} assembly, and that disruption of specific {rna} molecules disables rearrangement of the corresponding gene. injection of artificial templates reprogrammes the {dna} rearrangement pathway, suggesting that {rna} molecules guide genome rearrangement.
1528	2057275	article	review of educational research	\N	\N	\N	31	77	1	2007	\N	2007-12-04 14:47:13	\N	the power of feedback	feedback is one of the most powerful influences on learning and achievement, but this impact can be either positive or negative. its power is frequently mentioned in articles about learning and teaching, but surprisingly few recent studies have systematically investigated its meaning. this article provides a conceptual analysis of feedback and reviews the evidence related to its impact on learning and achievement. this evidence shows that although feedback is among the major influences, the type of feedback and the way it is given can be differentially effective. a model of feedback is then proposed that identifies the particular properties and circumstances that make it effective, and some typically thorny issues are discussed, including the timing of feedback and the effects of positive and negative feedback. finally, this analysis is used to suggest ways in which feedback can be used to enhance its effectiveness in classrooms.
1529	2064778	article	plos computational biology	\N	\N	\N	\N	preprint	2007	2007	dec	2007-12-06 02:03:53	\N	noise propagation and signaling sensitivity in biological networks: a role for positive feedback	interactions between genes and proteins are crucial for efficient processing of internal or external signals, but this connectivity also amplifies stochastic fluctuations by propagating noise between components. linear (unbranched) cascades were shown to exhibit an interplay between the sensitivity to changes in input signals and the ability to buffer noise. we searched for biological circuits that can maintain signaling sensitivity while minimizing noise propagation, focusing on cases where the noise is characterized by rapid fluctuations. negative feedback can buffer this type of noise, but this buffering comes at the expense of an even greater reduction in signaling sensitivity. by systematically analyzing all three-component circuits, we identify positive feedback as a central motif allowing for the buffering of propagated noise while maintaining sensitivity to long-term changes in input signals. we show analytically that noise reduction in the presence of positive feedback results from improved averaging of rapid fluctuations over time, and discuss in detail a particular implementation in the control of nutrient homeostasis in yeast. as the design of biological networks optimizes for multiple constrains, positive feedback can be used to improve sensitivity without a compromise the ability to buffer propagated noise.
1530	2094129	article	j. am. chem. soc.	\N	\N	\N	\N	110	\N	1988	\N	2007-12-11 21:31:37	\N	the {opls} [optimized potentials for liquid simulations] potential functions for proteins, energy minimizations for crystals of cyclic peptides and crambin	a complete set of intermolecular potential functions has beendeveloped for use in computer simulations of proteins in their nativeenvironment. parameters are reported for 25 peptide residues as wellas the common neutral and charged terminal groups. the potentialfunctions have the simple coulomb plus lennard-jones form and arecompatible with the widely used models for water, tip4p, tip3p, andspc. the parameters were obtained and tested primarily in conjunctionwith monte carlo statistical mechanics simulations of 36 pure organicliquids and numerous aqueous solutions of organic ions representativeof subunits in the side chains and backbones of proteins. bondstretch, angle bend, and torsional terms have been adopted from theamber united-atom force field. as reported here, further testing hasinvolved studies of conformational energy surfaces and optimizationsof the crystal structures for four cyclic hexapeptides and a cyclicpentapeptide. the average root-mean-square deviation from the x-raystructures of the crystals is only 0.17 \\aa for the atomic positions and 3% for the unit cell volumes. a more critical test was then providedby performing energy minimizations for the complete crystal of theprotein crambin, including 182 water molecules that were initiallyplaced via a monte carlo simulation. the resultant root-mean-squaredeviation for the non-hydrogen atoms is still ca. 0.2 \\aa and thevariation in the errors for charged, polar, and nonpolar residues issmall. improvement is apparent over the amber united-atom force fieldwhich has previously been demonstrated to be superior to manyalternatives.
1531	2094310	article	j. chem. phys.	\N	\N	\N	12	107	11	1997	sep	2007-12-11 21:31:40	\N	dissipative particle dynamics: bridging the gap between atomistic and mesoscopic simulation	we critically review dissipative particle dynamics ({dpd}) as a mesoscopic simulation method. we have established useful parameter ranges for simulations, and have made a link between these parameters and \\$\\xi\\$-parameters in {flory-huggins}-type models. this is possible because the equation of state of the {dpd} fluid is essentially quadratic in density. this link opens the way to do large scale simulations, effectively describing millions of atoms, by firstly performing simulations of molecular fragments retaining all atomistic details to derive \\$xi\\$-parameters, then secondly using these results as input to a {dpd} simulation to study the formation of micelles, networks, mesophases and so forth. as an example application, we have calculated the interfacial tension between homopolymer melts as a function of \\$\\xi\\$ and \\$n\\$ and have found a universal scaling collapse when \\$\\sigma/\\rho {k\\_bt} \\xi^0.4\\$ is plotted against \\$\\{xi^n}\\$ for \\$n > 1\\$. we also discuss the use of {dpd} to simulate the dynamics of mesoscopic systems, and indicate a possible problem with the timescale separation between particle diffusion and momentum diffusion (viscosity).
1532	2097686	article	proceedings of the national academy of sciences of the united states of america	\N	\N	\N	5	104	51	2007	dec	2007-12-12 10:25:20	\N	a systematic interaction map of validated kinase inhibitors with {ser/thr} kinases.	10.1073/pnas.0708800104 protein kinases play a pivotal role in cell signaling, and dysregulation of many kinases has been linked to disease development. a large number of kinase inhibitors are therefore currently under investigation in clinical trials, and so far seven inhibitors have been approved as anti-cancer drugs. in addition, kinase inhibitors are widely used as specific probes to study cell signaling, but systematic studies describing selectivity of these reagents across a panel of diverse kinases are largely lacking. here we evaluated the specificity of 156 validated kinase inhibitors, including inhibitors used in clinical trials, against 60 human ser/thr kinases using a thermal stability shift assay. our analysis revealed many unexpected cross-reactivities for inhibitors thought to be specific for certain targets. we also found that certain combinations of active-site residues in the atp-binding site correlated with the detected ligand promiscuity and that some kinases are highly sensitive to inhibition using diverse chemotypes, suggesting them as preferred intervention points. our results uncovered also inhibitor cross-reactivities that may lead to alternate clinical applications. for example, ly333â€²531, a pkcÎ² inhibitor currently in phase iii clinical trials, efficiently inhibited pim1 kinase in our screen, a suggested target for treatment of leukemia. we determined the binding mode of this inhibitor by x-ray crystallography and in addition showed that ly333â€²531 induced cell death and significantly suppressed growth of leukemic cells from acute myeloid leukemia patients.
1533	2102115	article	nature	\N	\N	nature publishing group	9	450	7172	2007	dec	2007-12-13 05:28:53	\N	the biological impact of mass-spectrometry-based proteomics.	in the past decade, there have been remarkable advances in proteomic technologies. mass spectrometry has emerged as the preferred method for in-depth characterization of the protein components of biological systems. using mass spectrometry, key insights into the composition, regulation and function of molecular complexes and pathways have been gained. from these studies, it is clear that mass-spectrometry-based proteomics is now a powerful 'hypothesis-generating engine' that, when combined with complementary molecular, cellular and pharmacological techniques, provides a framework for translating large data sets into an understanding of complex biological processes.
1534	2111163	article	bioinformatics (oxford, england)	\N	\N	oxford university press	2	24	2	2008	jan	2007-12-14 04:18:26	department of computational biology and applied algorithmics, max planck institute for informatics, stuhlsatzenhausweg 85, 66123 saarbr\\"{u}cken, german.	computing topological parameters of biological networks.	summary: rapidly increasing amounts of molecular interaction data are being produced by various experimental techniques and computational prediction methods. in order to gain insight into the organization and structure of the resultant large complex networks formed by the interacting molecules, we have developed the versatile cytoscape plugin networkanalyzer. it computes and displays a comprehensive set of topological parameters, which includes the number of nodes, edges, and connected components, the network diameter, radius, density, centralization, heterogeneity, and clustering coefficient, the characteristic path length, and the distributions of node degrees, neighborhood connectivities, average clustering coefficients, and shortest path lengths. networkanalyzer can be applied to both directed and undirected networks and also contains extra functionality to construct the intersection or union of two networks. it is an interactive and highly customizable application that requires no expert knowledge in graph theory from the user.  availability: networkanalyzer can be downloaded via the cytoscape web site: http://www.cytoscape.org  contact: mario.albrecht@mpi-inf.mpg.de  supplementary information: supplementary data are available at bioinformatics online. 10.1093/bioinformatics/btm554
1535	2111611	article	nature	\N	\N	nature publishing group	8	450	7172	2007	dec	2007-12-17 11:47:53	\N	reaching for high-hanging fruit in drug discovery at protein-protein interfaces	[ndash] protein interfaces targeting the interfaces between proteins has huge therapeutic potential, but discovering small-molecule drugs that disrupt proteinâ€“protein interactions is an enormous challenge. several recent success stories, however, indicate that proteinâ€“protein interfaces might be more tractable than has been thought. these studies discovered small molecules that bind with drug-like potencies to 'hotspots' on the contact surfaces involved in proteinâ€“protein interactions. remarkably, these small molecules bind deeper within the contact surface of the target protein, and bind with much higher efficiencies, than do the contact atoms of the natural protein partner. some of these small molecules are now making their way through clinical trials, so this high-hanging fruit might not be far out of reach.
1536	2111612	article	nature	\N	\N	nature publishing group	7	450	7172	2007	dec	2007-12-18 13:40:36	\N	the origin of protein interactions and allostery in colocalization.	two fundamental principles can account for how regulated networks of interacting proteins originated in cells. these are the law of mass action, which holds that the binding of one molecule to another increases with concentration, and the fact that the colocalization of molecules vastly increases their local concentrations. it follows that colocalization can amplify the effect on one protein of random mutations in another protein and can therefore, through natural selection, lead to interactions between proteins and to a startling variety of complex allosteric controls. it also follows that allostery is common and that homologous proteins can have different allosteric mechanisms. thus, the regulated protein networks of organisms seem to be the inevitable consequence of natural selection operating under physical laws.
1537	2111613	article	nature	\N	\N	nature publishing group	9	450	7172	2007	dec	2007-12-17 22:06:00	\N	the molecular sociology of the cell	proteomic studies have yielded detailed lists of the proteins present in a cell. comparatively little is known, however, about how these proteins interact and are spatially arranged within the 'functional modules' of the cell: that is, the 'molecular sociology' of the cell. this gap is now being bridged by using emerging experimental techniques, such as mass spectrometry of complexes and single-particle cryo-electron microscopy, to complement traditional biochemical and biophysical methods. with the development of integrative computational methods to exploit the data obtained, such hybrid approaches will uncover the molecular architectures, and perhaps even atomic models, of many protein complexes. with these structures in hand, researchers will be poised to use cryo-electron tomography to view protein complexes in action within cells, providing unprecedented insights into protein-interaction networks.
1538	2139257	article	{ieee} trans. acoustics, speech and signal processing	\N	\N	\N	6	26	1	1978	feb	2007-12-18 03:18:48	\N	dynamic programming algorithm optimization for spoken word recognition	abstract-this paper reports on an optimum dynamic programming (dp) based time-normalization algorithm for spoken word recognition. first, a general principle of time-normalization is given using timewarping function. then, two time-normalized distance definitions, ded symmetric and asymmetric forms, are derived from the principle. these two forms are compared with each other through theoretical discussions and experimental studies. the symmetric form algorithm superiority is established. a new technique, called slope constraint, is successfully introduced, in which the warping function slope is restricted so as to improve discrimination between words in different categories. and seibi chiba vestigations were made, based on the assumption that speech patterns are time-sampled with a common and uniform sam-pling period, as in most general cases. one of the problems discussed in this paper involves the relative superiority of either a symmetric form of dp-matching or an asymmetric one. in the asymmetric form, time-normalization is achieved by trans-forming the time axis of a speech pattern onto that of the other. in the symmetric form, on the other hand, both time axes are transformed onto a temporarily defined common axis. theoretical and experimental comparisons show that the sym-metric form gives better recognition than the asymmetric one. another problem discussed concerns slope constraint technique. since too much of the warping function flexibility sometimes results in poor discrimination between words in different the effective slope constraint characteristic is qualitatively analyzed, and the optimum slope constraint condition is determined through experiments. the optimized algorithm is then extensively subjected to experimentat comparison with various dp-algorithms, previously applied to spoken word recognition by different research groups. the experiment shows that the present algorithm gives no more than about twothirds errors, even compared to the best conventional algorithm. categories, a constraint is newly introduced on the warping i.
1539	2139378	article	j. comput. chem.	\N	\N	john wiley \\& sons, inc.	20	25	13	2004	oct	2007-12-18 04:19:59	laboratory of physical chemistry, swiss federal institute of technology, eth-h\\"{o}nggerberg, 8093 z\\"{u}rich, switzerland; groningen biomolecular sciences and biotechnology institute (gbb), department of biophysical chemistry, university of groningen, nijenborgh 4, 9747 ag groningen, the netherlands	a biomolecular force field based on the free enthalpy of hydration and solvation: the {gromos} force-field parameter sets {53a5} and {53a6}	successive parameterizations of the {gromos} force field have been used successfully to simulate biomolecular systems over a long period of time. the continuing expansion of computational power with time makes it possible to compute ever more properties for an increasing variety of molecular systems with greater precision. this has led to recurrent parameterizations of the {gromos} force field all aimed at achieving better agreement with experimental data. here we report the results of the latest, extensive reparameterization of the {gromos} force field. in contrast to the parameterization of other biomolecular force fields, this parameterization of the {gromos} force field is based primarily on reproducing the free enthalpies of hydration and apolar solvation for a range of compounds. this approach was chosen because the relative free enthalpy of solvation between polar and apolar environments is a key property in many biomolecular processes of interest, such as protein folding, biomolecular association, membrane formation, and transport over membranes. the newest parameter sets, {53a5} and {53a6}, were optimized by first fitting to reproduce the thermodynamic properties of pure liquids of a range of small polar molecules and the solvation free enthalpies of amino acid analogs in cyclohexane ({53a5}). the partial charges were then adjusted to reproduce the hydration free enthalpies in water ({53a6}). both parameter sets are fully documented, and the differences between these and previous parameter sets are discussed. {\\copyright} 2004 wiley periodicals, inc. j comput chem 25: 1656–1676, 2004
1540	2144477	book	\N	\N	\N	{blackstone audio inc.}	\N	\N	\N	2007	apr	2007-12-19 07:14:24	\N	the {4-hour} work week: escape 9-5, live anywhere, and join the new rich	{tim ferriss is an extraordinary young man on a mission. the twenty-eight-year-old serial vagabond and successful entrepreneur has been teaching a wildly popular course at princeton university for the past four years--a how-to and why-to guide to throwing out the old tools and methods for success (balancing life and work, retiring well, having a great nest egg) and replacing them with a whole new way of living. readers can lead a rich life by working only four hours a week, freeing up the rest of their time to spend it living the lives they want.}
1541	2151562	article	educational technology research and development	educational technology research and development	\N	kluwer academic publishers-plenum publishers	20	55	3	2007	jun	2007-12-20 12:53:09	\N	game design and learning: a conjectural analysis of how massively multiple online role-playing games ({mmorpgs}) foster intrinsic motivation	during the past two decades, the popularity of computer and video games has prompted games to become a source of study for educational researchers and instructional designers investigating how various aspects of game design might be appropriated, borrowed, and re-purposed for the design of educational materials. the purpose of this paper is to present an analysis of how the structure in massively multiple online role-playing games ({mmorpgs}) might inform the design of interactive learning and game-based learning environments by looking at the elements which support intrinsic motivation. specifically, this analysis presents (a) an overview of the two primary elements in {mmorpgs} game design: character design and narrative environment, (b) a discussion of intrinsic motivation in character role-playing, (c) a discussion of intrinsic motivational supports and cognitive support of the narrative structure of small quests, and (d) a discussion of how the narrative structure of {mmorpgs} might foster learning in various types of knowledge.
1542	2189904	inproceedings	\N	2007 future of software engineering	fose	ieee computer society	17	\N	\N	2007	\N	2008-01-02 21:26:04	washington, dc, usa	software design and architecture the once and future focus of software engineering	the design of software has been a focus of software engineering research since the field's beginning. this paper explores key aspects of this research focus and shows why design will remain a principal focus. the intrinsic elements of software design, both process and product, are discussed: concept formation, use of experience, and means for representation, reasoning, and directing the design activity. design is presented as being an activity engaged by a wide range of stakeholders, acting throughout most of a system's lifecycle, making a set of key choices which constitute the application's architecture. directions for design research are outlined, including: (a) drawing lessons, inspiration, and techniques from design fields outside of computer science, (b) emphasizing the design of application "character" (functionality and style) as well as the application's structure, and (c) expanding the notion of software to encompass the design of additional kinds of intangible complex artifacts.
1543	2191303	article	nucleic acids research	\N	\N	oxford university press	5	36	suppl 1	2008	jan	2008-01-18 03:56:58	\N	{drugbank}: a knowledgebase for drugs, drug actions and drug targets	{drugbank} is a richly annotated resource that combines detailed drug data with comprehensive drug target and drug action information. since its first release in 2006, {drugbank} has been widely used to facilitate in silico drug target discovery, drug design, drug docking or screening, drug metabolism prediction, drug interaction prediction and general pharmaceutical education. the latest version of {drugbank} (release 2.0) has been expanded significantly over the previous release. with ?4900 drug entries, it now contains 60\\% more {fda}-approved small molecule and biotech drugs including 10\\% more 'experimental' drugs. significantly, more protein target data has also been added to the database, with the latest version of {drugbank} containing three times as many non-redundant protein or drug target sequences as before (1565 versus 524). each {drugcard} entry now contains more than 100 data fields with half of the information being devoted to drug/chemical data and the other half devoted to pharmacological, pharmacogenomic and molecular biological data. a number of new data fields, including food–drug interactions, drug–drug interactions and experimental {adme} data have been added in response to numerous user requests. {drugbank} has also significantly improved the power and simplicity of its structure query and text query searches. {drugbank} is available at http://www.drugbank.ca
1544	2207732	article	american journal of sociology	\N	\N	\N	50	110	2	2004	\N	2008-01-08 13:58:38	\N	structural holes and good ideas	this article outlines the mechanism by which brokerage provides social capital. opinion and behavior are more homogeneous within than between groups, so people connected across groups are more familiar with alternative ways of thinking and behaving. brokerage across the structural holes between groups provides a vision of options otherwise unseen, which is the mechanism by which brokerage becomes social capital. i review evidence consistent with the hypothesis, then look at the networks around managers in a large american electronics company. the organization is rife with structural holes, and brokerage has its expected correlates. compensation, positive performance evaluations, promotions, and good ideas are disproportionately in the hands of people whose networks span structural holes. the between?group brokers are more likely to express ideas, less likely to have ideas dismissed, and more likely to have ideas evaluated as valuable. i close with implications for creativity and structural change.
1545	2208356	article	nature biotechnology	\N	\N	nature publishing group	5	26	1	2008	jan	2008-01-09 18:36:26	\N	a quantitative analysis of kinase inhibitor selectivity.	kinase inhibitors are a new class of therapeutics with a propensity to inhibit multiple targets. the biological consequences of multi-kinase activity are poorly defined, and an important step toward understanding the relationship between selectivity, efficacy and safety is the exploration of how inhibitors interact with the human kinome. we present interaction maps for 38 kinase inhibitors across a panel of 317 kinases representing >50\\% of the predicted human protein kinome. the data constitute the most comprehensive study of kinase inhibitor selectivity to date and reveal a wide diversity of interaction patterns. to enable a global analysis of the results, we introduce the concept of a selectivity score as a general tool to quantify and differentiate the observed interaction patterns. we further investigate the impact of panel size and find that small assay panels do not provide a robust measure of selectivity.
1546	2219600	article	\N	\N	\N	\N	\N	\N	\N	2007	aug	2008-01-11 16:07:08	\N	a correlated topic model of science	topic models, such as latent dirichlet allocation (lda), can be useful tools for the statistical analysis of document collections and other discrete data. the lda model assumes that the words of each document arise from a mixture of topics, each of which is a distribution over the vocabulary. a limitation of lda is the inability to model topic correlation even though, for example, a document about genetics is more likely to also be about disease than x-ray astronomy. this limitation stems from the use of the dirichlet distribution to model the variability among the topic proportions. in this paper we develop the correlated topic model (ctm), where the topic proportions exhibit correlation via the logistic normal distribution [j. roy. statist. soc. ser. b 44 (1982) 139â€“177]. we derive a fast variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. we apply the ctm to the articles from science published from 1990â€“1999, a data set that comprises 57m words. the ctm gives a better fit of the data than lda, and we demonstrate its use as an exploratory tool of large document collections. 1. introduction. large
1547	2226050	article	communciation pure application	\N	\N	\N	44	LVII	\N	2004	\N	2008-01-13 14:45:49	\N	an iterative thresholding algorithm for linear inverse problems with a sparsity constraint	we consider linear inverse problems where the solution is assumed to have a sparse expansion on an arbitrary preassigned orthonormal basis. we prove that replacing the usual quadratic regularizing penalties by weighted &lscr;p-penalties on the coefficients of such expansions, with 1 le p le 2, still regularizes the problem. use of such &lscr;p-penalized problems with p &lt; 2 is often advocated when one expects the underlying ideal noiseless solution to have a sparse expansion with respect to the basis under consideration. to compute the corresponding regularized solutions, we analyze an iterative algorithm that amounts to a landweber iteration with thresholding (or nonlinear shrinkage) applied at each iteration step. we prove that this algorithm converges in norm. Â© 2004 wiley periodicals, inc.
1548	2242051	article	nature	\N	\N	nature publishing group	4	451	7176	2008	jan	2008-01-17 00:29:29	\N	programming biomolecular self-assembly pathways	in nature, self-assembling and disassembling complexes of proteins and nucleic acids bound to a variety of ligands perform intricate and diverse dynamic functions. in contrast, attempts to rationally encode structure and function into synthetic amino acid and nucleic acid sequences have largely focused on engineering molecules that self-assemble into prescribed target structures, rather than on engineering transient system dynamics1, 2. to design systems that perform dynamic functions without human intervention, it is necessary to encode within the biopolymer sequences the reaction pathways by which self-assembly occurs. nucleic acids show promise as a design medium for engineering dynamic functions, including catalytic hybridization3, 4, 5, 6, triggered self-assembly7 and molecular computation8, 9. here, we program diverse molecular self-assembly and disassembly pathways using a 'reaction graph' abstraction to specify complementarity relationships between modular domains in a versatile {dna} hairpin motif. molecular programs are executed for a variety of dynamic functions: catalytic formation of branched junctions, autocatalytic duplex formation by a cross-catalytic circuit, nucleated dendritic growth of a binary molecular 'tree', and autonomous locomotion of a bipedal walker.
1549	2266473	inproceedings	\N	ijcai	\N	\N	5	\N	\N	2001	\N	2008-01-21 08:52:01	\N	the foundations of {cost-sensitive} learning	this paper revisits the problem of optimal learning and decision-making when different misclassification errors incur different penalties. we characterize precisely but intuitively when a cost matrix is reasonable, and we show how to avoid the mistake of defining a cost matrix that is economically incoherent. for the two-class case, we prove a theorem that shows how to change the proportion of negative examples in a training set in order to make optimal cost-sensitive classification decisions...
1550	2269410	article	nature	\N	\N	nature publishing group	3	397	6720	1999	feb	2008-01-21 17:05:08	department of psychology, boston university, massachusetts 02215, usa.	the global record of memory in hippocampal neuronal activity	in humans the hippocampal region of the brain is crucial for declarative1 or episodic2 memory for a broad range of materials. in contrast, there has been controversy over whether the hippocampus mediates a similarly general memory function in other species, or whether it is dedicated to spatial memory processing3, 4, 5, 6. evidence for the spatial view is derived principally from the observations of 'place cells'—hippocampal neurons that fire whenever the animal is in a particular location in its environment7, 8, or when it perceives a specific stimulus or performs a specific behaviour in a particular place3, 4. we trained rats to perform the same recognition memory task in several distinct locations in a rich spatial environment and found that the activity of many hippocampal neurons was related consistently to perceptual, behavioural or cognitive events, regardless of the location where these events occurred. these results indicate that non-spatial events are fundamental elements of hippocampal representation, and support the view that, across species, the hippocampus has a broad role in information processing associated with memory.
1551	2269925	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2008-01-21 20:12:42	\N	on testing the significance of sets of genes	this paper discusses the problem of identifying differentially expressed groups of genes from a microarray experiment. the groups of genes are externally defined, for example, sets of gene pathways derived from biological databases. our starting point is the interesting gene set enrichment analysis (gsea) procedure of subramanian et al. [proc. natl. acad. sci. usa 102 (2005) 15545â€“15550]. we study the problem in some generality and propose two potential improvements to gsea: the maxmean statistic for summarizing gene-sets, and restandardization for more accurate inferences. we discuss a variety of examples and extensions, including the use of gene-set scores for class predictions. we also describe a new r language package gsa that implements our ideas.
1552	2283005	article	bmc bioinformatics	\N	\N	\N	\N	9	1	2008	jan	2008-04-02 20:47:25	\N	a statistical toolbox for metagenomics: assessing functional diversity in microbial communities.	the results of comparisons between the three habitats were surprising considering the relatively low overlap of membership and the distinctively different characteristics of the three habitats. these tools will facilitate the use of metagenomics to pursue statistically sound genome-based ecological analyses.
1553	2286684	article	nucleic acids research	\N	\N	oxford university press	\N	36	2	2008	feb	2008-01-25 02:23:11	\N	model-based variance-stabilizing transformation for illumina microarray data	variance stabilization is a step in the preprocessing of microarray data that can greatly benefit the performance of subsequent statistical modeling and inference. due to the often limited number of technical replicates for affymetrix and {cdna} arrays, achieving variance stabilization can be difficult. although the illumina microarray platform provides a larger number of technical replicates on each array (usually over 30 randomly distributed beads per probe), these replicates have not been leveraged in the current log2 data transformation process. we devised a variance-stabilizing transformation ({vst}) method that takes advantage of the technical replicates available on an illumina microarray. we have compared {vst} with log2 and variance-stabilizing normalization ({vsn}) by using the kruglyak bead-level data (2006) and barnes titration data (2005). the results of the kruglyak data suggest that {vst} stabilizes variances of bead-replicates within an array. the results of the barnes data show that {vst} can improve the detection of differentially expressed genes and reduce false-positive identifications. we conclude that although both {vst} and {vsn} are built upon the same model of measurement noise, {vst} stabilizes the variance better and more efficiently for the illumina platform by leveraging the availability of a larger number of within-array replicates. the algorithms and supplementary data are included in the lumi package of bioconductor, available at: www.bioconductor.org.
1554	2288901	article	bmc bioinformatics	\N	\N	\N	\N	9	1	2008	\N	2008-01-25 09:57:45	\N	{ezarray}: a web-based highly automated affymetrix expression array data management and analysis system.	{background}: though microarray experiments are very popular in life science research, managing and analyzing microarray data are still challenging tasks for many biologists. most microarray programs require users to have sophisticated knowledge of mathematics, statistics and computer skills for usage. with accumulating microarray data deposited in public databases, easy-to-use programs to re-analyze previously published microarray data are in high demand. {results}: {ezarray} is a web-based affymetrix expression array data management and analysis system for researchers who need to organize microarray data efficiently and get data analyzed instantly. {ezarray} organizes microarray data into projects that can be analyzed online with predefined or custom procedures. {ezarray} performs data preprocessing and detection of differentially expressed genes with statistical methods. all analysis procedures are optimized and highly automated so that even novice users with limited pre-knowledge of microarray data analysis can complete initial analysis quickly. since all input files, analysis parameters, and executed scripts can be downloaded, {ezarray} provides maximum reproducibility for each analysis. in addition, {ezarray} integrates with gene expression omnibus ({geo}) and allows instantaneous re-analysis of published array data. {conclusion}: {ezarray} is a novel affymetrix expression array data analysis and sharing system. {ezarray} provides easy-to-use tools for re-analyzing published microarray data and will help both novice and experienced users perform initial analysis of their microarray data from the location of data storage. we believe {ezarray} will be a useful system for facilities with microarray services and laboratories with multiple members involved in microarray data analysis. {ezarray} is freely available from http://www.ezarray.com/.
1555	2289016	article	nature methods	\N	\N	nature publishing group	5	5	2	2008	jan	2008-02-21 17:20:30	[1] washington university school of medicine, department of genetics and genome sequencing center, 4444 forest park blvd., st. louis, missouri 63108, usa. [2] these authors contributed equally to this work.	whole-genome sequencing and variant discovery in c. elegans	massively parallel sequencing instruments enable rapid and inexpensive {dna} sequence data production. because these instruments are new, their data require characterization with respect to accuracy and utility. to address this, we sequenced a caernohabditis elegans n2 bristol strain isolate using the solexa sequence analyzer, and compared the reads to the reference genome to characterize the data and to evaluate coverage and representation. massively parallel sequencing facilitates strain-to-reference comparison for genome-wide sequence variant discovery. owing to the short-read-length sequences produced, we developed a revised approach to determine the regions of the genome to which short reads could be uniquely mapped. we then aligned solexa reads from c. elegans strain {cb4858} to the reference, and screened for single-nucleotide polymorphisms ({snps}) and small indels. this study demonstrates the utility of massively parallel short read sequencing for whole genome resequencing and for accurate discovery of genome-wide polymorphisms.
1556	2310535	article	proceedings of the national academy of sciences	\N	\N	national academy of sciences	5	105	5	2008	feb	2008-01-31 10:51:39	department of urology, veterans affairs medical center and university of california, san francisco, ca 94121.	{microrna}-373 induces expression of genes with complementary promoter sequences	recent studies have shown that {microrna} ({mirna}) regulates gene expression by repressing translation or directing sequence-specific degradation of complementary {mrna}. here, we report new evidence in which {mirna} may also function to induce gene expression. by scanning gene promoters in silico for sequences complementary to known {mirnas}, we identified a putative {mir}-373 target site in the promoter of e-cadherin. transfection of {mir}-373 and its precursor hairpin {rna} ({pre-mir}-373) into {pc}-3 cells readily induced e-cadherin expression. knockdown experiments confirmed that induction of e-cadherin by {pre-mir}-373 required the {mirna} maturation protein dicer. further analysis revealed that cold-shock domain-containing protein c2 ({csdc2}), which possesses a putative {mir}-373 target site within its promoter, was also readily induced in response to {mir}-373 and {pre-mir}-373. furthermore, enrichment of {rna} polymerase {ii} was detected at both e-cadherin and {csdc2} promoters after {mir}-373 transfection. mismatch mutations to {mir}-373 indicated that gene induction was specific to the {mir}-373 sequence. transfection of promoter-specific {dsrnas} revealed that the concurrent induction of e-cadherin and {csdc2} by {mir}-373 required the {mirna} target sites in both promoters. in conclusion, we have identified a {mirna} that targets promoter sequences and induces gene expression. these findings reveal a new mode by which {mirnas} may regulate gene expression.
1557	2311249	article	nature	\N	\N	nature publishing group	3	451	7179	2008	jan	2008-04-08 22:37:15	\N	bacterial carbon processing by generalist species in the coastal ocean	the assimilation and mineralization of dissolved organic carbon ({doc}) by marine bacterioplankton is a major process in the ocean carbon cycle1. however, little information exists on the specific metabolic functions of participating bacteria and on whether individual taxa specialize on particular components of the marine {doc} pool2. here we use experimental metagenomics to show that coastal communities are populated by taxa capable of metabolizing a wide variety of organic carbon compounds. genomic {dna} captured from bacterial community subsets metabolizing a single model component of the {doc} pool (either dimethylsulphoniopropionate or vanillate) showed substantial overlap in gene composition as well as a diversity of carbon-processing capabilities beyond the selected phenotypes. our direct measure of niche breadth for bacterial functional assemblages indicates that, in accordance with ecological theory, heterogeneity in the composition and supply of organic carbon to coastal oceans may favour generalist bacteria. in the important interplay between microbial community structure and biogeochemical cycling, coastal heterotrophic communities may be controlled less by transient changes in the carbon reservoir that they process and more by factors such as trophic interactions and physical conditions.
1558	2320347	article	genome research	\N	\N	\N	4	18	2	2008	feb	2008-02-01 19:09:40	\N	qualifying the relationship between sequence conservation and molecular function	quantification of evolutionary constraints via sequence conservation can be leveraged to annotate genomic functional sequences. recent efforts addressing the converse of this relationship have identified many sites in metazoan genomes with molecular function but without detectable conservation between related species. here, we discuss explanations and implications for these results considering both practical and theoretical issues. in particular, phylogenetic scope influences the relationship between sequence conservation and function. comparisons of distantly related species can detect constraint with high specificity due to the loss of conserved neutral sequence, but sensitivity is sacrificed as a result of functional changes related to lineage-specific biology. the strength of natural selection operating on functional sequence is also important. mutations to functional sequences that result in small fitness effects are subject to weaker constraints. therefore, particularly when comparing highly divergent species, functional sequences that are degenerate or biologically redundant will be prone to turnover, wherein functional sequences are replaced by effectively equivalent, but nonorthologous counterparts. finally, considering the size and complexity of metazoan genomes and the fact that many nonconserved sequences are associated with sequence-degenerate, low-level molecular functions, we find it likely that there exist many biochemically functional sequences that are not under constraint. this hypothesis does not lead to the conclusion that huge amounts of vertebrate genomes are functionally important, but rather that such  ” functionality” represents molecular noise that has weak or no effect on organismal phenotypes.
1559	2321560	article	bmc bioinformatics	\N	\N	biomed central ltd	\N	9	1	2008	jan	2008-02-05 11:58:49	\N	{opendmap}: an open source, ontology-driven concept analysis engine, with applications to capturing knowledge regarding protein transport, protein interactions and cell-type-specific gene expression	information extraction ({ie}) efforts are widely acknowledged to be important in harnessing the rapid advance of biomedical knowledge, particularly in areas where important factual information is published in a diverse literature. here we report on the design, implementation and several evaluations of {opendmap}, an ontology-driven, integrated concept analysis system. it significantly advances the state of the art in information extraction by leveraging knowledge in ontological resources, integrating diverse text processing applications, and using an expanded pattern language that allows the mixing of syntactic and semantic elements and variable ordering.
1560	2323708	article	\N	\N	\N	\N	\N	\N	\N	1999	oct	2008-02-02 19:50:00	\N	shot noise in mesoscopic conductors	theoretical and experimental work concerned with dynamic fluctuations has developed into a very active and fascinating subfield of mesoscopic physics. we present a review of this development focusing on shot noise in small electric conductors. shot noise is a consequence of the quantization of charge. it can be used to obtain information on a system which is not available through conductance measurements. in particular, shot noise experiments can determine the charge and statistics of the quasiparticles relevant for transport, and reveal information on the potential profile and internal energy scales of mesoscopic systems. shot noise is generally more sensitive to the effects of electron-electron interactions than the average conductance. we present a discussion based on the conceptually transparent scattering approach and on the classical langevin and {boltzmann-langevin} methods; in addition a discussion of results which cannot be obtained by these methods is provided. we conclude the review by pointing out a number of unsolved problems and an outlook on the likely future development of the field.
1561	2342470	article	bioinformatics (oxford, england)	\N	\N	oxford university press	1	24	6	2008	mar	2008-02-06 15:52:57	\N	{libsbml}: an {api} library for {sbml}.	summary: libsbml is an application programming interface library for reading, writing, manipulating and validating content expressed in the systems biology markup language (sbml) format. it is written in iso c and c++, provides language bindings for common lisp, java, python, perl, matlab and octave, and includes many features that facilitate adoption and use of both sbml and the li-brary. developers can embed libsbml in their applications, saving themselves the work of implementing their own sbml parsing, ma-nipulation, and validation software.  availability: libsbml 3 was released in august 2007. source code, binaries and documentation are freely available under lgpl open-source terms from http://sbml.org/software/libsbml.  contact: sbml-team@caltech.edu 10.1093/bioinformatics/btn051
1562	2345304	article	systematic biology	\N	\N	\N	11	52	5	2003	\N	2008-02-06 23:58:30	\N	comparison of likelihood and bayesian methods for estimating divergence times using multiple gene loci and calibration points, with application to a radiation of cute-looking mouse lemur species	divergence time and substitution rate are seriously confounded in phylogenetic analysis, making it difficult to estimate divergence times when the molecular clock (rate constancy among lineages) is violated. this problem can be alleviated to some extent by analyzing multiple gene loci simultaneously and by using multiple calibration points. while different genes may have different patterns of evolutionary rate change, they share the same divergence times. indeed, the fact that each gene may violate the molecular clock differently leads to the advantage of simultaneous analysis of multiple loci. multiple calibration points provide the means for characterizing the local evolutionary rates on the phylogeny. in this paper, we extend previous likelihood models of local molecular clock for estimating species divergence times to accommodate multiple calibration points and multiple genes. heterogeneity among different genes in evolutionary rate and in substitution process is accounted for by the models. we apply the likelihood models to analyze two mitochondrial protein-coding genes, cytochrome oxidase {ii} and cytochrome b, to estimate divergence times of malagasy mouse lemurs and related outgroups. the likelihood method is compared with the bayes method of thorne et al. (1998, mol. biol. evol. 15: 1647-1657), which uses a probabilistic model to describe the change in evolutionary rate over time and uses the markov chain monte carlo procedure to derive the posterior distribution of rates and times. our likelihood implementation has the drawbacks of failing to accommodate uncertainties in fossil calibrations and of requiring the researcher to classify branches on the tree into different rate groups. both problems are avoided in the bayes method. despite the differences in the two methods, however, data partitions and model assumptions had the greatest impact on date estimation. the three codon positions have very different substitution rates and evolutionary dynamics, and assumptions in the substitution model affect date estimation in both likelihood and bayes analyses. the results demonstrate that the separate analysis is unreliable, with dates variable among codon positions and between methods, and that the combined analysis is much more reliable. when the three codon positions were analyzed simultaneously under the most realistic models using all available calibration information, the two methods produced similar results. the divergence of the mouse lemurs is dated to be around 7-10 million years ago, indicating a surprisingly early species radiation for such a morphologically uniform group of primates.
1563	2349995	article	pew/internet	\N	\N	\N	\N	\N	\N	-1	\N	2008-02-07 18:15:24	\N	teens and social media	the use of social media â€“ from blogging to online social networking to creation of all kinds of digital material â€“ is central to many teenagersâ€™ lives. some 93% of teens use the internet, and more of them than ever are treating it as a venue for social interaction â€“ a place where they can share creations, tell stories, and interact with others. the pew internet & american life project has found that 64% of online teens ages 12-17 have participated in one or more among a wide range of content-creating activities on the internet, up from 57% of online teens in a similar survey at the end of 2004. Ã´Â€Â‚Â„ 39% of online teens share their own artistic creations online, such as artwork, photos, stories, or videos, up from 33% in 2004. Ã´Â€Â‚Â„ 33% create or work on webpages or blogs for others, including those for groups they belong to, friends, or school assignments, basically unchanged from 2004 (32%). Ã´Â€Â‚Â„ 28% have created their own online journal or blog, up from 19% in 2004. Ã´Â€Â‚Â„ 27% maintain their own personal webpage, up from 22% in 2004. Ã´Â€Â‚Â„ 26% remix content they find online into their own creations, up from 19% in 2004. the percentage of those ages 12-17 who said â€œyesâ€ to at least one of those five content-creation activities is 64% of online teens, or 59% of all teens. in addition to those core elements of content creation, 55% of online teens ages 12-17 have created a profile on a social networking site such as facebook or myspace; 47% of online teens have uploaded photos where others can see them, though many restrict access to the photos in some way; and 14% of online teens have posted videos online. the current survey marks the first time questions about video posting and sharing were asked.
1564	2354628	article	plos genet	\N	\N	public library of science	\N	4	2	2008	feb	2008-02-08 19:59:29	\N	comparing patterns of natural selection across species using selective signatures	comparing gene expression profiles over many different conditions has led to insights that were not obvious from single experiments. in the same way, comparing patterns of natural selection across a set of ecologically distinct species may extend what can be learned from individual genome-wide surveys. toward this end, we show how variation in protein evolutionary rates, after correcting for genome-wide effects such as mutation rate and demographic factors, can be used to estimate the level and types of natural selection acting on genes across different species. we identify unusually rapidly and slowly evolving genes, relative to empirically derived genome-wide and gene family-specific background rates for 744 core protein families in 30 ?-proteobacterial species. we describe the pattern of fast or slow evolution across species as the  ” selective signature” of a gene. selective signatures represent a profile of selection across species that is predictive of gene function: pairs of genes with correlated selective signatures are more likely to share the same cellular function, and genes in the same pathway can evolve in concert. for example, glycolysis and phenylalanine metabolism genes evolve rapidly in idiomarina loihiensis, mirroring an ecological shift in carbon source from sugars to amino acids. in a broader context, our results suggest that the genomic landscape is organized into functional modules even at the level of natural selection, and thus it may be easier than expected to understand the complex evolutionary pressures on a cell. natural selection promotes the survival of the fittest individuals within a species. over many generations, this may result in the maintenance of ancestral traits (conservation through purifying selection), or the emergence of newly beneficial traits (adaptation through positive selection). at the genetic level, long-term purifying or positive selection can cause genes to evolve more slowly, or more rapidly, providing a way to identify these evolutionary forces. while some genes are subject to consistent purifying or positive selection in most species, other genes show unexpected levels of selection in a particular species or group of species—a pattern we refer to as the  ” selective signature” of the gene. in this work, we demonstrate that these patterns of natural selection can be mined for information about gene function and species ecology. in the future, this method could be applied to any set of related species with fully sequenced genomes to better understand the genetic basis of ecological divergence.
1565	2357901	article	j. am. soc. inf. sci. technol.	\N	\N	john wiley \\& sons, inc.	14	59	2	2008	jan	2008-02-09 15:08:35	new york, ny, usa	novelty and topicality in interactive information retrieval	the information science research community is characterized by a paradigm split, with a system-centered cluster working on information retrieval ({ir}) algorithms and a user-centered cluster working on user behavior. the two clusters rarely leverage each other's insight and strength. one major suggestion from user-centered studies is to treat the relevance judgment of documents as a subjective, multidimensional, and dynamic concept rather than treating it as objective and based on topicality only. this study explores the possibility to enhance users' topicality-based relevance judgment with subjective novelty judgment in interactive {ir}. a set of systems is developed which differs in the way the novelty judgment is incorporated. in particular, this study compares systems which assume that users' novelty judgment is directed to a certain subtopic area and those which assume that users' novelty judgment is undirected. this study also compares systems which assume that users judge a document based on topicality first and then novelty in a stepwise, noncompensatory fashion and those which assume that users consider topicality and novelty simultaneously and as compensatory to each other. the user study shows that systems assuming directed novelty in general have higher relevance precision, but systems assuming a stepwise judgment process and systems assuming a compensatory judgment process are not significantly different. \\&copy; 2008 wiley periodicals, inc.
1566	2357917	article	psychological review	\N	\N	apa	36	114	1	2007	jan	2008-02-09 15:17:53	\N	representing word meaning and order information in a composite holographic lexicon	the authors present a computational model that builds a holographic lexicon representing both word meaning and word order from unsupervised experience with natural language. the model uses simple convolution and superposition mechanisms (cf. b. b. murdock, 1982) to learn distributed holographic representations for words. the structure of the resulting lexicon can account for empirical data from classic experiments studying semantic typicality, categorization, priming, and semantic constraint in sentence completions. furthermore, order information can be retrieved from the holographic representations, allowing the model to account for limited word transitions without the need for built-in transition rules. the model demonstrates that a broad range of psychological data can be accounted for directly from the structure of lexical representations learned in this way, without the need for complexity to be built into either the processing mechanisms or the representations. the holographic representations are an appropriate knowledge representation to be used by higher order models of language comprehension, relieving the complexity required at the higher level.
1567	2366929	article	nucleic acids research	\N	\N	oxford university press	8	32	4	2004	mar	2009-01-25 15:41:17	lehrstuhl f\\"{u}r mikrobiologie, technische universit\\"{a}t m\\"{u}nchen, d-853530 freising, germany. ludwig@mikro.biologie.tu-muenchen.de	{arb}: a software environment for sequence data	the {arb} (from latin arbor, tree) project was initiated almost 10 years ago. the {arb} program package comprises a variety of directly interacting software tools for sequence database maintenance and analysis which are controlled by a common graphical user interface. although it was initially designed for ribosomal {rna} data, it can be used for any nucleic and amino acid sequence data as well. a central database contains processed (aligned) primary structure data. any additional descriptive data can be stored in database fields assigned to the individual sequences or linked via local or worldwide networks. a phylogenetic tree visualized in the main window can be used for data access and visualization. the package comprises additional tools for data import and export, sequence alignment, primary and secondary structure editing, profile and filter calculation, phylogenetic analyses, specific hybridization probe design and evaluation and other components for data analysis. currently, the package is used by numerous working groups worldwide.
1568	2369506	article	trends genet	\N	\N	elsevier trends journals	7	24	3	2008	mar	2008-02-13 12:34:02	\N	bioinformatics challenges of new sequencing technology	new {dna} sequencing technologies can sequence up to one billion bases in a single day at low cost, putting large-scale sequencing within the reach of many scientists. many researchers are forging ahead with projects to sequence a range of species using the new technologies. however, these new technologies produce read lengths as short as 3540 nucleotides, posing challenges for genome assembly and annotation. here we review the challenges and describe some of the bioinformatics systems that are being proposed to solve them. we specifically address issues arising from using these technologies in assembly projects, both de novo and for resequencing purposes, as well as efforts to improve genome annotation in the fragmented assemblies produced by short read lengths.
1569	2374088	book	\N	\N	\N	cambridge university press	\N	\N	\N	1997	\N	2008-02-14 13:58:18	\N	quantum optics	{quantum optics has witnessed significant theoretical and experimental developments in recent years. this book provides an in-depth and wide-ranging introduction to the subject, emphasizing throughout the basic principles and their applications. the book begins by developing the basic tools of quantum optics, and goes on to show the application of these tools in a variety of quantum optical systems, including lasing without inversion, squeezed states, and atom optics. the final four chapters discuss quantum optical tests of the foundations of quantum mechanics, and particular aspects of measurement theory. assuming only a background of standard quantum mechanics and electromagnetic theory, and containing many problems and references, this book will be invaluable to graduate students of quantum optics, as well as to researchers in this field.}
1570	2380446	inproceedings	\N	proceedings of the 2005 ieee congress on evolutionary computation (cec-2005)	\N	\N	7	\N	\N	2005	\N	2008-02-14 17:55:00	\N	evolution of the driving styles of anticipatory agent remotely operating a scaled model of racing car	we present an approach for automated evolutionary design of driving agent, able to remotely operate a scale model of racing car running in a fastest possible way. the agent's actions are conveyed to the car via standard radio control transmitter. the agent perceives the environment from a live video feedback of an overhead camera. in order to cope with the inherent video feed latency, which renders even the straightforward tasks of following simple routes unsolvable, we implement an anticipatory modeling - the agent considers its current actions based on anticipated intrinsic (rather than currently available, outdated) state of the car and its surrounding. the driving style (i.e. the driving line combined with the speed at which the car travels along this line) is first evolved offline on a software simulator of the car and then adapted online to the real world. experimental results demonstrate that on long runs the agent-operated car is only marginally slower than a human-operated one, while the consistence of lap times posted by the evolved driving style of the agent is better than that of a human. this work can be viewed as a step towards the development of a framework for automated design of the controllers of remotely operated vehicles capable to find an optimal solution to various tasks in different traffic situations and road conditions.
1571	2393670	article	trends in pharmacological sciences	special issue on allosterism and collateral efficacy	\N	\N	9	28	8	2007	aug	2008-02-18 12:00:53	\N	conformational complexity of g-protein-coupled receptors	g-protein-coupled receptors (gpcrs) are remarkably versatile signaling molecules. members of this large family of membrane proteins respond to structurally diverse ligands and mediate most transmembrane signal transduction in response to hormones and neurotransmitters, and in response to the senses of sight, smell and taste. individual gpcrs can signal through several g-protein subtypes and through g-protein-independent pathways, often in a ligand-specific manner. this functional plasticity can be attributed to structural flexibility of gpcrs and the ability of ligands to induce or to stabilize ligand-specific conformations. here, we review what has been learned about the dynamic nature of the structure and mechanism of gpcr activation, primarily focusing on spectroscopic studies of purified human [beta]2 adrenergic receptor.
1572	2407908	inproceedings	\N	advances in neural information processing systems	\N	the mit press	6	8	\N	1996	\N	2008-02-21 15:35:36	\N	is learning the \\$n\\$-th thing any easier than learning the first?	this paper investigates learning in a lifelong context. lifelong learning addresses situations in which a learner faces a whole stream of learning tasks. such scenarios provide the opportunity to transfer knowledge across multiple learning tasks, in order to generalize more accurately from less training data. in this paper, several different approaches to lifelong learning are described, and applied in an object recognition domain. it is shown that across the board, lifelong learning approaches generalize consistently more accurately from less training data, by their ability to transfer knowledge across learning tasks. 1
1573	2413970	article	molecular systems biology	\N	\N	embo and nature publishing group	\N	4	\N	2008	feb	2008-02-22 11:05:41	\N	modular cell biology: retroactivity and insulation.	modularity plays a fundamental role in the prediction of the behavior of a system from the behavior of its components, guaranteeing that the properties of individual components do not change upon interconnection. just as electrical, hydraulic, and other physical systems often do not display modularity, nor do many biochemical systems, and specifically, genetic networks. here, we study the effect of interconnections on the inputâ€“output dynamic characteristics of transcriptional components, focusing on a property, which we call 'retroactivity', that plays a role analogous to non-zero output impedance in electrical systems. in transcriptional networks, retroactivity is large when the amount of transcription factor is comparable to, or smaller than, the amount of promoter-binding sites, or when the affinity of such binding sites is high. to attenuate the effect of retroactivity, we propose a feedback mechanism inspired by the design of amplifiers in electronics. we introduce, in particular, a mechanism based on a phosphorylationâ€“dephosphorylation cycle. this mechanism enjoys a remarkable insulation property, due to the fast timescales of the phosphorylation and dephosphorylation reactions.
1574	2417721	article	science	\N	\N	\N	3	288	5463	2000	apr	2008-02-23 06:41:32	\N	monolithic microfabricated valves and pumps by multilayer soft lithography	soft lithography is an alternative to silicon-based micromachining that uses replica molding of nontraditional elastomeric materials to fabricate stamps and microfluidic channels. we describe here an extension to the soft lithography paradigm, multilayer soft lithography, with which devices consisting of multiple layers may be fabricated from soft materials. we used this technique to build active microfluidic systems containing on-off valves, switching valves, and pumps entirely out of elastomer. the softness of these materials allows the device areas to be reduced by more than two orders of magnitude compared with silicon-based devices. the other advantages of soft lithography, such as rapid prototyping, ease of fabrication, and biocompatibility, are retained.
1575	2423543	article	proceedings of the national academy of sciences	\N	\N	national academy of sciences	5	105	6	2008	feb	2008-02-24 22:13:13	ministry of education key laboratory of systems biomedicine, shanghai center for systems biomedicine at shanghai jiao tong university, shanghai 200240, china.	symbiotic gut microbes modulate human metabolic phenotypes	humans have evolved intimate symbiotic relationships with a consortium of gut microbes (microbiome) and individual variations in the microbiome influence host health, may be implicated in disease etiology, and affect drug metabolism, toxicity, and efficacy. however, the molecular basis of these microbe–host interactions and the roles of individual bacterial species are obscure. we now demonstrate a ” transgenomic” approach to link gut microbiome and metabolic phenotype (metabotype) variation. we have used a combination of spectroscopic, microbiomic, and multivariate statistical tools to analyze fecal and urinary samples from seven chinese individuals (sampled twice) and to model the microbial–host metabolic connectivities. at the species level, we found structural differences in the chinese family gut microbiomes and those reported for american volunteers, which is consistent with population microbial cometabolic differences reported in epidemiological studies. we also introduce the concept of functional metagenomics, defined as  ” the characterization of key functional members of the microbiome that most influence host metabolism and hence health.” for example, faecalibacterium prausnitzii population variation is associated with modulation of eight urinary metabolites of diverse structure, indicating that this species is a highly functionally active member of the microbiome, influencing numerous host pathways. other species were identified showing different and varied metabolic interactions. our approach for understanding the dynamic basis of host–microbiome symbiosis provides a foundation for the development of functional metagenomics as a probe of systemic effects of drugs and diet that are of relevance to personal and public health care solutions.
1576	2466088	article	molecular systems biology	\N	\N	embo and nature publishing group	\N	4	\N	2008	mar	2008-03-04 17:17:46	\N	selection to minimise noise in living systems and its implications for the evolution of gene expression.	gene expression, like many biological processes, is subject to noise. this noise has been measured on a global scale, but its general importance to the fitness of an organism is unclear. here, i show that noise in gene expression in yeast has evolved to prevent harmful stochastic variation in the levels of genes that reduce fitness when their expression levels change. therefore, there has probably been widespread selection to minimise noise in gene expression. selection to minimise noise, because it results in gene expression that is stable to stochastic variation in cellular components, may also constrain the ability of gene expression to respond to non-stochastic variation. i present evidence that this has indeed been the case in yeast. i therefore conclude that gene expression noise is an important biological trait, and one that probably limits the evolvability of complex living systems.
1577	2476650	article	nature	\N	\N	nature publishing group	3	452	7183	2008	mar	2008-07-15 13:35:42	\N	strong dispersive coupling of a high-finesse cavity to a micromechanical membrane	macroscopic mechanical objects and electromagnetic degrees of freedom can couple to each other through radiation pressure. optomechanical systems in which this coupling is sufficiently strong are predicted to show quantum effects and are a topic of considerable interest. devices in this regime would offer new types of control over the quantum state of both light and matter1, 2, 3, 4, and would provide a new arena in which to explore the boundary between quantum and classical physics5, 6, 7. experiments so far have achieved sufficient optomechanical coupling to laser-cool mechanical devices8, 9, 10, 11, 12, but have not yet reached the quantum regime. the outstanding technical challenge in this field is integrating sensitive micromechanical elements (which must be small, light and flexible) into high-finesse cavities (which are typically rigid and massive) without compromising the mechanical or optical properties of either. a second, and more fundamental, challenge is to read out the mechanical element's energy eigenstate. displacement measurements (no matter how sensitive) cannot determine an oscillator's energy eigenstate13, and measurements coupling to quantities other than displacement14, 15, 16 have been difficult to realize in practice. here we present an optomechanical system that has the potential to resolve both of these challenges. we demonstrate a cavity which is detuned by the motion of a 50-nm-thick dielectric membrane placed between two macroscopic, rigid, high-finesse mirrors. this approach segregates optical and mechanical functionality to physically distinct structures and avoids compromising either. it also allows for direct measurement of the square of the membrane's displacement, and thus in principle the membrane's energy eigenstate. we estimate that it should be practical to use this scheme to observe quantum jumps of a mechanical system, an important goal in the field of quantum measurement.
1578	2479514	article	bioinformatics (oxford, england)	\N	\N	\N	6	24	9	2008	may	2008-04-29 18:55:28	\N	merging two gene-expression studies via cross-platform normalization.	gene-expression microarrays are currently being applied in a variety of biomedical applications. this article considers the problem of how to merge datasets arising from different gene-expression studies of a common organism and phenotype. of particular interest is how to merge data from different technological platforms. the article makes two contributions to the problem. the first is a simple cross-study normalization method, which is based on linked gene/sample clustering of the given datasets. the second is the introduction and description of several general validation measures that can be used to assess and compare cross-study normalization methods. the proposed normalization method is applied to three existing breast cancer datasets, and is compared to several competing normalization methods using the proposed validation measures. the supplementary materials and {xpn} matlab code are publicly available at website: https://genome.unc.edu/xpn
1579	2506264	article	annual review of ecology and systematics	\N	\N	\N	23	31	1	2000	\N	2008-03-11 07:35:15	\N	mechanisms of maintenance of species diversity	? abstract? the focus of most ideas on diversity maintenance is species coexistence, which may be stable or unstable. stable coexistence can be quantified by the long-term rates at which community members recover from low density. quantification shows that coexistence mechanisms function in two major ways: they may be (a) equalizing because they tend to minimize average fitness differences between species, or (b) stabilizing because they tend to increase negative intraspecific interactions relative to negative interspecific interactions. stabilizing mechanisms are essential for species coexistence and include traditional mechanisms such as resource partitioning and frequency-dependent predation, as well as mechanisms that depend on fluctuations in population densities and environmental factors in space and time. equalizing mechanisms contribute to stable coexistence because they reduce large average fitness inequalities which might negate the effects of stabilizing mechanisms. models of unstable coexitence, in which species diversity slowly decays over time, have focused almost exclusively on equalizing mechanisms. these models would be more robust if they also included stabilizing mechanisms, which arise in many and varied ways but need not be adequate for full stability of a system. models of unstable coexistence invite a broader view of diversity maintenance incorporating species turnover.
1580	2516366	inproceedings	\N	proceedings of the fourth international workshop on mining software repositories	msr	ieee computer society	\N	\N	\N	2007	may	2008-03-11 19:35:02	washington, dc, usa	how long will it take to fix this bug?	predicting the time and effort for a software problem has long been a difficult task. we present an approach that automatically predicts the fixing effort, i.e., the person-hours spent on fixing an issue. our technique leverages existing issue tracking systems: given a new issue report, we use the lucene framework to search for similar, earlier reports and use their average time as a prediction. our approach thus allows for early effort estimation, helping in assigning issues and scheduling stable releases. we evaluated our approach using effort data from the {jboss} project. given a sufficient number of issues reports, our automatic predictions are close to the actual effort; for issues that are bugs, we are off by only one hour, beating na\\^{a}¨\\'{y}ve predictions by a factor of four.
1581	2518492	article	proceedings of the national academy of sciences of the united states of america	proceedings of the national academy of sciences	\N	\N	5	105	11	2008	mar	2008-03-12 07:45:03	department of biomedical informatics, center of computational biology and bioinformatics, columbia university, new york, ny 10032.	network properties of genes harboring inherited disease mutations.	by analyzing, in parallel, large literature-derived and high-throughput experimental datasets we investigate genes harboring human inherited disease mutations in the context of molecular interaction networks. our results demonstrate that network properties influence the likelihood and phenotypic consequences of disease mutations. genes with intermediate connectivities have the highest probability of harboring germ-line disease mutations, suggesting that disease genes tend to occupy an intermediate niche in terms of their physiological and cellular importance. our analysis of tissue expression profiles supports this view. we show that disease mutations are less likely to occur in essential genes compared with all human genes. disease genes display significant functional clustering in the analyzed molecular network. for about one-third of known disorders with two or more associated genes we find physical clusters of genes with the same phenotype. these clusters are likely to represent disorder-specific functional modules and suggest a framework for identifying yet-undiscovered disease genes.
1582	2521939	article	inf. process. manage.	\N	\N	pergamon press, inc.	22	44	2	2008	mar	2008-03-17 11:06:35	tarrytown, ny, usa	an evaluation of adaptive filtering in the context of realistic task-based information exploration	exploratory search increasingly becomes an important research topic. our interests focus on task-based information exploration, a specific type of exploratory search performed by a range of professional users, such as intelligence analysts. in this paper, we present an evaluation framework designed specifically for assessing and comparing performance of innovative information access tools created to support the work of intelligence analysts in the context of task-based information exploration. the motivation for the development of this framework came from our needs for testing systems in taskbased information exploration, which cannot be satisfied by existing frameworks. the new framework is closely tied with the kind of tasks that intelligence analysts perform: complex, dynamic, and multiple facets and multiple stages. it views the user rather than the information system as the center of the evaluation, and examines how well users are served by the systems in their tasks. the evaluation framework examines the support of the systems at usersâ€™ major information access stages, such as information foraging and sense-making. the framework is accompanied by a reference test collection that has 18 tasks scenarios and corresponding passage-level ground truth annotations. to demonstrate the usage of the framework and the reference test collection, we present a specific evaluation study on cafeÂ´ , an adaptive filtering engine designed for supporting task-based information exploration. this study is a successful use case of the framework, and the study indeed revealed various aspects of the information systems and their roles in supporting task-based information exploration.
1583	2526029	article	ieee comput. graph. appl.	computer graphics and applications, ieee	\N	ieee computer society press	7	25	4	2005	\N	2008-03-13 12:14:32	los alamitos, ca, usa	the {large-display} user experience	as large displays become more affordable, researchers are investigating the effects on productivity, and techniques for making the large-display user experience more effective. recent work has demonstrated significant productivity benefits, but has also identified numerous usability issues that inhibit productivity. studies show that larger displays enable users to create and manage many windows, as well as to engage in complex multitasking behavior. this article describes various usability issues, including losing track of the cursor, accessing windows and icons at a distance, dealing with bezels in multimonitor displays, window management, and task management. it also presents several novel techniques that address these issues and make users more productive on large-display systems.
1584	2527434	article	journal of computational biology : a journal of computational molecular cell biology	\N	\N	\N	11	15	3	2008	apr	2008-03-13 14:58:10	department of statistics, university of glasgow, glasgow, united kingdom.	computational modeling of post-transcriptional gene regulation by {micrornas}.	micrornas (mirnas) have recently emerged as a new complex layer of gene regulation. mirnas act post-transcriptionally, influencing the stability, compartmentalization, and translation of their target mrnas. computational efforts to understand the post-transcriptional gene regulation by mirnas have been focused on the target prediction tools, while quantitative kinetic models of gene regulation by mirnas have so far largely been overlooked. we here develop a kinetic model of post-transcriptional gene regulation by mirnas, focusing on the mirnas' effect on increasing the target mrnas degradation rates. the model is fitted to a temporal microarray dataset where human mrnas are measured upon transfection with a specific mirna (mirna124a). the proposed model exhibits good fit with many target mrna profiles, indicating that such type of models can be used for studying post-transcriptional gene regulation by mirna. in particular, the proposed kinetic model can be used for quantifying the mirna-mediated effects on its targets in the mirna mis-expression experiments. the model makes an experimentally verifiable prediction of the mirna124a decay rate, quantifies the mirna-mediated effect on the target mrnas degradation, and yields a good correspondence between the inferred and experimentally measured decay rates of human target mrnas.
1585	2535886	incollection	natural language processing and information systems	\N	\N	\N	11	\N	\N	2007	\N	2008-03-15 06:16:52	\N	a lightweight approach to semantic annotation of research papers	this paper presents a novel application of a semantic annotation system, named cerno, to analyze research publications in electronic format. specifically, we address the problem of providing automatic support for authors who need to deal with large volumes of research documents. to this end, we have developed biblio, a user-friendly tool based on cerno. the tool directs the user's attention to the most important elements of the papers and provides assistance by generating automatically a list of references and an annotated bibliography given a collection of published research articles. the tool performance has been evaluated on a set of papers and preliminary evaluation results are promising. the backend of biblio uses a standard relational database to store the results.
1586	2545283	article	annual review of biochemistry	\N	\N	\N	26	77	1	2008	\N	2008-03-19 08:28:27	\N	analyzing protein interaction networks using structural information	determining protein interaction networks and predicting network changes in time and space are crucial to understanding and modeling a biological system. in the past few years, the combination of experimental and computational tools has allowed great progress toward reaching this goal. experimental methods include the large-scale determination of protein interactions using two-hybrid or pull-down analysis as well as proteomics. the latter one is especially valuable when changes in protein concentrations over time are recorded. computational tools include methods to predict and validate protein interactions on the basis of structural information and bioinformatics tools that analyze and integrate data for the same purpose. in this review, we focus on the use of structural information in combination with computational tools to predict new protein interactions, to determine which interactions are compatible with each other, to obtain some functional insight into single and multiple mutations, and to estimate equilibrium and kinetic parameters. finally, we discuss the importance of establishing criteria to biologically validate protein interactions.
1587	2547218	article	nature neuroscience	\N	\N	nature publishing group	8	11	4	2008	apr	2008-03-17 17:05:04	department of neuroscience, 116 johnson pavilion, 3610 hamilton walk, university of pennsylvania, philadelphia, pennsylvania 19104-6074, usa.	neural correlates of perceptual learning in a sensory-motor, but not a sensory, cortical area.	this study aimed to identify neural mechanisms that underlie perceptual learning in a visual-discrimination task. we trained two monkeys (macaca mulatta) to determine the direction of visual motion while we recorded from their middle temporal area ({mt}), which in trained monkeys represents motion information that is used to solve the task, and lateral intraparietal area ({lip}), which represents the transformation of motion information into a saccadic choice. during training, improved behavioral sensitivity to weak motion signals was accompanied by changes in motion-driven responses of neurons in {lip}, but not in {mt}. the time course and magnitude of the changes in {lip} correlated with the changes in behavioral sensitivity throughout training. thus, for this task, perceptual learning does not appear to involve improvements in how sensory information is represented in the brain, but rather how the sensory representation is interpreted to form the decision that guides behavior.
1588	2547927	article	genome research	\N	\N	cold spring harbor laboratory press	7	18	5	2008	may	2008-03-17 23:46:07	geneva university hospitals;	de novo bacterial genome sequencing: millions of very short reads assembled on a desktop computer	an international, peer-reviewed genome sciences journal featuring outstanding original research that offers novel insights into the biology of all organisms
1589	2568883	article	bioinformatics (oxford, england)	\N	\N	\N	6	24	9	2008	may	2008-03-21 05:03:23	department of statistics, university of \\"{o}rebro, sweden; department of medical epidemiology and biostatistics, karolinska institutet, stockholm, sweden; department of biomedical sciences and biotechnology, university of brescia, italy.	unequal group variances in microarray data analyses.	motivation: in searching for differentially expressed (de) genes in microarray data, we often observe a fraction of the genes to have unequal variability between groups. this is not an issue in large samples, where a valid test exists that uses individual variances separately. the problem arises in the small-sample setting, where the approximately valid welch test lacks sensitivity, while the more sensitive moderated t-test assumes equal variance.  methods: we introduce a moderated welch test (mwt) that allows unequal variance between groups. it is based on (i) weighting of pooled and unpooled standard errors and (ii) improved estimation of the gene-level variance that exploits the information from across the genes.  results: when a non-trivial proportion of genes has unequal variability, false discovery rate (fdr) estimates based on the standard t and moderated t-tests are often too optimistic, while the standard welch test has low sensitivity. the mwt is shown to (i) perform better than the standard t, the standard welch and the moderated t-tests when the variances are unequal between groups and (ii) perform similarly to the moderated t, and better than the standard t and welch tests when the group variances are equal. these results mean that mwt is more reliable than other existing tests over wider range of data conditions.  availability: r package to perform mwt is available at http://www.meb.ki.se/~yudpaw  contact: yudi.pawitan@ki.se  supplementary information: supplementary data are available at bioinformatics online. 10.1093/bioinformatics/btn100
1590	2580334	article	archival science	\N	\N	\N	14	2	3	2002	\N	2008-03-24 11:59:35	\N	archives, records, and power: from (postmodern) theory to (archival) performance	this article is the continuationand conclusion of our introduction, as theguest editors, that appeared in the first ofthese two special issues of {archivalscience}, which together are devoted to thetheme,  ” archives, records, and power.” it argues that, inperforming their work, archivists follow ascript that has been naturalized by the routinerepetition of past practice. they act in waysthat they anticipate their various audienceswould desire. if archival practice is to beinfluenced by the postmodern ideas of theauthors of the essays in these two volumes,then archivists must see that the script,stage, and audiences have changed. theory andpractice are not opposites, not evenpolarities, but integrated aspects of thearchivist's professional role andresponsibility. transparency of process aboutthe archivist's performance will facilitatethis integration, stimulate the building ofarchival knowledge, and enable present andfuture generations to hold the professionaccountable for its choices in exercising powerover the making of modern memory.
1591	2584166	book	\N	\N	\N	morgan kaufmann	\N	\N	\N	2008	may	2008-03-25 10:14:33	\N	semantic web for the working ontologist: effective modeling in {rdfs} and {owl}	{the promise of the semantic web to provide a universal medium to exchange data information and knowledge has been well publicized.  there are many sources too for basic information on the extensions to the www that permit content to be expressed in natural language yet used by software agents to easily find, share and integrate information. until now individuals engaged in creating ontologies-- formal descriptions of the concepts, terms, and relationships within a given knowledge domain-- have had no sources beyond the technical standards documents.    <br><br>semantic web for the working ontologist transforms this information into the practical knowledge that programmers and subject domain experts need.  authors allemang and hendler begin with solutions to the basic problems, but dont stop there:  they demonstrate how to develop your own solutions to problems of increasing complexity and ensure that your skills will keep pace with the continued evolution of the semantic web.<br><br> provides practical information for all programmers and subject matter experts engaged in modeling data to fit the requirements of the semantic web.<br> de-emphasizes algorithms and proofs, focusing instead on real-world problems, creative solutions, and highly illustrative examples. <br> presents detailed, ready-to-apply recipes for use in many specific situations.<br> shows how to create new recipes from rdf, rdfs, and owl constructs.}
1592	2587504	article	bmc bioinformatics	\N	\N	\N	\N	9 Suppl 1	Suppl 1	2008	\N	2008-03-25 19:23:35	\N	{gorouter}: an {rdf} model for providing semantic query and inference services for gene ontology and its associations.	{background}: the most renowned biological ontology, gene ontology ({go}) is widely used for annotations of genes and gene products of different organisms. however, there are shortcomings in the resource description framework ({rdf}) data file provided by the {go} consortium: 1) lack of sufficient semantic relationships between pairs of terms coming from the three independent {go} sub-ontologies, that limit the power to provide complex semantic queries and inference services based on it. 2) the term-centric view of {go} annotation data and the fact that all information is stored in a single file. this makes attempts to retrieve {go} annotations based on big volume datasets unmanageable. 3) no support of {goslim}. {results}: we propose a {rdf} model, {gorouter}, which encodes heterogeneous original data in a uniform {rdf} format, creates additional ontology mappings between {go} terms, and introduces a set of inference rulebases. furthermore, we use the oracle network data model ({ndm}) as the native {rdf} data repository and the table function {rdf\\_match} to seamlessly combine the result of {rdf} queries with traditional relational data. as a result, the scale of {gorouter} is minimized; information not directly involved in semantic inference is put into relational tables. {conclusion}: our work demonstrates how to use multiple semantic web tools and techniques to provide a mixture of semantic query and inference solutions of {go} and its associations. {gorouter} is licensed under apache license version 2.0, and is accessible via the website: http://www.scbit.org/gorouter/.
1593	2601345	article	science	\N	\N	american association for the advancement of science	3	319	5871	2008	mar	2008-03-27 09:45:41	molecular genetics and evolution, arc centre for the molecular genetics of development, research school of biological sciences, the australian national university, canberra act 0200, australia.	nutritional control of reproductive status in honeybees via {dna} methylation	fertile queens and sterile workers are alternative forms of the adult female honeybee that develop from genetically identical larvae following differential feeding with royal jelly. we show that silencing the expression of {dna} methyltransferase dnmt3, a key driver of epigenetic global reprogramming, in newly hatched larvae led to a royal jelly–like effect on the larval developmental trajectory; the majority of dnmt3 small interfering {rna}–treated individuals emerged as queens with fully developed ovaries. our results suggest that {dna} methylation in apis is used for storing epigenetic information, that the use of that information can be differentially altered by nutritional input, and that the flexibility of epigenetic modifications underpins, profound shifts in developmental fates, with massive implications for reproductive and behavioral status.
1594	2602869	book	\N	\N	\N	harpercollins	\N	\N	\N	2008	feb	2008-04-28 18:52:34	\N	predictably irrational: the hidden forces that shape our decisions	{<p> <ul> <li>why do our headaches persist after taking a one-cent aspirin but disappear when we take a 50-cent aspirin? </li> </p> <p> <li>why does recalling the ten commandments reduce our tendency to lie, even when we couldn't possibly be caught? </li> </p> <p> <li>why do we splurge on a lavish meal but cut coupons to save twenty-five cents on a can of soup?</li> </p> <p> <li>why do we go back for second helpings at the unlimited buffet, even when our stomachs are already full?</li> </p> <p> <li>and how did we ever start spending \\$4.15 on a cup of coffee when, just a few years ago, we used to pay less than a dollar?</li> </ul> </p> <p> when it comes to making decisions in our lives, we think we're in control. we think we're making smart, rational choices. but are we? </p> <p> in a series of illuminating, often surprising experiments, mit behavioral economist dan ariely refutes the common assumption that we behave in fundamentally rational ways. blending everyday experience with groundbreaking research, ariely explains how expectations, emotions, social norms, and other invisible, seemingly illogical forces skew our reasoning abilities. </p> <p> not only do we make astonishingly simple mistakes every day, but we make the same <i>types</i> of mistakes, ariely discovers. we consistently overpay, underestimate, and procrastinate. we fail to understand the profound effects of our emotions on what we want, and we overvalue what we already own. yet these misguided behaviors are neither random nor senseless. they're systematic and predictable—making us <i>predictably</i> irrational. </p> <p> from drinking coffee to losing weight, from buying a car to choosing a romantic partner, ariely explains how to break through these systematic patterns of thought to make better decisions. <i>predictably irrational</i> will change the way we interact with the world—one small decision at a time. </p>}
1595	2607936	article	nat meth	\N	\N	nature publishing group	6	5	4	2008	apr	2008-03-31 17:02:05	\N	systematic identification of mammalian regulatory motifs' target genes and functions	we developed an algorithm, lever, that systematically maps metazoan {dna} regulatory motifs or motif combinations to sets of genes. lever assesses whether the motifs are enriched in cis-regulatory modules ({crms}), predicted by our {phylcrm} algorithm, in the noncoding sequences surrounding the genes. lever analysis allows unbiased inference of functional annotations to regulatory motifs and candidate {crms}. we used human myogenic differentiation as a model system to statistically assess greater than 25,000 pairings of gene sets and motifs or motif combinations. we assigned functional annotations to candidate regulatory motifs predicted previously and identified gene sets that are likely to be co-regulated via shared regulatory motifs. lever allows moving beyond the identification of putative regulatory motifs in mammalian genomes, toward understanding their biological roles. this approach is general and can be applied readily to any cell type, gene expression pattern or organism of interest.
1596	2616003	article	j. am. soc. inf. sci. technol.	\N	\N	john wiley \\& sons, inc.	18	58	13	2007	nov	2008-03-31 13:11:53	new york, ny, usa	relevance: a review of the literature and a framework for thinking on the notion in information science. part {ii}: nature and manifestations of relevance	all is flux. —plato on knowledge in the theaetetus (about 369 {bc}) relevance is a, if not even the, key notion in information science in general and information retrieval in particular. this two-part critical review traces and synthesizes the scholarship on relevance over the past 30 years or so and provides an updated framework within which the still widely dissonant ideas and works about relevance might be interpreted and related. it is a continuation and update of a similar review that appeared in 1975 under the same title, considered here as being part i. the present review is organized in two parts: part {ii} addresses the questions related to nature and manifestations of relevance, and part {iii} addresses questions related to relevance behavior and effects. in part {ii}, the nature of relevance is discussed in terms of meaning ascribed to relevance, theories used or proposed, and models that have been developed. the manifestations of relevance are classified as to several kinds of relevance that form an interdependent system of relevancies. in part {iii}, relevance behavior and effects are synthesized using experimental and observational works that incorporated data. in both parts, each section concludes with a summary that in effect provides an interpretation and synthesis of contemporary thinking on the topic treated or suggests hypotheses for future research. analyses of some of the major trends that shape relevance work are offered in conclusions. {\\copyright} 2007 wiley periodicals, inc.
1597	2620752	article	genome research	\N	\N	cold spring harbor laboratory press	8	18	4	2008	apr	2008-04-01 19:32:16	\N	protein networks in disease.	during a decade of proof-of-principle analysis in model organisms, protein networks have been used to further the study of molecular evolution, to gain insight into the robustness of cells to perturbation, and for assignment of new protein functions. following these analyses, and with the recent rise of protein interaction measurements in mammals, protein networks are increasingly serving as tools to unravel the molecular basis of disease. we review promising applications of protein networks to disease in four major areas: identifying new disease genes; the study of their network properties; identifying disease-related subnetworks; and network-based disease classification. applications in infectious disease, personalized medicine, and pharmacology are also forthcoming as the available protein network information improves in quality and coverage.
1598	2623645	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2008-04-02 15:57:23	\N	robust face recognition via sparse representation	we consider the problem of automatically recognizing human faces from frontal views with varying expression and illumination, as well as occlusion and disguise. we cast the recognition problem as one of classifying among multiple linear regression models and argue that new theory from sparse signal representation offers the key to addressing this problem. based on a sparse representation computed by \\ell^{1}-minimization, we propose a general classification algorithm for (image-based) object recognition. this new framework provides new insights into two crucial issues in face recognition: feature extraction and robustness to occlusion. for feature extraction, we show that if sparsity in the recognition problem is properly harnessed, the choice of features is no longer critical. what is critical, however, is whether the number of features is sufficiently large and whether the sparse representation is correctly computed. unconventional features such as downsampled images and random projections perform just as well as conventional features such as eigenfaces and laplacianfaces, as long as the dimension of the feature space surpasses certain threshold, predicted by the theory of sparse representation. this framework can handle errors due to occlusion and corruption uniformly by exploiting the fact that these errors are often sparse with respect to the standard (pixel) basis. the theory of sparse representation helps predict how much occlusion the recognition algorithm can handle and how to choose the training images to maximize robustness to occlusion. we conduct extensive experiments on publicly available databases to verify the efficacy of the proposed algorithm and corroborate the above claims.
1599	2630042	article	science	\N	\N	american association for the advancement of science	3	320	5872	2008	apr	2008-04-04 16:36:25	\N	{single-molecule} {dna} sequencing of a viral genome	the full promise of human genomics will be realized only when the genomes of thousands of individuals can be sequenced for comparative analysis. a reference sequence enables the use of short read length. we report an amplification-free method for determining the nucleotide sequence of more than 280,000 individual {dna} molecules simultaneously. a {dna} polymerase adds labeled nucleotides to surface-immobilized primer-template duplexes in stepwise fashion, and the asynchronous growth of individual {dna} molecules was monitored by fluorescence imaging. read lengths of >25 bases and equivalent phred software program quality scores approaching 30 were achieved. we used this method to sequence the m13 virus to an average depth of >150× and with 100\\% coverage; thus, we resequenced the m13 genome with high-sensitivity mutation detection. this demonstrates a strategy for high-throughput low-cost resequencing.
1600	2647614	article	science (new york, n.y.)	\N	\N	american association for the advancement of science	3	320	5872	2008	apr	2008-04-10 02:02:10	cognitive neuroscience and schizophrenia program, nathan kline institute, orangeburg, ny 10962, usa.	entrainment of neuronal oscillations as a mechanism of attentional selection.	whereas gamma-band neuronal oscillations clearly appear integral to visual attention, the role of lower-frequency oscillations is still being debated. mounting evidence indicates that a key functional property of these oscillations is the rhythmic shifting of excitability in local neuronal ensembles. here, we show that when attended stimuli are in a rhythmic stream, delta-band oscillations in the primary visual cortex entrain to the rhythm of the stream, resulting in increased response gain for task-relevant events and decreased reaction times. because of hierarchical cross-frequency coupling, delta phase also determines momentary power in higher-frequency activity. these instrumental functions of low-frequency oscillations support a conceptual framework that integrates numerous earlier findings.
1601	2647679	inproceedings	\N	proceedings of the second international conference on weblogs and social media	\N	\N	\N	\N	\N	2008	\N	2008-04-10 02:46:54	\N	understanding the efficiency of social tagging systems using information theory	given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? does this distributed process really provide a way to circumnavigate the traditional \\vocabulary problem" with ontology? we analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. we show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. our results indicate the eciency of tags appears to be wan- ing. we discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software.understanding the efficiency of social tagging systems using information theory ed h. chi palo alto research center 3333 coyote hill road, palo alto, ca echi@parc.com todd mytkowicz dept. of computer science university of colorado, boulder, co todd.mytkowicz@colorado.edu abstract given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? does this distributed process really provide a way to circumnavigate the traditional \\vocabulary problem" with ontology? we analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. we show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. our results indicate the eciency of tags appears to be wan- ing. we discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software.understanding the efficiency of social tagging systems using information theory ed h. chi palo alto research center 3333 coyote hill road, palo alto, ca echi@parc.com todd mytkowicz dept. of computer science university of colorado, boulder, co todd.mytkowicz@colorado.edu abstract given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? does this distributed process really provide a way to circumnavigate the traditional \\vocabulary problem" with ontology? we analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. we show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. our results indicate the eciency of tags appears to be wan- ing. we discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software.understanding the efficiency of social tagging systems using information theory ed h. chi palo alto research center 3333 coyote hill road, palo alto, ca echi@parc.com todd mytkowicz dept. of computer science university of colorado, boulder, co todd.mytkowicz@colorado.edu abstract given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? does this distributed process really provide a way to circumnavigate the traditional \\vocabulary problem" with ontology? we analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. we show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. our results indicate the eciency of tags appears to be wan- ing. we discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software.understanding the efficiency of social tagging systems using information theory ed h. chi palo alto research center 3333 coyote hill road, palo alto, ca echi@parc.com todd mytkowicz dept. of computer science university of colorado, boulder, co todd.mytkowicz@colorado.edu abstract given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? does this distributed process really provide a way to circumnavigate the traditional \\vocabulary problem" with ontology? we analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. we show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. our results indicate the eciency of tags appears to be wan- ing. we discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software.understanding the efficiency of social tagging systems using information theory ed h. chi palo alto research center 3333 coyote hill road, palo alto, ca echi@parc.com todd mytkowicz dept. of computer science university of colorado, boulder, co todd.mytkowicz@colorado.edu abstract given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? does this distributed process really provide a way to circumnavigate the traditional \\vocabulary problem" with ontology? we analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. we show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. our results indicate the eciency of tags appears to be wan- ing. we discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software.understanding the efficiency of social tagging systems using information theory ed h. chi palo alto research center 3333 coyote hill road, palo alto, ca echi@parc.com todd mytkowicz dept. of computer science university of colorado, boulder, co todd.mytkowicz@colorado.edu abstract given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? does this distributed process really provide a way to circumnavigate the traditional \\vocabulary problem" with ontology? we analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. we show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. our results indicate the eciency of tags appears to be wan- ing. we discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software.
1602	2649220	article	structure	\N	\N	\N	9	16	4	2008	apr	2008-04-10 14:14:47	\N	{coarse-grained} {md} simulations of membrane {protein-bilayer} {self-assembly}	complete determination of a membrane protein structure requires knowledge of the protein position within the lipid bilayer. as the number of determined structures of membrane proteins increases so does the need for computational methods which predict their position in the lipid bilayer. here we present a coarse-grained molecular dynamics approach to lipid bilayer self-assembly around membrane proteins. we demonstrate that this method can be used to predict accurately the protein position in the bilayer for membrane proteins with a range of different sizes and architectures.
1603	2669725	article	science	\N	\N	american association for the advancement of science	2	305	5682	2004	jul	2008-04-14 17:33:46	\N	calorie restriction promotes mammalian cell survival by inducing the {sirt1} deacetylase	a major cause of aging is thought to result from the cumulative effects of cell loss over time. in yeast, caloric restriction ({cr}) delays aging by activating the sir2 deacetylase. here we show that expression of mammalian sir2 ({sirt1}) is induced in {cr} rats as well as in human cells that are treated with serum from these animals. insulin and insulin-like growth factor 1 ({igf}-1) attenuated this response. {sirt1} deacetylates the {dna} repair factor ku70, causing it to sequester the proapoptotic factor bax away from mitochondria, thereby inhibiting stress-induced apoptotic cell death. thus, {cr} could extend life-span by inducing {sirt1} expression and promoting the long-term survival of irreplaceable cells.
1604	2675151	article	molecular systems biology	\N	\N	embo and nature publishing group	\N	4	\N	2008	apr	2008-04-15 20:19:27	\N	a map of human protein interactions derived from co-expression of human {mrnas} and their orthologs.	the human protein interaction network will offer global insights into the molecular organization of cells and provide a framework for modeling human disease, but the network's large scale demands new approaches. we report a set of 7000 physical associations among human proteins inferred from indirect evidence: the comparison of human mrna co-expression patterns with those of orthologous genes in five other eukaryotes, which we demonstrate identifies proteins in the same physical complexes. to evaluate the accuracy of the predicted physical associations, we apply quantitative mass spectrometry shotgun proteomics to measure elution profiles of 3013 human proteins during native biochemical fractionation, demonstrating systematically that putative interaction partners tend to co-sediment. we further validate uncharacterized proteins implicated by the associations in ribosome biogenesis, including wbscr20c, associated with williams-beuren syndrome. this meta-analysis therefore exploits non-protein-based data, but successfully predicts associations, including 5589 novel human physical protein associations, with measured accuracies of 54+/-10%, comparable to direct large-scale interaction assays. the new associations' derivation from conserved in vivo phenomena argues strongly for their biological relevance.
1605	2686437	inproceedings	\N	proceedings of the twenty-sixth annual sigchi conference on human factors in computing systems	chi	acm	9	\N	\N	2008	\N	2008-04-18 06:23:11	new york, ny, usa	reality-based interaction: a framework for {post-wimp} interfaces	we are in the midst of an explosion of emerging human-computer interaction techniques that redefine our understanding of both computers and interaction. we propose the notion of {reality-based} interaction ({rbi}) as a unifying concept that ties together a large subset of these emerging interaction styles. based on this concept of {rbi}, we provide a framework that can be used to understand, compare, and relate current paths of recent {hci} research as well as to analyze specific interaction designs. we believe that viewing interaction through the lens of {rbi} provides insights for design and uncovers gaps or opportunities for future research.
1606	2686454	inproceedings	\N	proceedings of the twenty-sixth annual sigchi conference on human factors in computing systems	chi	acm	3	\N	\N	2008	\N	2008-04-18 06:31:20	new york, ny, usa	crowdsourcing user studies with mechanical turk	user studies are important for many aspects of the design process and involve techniques ranging from informal surveys to rigorous laboratory studies. however, the costs involved in engaging users often requires practitioners to trade off between sample size, time requirements, and monetary costs. micro-task markets, such as amazon's mechanical turk, offer a potential paradigm for engaging a large number of users for low time and monetary costs. here we investigate the utility of a micro-task market for collecting user measurements, and discuss design considerations for developing remote micro user evaluation tasks. although micro-task markets have great potential for rapidly collecting user measurements at low costs, we found that special care is needed in formulating tasks in order to harness the capabilities of the approach.
1607	2724724	inbook	\N	annual review of biomedical engineering	\N	\N	23	2	\N	2000	\N	2008-04-27 22:03:48	\N	current methods in medical image segmentation	image segmentation plays a crucial role in many medical imaging applications by automating or facilitating the delineation of anatomical structures and other regions of interest. we present herein a critical appraisal of the current status of semi-automated and automated methods for the segmentation of anatomical medical images. current segmentation approaches are reviewed with an emphasis placed on revealing the advantages and disadvantages of these methods for medical imaging applications. the use of image segmentation in different imaging modalities is also described along with the difficulties encountered in each modality. we conclude with a discussion on the future of image segmentation methods in biomedical research.
1608	2729940	inproceedings	\N	www2008	\N	\N	\N	\N	\N	2008	\N	2008-04-28 13:58:57	\N	{pagerank} for product image search	in this paper, we cast the image-ranking problem into the task of identifying â€œauthorityâ€ nodes on an inferred visual similarity graph and propose an algorithm to analyze the visual link structure that can be created among a group of images. through an iterative procedure based on the pagerank computation, a numerical weight is assigned to each image; this measures its relative importance to the other images being considered. the incorporation of visual signals in this process differs from the majority of large- scale commercial-search engines in use today. commercial search-engines often solely rely on the text clues of the pages in which images are embedded to rank images, and often en- tirely ignore the content of the images themselves as a rank- ing signal. to quantify the performance of our approach in a real-world system, we conducted a series of experiments based on the task of retrieving images for 2000 of the most popular products queries. our experimental results show significant improvement, in terms of user satisfaction and relevancy, in comparison to the most recent google image search results.
1609	2730940	article	nature genetics	\N	\N	nature publishing group	3	40	5	2008	apr	2008-04-28 21:07:19	\N	interpreting principal component analyses of spatial population genetic variation	nearly 30 years ago, {cavalli-sforza} et al. pioneered the use of principal component analysis ({pca}) in population genetics and used {pca} to produce maps summarizing human genetic variation across continental regions1. they interpreted gradient and wave patterns in these maps as signatures of specific migration events1, 2, 3. these interpretations have been controversial4, 5, 6, 7, but influential8, and the use of {pca} has become widespread in analysis of population genetics data9, 10, 11, 12, 13. however, the behavior of {pca} for genetic data showing continuous spatial variation, such as might exist within human continental groups, has been less well characterized. here, we find that gradients and waves observed in {cavalli-sforza} et al.'s maps resemble sinusoidal mathematical artifacts that arise generally when {pca} is applied to spatial data, implying that the patterns do not necessarily reflect specific migration events. our findings aid interpretation of {pca} results and suggest how {pca} can help correct for continuous population structure in association studies.
1610	2733029	inproceedings	\N	proceeding of the twenty-sixth annual sigchi conference on human factors in computing systems	chi	acm	3	\N	\N	2008	\N	2008-04-29 10:12:44	new york, ny, usa	lifting the veil: improving accountability and social transparency in wikipedia with wikidashboard	wikis are collaborative systems in which virtually anyone can edit anything. although wikis have become highly popular in many domains, their mutable nature often leads them to be distrusted as a reliable source of information. here we describe a social dynamic analysis tool called {wikidashboard} which aims to improve social transparency and accountability on wikipedia articles. early reactions from users suggest that the increased transparency afforded by the tool can improve the interpretation, communication, and trustworthiness of wikipedia articles.
1611	2733191	article	bmc bioinformatics	\N	\N	\N	\N	9	1	2008	apr	2008-06-03 13:14:15	\N	binning sequences using very sparse labels within a metagenome.	in the task of binning using semi-supervised learning methods, results indicate {s-gsom} to be the best of the methods tested. most importantly, the proposed method does not require knowledge from known genomes and uses only very few labels (one per species is sufficient in most cases), which are extracted from the metagenome itself. these advantages make it a very attractive binning method. {s-gsom} outperformed the binning methods that depend on already-sequenced genomes, and compares well to the current most advanced binning method, {phylopythia}.
1612	2739782	book	\N	\N	\N	{mcgraw hill higher education}	\N	\N	\N	2003	dec	2008-04-30 20:21:17	\N	strategic management of technology and innovation	{burgelman, maidique, and wheelwright have written the market leading text for a course in technology and innovation. this text covers the latest research by using a combination of text, readings, and cases.  based on reviewer response to a survey, the authors have updated many of the cases and instructors found outdated or lacking.  as in the current edition, the book has a strong case foundation at harvard and stanford.  classic cases such as claire mccloud have been kept, while newer cases such as intel corporation in 1999 have been added.   there is also a strong set of readings from sources such as harvard business review, california management review, and sloan management review.}
1613	2739855	article	nature	\N	\N	nature publishing group	3	453	7191	2008	may	2008-05-07 01:58:25	\N	the missing memristor found	anyone who ever took an electronics laboratory class will be familiar with the fundamental passive circuit elements: the resistor, the capacitor and the inductor. however, in 1971 leon chua reasoned from symmetry arguments that there should be a fourth fundamental element, which he called a memristor (short for memory resistor)1. although he showed that such an element has many interesting and valuable circuit properties, until now no one has presented either a useful physical model or an example of a memristor. here we show, using a simple analytical example, that memristance arises naturally in nanoscale systems in which solid-state electronic and ionic transport are coupled under an external bias voltage. these results serve as the foundation for understanding a wide range of hysteretic current–voltage behaviour observed in many nanoscale electronic devices2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19 that involve the motion of charged atomic or molecular species, in particular certain titanium dioxide cross-point switches20, 21, 22.
1614	2752882	inproceedings	\N	proceedings of the sigchi conference on human factors in computing systems	chi	acm	9	\N	\N	2008	\N	2008-05-04 06:23:42	new york, ny, usa	usability evaluation considered harmful (some of the time)	current practice in human computer interaction as encouraged by educational institutes, academic review processes, and institutions with usability groups advocate usability evaluation as a critical part of every design process. this is for good reason: usability evaluation has a significant role to play when conditions warrant it. yet evaluation can be ineffective and even harmful if naively done 'by rule' rather than 'by thought'. if done during early stage design, it can mute creative ideas that do not conform to current interface norms. if done to test radical innovations, the many interface issues that would likely arise from an immature technology can quash what could have been an inspired vision. if done to validate an academic prototype, it may incorrectly suggest a design's scientific worthiness rather than offer a meaningful critique of how it would be adopted and used in everyday practice. if done without regard to how cultures adopt technology over time, then today's reluctant reactions by users will forestall tomorrow's eager acceptance. the choice of evaluation methodology - if any - must arise from and be appropriate for the actual problem or research question under consideration.
1615	2760213	article	plos biology	\N	\N	\N	\N	6	5	2008	may	2008-05-06 09:52:44	\N	mapping the genetic architecture of gene expression in human liver.	genetic variants that are associated with common human diseases do not lead directly to disease, but instead act on intermediate, molecular phenotypes that in turn induce changes in higher-order disease traits. therefore, identifying the molecular phenotypes that vary in response to changes in dna and that also associate with changes in disease traits has the potential to provide the functional information required to not only identify and validate the susceptibility genes that are directly affected by changes in dna, but also to understand the molecular networks in which such genes operate and how changes in these networks lead to changes in disease traits. toward that end, we profiled more than 39,000 transcripts and we genotyped 782,476 unique single nucleotide polymorphisms (snps) in more than 400 human liver samples to characterize the genetic architecture of gene expression in the human liver, a metabolically active tissue that is important in a number of common human diseases, including obesity, diabetes, and atherosclerosis. this genome-wide association study of gene expression resulted in the detection of more than 6,000 associations between snp genotypes and liver gene expression traits, where many of the corresponding genes identified have already been implicated in a number of human diseases. the utility of these data for elucidating the causes of common human diseases is demonstrated by integrating them with genotypic and expression data from other human and mouse populations. this provides much-needed functional support for the candidate susceptibility genes being identified at a growing number of genetic loci that have been identified as key drivers of disease from genome-wide association studies of disease. by using an integrative genomics approach, we highlight how the gene rps26 and not erbb3 is supported by our data as the most likely susceptibility gene for a novel type 1 diabetes locus recently identified in a large-scale, genome-wide association study. we also identify sort1 and celsr2 as candidate susceptibility genes for a locus recently associated with coronary artery disease and plasma low-density lipoprotein cholesterol levels in the process.
1616	2762934	article	bmc genomics	\N	\N	\N	\N	9 Suppl 1	Suppl 1	2008	\N	2008-05-06 22:14:32	\N	bayesian biclustering of gene expression data.	{background}: biclustering of gene expression data searches for local patterns of gene expression. a bicluster (or a two-way cluster) is defined as a set of genes whose expression profiles are mutually similar within a subset of experimental conditions/samples. although several biclustering algorithms have been studied, few are based on rigorous statistical models. {results}: we developed a bayesian biclustering model ({bbc}), and implemented a gibbs sampling procedure for its statistical inference. we showed that bayesian biclustering model can correctly identify multiple clusters of gene expression data. using simulated data both from the model and with realistic characters, we demonstrated the {bbc} algorithm outperforms other methods in both robustness and accuracy. we also showed that the model is stable for two normalization methods, the interquartile range normalization and the smallest quartile range normalization. applying the {bbc} algorithm to the yeast expression data, we observed that majority of the biclusters we found are supported by significant biological evidences, such as enrichments of gene functions and transcription factor binding sites in the corresponding promoter sequences. {conclusions}: the {bbc} algorithm is shown to be a robust model-based biclustering method that can discover biologically significant gene-condition clusters in microarray data. the {bbc} model can easily handle missing data via monte carlo imputation and has the potential to be extended to integrated study of gene transcription networks.
1617	2773606	article	science	science	\N	\N	4	320	5881	2008	jun	2008-05-09 11:31:42	\N	predictive behavior within microbial genetic networks	the homeostatic framework has dominated our understanding of cellular physiology. we question whether homeostasis alone adequately explains microbial responses to environmental stimuli, and explore the capacity of intracellular networks for predictive behavior in a fashion similar to metazoan nervous systems. we show that in silico biochemical networks, evolving randomly under precisely defined complex habitats, capture the dynamical, multidimensional structure of diverse environments by forming internal representations that allow prediction of environmental change. we provide evidence for such anticipatory behavior by revealing striking correlations of escherichia coli transcriptional responses to temperature and oxygen perturbations--precisely mirroring the covariation of these parameters upon transitions between the outside world and the mammalian gastrointestinal tract. we further show that these internal correlations reflect a true associative learning paradigm, because they show rapid decoupling upon exposure to novel environments.
1618	2775897	article	bmc bioinformatics	\N	\N	\N	\N	9 Suppl 3	Suppl 3	2008	\N	2008-05-09 14:45:05	european bioinformatics institute, wellcome trust genome campus, hinxton, cambridge, cb10 1sd, uk. yepes@ebi.ac.uk	assessment of disease named entity recognition on a corpus of annotated sentences.	{background}: in recent years, the recognition of semantic types from the biomedical scientific literature has been focused on named entities like protein and gene names ({pgns}) and gene ontology terms ({go} terms). other semantic types like diseases have not received the same level of attention. different solutions have been proposed to identify disease named entities in the scientific literature. while matching the terminology with language patterns suffers from low recall (e.g., whatizit) other solutions make use of morpho-syntactic features to better cover the full scope of terminological variability (e.g., {metamap}). currently, {metamap} that is provided from the national library of medicine ({nlm}) is the state of the art solution for the annotation of concepts from {umls} (unified medical language system) in the literature. nonetheless, its performance has not yet been assessed on an annotated corpus. in addition, little effort has been invested so far to generate an annotated dataset that links disease entities in text to disease entries in a database, thesaurus or ontology and that could serve as a gold standard to benchmark text mining solutions. {results}: as part of our research work, we have taken a corpus that has been delivered in the past for the identification of associations of genes to diseases based on the {umls} metathesaurus and we have reprocessed and re-annotated the corpus. we have gathered annotations for disease entities from two curators, analyzed their disagreement (0.51 in the kappa-statistic) and composed a single annotated corpus for public use. thereafter, three solutions for disease named entity recognition including {metamap} have been applied to the corpus to automatically annotate it with {umls} metathesaurus concepts. the resulting annotations have been benchmarked to compare their performance. {conclusions}: the annotated corpus is publicly available at ftp://ftp.ebi.ac.uk/pub/software/textmining/corpora/diseases and can serve as a benchmark to other systems. in addition, we found that dictionary look-up already provides competitive results indicating that the use of disease terminology is highly standardized throughout the terminologies and the literature. {metamap} generates precise results at the expense of insufficient recall while our statistical method obtains better recall at a lower precision rate. even better results in terms of precision are achieved by combining at least two of the three methods leading, but this approach again lowers recall. altogether, our analysis gives a better understanding of the complexity of disease annotations in the literature. {metamap} and the dictionary based approach are available through the whatizit web service infrastructure ({rebholz-schuhmann} d, arregui m, gaudan s, kirsch h, jimeno a: text processing through web services: calling whatizit. bioinformatics 2008, 24:296-298).
1619	2793908	article	medical teacher	medical teacher	\N	informa allied health	5	30	2	2008	jan	2008-05-13 09:19:15	university of pittsburgh school of medicine, usa.	what medical educators need to know about "web 2.0".	â€˜â€˜web 2.0â€™â€™ describes a collection of web-based technologies which share a user-focused approach to design and functionality, where users actively participate in content creation and editing through open collaboration between members of communities of practice. the current generation of students in medical school made web 2.0 websites such as facebook and myspace some of the most popular on the internet. medical educators and designers of educational software applications can benefit from understanding and applying web 2.0 concepts to the curriculum and related websites. health science schools have begun experimenting with wikis, blogs and other web 2.0 applications and have identified both advantages and potential problems with these relatively open, student-focused communication tools. this paper reviews the unique features of web 2.0 technologies, addresses questions regarding potential pitfalls and suggests valuable applications in health science education.
1620	2796032	article	bioinformatics	\N	\N	\N	\N	\N	\N	2008	\N	2008-05-13 20:51:05	\N	{patman: rapid alignment of short sequences to large databases}	summary: we present a tool suited for searching for many short nucleotide sequences in large databases, allowing for a pre-defined number of gaps and mismatches. the commandline-driven program implements a nondeterministic automata matching-algorithm on a keyword tree of the search strings. both queries with and without ambiguity codes can be searched. search time is short for perfect matches, and retrieval time rises exponentially with the number of edits allowed. availability: the c++ source code for {patman} is distributed under the {gnu} general public license and has been tested on the {gnu}/linux operating system. it is available from http://bioinf.eva.mpg.de/patman. contact: pruefer@eva.mpg.de
1621	2800564	article	bioinformatics (oxford, england)	bioinformatics	\N	\N	5	24	13	2008	jul	2008-05-15 03:58:46	sissa-isas, international school for advanced studies, via beirut 2-4, 34014 trieste, italy.	discerning static and causal interactions in genome-wide reverse engineering problems.	{background}: in the past years devising methods for discovering gene regulatory mechanisms at a genome-wide level has become a fundamental topic in the field of systems biology. the aim is to infer gene-gene interactions in an increasingly sophisticated and reliable way through the continuous improvement of reverse engineering algorithms exploiting microarray data. {motivation}: this work is inspired by the several studies suggesting that coexpression is mostly related to 'static' stable binding relationships, like belonging to the same protein complex, rather than other types of interactions more of a 'causal' and transient nature (e.g. transcription factor-binding site interactions). the aim of this work is to verify if direct or conditional network inference algorithms (e.g. pearson correlation for the former, partial pearson correlation for the latter) are indeed useful in discerning static from causal dependencies in artificial and real gene networks (derived from escherichia coli and saccharomyces cerevisiae). {results}: even in the regime of weak inference power we have to work in, our analysis confirms the differences in the performances of the algorithms: direct methods are more robust in detecting stable interactions, conditional ones are better for causal interactions especially in presence of combinatorial transcriptional regulation. {supplementary} {information}: supplementary data are available at bioinformatics online.
1622	2805405	article	nucleic acids research	\N	\N	oxford university press	6	36	suppl 2	2008	jul	2008-05-16 15:44:44	\N	{polysearch}: a web-based text mining system for extracting relationships between human diseases, genes, mutations, drugs and metabolites	a particular challenge in biomedical text mining is to find ways of handling 'comprehensive' or 'associative' queries such as 'find all genes associated with breast cancer'. given that many queries in genomics, proteomics or metabolomics involve these kind of comprehensive searches we believe that a web-based tool that could support these searches would be quite useful. in response to this need, we have developed the {polysearch} web server. {polysearch} supports >50 different classes of queries against nearly a dozen different types of text, scientific abstract or bioinformatic databases. the typical query supported by {polysearch} is 'given x, find all y's' where x or y can be diseases, tissues, cell compartments, gene/protein names, {snps}, mutations, drugs and metabolites. {polysearch} also exploits a variety of techniques in text mining and information retrieval to identify, highlight and rank informative abstracts, paragraphs or sentences. {polysearch}'s performance has been assessed in tasks such as gene synonym identification, protein–protein interaction identification and disease gene identification using a variety of manually assembled 'gold standard' text corpuses. its f-measure on these tasks is 88, 81 and 79\\%, respectively. these values are between 5 and 50\\% better than other published tools. the server is freely available at http://wishart.biology.ualberta.ca/polysearch
1623	2807037	article	nat rev micro	\N	\N	nature publishing group	11	6	6	2008	jun	2008-05-20 05:02:27	\N	microbiology in the post-genomic era	genomics has revolutionized every aspect of microbiology. now, 13 years after the first bacterial genome was sequenced, it is important to pause and consider what has changed in microbiology research as a consequence of genomics. in this article, we review the evolving field of bacterial typing and the genomic technologies that enable comparative analysis of multiple genomes and the metagenomes of complex microbial environments, and address the implications of the genomic era for the future of microbiology.
1624	2810294	article	plos comput biol	\N	\N	public library of science	\N	4	5	2008	may	2008-05-18 18:51:57	department of biomedical engineering, university of virginia health system, charlottesville, virginia, united states of america.	dynamic analysis of integrated signaling, metabolic, and regulatory networks	extracellular cues affect signaling, metabolic, and regulatory processes to elicit cellular responses. although intracellular signaling, metabolic, and regulatory networks are highly integrated, previous analyses have largely focused on independent processes (e.g., metabolism) without considering the interplay that exists among them. however, there is evidence that many diseases arise from multifunctional components with roles throughout signaling, metabolic, and regulatory networks. therefore, in this study, we propose a flux balance analysis ({fba})–based strategy, referred to as integrated dynamic {fba} ({idfba}), that dynamically simulates cellular phenotypes arising from integrated networks. the {idfba} framework requires an integrated stoichiometric reconstruction of signaling, metabolic, and regulatory processes. it assumes quasi-steady-state conditions for  ” fast” reactions and incorporates  ” slow” reactions into the stoichiometric formalism in a time-delayed manner. to assess the efficacy of {idfba}, we developed a prototypic integrated system comprising signaling, metabolic, and regulatory processes with network features characteristic of actual systems and incorporating kinetic parameters based on typical time scales observed in literature. {idfba} was applied to the prototypic system, which was evaluated for different environments and gene regulatory rules. in addition, we applied the {idfba} framework in a similar manner to a representative module of the single-cell eukaryotic organism saccharomyces cerevisiae. ultimately, {idfba} facilitated quantitative, dynamic analysis of systemic effects of extracellular cues on cellular phenotypes and generated comparable time-course predictions when contrasted with an equivalent kinetic model. since {idfba} solves a linear programming problem and does not require an exhaustive list of detailed kinetic parameters, it may be efficiently scaled to integrated intracellular systems that incorporate signaling, metabolic, and regulatory processes at the genome scale, such as the s. cerevisiae system presented here. cellular systems comprise many diverse components and component interactions spanning signal transduction, transcriptional regulation, and metabolism. although signaling, metabolic, and regulatory activities are often investigated independently of one another, there is growing evidence that considerable interplay occurs among them, and that the malfunctioning of this interplay is associated with disease. the computational analysis of integrated networks has been challenging because of the varying time scales involved as well as the sheer magnitude of such systems (e.g., the numbers of rate constants involved). to this end, we developed a novel computational framework called integrated dynamic flux balance analysis ({idfba}) that generates quantitative, dynamic predictions of species concentrations spanning signaling, regulatory, and metabolic processes. {idfba} extends an existing approach called flux balance analysis ({fba}) in that it couples  ” fast” and  ” slow” reactions, thereby facilitating the study of whole-cell phenotypes and not just sub-cellular network properties. we applied this framework to a prototypic integrated system derived from literature as well as a representative integrated yeast module (the high-osmolarity glycerol [{hog}] pathway) and generated time-course predictions that matched with available experimental data. by extending this framework to larger-scale systems, phenotypic profiles of whole-cell systems could be attained expeditiously.
1625	2841481	article	nature materials	\N	\N	nature publishing group	11	7	6	2008	jun	2008-05-28 14:38:11	\N	biosensing with plasmonic nanosensors	recent developments have greatly improved the sensitivity of optical sensors based on metal nanoparticle arrays and single nanoparticles. we introduce the localized surface plasmon resonance ({lspr}) sensor and describe how its exquisite sensitivity to size, shape and environment can be harnessed to detect molecular binding events and changes in molecular conformation. we then describe recent progress in three areas representing the most significant challenges: pushing sensitivity towards the single-molecule detection limit, combining {lspr} with complementary molecular identification techniques such as surface-enhanced raman spectroscopy, and practical development of sensors and instrumentation for routine use and high-throughput detection. this review highlights several exceptionally promising research directions and discusses how diverse applications of plasmonic nanoparticles can be integrated in the near future.
1626	2841667	inproceedings	proceedings of the 16th annual international conference on intelligent systems in molecular biology (ismb).	\N	\N	\N	\N	\N	\N	2008	\N	2008-05-28 15:34:02	\N	a robust framework for detecting structural variations in a genome	motivation: recently, structural genomic variants have come to the forefront as a significant source of variation in the human population, but the identification of these variants in a large genome remains a challenge. the complete sequencing of a human individual is prohibitive at current costs, while current polymorphism detection technologies, such as {snp} arrays, are not able to identify many of the large scale events. one of the most promising methods to detect such variants is the computational mapping of clone-end sequences to a reference genome. results: here, we present a probabilistic framework for the identification of structural variants using clone-end sequencing. unlike previous methods, our approach does not rely on an a priori determined mapping of all reads to the reference. instead, we build a framework for finding the most probable assignment of sequenced clones to potential structural variants based on the other clones. we compare our predictions with the structural variants identified in three previous studies. while there is a statistically significant correlation between the predictions, we also find a significant number of previously uncharacterized structural variants. furthermore, we identify a number of putative cross-chromosomal events, primarily located proximally to the centromeres of the chromosomes. availability: our dataset, results, and source code are available at http://compbio.cs.toronto.edu/structvar/
1627	2844577	article	nature	\N	\N	nature publishing group	3	453	7195	2008	may	2008-06-10 19:34:03	\N	a phase diagram for jammed matter	the problem of finding the most efficient way to pack spheres has a long history, dating back to the crystalline arrays conjectured1 by kepler and the random geometries explored2 by bernal. apart from its mathematical interest, the problem has practical relevance3 in a wide range of fields, from granular processing to fruit packing. there are currently numerous experiments showing that the loosest way to pack spheres (random loose packing) gives a density of \\~{}55 per cent4, 5, 6. on the other hand, the most compact way to pack spheres (random close packing) results in a maximum density of \\~{}64 per cent2, 4, 6. although these values seem to be robust, there is as yet no physical interpretation for them. here we present a statistical description of jammed states7 in which random close packing can be interpreted as the ground state of the ensemble of jammed matter. our approach demonstrates that random packings of hard spheres in three dimensions cannot exceed a density limit of \\~{}63.4 per cent. we construct a phase diagram that provides a unified view of the hard-sphere packing problem and illuminates various data, including the random-loose-packed state.
1628	2845944	article	nature methods	\N	\N	nature publishing group	9	5	6	2008	may	2008-05-29 22:51:21	\N	a practical guide to single-molecule {fret}	single-molecule fluorescence resonance energy transfer ({smfret}) is one of the most general and adaptable single-molecule techniques. despite the explosive growth in the application of {smfret} to answer biological questions in the last decade, the technique has been practiced mostly by biophysicists. we provide a practical guide to using {smfret}, focusing on the study of immobilized molecules that allow measurements of single-molecule reaction trajectories from 1 ms to many minutes. we discuss issues a biologist must consider to conduct successful {smfret} experiments, including experimental design, sample preparation, single-molecule detection and data analysis. we also describe how a {smfret}-capable instrument can be built at a reasonable cost with off-the-shelf components and operated reliably using well-established protocols and freely available software.
1629	2851066	article	\N	\N	\N	\N	\N	\N	\N	-1	\N	2008-05-30 22:29:42	\N	assessing the evolutionary impact of amino acid mutations in the human genome	quantifying the distribution of fitness effects among newly arising mutations in the human genome is key to resolving important debates in medical and evolutionary genetics. here, we present a method for inferring this distribution using single nucleotide polymorphism (snp) data from a population with non-stationary demographic history (such as that of modern humans). application of our method to 47,576 coding snps found by direct resequencing of 11,404 protein coding-genes in 35 individuals (20 european americans and 15 african americans) allows us to assess the relative contribution of demographic and selective effects to patterning amino acid variation in the human genome. we find evidence of an ancient population expansion in the sample with african ancestry and a relatively recent bottleneck in the sample with european ancestry. after accounting for these demographic effects, we find strong evidence for great variability in the selective effects of new amino acid replacing mutations. in both populations, the patterns of variation are consistent with a leptokurtic distribution of selection coefficients (e.g., gamma or log-normal) peaked near neutrality. specifically, we predict 27â€šÃ„Ã¬29% of amino acid changing (nonsynonymous) mutations are neutral or nearly neutral ( s <0.01%), 30â€šÃ„Ã¬42% are moderately deleterious (0.01%< s <1%), and nearly all the remainder are highly deleterious or lethal ( s >1%). our results are consistent with 10â€šÃ„Ã¬20% of amino acid differences between humans and chimpanzees having been fixed by positive selection with the remainder of differences being neutral or nearly neutral. our analysis also predicts that many of the alleles identified via whole-genome association mapping may be selectively neutral or (formerly) positively selected, implying that deleterious genetic variation affecting disease phenotype may be missed by this widely used approach for mapping genes underlying complex traits.
1630	2853154	article	gene	\N	\N	\N	3	417	1-2	2008	jul	2008-05-31 14:42:49	\N	what is a gene? an updated operational definition	a crucial pre-requisite for large-scale annotation of eukaryotic genomes is the definition of what constitutes a gene. this issue is addressed here in the light of novel and surprising gene features that have recently emerged from large-scale genomic and transcriptomic analyses. the updated operational definition proposed here is:  ” a gene is a discrete genomic region whose transcription is regulated by one or more promoters and distal regulatory elements and which contains the information for the synthesis of functional proteins or non-coding {rnas}, related by the sharing of a portion of genetic information at the level of the ultimate products (proteins or {rnas})”. this definition is specifically designed for eukaryotic chromosomal genes and emphasizes the commonality of the genetic material that gives rise to final, functional products ({ncrnas} or proteins) derived from a single gene. it may be useful in several applications and should help in the provision of a comprehensive inventory of the genes of a given organism, finally allowing answers to the basic question of  ” how many genes” are encoded in its genome.
1631	2855355	article	science technology human values	\N	\N	\N	21	33	5	2008	sep	2008-06-01 22:02:34	\N	new knowledge from old data: the role of standards in the sharing and reuse of ecological data	this article analyzes the experiences of ecologists who used data they did not collect themselves. specifically, the author examines the processes by which ecologists understand and assess the quality of the data they reuse, and investigates the role that standard methods of data collection play in these processes. standardization is one means by which scientific knowledge is transported from local to public spheres. while standards can be helpful, the results show that knowledge of the local context is critical to ecologists' reuse of data. yet, this information is often left behind as data move from the private to the public world. the knowledge that ecologists acquire through fieldwork enables them to recover the local details that are so critical to their comprehension of data collected by others. social processes also play a role in ecologists' efforts to judge the quality of data they reuse. 10.1177/0162243907306704
1632	2855780	article	nature methods	\N	\N	\N	\N	\N	\N	2008	\N	2008-06-02 04:56:03	\N	stem cell transcriptome profiling via massive-scale {mrna} sequencing	we developed a massive-scale rna sequencing protocol, short quantitative random rna libraries or sqrl, to survey the complexity, dynamics and sequence content of transcriptomes in a near-complete fashion. this method generates directional, random-primed, linear cdna libraries that are optimized for next-generation short-tag sequencing. we surveyed the poly(a)(+) transcriptomes of undifferentiated mouse embryonic stem cells (escs) and embryoid bodies (ebs) at an unprecedented depth (10 gb), using the applied biosystems solid technology. these libraries capture the genomic landscape of expression, state-specific expression, single-nucleotide polymorphisms (snps), the transcriptional activity of repeat elements, and both known and new alternative splicing events. we investigated the impact of transcriptional complexity on current models of key signaling pathways controlling esc pluripotency and differentiation, highlighting how sqrl can be used to characterize transcriptome content and dynamics in a quantitative and reproducible manner, and suggesting that our understanding of transcriptional complexity is far from complete.
1633	2857666	article	drug discovery today	\N	\N	\N	4	13	11	2008	may	2008-06-02 19:52:52	\N	internet-based tools for communication and collaboration in chemistry	web-based technologies, coupled with a drive for improved communication between scientists, have resulted in the proliferation of scientific opinion, data and knowledge at an ever-increasing rate. the availability of tools to host wikis and blogs has provided the necessary building blocks for scientists with only a rudimentary understanding of computer software science to communicate to the masses. this newfound freedom has the ability to speed up research and sharing of results, develop extensive collaborations, conduct science in public, and in near-real time. the technologies supporting chemistry, while immature, are fast developing to support chemical structures and reactions, analytical data support and integration to related data sources via supporting software technologies. communication in chemistry is already witnessing a new revolution.
1634	2859483	article	trends in cognitive sciences	\N	\N	\N	4	12	5	2008	may	2008-06-03 16:44:37	\N	object-based auditory and visual attention.	theories of visual attention argue that attention operates on perceptual objects, and thus that interactions between object formation and selective attention determine how competing sources interfere with perception. in auditory perception, theories of attention are less mature and no comprehensive framework exists to explain how attention influences perceptual abilities. however, the same principles that govern visual perception can explain many seemingly disparate auditory phenomena. in particular, many recent studies of 'informational masking' can be explained by failures of either auditory object formation or auditory object selection. this similarity suggests that the same neural mechanisms control attention and influence perception across different sensory modalities.
1635	2862245	book	\N	\N	\N	\N	\N	\N	\N	-1	\N	2008-06-04 19:37:57	\N	forms of talk	"what makes [goffman] compelling is not just the aptness of his observations or the rigors of his theoretical scheme but also his considerable gifts for rendering the everyday as bizarre and amusing."-new york times forms of talk extends erving goffman's interactional analyses of face-to-face communication to ordinary conversations and vebal exchanges. in this, his most sociolinguistic work, goffman relates to certain forms of talk some of the issues that concerned him in his work on frame analysis. this book brings together five of goffman's essays: "replies and responses," "response cries," "footing," "the lecture," and "radio talk." of lasting value in goffman's work is his insistence that behavior-verbal or nonverbal-be examined along with the context of that behavior. in all of these classic essays, there is a "topic" at hand for discussion and analysis. in addition, as those familiar with goffman's work have come to expect, there is the wider context in which the topic can be viewed and related to other topics-a characteristic move of goffman's that has made his work so necessary for students of interaction in many disciplines.
1636	2865447	article	journal of machine learning research	\N	\N	\N	34	7	\N	2006	jul	2008-06-05 14:25:26	\N	large scale multiple kernel learning	while classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. lanckriet et al. (2004) considered conic combinations of kernel matrices for classification, leading to a convex quadratically constrained quadratic program. we show that it can be rewritten as a semi-infinite linear program that can be efficiently solved by recycling the standard {svm} implementations. moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classification. experimental results show that the proposed algorithm works for hundred thousands of examples or hundreds of kernels to be combined, and helps for automatic model selection, improving the interpretability of the learning result. in a second part we discuss general speed up mechanism for {svms}, especially when used with sparse feature maps as appear for string kernels, allowing us to train a string kernel {svm} on a 10 million real-world splice data set from computational biology. we integrated multiple kernel learning in our machine learning toolbox {shogun} for which the source code is publicly available at http://www.fml.tuebingen.mpg.de/raetsch/projects/shogun.
1637	2865708	article	proceedings of the national academy of sciences of the united states of america	\N	\N	national academy of sciences	7	105	23	2008	jun	2008-06-05 15:40:16	\N	historical contingency and the evolution of a key innovation in an experimental population of escherichia coli.	the role of historical contingency in evolution has been much debated, but rarely tested. twelve initially identical populations of escherichia coli were founded in 1988 to investigate this issue. they have since evolved in a glucose-limited medium that also contains citrate, which e. coli cannot use as a carbon source under oxic conditions. no population evolved the capacity to exploit citrate for >30,000 generations, although each population tested billions of mutations. a citrate-using (cit+) variant finally evolved in one population by 31,500 generations, causing an increase in population size and diversity. the long-delayed and unique evolution of this function might indicate the involvement of some extremely rare mutation. alternately, it may involve an ordinary mutation, but one whose physical occurrence or phenotypic expression is contingent on prior mutations in that population. we tested these hypotheses in experiments that "replayed" evolution from different points in that population's history. we observed no cit+ mutants among 8.4 x 10(12) ancestral cells, nor among 9 x 10(12) cells from 60 clones sampled in the first 15,000 generations. however, we observed a significantly greater tendency for later clones to evolve cit+, indicating that some potentiating mutation arose by 20,000 generations. this potentiating change increased the mutation rate to cit+ but did not cause generalized hypermutability. thus, the evolution of this phenotype was contingent on the particular history of that population. more generally, we suggest that historical contingency is especially important when it facilitates the evolution of key innovations that are not easily evolved by gradual, cumulative selection.
1638	2870248	article	bmc bioinformatics	\N	\N	\N	\N	9	1	2008	\N	2008-06-08 08:57:17	\N	{pagerank} without hyperlinks: reranking with {pubmed} related article networks for biomedical text retrieval.	{background}: graph analysis algorithms such as {pagerank} and {hits} have been successful in web environments because they are able to extract important inter-document relationships from manually-created hyperlinks. we consider the application of these techniques to biomedical text retrieval. in the current {pubmed}(r) search interface, a {medline}(r) citation is connected to a number of related citations, which are in turn connected to other citations. thus, a {medline} record represents a node in a vast content-similarity network. this article explores the hypothesis that these networks can be exploited for text retrieval, in the same manner as hyperlink graphs on the web. {results}: we conducted a number of reranking experiments using the {trec} 2005 genomics track test collection in which scores extracted from {pagerank} and {hits} analysis were combined with scores returned by an off-the-shelf retrieval engine. experiments demonstrate that incorporating {pagerank} scores yields significant improvements in terms of standard ranked-retrieval metrics. {conclusion}: the link structure of content-similarity networks can be exploited to improve the effectiveness of information retrieval systems. these results generalize the applicability of graph analysis algorithms to text retrieval in the biomedical domain.
1639	2873914	book	\N	\N	\N	mcgraw-hill	\N	\N	\N	2008	may	2008-06-08 21:07:13	\N	disrupting class: how disruptive innovation will change the way the world learns	**a crash course in the business of learning-from the bestselling author of _the innovator's dilemma_ and _the innovator's solution_â€¦**  â€œa brilliant teacher, christensen brings clarity to a muddled and chaotic world of education.â€  **-jim collins, bestselling author of _good to great_**  according to recent studies in neuroscience, the way we learn doesn't always match up with the way we are taught. if we hope to stay competitive- academically, economically, and technologically-we need to rethink our understanding of intelligence, reevaluate our educational system, and reinvigorate our commitment to learning. in other words, we need â€œdisruptive innovation.â€  now, in his long-awaited new book, clayton m. christensen and coauthors michael b. horn and curtis w. johnson take one of the most important issues of our time-education-and apply christensen's now-famous theories of â€œdisruptiveâ€ change using a wide range of real-life examples. whether you're a school administrator, government official, business leader, parent, teacher, or entrepreneur, you'll discover surprising new ideas, outside-the-box strategies, and straight-a success stories.  you'll learn how  * customized learning will help many more students succeed in school  * student-centric classrooms will increase the demand for new technology  * computers must be disruptively deployed to every student  * disruptive innovation can circumvent roadblocks that have prevented other attempts at school reform  * we can compete in the global classroom-and get ahead in the global market  filled with fascinating case studies, scientific findings, and unprecedented insights on how innovation must be managed, _disrupting class_ will open your eyes to new possibilities, unlock hidden potential, and get you to think differently. professor christensen and his coauthors provide a bold new lesson in innovation that will help you make the grade for years to come.  the future is now. class is in session.
1640	2877638	article	trends in ecology \\& evolution	\N	\N	\N	6	23	6	2008	jun	2008-06-09 17:55:47	national center for ecological analysis and synthesis, santa barbara, ca 93101, usa.	the evolutionary ecology of metacommunities	research on the interactions between evolutionary and ecological dynamics has largely focused on local spatial scales and on relatively simple ecological communities. however, recent work demonstrates that dispersal can drastically alter the interplay between ecological and evolutionary dynamics, often in unexpected ways. we argue that a dispersal-centered synthesis of metacommunity ecology and evolution is necessary to make further progress in this important area of research. we demonstrate that such an approach generates several novel outcomes and substantially enhances understanding of both ecological and evolutionary phenomena in three core research areas at the interface of ecology and evolution.
1641	2885336	article	bmc bioinformatics	\N	\N	\N	\N	9	1	2008	\N	2008-06-12 06:38:45	\N	discovering functional interaction patterns in protein-protein interaction networks	{background}:in recent years, a considerable amount of research effort has been directed to the analysis of biological networks with the availability of genome-scale networks of genes and/or proteins of an increasing number of organisms. a protein-protein interaction ({ppi}) network is a particular biological network which represents physical interactions between pairs of proteins of an organism. major research on {ppi} networks has focused on understanding the topological organization of {ppi} networks, evolution of {ppi} networks and identification of conserved subnetworks across different species, discovery of modules of interaction, use of {ppi} networks for functional annotation of uncharacterized proteins, and improvement of the accuracy of currently available {networks.results}:in this article, we map known functional annotations of proteins onto a {ppi} network in order to identify frequently occurring interaction patterns in the functional space. we propose a new frequent pattern identification technique, {ppispan}, adapted specifically for {ppi} networks from a well-known frequent subgraph identification method, {gspan}. existing module discovery techniques either look for specific clique-like highly interacting protein clusters or linear paths of interaction. however, our goal is different; instead of single clusters or pathways, we look for recurring functional interaction patterns in arbitrary topologies. we have applied {ppispan} on {ppi} networks of saccharomyces cerevisiae and identified a number of frequently occurring functional interaction {patterns.conclusion}:with the help of {ppispan}, recurring functional interaction patterns in an organism's {ppi} network can be identified. such an analysis offers a new perspective on the modular organization of {ppi} networks. the complete list of identified functional interaction patterns is available at {http://bioserver.ceng.metu.edu.tr/ppispan}/.
1642	2890226	article	nat biotech	\N	\N	nature publishing group	5	26	8	2008	aug	2008-06-13 01:11:30	delaware biotechnology institute, university of delaware, newark, delaware 19711, usa.	global identification of {microrna}-target {rna} pairs by parallel analysis of {rna} ends	micrornas (mirnas) are important regulatory molecules in most eukaryotes and identification of their target mrnas is essential for their functional analysis. whereas conventional methods rely on computational prediction and subsequent experimental validation of target rnas, we directly sequenced >28,000,000 signatures from the 5' ends of polyadenylated products of mirna-mediated mrna decay, isolated from inflorescence tissue of arabidopsis thaliana, to discover novel mirna-target rna pairs. within the set of approximately 27,000 transcripts included in the 8,000,000 nonredundant signatures, several previously predicted but nonvalidated targets of mirnas were found. like validated targets, most showed a single abundant signature at the mirna cleavage site, particularly in libraries from a mutant deficient in the 5'-to-3' exonuclease atxrn4. although mirnas in arabidopsis have been extensively investigated, working in reverse from the cleaved targets resulted in the identification and validation of novel mirnas. this versatile approach will affect the study of other aspects of rna processing beyond mirna-target rna pairs.
1643	2890240	article	science (new york, n.y.)	\N	\N	\N	1	320	5882	2008	jun	2008-06-13 01:25:26	\N	biochemistry. how do proteins interact?	10.1126/science.1158818
1644	2899290	book	\N	\N	\N	duke university press	\N	\N	\N	2008	jun	2008-06-16 19:11:02	\N	two bits: the cultural significance of free software	in two bits, christopher m. kelty investigates the history and cultural significance of free software, revealing the people and practices that have transformed not only software, but also music, film, science, and education. free software is a set of practices devoted to the collaborative creation of software source code that is made openly and freely available through an unconventional use of copyright law. kelty shows how these specific practices have reoriented the relations of power around the creation, dissemination, and authorization of all kinds of knowledge after the arrival of the internet. two bits also makes an important contribution to discussions of public spheres and social imaginaries by demonstrating how free software is a "recursive public" --a public organized around the ability to build, modify, and maintain the very infrastructure that gives it life in the first place.   <{p>drawing} on ethnographic research that took him from an internet healthcare start-up company in boston to media labs in berlin to young entrepreneurs in bangalore, kelty describes the technologies and the moral vision that binds together hackers, geeks, lawyers, and other free software advocates. in each case, he shows how their practices and way of life include not only the sharing of software source code but also ways of conceptualizing openness, writing copyright licenses, coordinating collaboration, and proselytizing for the movement. by exploring in detail how these practices came together as the free software movement from the 1970s to the 1990s, kelty also shows how it is possible to understand the new movements that are emerging out of free software: projects such as creative commons, a nonprofit organization that creates copyright licenses, and connexions, a project to create an online scholarly textbook commons.
1645	2900445	article	current opinion in structural biology	\N	\N	\N	6	18	4	2008	aug	2008-06-17 02:24:31	\N	structural specificity in coiled-coil interactions	coiled coils have a rich history in the field of protein design and engineering. novel structures, such as the first seven-helix coiled coil, continue to provide surprises and insights. large-scale datasets quantifying the influence of systematic mutations on coiled-coil stability are a valuable new asset to the area. scoring methods based on sequence and/or structure can predict interaction preferences in coiled-coil-mediated {bzip} transcription factor dimerization. experimental and computational methods for dealing with the near-degeneracy of many coiled-coil structures appear promising for future design applications.
1646	2902291	article	current opinion in structural biology	nucleic acids / sequences and topology	\N	\N	6	18	3	2008	jun	2008-06-17 13:08:55	\N	progress and challenges in protein structure prediction.	depending on whether similar structures are found in the {pdb} library, the protein structure prediction can be categorized into template-based modeling and free modeling. although threading is an efficient tool to detect the structural analogs, the advancements in methodology development have come to a steady state. encouraging progress is observed in structure refinement which aims at drawing template structures closer to the native; this has been mainly driven by the use of multiple structure templates and the development of hybrid knowledge-based and physics-based force fields. for free modeling, exciting examples have been witnessed in folding small proteins to atomic resolutions. however, predicting structures for proteins larger than 150 residues still remains a challenge, with bottlenecks from both force field and conformational search.
1647	2904261	proceedings	vehicular technology conference, 2008. vtc spring 2008. ieee	vehicular technology conference, 2008. vtc spring 2008. ieee	\N	\N	4	\N	\N	2008	may	2008-06-18 06:42:40	\N	{ieee} 802.11p: towards an international standard for wireless access in vehicular environments	vehicular environments impose a set of new requirements on today's wireless communication systems. vehicular safety communications applications cannot tolerate long connection establishment delays before being enabled to communicate with other vehicles encountered on the road. similarly, non-safety applications also demand efficient connection setup with roadside stations providing services (e.g. digital map update) because of the limited time it takes for a car to drive through the coverage area. additionally, the rapidly moving vehicles and complex roadway environment present challenges at the {phy} level. the {ieee} 802.11 standard body is currently working on a new amendment, {ieee} 802.1 lp, to address these concerns. this document is named wireless access in vehicular environment, also known as {wave}. as of writing, the draft document for {ieee} 802.11p is making progress and moving closer towards acceptance by the general {ieee} 802.11 working group. it is projected to pass letter ballot in the first half of 2008. this paper provides an overview of the latest draft proposed for {ieee} 802.11p. it is intended to provide an insight into the reasoning and approaches behind the document.
1648	2909276	article	science	science	\N	\N	3	320	5883	2008	jun	2008-06-20 02:52:09	\N	{single-cycle} nonlinear optics	nonlinear optics plays a central role in the advancement of optical science and laser-based technologies. we report on the confinement of the nonlinear interaction of light with matter to a single wave cycle and demonstrate its utility for time-resolved and strong-field science. the electric field of 3.3-femtosecond, 0.72-micron laser pulses with a controlled and measured waveform ionizes atoms near the crests of the central wave cycle, with ionization being virtually switched off outside this interval. isolated sub-100-attosecond pulses of extreme ultraviolet light (photon energy [\\~{}] 80 electron volts), containing [\\~{}]0.5 nanojoule of energy, emerge from the interaction with a conversion efficiency of [\\~{}]10-6. these tools enable the study of the precision control of electron motion with light fields and electron-electron interactions with a resolution approaching the atomic unit of time ([\\~{}]24 attoseconds). 10.1126/science.1157846
1649	2910062	article	science (new york, n.y.)	\N	\N	\N	5	320	5883	2008	jun	2008-06-20 11:33:23	\N	tuned responses of astrocytes and their influence on hemodynamic signals in the visual cortex.	astrocytes have long been thought to act as a support network for neurons, with little role in information representation or processing. we used two-photon imaging of calcium signals in the ferret visual cortex in vivo to discover that astrocytes, like neurons, respond to visual stimuli, with distinct spatial receptive fields and sharp tuning to visual stimulus features including orientation and spatial frequency. the stimulus-feature preferences of astrocytes were exquisitely mapped across the cortical surface, in close register with neuronal maps. the spatially restricted stimulus-specific component of the intrinsic hemodynamic mapping signal was highly sensitive to astrocyte activation, indicating that astrocytes have a key role in coupling neuronal organization to mapping signals critical for noninvasive brain imaging. furthermore, blocking astrocyte glutamate transporters influenced the magnitude and duration of adjacent visually driven neuronal responses.
1650	2911654	article	genome biology	\N	\N	\N	\N	9	6	2008	\N	2008-06-21 01:26:34	\N	{gmodweb}: a web framework for the generic model organism database	the generic model organism database ({gmod}) initiative provides species-agnostic data models and software tools for representing curated model organism data. here we describe {gmodweb}, a {gmod} project designed to speed the development of model organism database ({mod}) websites. sites created with {gmodweb} provide integration with other {gmod} tools and allow users to browse and search through a variety of data types. {gmodweb} was built using the open source turnkey web framework and is available from http://turnkey.sourceforge.net.
1651	2914632	article	proceeding of chi 2008	\N	\N	\N	\N	\N	\N	2008	\N	2008-06-22 06:00:35	\N	harvesting with {sonar}: the value of aggregating social network information	web 2.0 gives people a substantial role in content and metadata creation. new interpersonal connections are formed and existing connections become evident through web 2.0 services. this newly created social network ({sn}) spans across multiple services and aggregating it could bring great value. in this work we present {sonar}, an {api} for gathering and sharing {sn} information. we give a detailed description of {sonar}, demonstrate its potential value through user scenarios, and show results from experiments we conducted with a {sonar}-based social networking application. these suggest that aggregating {sn} information across diverse data sources enriches the {sn} picture and makes it more complete and useful for the end user.
1652	2933156	article	bioinformatics	\N	\N	\N	8	24	13	2008	jul	2008-06-27 10:29:23	\N	the {exact} description of biomedical protocols	motivation: many published manuscripts contain experiment protocols which are poorly described or deficient in information. this means that the published results are very hard or impossible to repeat. this problem is being made worse by the increasing complexity of high-throughput/automated methods. there is therefore a growing need to represent experiment protocols in an efficient and unambiguous way.  results: we have developed the experiment {actions} ({exact}) ontology as the basis of a method of representing biological laboratory protocols. we provide example protocols that have been formalized using {exact}, and demonstrate the advantages and opportunities created by using this formalization. we argue that the use of {exact} will result in the publication of protocols with increased clarity and usefulness to the scientific community.  availability: the ontology, examples and code can be downloaded from {http://www.aber.ac.uk/compsci/research}/{bio/dss/exact}/  contact: larisa soldatova lss@aber.ac.uk 10.1093/bioinformatics/btn156
1653	2933935	article	organization science	\N	\N	\N	\N	\N	5	-1	\N	2008-06-27 12:17:25	\N	coevolution of firm absorptive capacity and knowledge environment: organizational forms and combinative capabilities	this paper advances the understanding of absorptive capacity for assimilating new knowledge as a mediating variable of organization adaptation. many scholars suggest a firm's absorptive capacity plays a key role in the process of coevolution (lewin et al., this issue). so far, most publications, in following cohen and levinthal (1990), have considered the level of prior related knowledge as the determinant of absorptive capacity. we suggest, however, that two specific organizational determinants of absorptive capacity should also be considered: organization forms and combinative capabilities. we will show how these organizational determinants influence the level of absorptive capacity, ceteris paribus the level of prior related knowledge. subsequently, we will develop a framework in which absorptive capacity is related to both micro- and macrocoevolutionary effects. this framework offers an explanation of how knowledge environments coevolve with the emergence of organization forms and combinative capabilities that are suitable for absorbing knowledge. we will illustrate the framework by discussing two longitudinal case studies of traditional publishing firms moving into the turbulent knowledge environment of an emerging multimedia industrial complex. 10.1287/orsc.10.5.551
1654	2937498	article	bioinformatics	bioinformatics	\N	oxford university press	6	24	13	2008	jul	2008-06-27 15:59:14	\N	predicting functional transcription factor binding through alignment-free and affinity-based analysis of orthologous promoter sequences	motivation: the identification of transcription factor ({tf}) binding sites and the regulatory circuitry that they define is currently an area of intense research. data from whole-genome chromatin immunoprecipitation ({chip}–chip), whole-genome expression microarrays, and sequencing of multiple closely related genomes have all proven useful. by and large, existing methods treat the interpretation of functional data as a classification problem (between bound and unbound {dna}), and the analysis of comparative data as a problem of local alignment (to recover phylogenetic footprints of presumably functional elements). both of these approaches suffer from the inability to model and detect low-affinity binding sites, which have recently been shown to be abundant and functional.
1655	2941039	article	annual review of genomics and human genetics	\N	\N	\N	15	9	1	2008	jun	2008-06-29 08:42:48	departments of genetics and molecular microbiology and genome sequencing center, washington university school of medicine, st. louis mo 63108; emardis@wustl.edu.	{next-generation} {dna} sequencing methods	recent scientific discoveries that resulted from the application of next-generation {dna} sequencing technologies highlight the striking impact of these massively parallel platforms on genetics. these new methods have expanded previously focused readouts from a variety of {dna} preparation protocols to a genome-wide scale and have fine-tuned their resolution to single base precision. the sequencing of {rna} also has transitioned and now includes full-length {cdna} analyses, serial analysis of gene expression ({sage})-based methods, and noncoding {rna} discovery. next-generation sequencing has also enabled novel applications such as the sequencing of ancient {dna} samples, and has substantially widened the scope of metagenomic analysis of environmentally derived samples. taken together, an astounding potential exists for these technologies to bring enormous change in genetic and biological research and to enhance our fundamental biological knowledge.
1656	2943535	article	bmc systems biology	\N	\N	biomed central ltd	\N	2	1	2008	jun	2008-06-30 11:00:21	\N	the systems biology research tool: evolvable open-source software	research in the field of systems biology requires software for a variety of purposes. software must be used to store, retrieve, analyze, and sometimes even to collect the data obtained from system-level (often high-throughput) experiments. software must also be used to implement mathematical models and algorithms required for simulation and theoretical predictions on the system-level.
1657	2948484	inproceedings	\N	proceedings of the nineteenth acm conference on hypertext and hypermedia	ht	acm	9	\N	\N	2008	\N	2008-10-20 08:53:42	new york, ny, usa	an epistemic dynamic model for tagging systems	in recent literature, several models were proposed for reproducing and understanding the tagging behavior of users. they all assume that the tagging behavior is influenced by the previous tag assignments of other users. but they are only partially successful in reproducing characteristic properties found in tag streams. we argue that this inadequacy of existing models results from their inability to include user's background knowledge into their model of tagging behavior. this paper presents a generative tagging model that integrates both components, the background knowledge and the influence of previous tag assignments. our model successfully reproduces characteristic properties of tag streams. it even explains effects of the user interface on the tag stream.
1658	2949866	article	proceedings of the national academy of sciences	\N	\N	national academy of sciences	3	87	12	1990	jun	2008-07-02 04:59:38	department of microbiology, university of illinois, urbana 61801.	towards a natural system of organisms: proposal for the domains archaea, bacteria, and eucarya.	molecular structures and sequences are generally more revealing of evolutionary relationships than are classical phenotypes (particularly so among microorganisms). consequently, the basis for the definition of taxa has progressively shifted from the organismal to the cellular to the molecular level. molecular comparisons show that life on this planet divides into three primary groupings, commonly known as the eubacteria, the archaebacteria, and the eukaryotes. the three are very dissimilar, the differences that separate them being of a more profound nature than the differences that separate typical kingdoms, such as animals and plants. unfortunately, neither of the conventionally accepted views of the natural relationships among living systems--i.e., the five-kingdom taxonomy or the eukaryote-prokaryote dichotomy--reflects this primary tripartite division of the living world. to remedy this situation we propose that a formal system of organisms be established in which above the level of kingdom there exists a new taxon called a "domain." life on this planet would then be seen as comprising three domains, the bacteria, the archaea, and the eucarya, each containing two or more kingdoms. (the eucarya, for example, contain animalia, plantae, fungi, and a number of others yet to be defined). although taxonomic structure within the bacteria and eucarya is not treated herein, archaea is formally subdivided into the two kingdoms euryarchaeota (encompassing the methanogens and their phenotypically diverse relatives) and crenarchaeota (comprising the relatively tight clustering of extremely thermophilic archaebacteria, whose general phenotype appears to resemble most the ancestral phenotype of the archaea.
1659	2966372	article	bioinformatics	\N	\N	\N	8	24	13	2008	jul	2008-07-05 16:26:30	\N	efficient algorithms for accurate hierarchical clustering of huge datasets: tackling the entire protein space	motivation: {upgma} (average linking) is probably the most popular algorithm for hierarchical data clustering, especially in computational biology. however, {upgma} requires the entire dissimilarity matrix in memory. due to this prohibitive requirement, {upgma} is not scalable to very large datasets. application: we present a novel class of memory-constrained {upgma} ({mc}-{upgma}) algorithms. given any practical memory size constraint, this framework guarantees the correct clustering solution without explicitly requiring all dissimilarities in memory. the algorithms are general and are applicable to any dataset. we present a data-dependent characterization of hardness and clustering efficiency. the presented concepts are applicable to any agglomerative clustering formulation. results: we apply our algorithm to the entire collection of protein sequences, to automatically build a comprehensive evolutionary-driven hierarchy of proteins from sequence alone. the newly created tree captures protein families better than state-of-the-art large-scale methods such as {clustr}, {protonet4} or single-linkage clustering. we demonstrate that leveraging the entire mass embodied in all sequence similarities allows to significantly improve on current protein family clusterings which are unable to directly tackle the sheer mass of this data. furthermore, we argue that non-metric constraints are an inherent complexity of the sequence space and should not be overlooked. the robustness of {upgma} allows significant improvement, especially for multidomain proteins, and for large or divergent families. availability: a comprehensive tree built from all {uniprot} sequence similarities, together with navigation and classification tools will be made available as part of the {protonet} service. a c++ implementation of the algorithm is available on request. contact: lonshy@cs.huji.ac.il 10.1093/bioinformatics/btn174
1660	3008520	article	brief bioinform	\N	\N	\N	12	9	6	2008	nov	2008-07-16 14:40:15	\N	experience using web services for biological sequence analysis	programmatic access to data and tools through the web using so-called web services has an important role to play in bioinformatics. in this article, we discuss the most popular approaches based on {soap}/{ws}-i and {rest} and describe our, a cross section of the community, experiences with providing and using web services in the context of biological sequence analysis. we briefly review main technological approaches as well as best practice hints that are useful for both users and developers. finally, syntactic and semantic data integration issues with multiple web services are discussed. 10.1093/bib/bbn029
1661	3023017	inproceedings	high performance computer architecture, 2007. hpca 2007. ieee 13th international symposium on	proceedings of the 2007 ieee 13th international symposium on high performance computer architecture	\N	ieee computer society	11	0	\N	2007	feb	2008-07-20 17:21:47	washington, dc, usa	evaluating {mapreduce} for multi-core and multiprocessor systems	this paper evaluates the suitability of the {mapreduce} model for multi-core and multi-processor systems. {mapreduce} was created by google for application development on data-centers with thousands of servers. it allows programmers to write functional-style code that is automaticatlly parallelized and scheduled in a distributed system. we describe phoenix, an implementation of {mapreduce} for shared-memory systems that includes a programming {api} and an efficient runtime system. the phoenix run-time automatically manages thread creation, dynamic task scheduling, data partitioning, and fault tolerance across processor nodes. we study phoenix with multi-core and symmetric multiprocessor systems and evaluate its performance potential and error recovery features. we also compare {mapreduce} code to code written in lower-level {apis} such as p-threads. overall, we establish that, given a careful implementation, {mapreduce} is a promising model for scalable performance on shared-memory systems with simple parallel code.
1662	3037500	article	bioinformatics	bioinformatics	\N	\N	1	24	17	2008	sep	2008-07-23 19:24:17	\N	{biobayes}: a software package for bayesian inference in systems biology	motivation: there are several levels of uncertainty involved in the mathematical modelling of biochemical systems. there often may be a degree of uncertainty about the values of kinetic parameters, about the general structure of the model and about the behaviour of biochemical species which cannot be observed directly. the methods of bayesian inference provide a consistent framework for modelling and predicting in these uncertain conditions. we present a software package for applying the bayesian inferential methodology to problems in systems {biology.results}: described herein is a software package, {biobayes}, which provides a framework for bayesian parameter estimation and evidential model ranking over models of biochemical systems defined using ordinary differential equations. the package is extensible allowing additional modules to be included by developers. there are no other such packages available which provide this {functionality.availability}: {http://www.dcs.gla.ac.uk/biobayes}/contact: vvv@dcs.gla.ac.uk
1663	3053207	inproceedings	\N	proceedings of the 31st annual international acm sigir conference on research and development in information retrieval	sigir	acm	7	\N	\N	2008	\N	2008-07-28 17:41:45	new york, ny, usa	selecting good expansion terms for pseudo-relevance feedback	pseudo-relevance feedback assumes that most frequent terms in the pseudo-feedback documents are useful for the retrieval. in this study, we re-examine this assumption and show that it does not hold in reality - many expansion terms identified in traditional approaches are indeed unrelated to the query and harmful to the retrieval. we also show that good expansion terms cannot be distinguished from bad ones merely on their distributions in the feedback documents and in the whole collection. we then propose to integrate a term classification process to predict the usefulness of expansion terms. multiple additional features can be integrated in this process. our experiments on three {trec} collections show that retrieval effectiveness can be much improved when term classification is used. in addition, we also demonstrate that good terms should be identified directly according to their possible impact on the retrieval effectiveness, i.e. using supervised learning, instead of unsupervised learning.
1664	3056638	inproceedings	\N	proceedings of the 31st annual international acm sigir conference on research and development in information retrieval	sigir	acm	7	\N	\N	2008	\N	2008-07-29 13:20:06	new york, ny, usa	{tf}-{idf} uncovered: a study of theories and probabilities	interpretations of {tf}-{idf} are based on binary independence retrieval, poisson, information theory, and language modelling. this paper contributes a review of existing interpretations, and then, {tf}-{idf} is systematically related to the probabilities p(q|d) and p(d|q). two approaches are explored: a space of independent, and a space of disjoint terms. for independent terms, an "extreme" query/non-query term assumption uncovers {tf}-{idf}, and an analogy of p(d|q) and the probabilistic odds o(r|d, q) mirrors relevance feedback. for disjoint terms, a relationship between probability theory and {tf}-{idf} is established through the integral + 1/x dx = log x. this study uncovers components such as divergence from randomness and pivoted document length to be inherent parts of a document-query independence ({dqi}) measure, and interestingly, an integral of the {dqi} over the term occurrence probability leads to {tf}-{idf}.
1665	3058452	article	bioinformatics	\N	\N	\N	1	24	18	2008	sep	2008-07-30 07:47:21	the auckland bioengineering institute, the university of auckland, auckland 1010, new zealand.	the {cellml} model repository	summary: the {cellml} model repository provides free access to over 330 biological models. the vast majority of these models are derived from published, peer-reviewed papers. model curation is an important and ongoing process to ensure the {cellml} model is able to accurately reproduce the published results. as the {cellml} community grows, and more people add their models to the repository, model annotation will become increasingly important to facilitate data searches and information retrieval.  availability: the {cellml} model repository is publicly accessible at http://www.cellml.org/models  contact: c.lloyd@auckland.ac.nz 10.1093/bioinformatics/btn390
1666	3063692	inproceedings	\N	proceedings of the 31st annual international acm sigir conference on research and development in information retrieval	sigir	acm	7	\N	\N	2008	\N	2008-07-30 17:22:16	new york, ny, usa	real-time automatic tag recommendation	tags are user-generated labels for entities. existing research on tag recommendation either focuses on improving its accuracy or on automating the process, while ignoring the efficiency issue. we propose a highly-automated novel framework for real-time tag recommendation. the tagged training documents are treated as triplets of (words, docs, tags), and represented in two bipartite graphs, which are partitioned into clusters by spectral recursive embedding ({sre}). tags in each topical cluster are ranked by our novel ranking algorithm. a two-way poisson mixture model ({pmm}) is proposed to model the document distribution into mixture components within each cluster and aggregate words into word clusters simultaneously. a new document is classified by the mixture model based on its posterior probabilities so that tags are recommended according to their ranks. experiments on large-scale tagging datasets of scientific documents ({citeulike}) and web pages del.icio.us) indicate that our framework is capable of making tag recommendation efficiently and effectively. the average tagging time for testing a document is around 1 second, with over 88\\% test documents correctly labeled with the top nine tags we suggested.
1667	3063696	inproceedings	\N	proceedings of the 31st annual international acm sigir conference on research and development in information retrieval	sigir	acm	7	\N	\N	2008	\N	2008-07-30 17:22:44	new york, ny, usa	efficient top-k querying over social-tagging networks	online communities have become popular for publishing and searching content, as well as for finding and connecting to other users. user-generated content includes, for example, personal blogs, bookmarks, and digital photos. these items can be annotated and rated by different users, and these social tags and derived user-specific scores can be leveraged for searching relevant content and discovering subjectively interesting items. moreover, the relationships among users can also be taken into consideration for ranking search results, the intuition being that you trust the recommendations of your close friends more than those of your casual acquaintances. queries for tag or keyword combinations that compute and rank the top-k results thus face a large variety of options that complicate the query processing and pose efficiency challenges. this paper addresses these issues by developing an incremental top-k algorithm with two-dimensional expansions: social expansion considers the strength of relations among users, and semantic expansion considers the relatedness of different tags. it presents a new algorithm, based on principles of threshold algorithms, by folding friends and related tags into the search space in an incremental on-demand manner. the excellent performance of the method is demonstrated by an experimental evaluation on three real-world datasets, crawled from deli.cio.us, flickr, and {librarything}.
1668	3065672	inproceedings	\N	proceedings of the workshop on current trends in biomedical natural language processing	\N	\N	7	\N	\N	2008	jun	2008-07-31 15:47:16	\N	accelerating the annotation of sparse named entities by dynamic sentence selection	background: previous studies of named entity recognition have shown that a reasonable level of recognition accuracy can be achieved by using machine learning models such as conditional random fields or support vector machines. however, the lack of training data (i.e. annotated corpora) makes it difficult for machine learning-based named entity recognizers to be used in building practical information extraction systems. results: this paper presents an active learning-like framework for reducing the human effort required to create named entity annotations in a corpus. in this framework, the annotation work is performed as an iterative and interactive process between the human annotator and a probabilistic named entity tagger. unlike active learning, our framework aims to annotate all occurrences of the target named entities in the given corpus, so that the resulting annotations are free from the sampling bias which is inevitable in active learning approaches. conclusion: we evaluate our framework by simulating the annotation process using two named entity corpora and show that our approach can reduce the number of sentences which need to be examined by the human annotator. the cost reduction achieved by the framework could be drastic when the target named entities are sparse.
1669	3066965	article	nat meth	\N	\N	nature publishing group	3	5	8	2008	aug	2008-08-16 15:06:32	\N	{alta-cyclic}: a self-optimizing base caller for next-generation sequencing	next-generation sequencing is limited to short read lengths and by high error rates. we systematically analyzed sources of noise in the illumina genome analyzer that contribute to these high error rates and developed a base caller, alta-cyclic, that uses machine learning to compensate for noise factors. alta-cyclic substantially improved the number of accurate reads for sequencing runs up to 78 bases and reduced systematic biases, facilitating confident identification of sequence variants.
1670	3080312	article	plos comput biol	\N	\N	public library of science	\N	4	8	2008	aug	2008-08-04 08:53:23	max-planck institute for informatics, saarbruecken, germany.	why do hubs in the yeast protein interaction network tend to be essential: reexamining the connection between the network topology and essentiality	the centrality-lethality rule, which notes that high-degree nodes in a protein interaction network tend to correspond to proteins that are essential, suggests that the topological prominence of a protein in a protein interaction network may be a good predictor of its biological importance. even though the correlation between degree and essentiality was confirmed by many independent studies, the reason for this correlation remains illusive. several hypotheses about putative connections between essentiality of hubs and the topology of protein–protein interaction networks have been proposed, but as we demonstrate, these explanations are not supported by the properties of protein interaction networks. to identify the main topological determinant of essentiality and to provide a biological explanation for the connection between the network topology and essentiality, we performed a rigorous analysis of six variants of the genomewide protein interaction network for saccharomyces cerevisiae obtained using different techniques. we demonstrated that the majority of hubs are essential due to their involvement in essential complex biological modules, a group of densely connected proteins with shared biological function that are enriched in essential proteins. moreover, we rejected two previously proposed explanations for the centrality-lethality rule, one relating the essentiality of hubs to their role in the overall network connectivity and another relying on the recently published essential protein interactions model. analysis of protein interaction networks in the budding yeast saccharomyces cerevisiae has revealed that a small number of proteins, the so-called hubs, interact with a disproportionately large number of other proteins. furthermore, many hub proteins have been shown to be essential for survival of the cell—that is, in optimal conditions, yeast cannot grow and multiply without them. this relation between essentiality and the number of neighbors in the protein–protein interaction network has been termed the centrality-lethality rule. however, why are such hubs essential? jeong and colleagues [1] suggested that overrepresentation of essential proteins among high-degree nodes can be attributed to the central role that hubs play in mediating interactions among numerous, less connected proteins. another view, proposed by he and zhang, suggested that that the majority of proteins are essential due to their involvement in one or more essential protein–protein interactions that are distributed uniformly at random along the network edges [2]. we find that none of the above reasons determines essentiality. instead, the majority of hubs are essential due to their involvement in essential complex biological modules, a group of densely connected proteins with shared biological function that are enriched in essential proteins. this study sheds new light on the topological complexity of protein interaction networks.
1671	3080474	article	plos genet	\N	\N	public library of science	\N	4	8	2008	aug	2008-08-04 10:01:34	\N	{high-precision}, {whole-genome} sequencing of laboratory strains facilitates genetic studies	whole-genome sequencing is a powerful technique for obtaining the reference sequence information of multiple organisms. its use can be dramatically expanded to rapidly identify genomic variations, which can be linked with phenotypes to obtain biological insights. we explored these potential applications using the emerging next-generation sequencing platform solexa genome analyzer, and the well-characterized model bacterium bacillus subtilis . combining sequencing with experimental verification, we first improved the accuracy of the published sequence of the b. subtilis reference strain 168, then obtained sequences of multiple related laboratory strains and different isolates of each strain. this provides a framework for comparing the divergence between different laboratory strains and between their individual isolates. we also demonstrated the power of solexa sequencing by using its results to predict a defect in the citrate signal transduction pathway of a common laboratory strain, which we verified experimentally. finally, we examined the molecular nature of spontaneously generated mutations that suppress the growth defect caused by deletion of the stringent response mediator {rela} . using whole-genome sequencing, we rapidly mapped these suppressor mutations to two small homologs of {rela} . interestingly, stable suppressor strains had mutations in both genes, with each mutation alone partially relieving the {rela} growth defect. this supports an intriguing three-locus interaction module that is not easily identifiable through traditional suppressor mapping. we conclude that whole-genome sequencing can drastically accelerate the identification of suppressor mutations and complex genetic interactions, and it can be applied as a standard tool to investigate the genetic traits of model organisms.
1672	3100446	article	american political science review	\N	\N	\N	15	78	\N	1984	\N	2008-08-08 13:25:44	\N	the new institutionalism: organizational factors in political life	contemporary theories of politics tend to portray politics as a reflection of society, political phenomena as the aggregate consequences of individual behavior, action as the result of choices based on calculated self-interest, history as efficient in reaching unique and appropriate outcomes, and decision making and the allocation of resources as the central foci of political life. some recent theoretical thought in political science, however, blends elements of these theoretical styles into an older concern with institutions. this new institutionalism emphasizes the relative autonomy of political institutions, possibilities for inefficiency in history, and the importance of symbolic action to an understanding of politics. such ideas have a resonable empirical basis, but they are not characterized by powerful theoretical forms. some directions for theoretical research may, however, be identified in institutionalist conceptions of political order.
1673	3102124	inproceedings	\N	jcdl	\N	acm	10	\N	\N	2008	\N	2008-08-08 18:51:21	new york, ny, usa	socialtrust: tamper-resilient trust establishment in online communities	web 2.0 promises rich opportunities for information sharing, electronic commerce, and new modes of social interaction, all centered around the "social web" of user-contributed content, social annotations, and person-to-person social connections. but the increasing reliance on this "social web" also places individuals and their computer systems at risk, creating opportunities for malicious participants to exploit the tight social fabric of these networks. with these problems in mind, we propose the {socialtrust} framework for tamper-resilient trust establishment in online communities. {socialtrust} provides community users with dynamic trust values by (i) distinguishing relationship quality from trust; (ii) incorporating a personalized feedback mechanism for adapting as the community evolves; and (iii) tracking user behavior. we experimentally evaluate the {socialtrust} framework using real online social networking data consisting of millions of {myspace} profiles and relationships. we find that {socialtrust} supports robust trust establishment even in the presence of large-scale collusion by malicious participants.
1674	3111773	article	bioinformatics	bioinformatics	\N	oxford university press	6	24	16	2008	aug	2008-08-12 13:15:31	\N	optimal spliced alignments of short sequence reads	motivation: next generation sequencing technologies open exciting new possibilities for genome and transcriptome sequencing. while reads produced by these technologies are relatively short and error prone compared to the sanger method their throughput is several magnitudes higher. to utilize such reads for transcriptome sequencing and gene structure identification, one needs to be able to accurately align the sequence reads over intron boundaries. this represents a significant challenge given their short length and inherent high error rate.
1675	3123493	article	plos one	\N	\N	public library of science	\N	3	8	2008	aug	2008-08-14 13:05:51	department of bioinformatics and computational biology, the university of texas m.d. anderson cancer center, houston, texas, united states of america.	a semantic web management model for integrative biomedical informatics.	background: data, data everywhere. the diversity and magnitude of the data generated in the life sciences defies automated articulation among complementary efforts. the additional need in this field for managing property and access permissions compounds the difficulty very significantly. this is particularly the case when the integration involves multiple domains and disciplines, even more so when it includes clinical and high throughput molecular data. methodology/principal findings: the emergence of semantic web technologies brings the promise of meaningful interoperation between data and analysis resources. in this report we identify a core model for biomedical knowledge engineering applications and demonstrate how this new technology can be used to weave a management model where multiple intertwined data structures can be hosted and managed by multiple authorities in a distributed management infrastructure. specifically, the demonstration is performed by linking data sources associated with the lung cancer spore awarded to the university of texas md anderson cancer center at houston and the southwestern medical center at dallas. a software prototype, available with open source at www.s3db.org, was developed and its proposed design has been made publicly available as an open source instrument for shared, distributed data management. conclusions/significance: the semantic web technologies have the potential to addresses the need for distributed and evolvable representations that are critical for systems biology and translational biomedical research. as this technology is incorporated into application development we can expect that both general purpose productivity software and domain specific software installed on our personal computers will become increasingly integrated with the relevant remote resources. in this scenario, the acquisition of a new dataset should automatically trigger the delegation of its analysis.
1676	3128959	article	asia pacific journal of management	\N	\N	\N	20	25	3	2008	\N	2008-08-16 19:34:28	\N	impact of personal and cultural factors on knowledge sharing in china	{abstract\\&nbsp;\\&nbsp;knowledge} sharing has been the focus of research for more than a decade and it is widely recognized that it can contribute to the success of an organisation. however, in comparison with other countries, relatively little work on this topic has been done in the chinese context. knowledge sharing is particularly interesting to study in the chinese context at the individual level, given the unique social and cultural characteristics of this environment. in this paper, we develop a theoretical model to explain how personal factors would affect people's intention to share their knowledge. the theory of reasoned action and social exchange theory are used in this paper, as are the time dimension of national culture, face, and guanxi. a survey methodology is used to test the model. face and guanxi orientation both exert a significant effect on the intention to share knowledge. theoretical and practical implications, as well as directions for future research, are discussed.
1677	3129721	article	genomics	\N	\N	\N	9	92	5	2008	nov	2008-08-17 13:13:42	bc cancer agency genome sciences centre, suite 100, 570 west 7th avenue, vancouver, bc v5z 4s6, canada.	applications of next-generation sequencing technologies in functional genomics	a new generation of sequencing technologies, from illumina/solexa, abi/solid, 454/roche, and helicos, has provided unprecedented opportunities for high-throughput functional genomic research. to date, these technologies have been applied in a variety of contexts, including whole-genome sequencing, targeted resequencing, discovery of transcription factor binding sites, and noncoding rna expression profiling. this review discusses applications of next-generation sequencing technologies in functional genomics research and highlights the transforming potential these technologies offer.
1678	3141401	article	proceedings of the national academy of sciences	\N	\N	\N	3	105	33	2008	aug	2008-08-20 19:15:13	\N	rapid shifts in plant distribution with recent climate change	10.1073/pnas.0802891105 a change in climate would be expected to shift plant distribution as species expand in newly favorable areas and decline in increasingly hostile locations. we compared surveys of plant cover that were made in 1977 and 2006–2007 along a 2,314-m elevation gradient in southern california's santa rosa mountains. southern california's climate warmed at the surface, the precipitation variability increased, and the amount of snow decreased during the 30-year period preceding the second survey. we found that the average elevation of the dominant plant species rose by ?65 m between the surveys. this shift cannot be attributed to changes in air pollution or fire frequency and appears to be a consequence of changes in regional climate.
1679	3145759	article	bioinformatics	\N	\N	\N	1	24	19	2008	oct	2008-08-21 19:40:03	\N	a toolkit for analysing large-scale plant small {rna} datasets	summary: recent developments in high-throughput sequencing technologies have generated considerable demand for tools to analyse large datasets of small {rna} sequences. here, we describe a suite of web-based tools for processing plant small {rna} datasets. our tools can be used to identify micro {rnas} and their targets, compare expression levels in {srna} loci, and find putative trans-acting {sirna} {loci.availability}: the tools are freely available for use at {http://srna-tools.cmp.uea.ac.ukcontact}: vincent.moulton@cmp.uea.ac.uk
1680	3175174	inproceedings	\N	sigir	\N	\N	\N	\N	\N	2008	\N	2008-08-30 19:17:09	\N	detecting synonyms in social tagging systems to improve content retrieval	collaborative tagging used in online social content systems is naturally characterized by many synonyms, causing low precision retrieval. we propose a mechanism based on user preference profiles to identify synonyms that can be used to retrieve more relevant documents by expanding the userâ€™s query. using a popular online book catalog we discuss the effectiveness of our method over usual similarity based ex- pansion methods.
1681	3177252	inproceedings	\N	asplos xiii: proceedings of the 13th international conference on architectural support for programming languages and operating systems	\N	acm	9	\N	\N	2008	\N	2008-09-01 03:06:47	new york, ny, usa	merge: a programming model for heterogeneous multi-core systems	in this paper we propose the merge framework, a general purpose programming model for heterogeneous multi-core systems. the merge framework replaces current ad hoc approaches to parallel programming on heterogeneous platforms with a rigorous, library-based methodology that can automatically distribute computation across heterogeneous cores to achieve increased energy and performance efficiency. the merge framework provides (1) a predicate dispatch-based library system for managing and invoking function variants for multiple architectures; (2) a high-level, library-oriented parallel language based on map-reduce; and (3) a compiler and runtime which implement the map-reduce language pattern by dynamically selecting the best available function implementations for a given input and machine configuration. using a generic sequencer architecture interface for heterogeneous accelerators, the merge framework can integrate function variants for specialized accelerators, offering the potential for to-the-metal performance for a wide range of heterogeneous architectures, all transparent to the user. the merge framework has been prototyped on a heterogeneous platform consisting of an intel core 2 duo {cpu} and an 8-core 32-thread intel graphics and media accelerator x3000, and a homogeneous 32-way unisys {smp} system with intel xeon processors. we implemented a set of benchmarks using the merge framework and enhanced the library with x3000 specific implementations, achieving speedups of 3.6x -- 8.5x using the x3000 and 5.2x -- 22x using the 32-way system relative to the straight c reference implementation on a single {ia32} core.
1682	3179558	article	annual review of biochemistry	\N	\N	\N	19	77	1	2008	\N	2008-09-01 17:37:58	department of biochemistry, university of washington, seattle, wa 98195, usa. rhiju@u.washington.edu	macromolecular modeling with rosetta.	advances over the past few years have begun to enable prediction and design of macromolecular structures at near-atomic accuracy. progress has stemmed from the development of reasonably accurate and efficiently computed all-atom potential functions as well as effective conformational sampling strategies appropriate for searching a highly rugged energy landscape, both driven by feedback from structure prediction and design tests. a unified energetic and kinematic framework in the rosetta program allows a wide range of molecular modeling problems, from fibril structure prediction to {rna} folding to the design of new protein interfaces, to be readily investigated and highlights areas for improvement. the methodology enables the creation of novel molecules with useful functions and holds promise for accelerating experimental structural inference. emerging connections to crystallographic phasing, {nmr} modeling, and lower-resolution approaches are described and critically assessed.
1683	3191538	article	adv. phys.	\N	\N	\N	124	46	\N	1997	\N	2008-09-04 13:02:18	\N	configurations of fluid membranes and vesicles	vesicles consisting of a bilayer membrane of amphiphilic lipid molecules are remarkably flexible surfaces that show an amazing variety of shapes of different symmetry and topology. owing to the fluidity of the membrane, shape transitions such as budding can be induced by temperature changes or the action of optical tweezers. thermally excited shape fluctuations are both strong and slow enough to be visible by video microscopy. depending on the physical conditions, vesicles adhere to and unbind from each other or a substrate. this article describes the systematic physical theory developed to understand the static and dynamic aspects of membrane and vesicle configurations. the preferred shapes arise from a competition between curvature energy, which derives from the bending elasticity of the membrane, geometrical constraints such as fixed surface area and fixed enclosed volume, and a signature of the bilayer aspect. these shapes of lowest energy are arranged into phase diagrams, which separate regions of different symmetry by continuous or discontinuous transitions. the geometrical constraints affect the fluctuations around these shapes by creating an effective tension. for vesicles of non-spherical topology, the conformal invariance of the curvature energy leads to conformal diffusion, which signifies a one-fold degeneracy of the ground state. unbinding and adhesion transitions arise from the balance between attractive interactions and entropic repulsion or a cost in bending energy, respectively. both the dynamics of equilibrium fluctuations and the dynamics of shape transformations are governed not only by viscous damping in the surrounding liquid but also by internal friction if the two monolayers slip over each other. more complex membranes such as that of the red blood cell exhibit a variety of new phenomena because of coupling between internal degrees of freedom and external geometry.
1684	3196658	article	nature biotechnology	nat biotech	\N	nature publishing group	7	26	9	2008	aug	2008-09-05 05:12:26	[1] school of computer science, tel-aviv university, tel-aviv 69978, israel. [2] these authors contributed equally to this work.	network-based prediction of human tissue-specific metabolism	direct in vivo investigation of mammalian metabolism is complicated by the distinct metabolic functions of different tissues. we present a computational method that successfully describes the tissue specificity of human metabolism on a large scale. by integrating tissue-specific gene- and protein-expression data with an existing comprehensive reconstruction of the global human metabolic network, we predict tissue-specific metabolic activity in ten human tissues. this reveals a central role for post-transcriptional regulation in shaping tissue-specific metabolic activity profiles. the predicted tissue specificity of genes responsible for metabolic diseases and tissue-specific differences in metabolite exchange with biofluids extend markedly beyond tissue-specific differences manifest in enzyme-expression data, and are validated by large-scale mining of tissue-specificity data. our results establish a computational basis for the genome-wide study of normal and abnormal human metabolism in a tissue-specific manner.
1685	3200250	article	bioinformatics (oxford, england)	\N	\N	\N	6	24	18	2008	sep	2008-09-07 04:21:08	\N	robust and efficient identification of biomarkers by classifying features on graphs.	{motivation}: a central problem in biomarker discovery from large-scale gene expression or single nucleotide polymorphism ({snp}) data is the computational challenge of taking into account the dependence among all the features. methods that ignore the dependence usually identify non-reproducible biomarkers across independent datasets. we introduce a new graph-based semi-supervised feature classification algorithm to identify discriminative disease markers by learning on bipartite graphs. our algorithm directly classifies the feature nodes in a bipartite graph as positive, negative or neutral with network propagation to capture the dependence among both samples and features (clinical and genetic variables) by exploring bi-cluster structures in a graph. two features of our algorithm are: (1) our algorithm can find a global optimal labeling to capture the dependence among all the features and thus, generates highly reproducible results across independent microarray or other high-thoughput datasets, (2) our algorithm is capable of handling hundreds of thousands of features and thus, is particularly useful for biomarker identification from high-throughput gene expression and {snp} data. in addition, although designed for classifying features, our algorithm can also simultaneously classify test samples for disease prognosis/diagnosis. {results}: we applied the network propagation algorithm to study three large-scale breast cancer datasets. our algorithm achieved competitive classification performance compared with {svms} and other baseline methods, and identified several markers with clinical or biological relevance with the disease. more importantly, our algorithm also identified highly reproducible marker genes and enriched functions from the independent datasets. {availability}: supplementary results and source code are available at {http://compbio.cs.umn.edu/feature\\_class}. {supplementary} {information}: supplementary data are available at bioinformatics online.
1686	3225800	article	nature genetics	\N	\N	nature publishing group	4	40	10	2008	oct	2008-09-11 17:09:24	[1] department of genome sciences, university of washington, seattle, washington 98195, usa. [2] these authors contributed equally to this work.	systematic assessment of copy number variant detection via genome-wide {snp} genotyping.	{snp} genotyping has emerged as a technology to incorporate copy number variants ({cnvs}) into genetic analyses of human traits. however, the extent to which {snp} platforms accurately capture {cnvs} remains unclear. using independent, sequence-based {cnv} maps, we find that commonly used {snp} platforms have limited or no probe coverage for a large fraction of {cnvs}. despite this, in 9 samples we inferred 368 {cnvs} using illumina {snp} genotyping data and experimentally validated over two-thirds of these. we also developed a method ({snp}-conditional mixture modeling, {scimm}) to robustly genotype deletions using as few as two {snp} probes. we find that {hapmap} {snps} are strongly correlated with 82\\% of common deletions, but the newest {snp} platforms effectively tag about 50\\%. we conclude that currently available genome-wide {snp} assays can capture {cnvs} accurately, but improvements in array designs, particularly in duplicated sequences, are necessary to facilitate more comprehensive analyses of genomic variation.
1687	3244398	article	bmc biotechnology	\N	\N	\N	\N	8	1	2008	sep	2008-09-21 15:46:38	\N	measuring {micrornas}: comparisons of microarray and quantitative {pcr} measurements, and of different total {rna} prep methods	{background}:determining the expression levels of {micrornas} ({mirnas}) is of great interest to researchers in many areas of biology, given the significant roles these molecules play in cellular regulation. two common methods for measuring {mirnas} in a total {rna} sample are microarrays and quantitative {rt}-{pcr} ({qpcr}). to understand the results of studies that use these two different techniques to measure {mirnas}, it is important to understand how well the results of these two analysis methods correlate. since both methods use total {rna} as a starting material, it is also critical to understand how measurement of {mirnas} might be affected by the particular method of total {rna} preparation {used.results}:we measured the expression of 470 human {mirnas} in nine human tissues using agilent microarrays, and compared these results to {qpcr} profiles of 61 {mirnas} in the same tissues. most expressed {mirnas} (53/60) correlated well (r > 0.9) between the two methods. using spiked-in synthetic {mirnas}, we further examined the two {mirnas} with the lowest correlations, and found the differences cannot be attributed to differential sensitivity of the two methods. we also tested three widely-used total {rna} sample prep methods using {mirna} microarrays. we found that while almost all {mirna} levels correspond between the three methods, there were a few {mirnas} whose levels consistently differed between the different prep techniques when measured by microarray analysis. these differences were corroborated by {qpcr} {measurements.conclusion}:the correlations between agilent {mirna} microarray results and {qpcr} results are generally excellent, as are the correlations between different total {rna} prep methods. however, there are a few {mirnas} whose levels do not correlate between the microarray and {qpcr} measurements, or between different sample prep methods. researchers should therefore take care when comparing results obtained using different analysis or sample preparation methods.
1688	3293311	article	bmc bioinformatics	\N	\N	\N	\N	9	Suppl 9	2008	\N	2008-09-19 16:32:26	\N	the balance of reproducibility, sensitivity, and specificity of lists of differentially expressed genes in microarray studies	{background}:reproducibility is a fundamental requirement in scientific experiments. some recent publications have claimed that microarrays are unreliable because lists of differentially expressed genes ({degs}) are not reproducible in similar experiments. meanwhile, new statistical methods for identifying {degs} continue to appear in the scientific literature. the resultant variety of existing and emerging methods exacerbates confusion and continuing debate in the microarray community on the appropriate choice of methods for identifying reliable {deg} {lists.results}:using the data sets generated by the {microarray} quality control ({maqc}) project, we investigated the impact on the reproducibility of {deg} lists of a few widely used gene selection procedures. we present comprehensive results from inter-site comparisons using the same microarray platform, cross-platform comparisons using multiple microarray platforms, and comparisons between microarray results and those from {taqman} - the widely regarded "standard" gene expression platform. our results demonstrate that (1) previously reported discordance between {deg} lists could simply result from ranking and selecting {degs} solely by statistical significance (p) derived from widely used simple t-tests; (2) when fold change ({fc}) is used as the ranking criterion with a non-stringent p-value cutoff filtering, the {deg} lists become much more reproducible, especially when fewer genes are selected as differentially expressed, as is the case in most microarray studies; and (3) the instability of short {deg} lists solely based on p-value ranking is an expected mathematical consequence of the high variability of the t-values; the more stringent the p-value threshold, the less reproducible the {deg} list is. these observations are also consistent with results from extensive simulation {calculations.conclusion}:we recommend the use of {fc}-ranking plus a non-stringent p cutoff as a straightforward and baseline practice in order to generate more reproducible {deg} lists. specifically, the p-value cutoff should not be stringent (too small) and {fc} should be as large as possible. our results provide practical guidance to choose the appropriate {fc} and p-value cutoffs when selecting a given number of {degs}. the {fc} criterion enhances reproducibility, whereas the p criterion balances sensitivity and specificity.
1689	3320298	article	science	\N	\N	aaas	4	321	5893	2008	\N	2008-09-23 09:54:11	\N	cavity optomechanics: {back-action} at the mesoscale	the coupling of optical and mechanical degrees of freedom is the underlying principle of many techniques to measure mechanical displacement, from macroscale gravitational wave detectors to microscale cantilevers used in scanning probe microscopy. recent experiments have reached a regime where the back-action of photons caused by radiation pressure can influence the optomechanical dynamics, giving rise to a host of long-anticipated phenomena. here we review these developments and discuss the opportunities for innovative technology as well as for fundamental science. 10.1126/science.1156032
1690	3337104	article	cell	\N	\N	\N	13	132	6	2008	mar	2008-09-25 16:38:57	\N	direct inhibition of the longevity-promoting factor {skn}-1 by insulin-like signaling in c. elegans.	{insulin/igf}-1-like signaling ({iis}) is central to growth and metabolism and has a conserved role in aging. in c. elegans, reductions in {iis} increase stress resistance and longevity, effects that require the {iis}-inhibited {foxo} protein {daf}-16. the c. elegans transcription factor {skn}-1 also defends against oxidative stress by mobilizing the conserved phase 2 detoxification response. here we show that {iis} not only opposes {daf}-16 but also directly inhibits {skn}-1 in parallel. the {iis} kinases {akt}-1, -2, and {sgk}-1 phosphorylate {skn}-1, and reduced {iis} leads to constitutive {skn}-1 nuclear accumulation in the intestine and {skn}-1 target gene activation. {skn}-1 contributes to the increased stress tolerance and longevity resulting from reduced {iis} and delays aging when expressed transgenically. furthermore, {skn}-1 that is constitutively active increases life span independently of {daf}-16. our findings indicate that the transcription network regulated by {skn}-1 promotes longevity and is an important direct target of {iis}.
1691	3338202	article	sociology	\N	\N	sage publications	18	42	5	2008	oct	2008-09-25 21:52:16	\N	digital ethnography	the rise of digital technologies has the potential to open new directions in ethnography. despite the ubiquity of these technologies, their infiltration into popular sociological research methods is still limited compared to the insatiable uptake of online scholarly research portals. this article argues that social researchers cannot afford to continue this trend. building upon pioneering work in `digital ethnography', i critically examine the possibilities and problems of four new technologies — online questionnaires, digital video, social networking websites, and blogs — and their potential impacts on the research relationship. the article concludes that a balanced combination of physical and digital ethnography not only gives researchers a larger and more exciting array of methods, but also enables them to demarginalize the voice of respondents. however, access to these technologies remains stratified by class, race, and gender of both researchers and respondents.
1692	3339772	book	\N	\N	\N	princeton university press	\N	\N	\N	2008	jul	2008-09-26 13:25:06	\N	ecological models and data in r	_ecological models and data in r_ is the first truly practical introduction to modern statistical methods for ecology. in step-by-step detail, the book teaches ecology graduate students and researchers everything they need to know in order to use maximum likelihood, information-theoretic, and bayesian techniques to analyze their own data using the programming language r. drawing on extensive experience teaching these techniques to graduate students in ecology, benjamin bolker shows how to choose among and construct statistical models for data, estimate their parameters and confidence limits, and interpret the results. the book also covers statistical frameworks, the philosophy of statistical modeling, and critical mathematical functions and probability distributions. it requires no programming background--only basic calculus and statistics.  * practical, beginner-friendly introduction to modern statistical techniques for ecology using the programming language r  * step-by-step instructions for fitting models to messy, real-world data  * balanced view of different statistical approaches  * wide coverage of techniques--from simple (distribution fitting) to complex (state-space modeling)  * techniques for data manipulation and graphical display  * companion web site with data and r code for all examples
1693	3367094	article	plos comput biol	\N	\N	public library of science	\N	4	9	2008	sep	2008-10-02 20:04:45	\N	structural biology by {nmr}: structure, dynamics, and interactions	the function of bio-macromolecules is determined by both their {3d} structure and conformational dynamics. these molecules are inherently flexible systems displaying a broad range of dynamics on time-scales from picoseconds to seconds. nuclear magnetic resonance ({nmr}) spectroscopy has emerged as the method of choice for studying both protein structure and dynamics in solution. typically, {nmr} experiments are sensitive both to structural features and to dynamics, and hence the measured data contain information on both. despite major progress in both experimental approaches and computational methods, obtaining a consistent view of structure and dynamics from experimental {nmr} data remains a challenge. molecular dynamics simulations have emerged as an indispensable tool in the analysis of {nmr} data.
1694	3388025	book	\N	\N	\N	springer	\N	\N	\N	2011	apr	2008-10-08 17:49:34	\N	the elements of statistical learning: data mining, inference, and prediction, second edition (springer series in statistics)	"during the past decade there has been an explosion in computation and information technology. with it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. the challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. many of these tools have common underpinnings but are often expressed with different terminology. this book describes the important ideas in these areas in a common conceptual framework. while the approach is statistical, the emphasis is on concepts rather than mathematics. many examples are given, with a liberal use of color {graphics."--jacket}.
1695	3391361	article	nature biotechnology	\N	\N	nature publishing group	7	26	10	2008	oct	2008-10-09 20:09:42	\N	the development and impact of 454 sequencing	the 454 sequencer has dramatically increased the volume of sequencing conducted by the scientific community and expanded the range of problems that can be addressed by the direct readouts of {dna} sequence. key breakthroughs in the development of the 454 sequencing platform included higher throughput, simplified all in vitro sample preparation and the miniaturization of sequencing chemistries, enabling massively parallel sequencing reactions to be carried out at a scale and cost not previously possible. together with other recently released next-generation technologies, the 454 platform has started to democratize sequencing, providing individual laboratories with access to capacities that rival those previously found only at a handful of large sequencing centers. over the past 18 months, 454 sequencing has led to a better understanding of the structure of the human genome, allowed the first {non-sanger} sequence of an individual human and opened up new approaches to identify small {rnas}. to make next-generation technologies more widely accessible, they must become easier to use and less costly. in the longer term, the principles established by 454 sequencing might reduce cost further, potentially enabling personalized genomics.
1696	3391362	article	nature biotechnology	\N	\N	nature publishing group	8	26	10	2008	oct	2008-10-09 20:09:48	\N	what would you do if you could sequence everything?	it could be argued that the greatest transformative aspect of the human genome project has been not the sequencing of the genome itself, but the resultant development of new technologies. a host of new approaches has fundamentally changed the way we approach problems in basic and translational research. now, a new generation of high-throughput sequencing technologies promises to again transform the scientific enterprise, potentially supplanting array-based technologies and opening up many new possibilities. by allowing {dna}/{rna} to be assayed more rapidly than previously possible, these next-generation platforms promise a deeper understanding of genome regulation and biology. significantly enhancing sequencing throughput will allow us to follow the evolution of viral and bacterial resistance in real time, to uncover the huge diversity of novel genes that are currently inaccessible, to understand nucleic acid therapeutics, to better integrate biological information for a complete picture of health and disease at a personalized level and to move to advances that we cannot yet imagine.
1697	3394146	article	plos genet	\N	\N	public library of science	\N	4	10	2008	oct	2008-10-10 18:06:09	\N	{high-resolution} mapping of {expression-qtls} yields insight into human gene regulation	recent studies of the {hapmap} lymphoblastoid cell lines have identified large numbers of quantitative trait loci for gene expression ({eqtls}). reanalyzing these data using a novel bayesian hierarchical model, we were able to create a surprisingly high-resolution map of the typical locations of sites that affect {mrna} levels in cis. strikingly, we found a strong enrichment of {eqtls} in the 250 bp just upstream of the transcription end site ({tes}), in addition to an enrichment around the transcription start site ({tss}). most {eqtls} lie either within genes or close to genes; for example, we estimate that only 5\\% of {eqtls} lie more than 20 kb upstream of the {tss}. after controlling for position effects, {snps} in exons are \\~{}2-fold more likely than {snps} in introns to be {eqtls}. our results suggest an important role for {mrna} stability in determining steady-state {mrna} levels, and highlight the potential of {eqtl} mapping as a high-resolution tool for studying the determinants of gene regulation. individual phenotypes within natural populations generally exhibit a large diversity resulting from a complex interplay of genes and environmental factors. since the advent of molecular markers in the 1980s, quantitative genetics has made a significant step toward unraveling the genetic bases of such complex traits, in particular by developing sophisticated tools to map the genomic locations of genes that affect complex traits. these regions are known as quantitative trait loci ({qtls}). more recently, these tools have been extended to the study of gene expression phenotypes on a massive scale. in this paper, we used a previously published dataset consisting of expression measurements of 11,446 genes in human cell lines derived from 210 unrelated human individuals that have been genetically characterized by the international {hapmap} project. our article develops and applies a framework for determining the genetic factors that impact gene regulation. we show that these factors cluster strongly near to the gene start and gene end and are enriched within the transcribed region. our approach suggests a general framework for studying the genetic factors that affect variation in gene expression.
1698	3403177	incollection	data warehousing and knowledge discovery	data warehousing and knowledge discovery	lecture notes in computer science	springer berlin heidelberg	9	5182	\N	2008	\N	2008-10-14 03:37:25	berlin, heidelberg	personalizing navigation in folksonomies using hierarchical tag clustering	the popularity of collaborative tagging, otherwise known as  ” folksonomies”, emanate from the flexibility they afford users in navigating large information spaces for resources, tags, or other users, unencumbered by a pre-defined navigational or conceptual hierarchy. despite its advantages, social tagging also increases user overhead in search and navigation: users are free to apply any tag they wish to a resource, often resulting in a large number of tags that are redundant, ambiguous, or idiosyncratic. data mining techniques such as clustering provide a means to overcome this problem by learning aggregate user models, and thus reducing noise. in this paper we propose a method to personalize search and navigation based on unsupervised hierarchical agglomerative tag clustering. given a user profile, represented as a vector of tags, the learned tag clusters provide the nexus between the user and those resources that correspond more closely to the user's intent. we validate this assertion through extensive evaluation of the proposed algorithm using data from a real collaborative tagging web site.
1699	3406993	unpublished	\N	\N	\N	\N	\N	\N	\N	-1	\N	2008-10-15 00:08:05	\N	forget time	following a line of research that i have developed for several years, i argue that the best strategy for understanding quantum gravity is to build a picture of the physical world where the notion of time plays no role at all. i summarize here this point of view, explaining why i think that in a fundamental description of nature we must â€œforget timeâ€, and how this can be done in the classical and in the quantum theory. the idea is to develop a formalism that treats dependent and independent variables on the same footing. in short, i propose to interpret mechanics as a theory of relations between variables, rather than the theory of the evolution of variables in time.
1700	3426312	article	mol biol evol	\N	\N	\N	10	26	2	2009	feb	2008-10-18 01:39:16	\N	an investigation of the statistical power of neutrality tests based on comparative and population genetic data	in this report, we investigate the statistical power of several tests of selective neutrality based on patterns of genetic diversity within and between species. the goal is to compare tests based solely on population genetic data with tests using comparative data or a combination of comparative and population genetic data. we show that in the presence of repeated selective sweeps on relatively neutral background, tests based on the {dn}/{ds} ratios in comparative data almost always have more power to detect selection than tests based on population genetic data, even if the overall level of divergence is low. tests based solely on the distribution of allele frequencies or the site frequency spectrum, such as the {ewens-watterson} test or tajima's d, have less power in detecting both positive and negative selection because of the transient nature of positive selection and the weak signal left by negative selection. the {hudson-kreitman}-aguade test is the most powerful test for detecting positive selection among the population genetic tests investigated, whereas {mcdonald}-kreitman test typically has more power to detect negative selection. we discuss our findings in the light of the discordant results obtained in several recently published genomic scans. 10.1093/molbev/msn231
1701	3428472	article	nucleic acids research	\N	\N	\N	6	37	suppl 1	2009	jan	2008-10-19 03:43:58	\N	{mir2disease}: a manually curated database for {microrna} deregulation in human disease	'{mir2disease}', a manually curated database, aims at providing a comprehensive resource of {microrna} deregulation in various human diseases. the current version of {mir2disease} documents 1939 curated relationships between 299 human {micrornas} and 94 human diseases by reviewing more than 600 published papers. around one-seventh of the {microrna}–disease relationships represent the pathogenic roles of deregulated {microrna} in human disease. each entry in the {mir2disease} contains detailed information on a {microrna}–disease relationship, including a {microrna} {id}, the disease name, a brief description of the {microrna}–disease relationship, an expression pattern of the {microrna}, the detection method for {microrna} expression, experimentally verified target gene(s) of the {microrna} and a literature reference. {mir2disease} provides a user-friendly interface for a convenient retrieval of each entry by {microrna} {id}, disease name, or target gene. in addition, {mir2disease} offers a submission page that allows researchers to submit established {microrna}–disease relationships that are not documented. once approved by the submission review committee, the submitted records will be included in the database. {mir2disease} is freely available at {http://www.mir2disease}.org.
1702	3443713	article	nucleic acids research	\N	\N	\N	4	37	Database issue	2009	jan	2008-11-10 14:19:38	\N	{interpro}: the integrative protein signature database.	the {interpro} database (http://www.ebi.ac.uk/interpro/) integrates together predictive models or 'signatures' representing protein domains, families and functional sites from multiple, diverse source databases: {gene3d}, {panther}, pfam, {pirsf}, {prints}, {prodom}, {prosite}, {smart}, {superfamily} and {tigrfams}. integration is performed manually and approximately half of the total approximately 58,000 signatures available in the source databases belong to an {interpro} entry. recently, we have started to also display the remaining un-integrated signatures via our web interface. other developments include the provision of non-signature data, such as structural data, in new {xml} files on our {ftp} site, as well as the inclusion of matchless {uniprotkb} proteins in the existing match {xml} files. the web interface has been extended and now links out to the {adan} predicted protein-protein interaction database and the {spice} and dasty viewers. the latest public release (v18.0) covers 79.8\\% of {uniprotkb} (v14.1) and consists of 16 549 entries. {interpro} data may be accessed either via the web address above, via web services, by downloading files by anonymous {ftp} or by using the {interproscan} search software ({http://www.ebi.ac.uk/tools}/{interproscan}/).
1703	3443777	article	proceedings of the national academy of sciences	\N	\N	\N	5	105	43	2008	oct	2008-10-23 16:20:00	\N	small-scale copy number variation and large-scale changes in gene expression	the expression dynamics of interacting genes depends, in part, on the structure of regulatory networks. genetic regulatory networks include an overrepresentation of subgraphs commonly known as network motifs. in this article, we demonstrate that gene copy number is an omnipresent parameter that can dramatically modify the dynamical function of network motifs. we consider positive feedback, bistable feedback, and toggle switch motifs and show that variation in gene copy number, on the order of a single or few copies, can lead to multiple orders of magnitude change in gene expression and, in some cases, switches in deterministic control. further, small changes in gene copy number for a 3-gene motif with successive inhibition (the  ” repressilator”) can lead to a qualitative switch in system behavior among oscillatory and equilibrium dynamics. in all cases, the qualitative change in expression is due to the nonlinear nature of transcriptional feedback in which duplicated motifs interact via common pools of transcription factors. we are able to implicitly determine the critical values of copy number which lead to qualitative shifts in system behavior. in some cases, we are able to solve for the sufficient condition for the existence of a bifurcation in terms of kinetic rates of transcription, translation, binding, and degradation. we discuss the relevance of our findings to ongoing efforts to link copy number variation with cell fate determination by viruses, dynamics of synthetic gene circuits, and constraints on evolutionary adaptation.
1704	3448578	article	plos comput biol	\N	\N	public library of science	\N	4	10	2008	oct	2008-10-24 19:25:52	\N	fusion and fission of genes define a metric between fungal genomes	gene fusion and fission events are key mechanisms in the evolution of gene architecture, whose effects are visible in protein architecture when they occur in coding sequences. until now, the detection of fusion and fission events has been performed at the level of protein sequences with a post facto removal of supernumerary links due to paralogy, and often did not include looking for events defined only in single genomes. we propose a method for the detection of these events, defined on groups of paralogs to compensate for the gene redundancy of eukaryotic genomes, and apply it to the proteomes of 12 fungal species. we collected an inventory of 1,680 elementary fusion and fission events. in half the cases, both composite and element genes are found in the same species. per-species counts of events correlate with the species genome size, suggesting a random mechanism of occurrence. some biological functions of the genes involved in fusion and fission events are slightly over- or under-represented. as already noted in previous studies, the genes involved in an event tend to belong to the same functional category. we inferred the position of each event in the evolution tree of the 12 fungal species. the event localization counts for all the segments of the tree provide a metric that depicts the \\^{a}€{\\oe}recombinational\\^{a}€? phylogeny among fungi. a possible interpretation of this metric as distance in adaptation space is proposed.
1705	3458224	article	molecular ecology	\N	\N	blackwell publishing ltd	10	17	21	2008	nov	2008-12-10 08:21:50	department of evolutionary biology, evolutionary biology centre, uppsala university, norbyvgen 18d, se-752 36 uppsala, sweden	comparative genomics and the study of evolution by natural selection	abstract genomics profoundly affects most areas of biology, including ecology and evolutionary biology. by examining genome sequences from multiple species, comparative genomics offers new insight into genome evolution and the way natural selection moulds {dna} sequence evolution. functional divergence, as manifested in the accumulation of nonsynonymous substitutions in protein-coding genes, differs among lineages in a manner seemingly related to population size. for example, the ratio of nonsynonymous to synonymous substitution ({dn}/{ds}) is higher in apes than in rodents, compatible with ohta's nearly neutral theory of molecular evolution, which suggests that the fixation of slightly deleterious mutations contributes to protein evolution at an extent negatively correlated with effective population size. while this supports the idea that functional evolution is not necessarily adaptive, comparative genomics is uncovering a role for positive darwinian selection in 10–40\\% of all genes in different lineages, estimates that are likely to increase when the addition of more genomes gives increased power. again, population size seems to matter also in this context, with a higher proportion of fixed amino acid changes representing advantageous mutations in large populations. genes that are particularly prone to be driven by positive selection include those involved with reproduction, immune response, sensory perception and apoptosis. genetic innovations are also frequently obtained by the gain or loss of complete gene sequences. moreover, it is increasingly realized, from comparative genomics, that purifying selection conserves much more than just the protein-coding part of the genome, and this points at an important role for regulatory elements in trait evolution. finally, genome sequencing using outbred or multiple individuals has provided a wealth of polymorphism data that gives information on population history, demography and marker evolution.
1706	3464173	article	briefings in bioinformatics	\N	\N	oxford university press	12	10	1	2009	jan	2008-10-30 05:06:42	\N	models of coding sequence evolution	probabilistic models of sequence evolution are in widespread use in phylogenetics and molecular sequence evolution. these models have become increasingly sophisticated and combined with statistical model comparison techniques have helped to shed light on how genes and proteins evolve. models of codon evolution have been particularly useful, because, in addition to providing a significant improvement in model realism for protein-coding sequences, codon models can also be designed to test hypotheses about the selective pressures that shape the evolution of the sequences. such models typically assume a phylogeny and can be used to identify sites or lineages that have evolved adaptively. recently some of the key assumptions that underlie phylogenetic tests of selection have been questioned, such as the assumption that the rate of synonymous changes is constant across sites or that a single phylogenetic tree can be assumed at all sites for recombining sequences. while some of these issues have been addressed through the development of novel methods, others remain as caveats that need to be considered on a case-by-case basis. here, we outline the theory of codon models and their application to the detection of positive selection. we review some of the more recent developments that have improved their power and utility, laying a foundation for further advances in the modeling of coding sequence evolution.
1707	3470400	article	genome biology	\N	\N	\N	\N	9	10	2008	oct	2008-11-03 15:18:22	\N	boolean implication networks derived from large scale, whole genome microarray datasets	we describe a method for extracting boolean implications (if-then relationships) in very large amounts of gene expression microarray data. a meta-analysis of data from thousands of microarrays for humans, mice, and fruit flies finds millions of implication relationships between genes that would be missed by other methods. these relationships capture gender differences, tissue differences, development, and differentiation. new relationships are discovered that are preserved across all three species.
1708	3480685	article	annual review of pathology	\N	\N	\N	28	4	\N	2009	sep	2008-11-04 18:49:25	\N	{micrornas} in cancer.	within the past few years, studies on {microrna} ({mirna}) and cancer have burst onto the scene. profiling of the {mirnome} (global {mirna} expression levels) has become prevalent, and abundant {mirnome} data are currently available for various cancers. the pattern of {mirna} expression can be correlated with cancer type, stage, and other clinical variables, so {mirna} profiling can be used as a tool for cancer diagnosis and prognosis. {mirna} expression analyses also suggest oncogenic (or tumor-suppressive) roles of {mirnas}. {mirnas} play roles in almost all aspects of cancer biology, such as proliferation, apoptosis, invasion/metastasis, and angiogenesis. given that many {mirnas} are deregulated in cancers but have not yet been further studied, it is expected that more {mirnas} will emerge as players in the etiology and progression of cancer. here we also discuss {mirnas} as a tool for cancer therapy.
1709	3482554	book	\N	\N	\N	o'reilly media	\N	\N	\N	2008	dec	2008-11-05 14:38:55	\N	real world haskell	this easy-to-use, fast-moving tutorial introduces you to functional programming with haskell. you'll learn how to use haskell in a variety of practical ways, from short scripts to large and demanding applications. _real world haskell_ takes you through the basics of functional programming at a brisk pace, and then helps you increase your understanding of haskell in real- world issues like i/o, performance, dealing with data, concurrency, and more as you move through each chapter.   with this book, you will:  * understand the differences between procedural and functional programming  * learn the features of haskell, and how to use it to develop useful programs  * interact with filesystems, databases, and network services  * write solid code with automated tests, code coverage, and error handling  * harness the power of multicore systems via concurrent and parallel programming  you'll find plenty of hands-on exercises, along with examples of real haskell programs that you can modify, compile, and run. whether or not you've used a functional language before, if you want to understand why haskell is coming into its own as a practical language in so many major organizations, _real world haskell_ is the best place to start.
1710	3483523	article	annual review of genetics	\N	\N	\N	22	42	1	2008	\N	2008-11-06 01:40:31	\N	the organization of the bacterial genome	many bacterial cellular processes interact intimately with the chromosome. such interplay is the major driving force of genome structure or organization. interactions take place at different scales—local for gene expression, global for replication—and lead to the differentiation of the chromosome into organizational units such as operons, replichores, or macrodomains. these processes are intermingled in the cell and create complex higher-level organizational features that are adaptive because they favor the interplay between the processes. the surprising result of selection for genome organization is that gene repertoires change much more quickly than chromosomal structure. comparative genomics and experimental genomic manipulations are untangling the different cellular and evolutionary mechanisms causing such resilience to change. since organization results from cellular processes, a better understanding of chromosome organization will help unravel the underlying cellular processes and their diversity.
1711	3484872	article	nat genet	\N	\N	nature publishing group	2	40	12	2008	dec	2008-11-06 15:33:40	\N	deep surveying of alternative splicing complexity in the human transcriptome by high-throughput sequencing	we carried out the first analysis of alternative splicing complexity in human tissues using {mrna}-seq data. new splice junctions were detected in approximately 20\\% of multiexon genes, many of which are tissue specific. by combining {mrna}-seq and {est}-{cdna} sequence data, we estimate that transcripts from approximately 95\\% of multiexon genes undergo alternative splicing and that there are approximately 100,000 intermediate- to high-abundance alternative splicing events in major human tissues. from a comparison with quantitative alternative splicing microarray profiling data, we also show that {mrna}-seq data provide reliable measurements for exon inclusion levels.
1712	3486430	article	nucleic acids research	\N	\N	\N	11	37	1	2009	jan	2008-11-12 10:43:25	\N	high-throughput chromatin information enables accurate tissue-specific prediction of transcription factor binding sites	in silico prediction of transcription factor binding sites ({tfbss}) is central to the task of gene regulatory network elucidation. genomic {dna} sequence information provides a basis for these predictions, due to the sequence specificity of {tf}-binding events. however, {dna} sequence alone is an impoverished source of information for the task of {tfbs} prediction in eukaryotes, as additional factors, such as chromatin structure regulate binding events. we show that incorporating high-throughput chromatin modification estimates can greatly improve the accuracy of in silico prediction of in vivo binding for a wide range of {tfs} in human and mouse. this improvement is superior to the improvement gained by equivalent use of either transcription start site proximity or phylogenetic conservation information. importantly, predictions made with the use of chromatin structure information are tissue specific. this result supports the biological hypothesis that chromatin modulates {tf} binding to produce tissue-specific binding profiles in higher eukaryotes, and suggests that the use of chromatin modification information can lead to accurate tissue-specific transcriptional regulatory network elucidation.
1713	3488831	article	plos computational biology	\N	\N	public library of science	\N	4	11	2008	nov	2008-11-09 22:53:47	\N	facilitated variation: how evolution learns from past environments to generalize to new environments.	one of the striking features of evolution is the appearance of novel structures in organisms. recently, kirschner and gerhart have integrated discoveries in evolution, genetics, and developmental biology to form a theory of facilitated variation ({fv}). the key observation is that organisms are designed such that random genetic changes are channeled in phenotypic directions that are potentially useful. an open question is how {fv} spontaneously emerges during evolution. here, we address this by means of computer simulations of two well-studied model systems, logic circuits and {rna} secondary structure. we find that evolution of {fv} is enhanced in environments that change from time to time in a systematic way: the varying environments are made of the same set of subgoals but in different combinations. we find that organisms that evolve under such varying goals not only remember their history but also generalize to future environments, exhibiting high adaptability to novel goals. rapid adaptation is seen to goals composed of the same subgoals in novel combinations, and to goals where one of the subgoals was never seen in the history of the organism. the mechanisms for such enhanced generation of novelty (generalization) are analyzed, as is the way that organisms store information in their genomes about their past environments. elements of facilitated variation theory, such as weak regulatory linkage, modularity, and reduced pleiotropy of mutations, evolve spontaneously under these conditions. thus, environments that change in a systematic, modular fashion seem to promote facilitated variation and allow evolution to generalize to novel conditions.
1714	3494817	article	nature biotechnology	\N	\N	nature publishing group	7	26	11	2008	nov	2008-11-10 09:21:39	\N	an integrated software system for analyzing {chip}-chip and {chip}-seq data	we present {cisgenome}, a software system for analyzing genome-wide chromatin immunoprecipitation ({chip}) data. {cisgenome} is designed to meet all basic needs of {chip} data analyses, including visualization, data normalization, peak detection, false discovery rate computation, gene-peak association, and sequence and motif analysis. in addition to implementing previously published {chip}–microarray ({chip}-chip) analysis methods, the software contains statistical methods designed specifically for {chlp} sequencing ({chip}-seq) data obtained by coupling {chip} with massively parallel sequencing. the modular design of {cisgenome} enables it to support interactive analyses through a graphic user interface as well as customized batch-mode computation for advanced data mining. a built-in browser allows visualization of array images, signals, gene structure, conservation, and {dna} sequence and motif information. we demonstrate the use of these tools by a comparative analysis of {chip}-chip and {chip}-seq data for the transcription factor {nrsf}/{rest}, a study of {chip}-seq analysis with or without a negative control sample, and an analysis of a new motif in nanog- and sox2-binding regions.
1715	3494821	article	nature biotechnology	\N	\N	nature publishing group	8	26	11	2008	nov	2008-11-09 01:42:18	\N	activity motifs reveal principles of timing in transcriptional control of the yeast metabolic network.	significant insight about biological networks arises from the study of network motifsâ€”overly abundant network subgraphs1, 2â€”but such wiring patterns do not specify when and how potential routes within a cellular network are used. to address this limitation, we introduce activity motifs, which capture patterns in the dynamic use of a network. using this framework to analyze transcription in saccharomyces cerevisiae metabolism, we find that cells use different timing activity motifs to optimize transcription timing in response to changing conditions: forward activation to produce metabolic compounds efficiently, backward shutoff to rapidly stop production of a detrimental product and synchronized activation for co-production of metabolites required for the same reaction. measuring protein abundance over a time course reveals that mrna timing motifs also occur at the protein level. timing motifs significantly overlap with binding activity motifs, where genes in a linear chain have ordered binding affinity to a transcription factor, suggesting a mechanism for ordered transcription. finely timed transcriptional regulation is therefore abundant in yeast metabolism, optimizing the organism's adaptation to new environmental conditions.
1716	3497386	article	nucleic acids research	\N	\N	\N	3	37	Database issue	2009	jan	2008-11-09 04:09:05	\N	reactome knowledgebase of human biological pathways and processes.	reactome (http://www.reactome.org) is an expert-authored, peer-reviewed knowledgebase of human reactions and pathways that functions as a data mining resource and electronic textbook. its current release includes 2975 human proteins, 2907 reactions and 4455 literature citations. a new entity-level pathway viewer and improved search and data mining tools facilitate searching and visualizing pathway data and the analysis of user-supplied high-throughput data sets. reactome has increased its utility to the model organism communities with improved orthology prediction methods allowing pathway inference for 22 species and through collaborations to create manually curated reactome pathway datasets for species including arabidopsis, oryza sativa (rice), drosophila and gallus gallus (chicken). reactome's data content and software can all be freely used and redistributed under open source terms.
1717	3504163	article	information and software technology	\N	\N	butterworth-heinemann	26	50	9-10	2008	aug	2008-11-11 17:31:29	newton, ma, usa	empirical studies of agile software development: a systematic review	agile software development represents a major departure from traditional, plan-based approaches to software engineering. a systematic review of empirical studies of agile software development up to and including 2005 was conducted. the search strategy identified 1996 studies, of which 36 were identified as empirical studies. the studies were grouped into four themes: introduction and adoption, human and social factors, perceptions on agile methods, and comparative studies. the review investigates what is currently known about the benefits and limitations of, and the strength of evidence for, agile methods. implications for research and practice are presented. the main implication for research is a need for more and better empirical studies of agile software development within a common research agenda. for the industrial readership, the review provides a map of findings, according to topic, that can be compared for relevance to their own settings and situations.
1718	3507817	article	bmc genomics	\N	\N	\N	\N	9	1	2008	\N	2008-11-13 04:58:52	\N	identifying positioned nucleosomes with epigenetic marks in human from {chip}-seq.	in vivo positioning and covalent modifications of nucleosomes play an important role in epigenetic regulation, but genome-wide studies of positioned nucleosomes and their modifications in human still remain limited. this paper describes a novel computational framework to efficiently identify positioned nucleosomes and their histone modification profiles from nucleosome-resolution histone modification {chip}-seq data. we applied the algorithm to histone methylation {chip}-seq data in human {cd4}+ t cells and identified over 438,000 positioned nucleosomes, which appear predominantly at functionally important regions such as genes, promoters, {dnase} i hypersensitive regions, and transcription factor binding sites. our analysis shows the identified nucleosomes play a key role in epigenetic gene regulation within those functionally important regions via their positioning and histone modifications. our method provides an effective framework for studying nucleosome positioning and epigenetic marks in mammalian genomes. the algorithm is open source and available at {http://liulab.dfci.harvard.edu/nps}/.
1719	3508967	article	genome biology	\N	\N	\N	\N	9	11	2008	\N	2008-11-13 14:22:22	\N	{arrayplex}: distributed, interactive and programmatic access to genome sequence, annotation, ontology, and analytical toolsets	{arrayplex} is a software package that centrally provides a large number of flexible toolsets useful for functional genomics including microarray data storage, quality assessments, data visualization, gene annotation retrieval, statistical tests, genomic sequence retrieval and motif analysis. it uses a client-server architecture based on open source components, provides graphical, command-line, and programmatic access to all needed resources, and is extensible by virtue of a documented application programming interface. {arrayplex} is available at http://sourceforge.net/projects/arrayplex/.
1720	3574100	article	the tohoku journal of experimental medicine	\N	\N	\N	11	196	\N	2002	\N	2008-11-18 11:43:03	\N	lead, chemical porphyria, and heme as a biological mediator	one of the most well-characterized symptoms of lead poisoning is porphyria. the biochemical signs of lead intoxication related to porphyria are ?-aminolevulinic aciduria, coproporphyrinuria, and accumulation of free and zinc protoporphyrin in erythrocytes. from the 1970s to the early 80s, almost all of the enzymes in the heme pathway had been purified and characterized, and it was demonstrated that ?-aminolevulinic aciduria is due to inhibition of ?-aminolevulinate dehydratase by lead. lead also inhibits purified ferrochelatase; however, the magnitude of inhibition was essentially nil even under pathological conditions. further study proved the disturbance of iron-reducing activity by moderate lead exposure. far different from these two enzymes, lead failed to inhibit purified coproporphyrinogen oxidase, i.e., the mechanism of coproporphyrinuria has not yet been understood. during the 80s to the 90s, the effects of environmental hazards including lead were elucidated through stress proteins, indicating the induction of some heme pathway enzymes as stress proteins. at that time, gene environment interaction was another focus of toxicology, since gene carriers of porphyrias are considered to be a high-risk group to chemical pollutants. toxicological studies from the 70s to the 90s focused on the direct effect of hazards on biological molecules, such as the heme pathway enzymes, and many environmental pollutants were proved to affect cytosolic heme. recently, we demonstrated the mechanism of the heme-controlled transcription system, which suggests that the indirect effects of environmental hazards are also important for elucidating toxicity, i.e., the hazards can affect cell functions through such biological mediators as regulatory heme. it is, therefore, probable that toxicology in the future will focus on biological systems such as gene regulation and signal transduction systems.
1721	3574337	article	experimental cell research	\N	\N	\N	11	290	2	2003	nov	2008-11-18 14:05:43	\N	transcription factor nrf2 activation by inorganic arsenic in cultured keratinocytes: involvement of hydrogen peroxide.	inorganic arsenic is a well-documented human carcinogen that targets the skin. the induction of oxidative stress, as shown with arsenic, may have a bearing on the carcinogenic mechanism of this metalloid. the transcription factor nrf2 is a key player in the regulation of genes encoding for many antioxidative response enzymes. thus, the effect of inorganic arsenic (as sodium arsenite) on nrf2 expression and localization was studied in {hacat} cells, an immortalized human keratinocyte cell line. we found, for the first time, that arsenic enhanced cellular expression of nrf2 at the transcriptional and protein levels and activated expression of nrf2-related genes in these cells. in addition, arsenic exposure caused nuclear accumulation of nrf2 in association with downstream activation of nrf2-mediated oxidative response genes. arsenic simultaneously increased the expression of keap1, a regulator of nrf2 activity. the coordinated induction of keap1 expression and nuclear nrf2 accumulation induced by arsenic suggests that keap1 is important to arsenic-induced nrf2 activation. furthermore, when cells were pretreated with scavengers of hydrogen peroxide ({h(2)o}(2)) such as catalase-polyethylene glycol ({peg}-{cat}) or tiron, arsenic-induced nuclear nrf2 accumulation was suppressed, whereas {cudipsh}, a cell-permeable superoxide dismutase ({sod}) mimic compound that produces {h(2)o}(2) from superoxide (*o(2)(-)), enhanced nrf2 nuclear accumulation. these results indicate that {h(2)o}(2), rather than *o(2)(-), is the mediator of nuclear nrf2 accumulation. additional study showed that arsenic causes increased cellular {h(2)o}(2) production and that {h(2)o}(2) itself has the ability to increase nrf2 expression at both the transcription and protein levels in {hacat} cells. taken together, these data clearly show that arsenic increases nrf2 expression and activity at multiple levels and that {h(2)o}(2) is one of the mediators of this process.
1722	3614773	article	nature reviews genetics	\N	\N	nature publishing group	6	10	1	2009	jan	2008-11-20 17:50:25	\N	{rna}-seq: a revolutionary tool for transcriptomics	{rna}-seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. {rna}-seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. this article describes the {rna}-seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes.
1723	3655691	article	nat rev mol cell biol	\N	\N	nature publishing group	10	9	12	2008	dec	2008-11-25 19:19:53	\N	design principles of biochemical oscillators	cellular rhythms are generated by complex interactions among genes, proteins and metabolites. they are used to control every aspect of cell physiology, from signalling, motility and development to growth, division and death. we consider specific examples of oscillatory processes and discuss four general requirements for biochemical oscillations: negative feedback, time delay, sufficient 'nonlinearity' of the reaction kinetics and proper balancing of the timescales of opposing chemical reactions. positive feedback is one mechanism to delay the negative-feedback signal. biological oscillators can be classified according to the topology of the positive- and negative-feedback loops in the underlying regulatory mechanism.
1724	3665188	article	the american naturalist	\N	\N	the university of chicago press	9	172	6	2008	dec	2009-03-11 12:44:37	\N	disintegration of the ecological community.	in this essay, i argue that the seemingly indestructible concept of the community as a local, interacting assemblage of species has hindered progress toward understanding species richness at local to regional scales. i suggest that the distributions of species within a region reveal more about the processes that generate diversity patterns than does the co-occurrence of species at any given point. the local community is an epiphenomenon that has relatively little explanatory power in ecology and evolutionary biology. local coexistence cannot provide insight into the ecogeographic distributions of species within a region, from which local assemblages of species derive, nor can local communities be used to test hypotheses concerning the origin, maintenance, and regulation of species richness, either locally or regionally. ecologists are moving toward a community concept based on interactions between populations over a continuum of spatial and temporal scales within entire regions, including the population and evolutionary processes that produce new species.
1725	3689841	article	plos one	\N	\N	public library of science	\N	3	11	2008	nov	2008-11-25 21:36:41	\N	articles by latin american authors in prestigious journals have fewer citations	the journal impact factor ({if}) is generally accepted to be a good measurement of the relevance/quality of articles that a journal publishes. in spite of an, apparently, homogenous peer-review process for a given journal, we hypothesize that the country affiliation of authors from developing latin american ({la}) countries affects the {if} of a journal detrimentally. seven prestigious international journals, one multidisciplinary journal and six serving specific branches of science, were examined in terms of their {if} in the web of science. two subsets of each journal were then selected to evaluate the influence of author's affiliation on the {if}. they comprised contributions (i) with authorship from four latin american ({la}) countries (argentina, brazil, chile and mexico) and (ii) with authorship from five developed countries (england, france, germany, japan and {usa}). both subsets were further subdivided into two groups: articles with authorship from one country only and collaborative articles with authorship from other countries. articles from the five developed countries had {if} close to the overall {if} of the journals and the influence of collaboration on this value was minor. in the case of {la} articles the effect of collaboration (virtually all with developed countries) was significant. the {ifs} for non-collaborative articles averaged 66\\% of the overall {if} of the journals whereas the articles in collaboration raised the {ifs} to values close to the overall {if}. the study shows a significantly lower {if} in the group of the subsets of non-collaborative {la} articles and thus that country affiliation of authors from non-developed {la} countries does affect the {if} of a journal detrimentally. there are no data to indicate whether the lower {ifs} of {la} articles were due to their inherent inferior quality/relevance or psycho-social trend towards under-citation of articles from these countries. however, further study is required since there are foreseeable consequences of this trend as it may stimulate strategies by editors to turn down articles that tend to be under-cited.
1726	3729371	article	biochemical pharmacology	\N	\N	\N	9	73	12	2007	jun	2008-11-30 12:28:05	\N	coordinate regulation of phase i and {ii} xenobiotic metabolisms by the ah receptor and nrf2.	the aryl hydrocarbon receptor ({ahr}) is a ligand-activated transcription factor with important roles in metabolic adaptation, normal physiology and dioxin toxicology. metabolic adaptation is based on coordinate regulation of a set of xenobiotic-metabolizing enzymes ({xmes}), termed {ahr} battery. coordination is achieved by {ahr}/arnt-binding to {xres} (xenobiotic response elements), identified in the 5' upstream region of {ahr} target genes. the {ahr} battery encodes phase i and {ii} enzymes. interestingly, these phase {ii} genes are linked to the nrf2 gene battery that encodes enzymes that are essential in protection against oxidative/electrophile stress. nrf2 binds to {ares} (antioxidant response elements) in the regulatory region of a large and distinct set of target genes. functionally characterized response elements such as {xres} and {ares} in the regulatory region of target genes may provide a genetic basis to understand {ahr}- and nrf2-induced genes. linkage between {ahr} and nrf2 batteries is probably achieved by multiple mechanisms, including nrf2 as a target gene of the {ahr}, indirect activation of nrf2 via {cyp1a1}-generated reactive oxygen species, and direct cross-interaction of {ahr}/{xre} and {nrf2/are} signaling. linkage appears to be species- and cell-dependent. however, mechanisms linking {xre}- and {are}-controlled phase {ii} genes need further investigation. tightened coupling between phases i and {ii} by {ahr}- and nrf2-induced {xmes} may greatly attenuate health risks posed by {cyp1a1}-generated toxic intermediates and reactive oxygen species. better recognition of coordinate phase i and {ii} metabolisms may improve risk assessment of reactive toxic intermediates in the extrapolation to low level endo- and xenobiotic exposure.
1727	3741343	article	proceedings of the national academy of sciences	\N	\N	\N	5	105	49	2008	dec	2008-12-03 16:39:09	\N	cell shape and cell-wall organization in gram-negative bacteria	in bacterial cells, the peptidoglycan cell wall is the stress-bearing structure that dictates cell shape. although many molecular details of the composition and assembly of cell-wall components are known, how the network of peptidoglycan subunits is organized to give the cell shape during normal growth and how it is reorganized in response to damage or environmental forces have been relatively unexplored. in this work, we introduce a quantitative physical model of the bacterial cell wall that predicts the mechanical response of cell shape to peptidoglycan damage and perturbation in the rod-shaped gram-negative bacterium escherichia coli. to test these predictions, we use time-lapse imaging experiments to show that damage often manifests as a bulge on the sidewall, coupled to large-scale bending of the cylindrical cell wall around the bulge. our physical model also suggests a surprising robustness of cell shape to peptidoglycan defects, helping explain the observed porosity of the cell wall and the ability of cells to grow and maintain their shape even under conditions that limit peptide crosslinking. finally, we show that many common bacterial cell shapes can be realized within the same model via simple spatial patterning of peptidoglycan defects, suggesting that minor patterning changes could underlie the great diversity of shapes observed in the bacterial kingdom.
1728	3749174	article	genome research	\N	\N	\N	\N	\N	\N	-1	\N	2008-12-04 22:16:00	\N	global analysis of the insulator binding protein {ctcf} in chromatin barrier regions reveals demarcation of active and repressive domains	insulators are {dna} elements that prevent inappropriate interactions between the neighboring regions of the genome. they can be functionally classified as either enhancer blockers or domain barriers. {ctcf} ({ccctc}-binding factor) is the only known major insulator-binding protein in the vertebrates and has been shown to bind many enhancer-blocking elements. however, it is not clear whether it plays a role in chromatin domain barriers between active and repressive domains. here, we used {chip}-seq to map the genome-wide binding sites of {ctcf} in three cell types and identified significant binding of {ctcf} to the boundaries of repressive chromatin domains marked by {h3k27me3}. although we find an extensive overlapping of {ctcf}-binding sites across the three cell types, its association with the domain boundaries is cell-type-specific. we further show that the nucleosomes flanking {ctcf}-binding sites are well positioned. interestingly, we found a complementary pattern between the repressive {h3k27me3} and the active {h2ak5ac} regions, which are separated by {ctcf}. our data indicate that {ctcf} may play important roles in the barrier activity of insulators, and this study provides a resource for further investigation of the {ctcf} function in organizing chromatin in the human genome.
1729	3749296	article	yearbook of medical informatics	\N	\N	\N	16	\N	\N	2008	\N	2008-12-05 01:17:48	\N	extracting information from textual documents in the electronic health record: a review of recent research.	objectives: we examine recent published research on the extraction of information from textual documents in the electronic health record (ehr). methods: literature review of the research published after 1995, based on pubmed, conference proceedings, and the acm digital library, as well as on relevant publications referenced in papers already included. results: 174 publications were selected and are discussed in this review in terms of methods used, pre-processing of textual documents, contextual features detection and analysis, extraction of information in general, extraction of codes and of information for decision-support and enrichment of the ehr, information extraction for surveillance, research, automated terminology management, and data mining, and de-identification of clinical text. conclusions: performance of information extraction systems with clinical text has improved since the last systematic review in 1995, but they are still rarely applied outside of the laboratory they have been developed in. competitive challenges for information extraction from clinical text, along with the availability of annotated clinical text corpora, and further improvements in system performance are important factors to stimulate advances in this field and to increase the acceptance and usage of these systems in concrete clinical and biomedical research contexts.
1730	3749403	article	bmj	\N	\N	british medical journal publishing group	\N	337	dec04\\_2	2008	jan	2008-12-05 07:37:10	\N	dynamic spread of happiness in a large social network: longitudinal analysis over 20 years in the framingham heart study	{abstractobjectives} to evaluate whether happiness can spread from person to person and whether niches of happiness form within social {networks.design} longitudinal social network {analysis.setting} framingham heart study social {network.participants} 4739 individuals followed from 1983 to {2003.main} outcome measures happiness measured with validated four item scale; broad array of attributes of social networks and diverse social {ties.results} clusters of happy and unhappy people are visible in the network, and the relationship between people's happiness extends up to three degrees of separation (for example, to the friends of one's friends' friends). people who are surrounded by many happy people and those who are central in the network are more likely to become happy in the future. longitudinal statistical models suggest that clusters of happiness result from the spread of happiness and not just a tendency for people to associate with similar individuals. a friend who lives within a mile (about 1.6 km) and who becomes happy increases the probability that a person is happy by 25\\% (95\\% confidence interval 1\\% to 57\\%). similar effects are seen in coresident spouses (8\\%, 0.2\\% to 16\\%), siblings who live within a mile (14\\%, 1\\% to 28\\%), and next door neighbours (34\\%, 7\\% to 70\\%). effects are not seen between coworkers. the effect decays with time and with geographical {separation.conclusions} people's happiness depends on the happiness of others with whom they are connected. this provides further justification for seeing happiness, like health, as a collective phenomenon.
1731	3750481	article	science (new york, n.y.)	\N	\N	american association for the advancement of science	2	322	5909	2008	dec	2008-12-05 17:35:31	\N	divergent transcription from active promoters.	transcription initiation by {rna} polymerase {ii} ({rnapii}) is thought to occur unidirectionally from most genes. here, we present evidence of widespread divergent transcription at protein-encoding gene promoters. transcription start site-associated {rnas} ({tssa}-{rnas}) nonrandomly flank active promoters, with peaks of antisense and sense short {rnas} at 250 nucleotides upstream and 50 nucleotides downstream of {tsss}, respectively. northern analysis shows that {tssa}-{rnas} are subsets of an {rna} population 20 to 90 nucleotides in length. promoter-associated {rnapii} and {h3k4}-trimethylated histones, transcription initiation hallmarks, colocalize at sense and antisense {tssa}-{rna} positions; however, {h3k79}-dimethylated histones, characteristic of elongating {rnapii}, are only present downstream of {tsss}. these results suggest that divergent transcription over short distances is common for active promoters and may help promoter regions maintain a state poised for subsequent regulation.
1732	3757771	article	genome research	\N	\N	\N	9	19	1	2009	jan	2008-12-08 18:01:20	\N	targeted screening of cis-regulatory variation in human haplotypes.	regulatory cis-acting variants account for a large proportion of gene expression variability in populations. cis-acting differences can be specifically measured by comparing relative levels of allelic transcripts within a sample. allelic expression ({ae}) mapping for cis-regulatory variant discovery has been hindered by the requirements of having informative or heterozygous single nucleotide polymorphisms ({snps}) within genes in order to assign the allelic origin of each transcript. in this study we have developed an approach to systematically screen for heritable cis-variants in common human haplotypes across >1,000 genes. in order to achieve the highest level of information per haplotype studied, we carried out allelic expression measurements by using both intronic and exonic {snps} in primary transcripts. we used a novel {rna} pooling strategy in immortalized lymphoblastoid cell lines ({lcls}) and primary human osteoblast cell lines ({hobs}) to allow for high-throughput {ae}. screening hits from {rna} pools were further validated by performing allelic expression mapping in individual samples. our results indicate that >10\\% of expressed genes in human {lcls} show genotype-linked {ae}. in addition, we have validated cis-acting variants in over 20 genes linked with common disease susceptibility in recent genome-wide studies. more generally, our results indicate that {rna} pooling coupled with {ae} read-out by second generation sequencing or by other methods provides a high-throughput tool for cataloging the impact of common noncoding variants in the human genome.
1733	3760539	article	philosophical transactions of the royal society of london, series b	\N	\N	\N	9	359	\N	2004	\N	2008-12-09 16:14:00	\N	biodiversity informatics managing and applying primary biodiversity data	recently, advances in information technology and an increased willingness to share primary biodiversity data are enabling unprecedented access to it. by combining presences of species data with electronic cartography via a number of algorithms, estimating niches of species and their areas of distribution becomes feasible at resolutions one to three orders of magnitude higher than it was possible a few years ago. some examples of the power of that technique are presented. for the method to work, limitations such as lack of high-quality taxonomic determination, precise georeferencing of the data and availability of high-quality and updated taxonomic treatments of the groups must be overcome. these are discussed, together with comments on the potential of these biodiversity informatics techniques not only for fundamental studies but also as a way for developing countries to apply state of the art bioinformatic methods and large quantities of data, in practical ways, to tackle issues of biodiversity management.
1734	3801011	article	journal of contemporary ethnography	\N	\N	\N	32	38	1	2009	feb	2008-12-17 10:24:36	\N	ethnographic approaches to the internet and {computer-mediated} communication	in this article we review ethnographic research on the internet and computer-mediated communication. the technologically mediated environment prevents researchers from directly observing research participants and often makes the interaction anonymous. in addition, in the online environment direct interaction with participants is replaced by computer-screen data that are largely textual, but may include combinations of textual, visual, aural, and kinetic components. we show how the online environment requires adjustments in how ethnographers define the setting of their research, conduct participant observation and interviews, obtain access to settings and research subjects, and deal with the ethical dilemmas posed by the medium.
1735	3837640	article	nature methods	\N	\N	nature publishing group	8	6	1	2008	dec	2009-04-03 20:48:35	\N	microscopy and its focal switch	until not very long ago, it was widely accepted that lens-based (far-field) optical microscopes cannot visualize details much finer than about half the wavelength of light. the advent of viable physical concepts for overcoming the limiting role of diffraction in the early 1990s set off a quest that has led to readily applicable and widely accessible fluorescence microscopes with nanoscale spatial resolution. here i discuss the principles of these methods together with their differences in implementation and operation. finally, i outline potential developments.
1736	3857936	article	studies in health technology and informatics	\N	\N	\N	5	136	\N	2008	\N	2009-01-07 18:44:52	\N	syntactical negation detection in clinical practice guidelines.	in clinical practice guidelines ({cpgs}) the medical information is stored in a narrative way. a large part of this information occurs in a negated form. the detection of negation in {cpgs} is an important task since it helps medical personnel to identify not occurring symptoms and diseases as well as treatment actions that should not be accomplished. we developed algorithms capable of negation detection in this kind of medical documents. according to our results, we are convinced that the involvement of syntactical methods can improve negation detection, not only in medical writings but also in arbitrary narrative texts.
1737	3886758	article	bmc genomics	\N	\N	\N	\N	10	1	2009	\N	2009-01-14 21:38:13	\N	{biomart}--biological queries made easy.	biologists need to perform complex queries, often across a variety of databases. typically, each data resource provides an advanced query interface, each of which must be learnt by the biologist before they can begin to query them. frequently, more than one data source is required and for high-throughput analysis, cutting and pasting results between websites is certainly very time consuming. therefore, many groups rely on local bioinformatics support to process queries by accessing the resource's programmatic interfaces if they exist. this is not an efficient solution in terms of cost and time. instead, it would be better if the biologist only had to learn one generic interface. {biomart} provides such a solution. {biomart} enables scientists to perform advanced querying of biological data sources through a single web interface. the power of the system comes from integrated querying of data sources regardless of their geographical locations. once these queries have been defined, they may be automated with its "scripting at the click of a button" functionality. {biomart}'s capabilities are extended by integration with several widely used software packages such as {bioconductor}, {das}, galaxy, cytoscape, taverna. in this paper, we describe all aspects of {biomart} from a user's perspective and demonstrate how it can be used to solve real biological use cases such as {snp} selection for candidate gene screening or annotation of microarray results. {biomart} is an easy to use, generic and scalable system and therefore, has become an integral part of large data resources including ensembl, {uniprot}, {hapmap}, wormbase, gramene, dictybase, {pride}, {msd} and reactome. {biomart} is freely accessible to use at http://www.biomart.org.
1738	3891587	article	nat rev genet	\N	\N	nature publishing group	14	10	2	2009	feb	2009-01-16 11:20:48	\N	small silencing {rnas}: an expanding universe	since the discovery in 1993 of the first small silencing rna, a dizzying number of small rna classes have been identified, including micrornas (mirnas), small interfering rnas (sirnas) and piwi-interacting rnas (pirnas). these classes differ in their biogenesis, their modes of target regulation and in the biological pathways they regulate. there is a growing realization that, despite their differences, these distinct small rna pathways are interconnected, and that small rna pathways compete and collaborate as they regulate genes and protect the genome from external and internal threats.
1739	3979340	article	trends in ecology \\& evolution	\N	\N	elsevier	8	24	3	2016	jan	2009-01-29 16:20:45	\N	generalized linear mixed models: a practical guide for ecology and evolution	how should ecologists and evolutionary biologists analyze nonnormal data that involve random effects? nonnormal data such as counts or proportions often defy classical statistical procedures. generalized linear mixed models ({glmms}) provide a more flexible approach for analyzing nonnormal data when random effects are present. the explosion of research on {glmms} in the last decade has generated considerable uncertainty for practitioners in ecology and evolution. despite the availability of accurate techniques for estimating {glmm} parameters in simple cases, complex {glmms} are challenging to fit and statistical inference such as hypothesis testing remains difficult. we review the use (and misuse) of {glmms} in ecology and evolution, discuss estimation and inference and summarize 'best-practice' data analysis procedures for scientists facing this challenge.
1740	3981307	article	j. phys. chem. b	\N	\N	american chemical society	12	113	8	2009	jan	2009-01-30 11:13:20	\N	computations of standard binding free energies with molecular dynamics simulations	an increasing number of studies have reported computations of the standard (absolute) binding free energy of small ligands to proteins using molecular dynamics ({md}) simulations and explicit solvent molecules that are in good agreement with experiments. this encouraging progress suggests that physics-based approaches hold the promise of making important contributions to the process of drug discovery and optimization in the near future. two types of approaches are principally used to compute binding free energies with {md} simulations. the most widely known is the alchemical double decoupling method, in which the interaction of the ligand with its surroundings are progressively switched off. it is also possible to use a potential of mean force ({pmf}) method, in which the ligand is physically separated from the protein receptor. for both of these computational approaches, restraining potentials may be activated and released during the simulation for sampling efficiently the changes in translational, rotational, and conformational freedom of the ligand and protein upon binding. because such restraining potentials add bias to the simulations, it is important that their effects be rigorously removed to yield a binding free energy that is properly unbiased with respect to the standard state. a review of recent results is presented, and differences in computational methods are discussed. examples of computations with t4-lysozyme mutants, {fkbp12}, {sh2} domain, and cytochrome p450 are discussed and compared. remaining difficulties and challenges are highlighted.
1741	3981325	article	genome research	\N	\N	\N	7	19	4	2009	apr	2009-01-30 11:16:20	\N	multiple whole-genome alignments without a reference organism.	10.1101/gr.081778.108 multiple sequence alignments have become one of the most commonly used resources in genomics research. most algorithms for multiple alignment of whole genomes rely either on a reference genome, against which all of the other sequences are laid out, or require a one-to-one mapping between the nucleotides of the genomes, preventing the alignment of recently duplicated regions. both approaches have drawbacks for whole-genome comparisons. in this paper we present a novel symmetric alignment algorithm. the resulting alignments not only represent all of the genomes equally well, but also include all relevant duplications that occurred since the divergence from the last common ancestor. our algorithm, implemented as a part of the vista genome pipeline (vgp), was used to align seven vertebrate and six  genomes. the resulting whole-genome alignments demonstrate a higher sensitivity and specificity than the pairwise alignments previously available through the vgp and have higher exon alignment accuracy than comparable public whole-genome alignments. of the multiple alignment methods tested, ours performed the best at aligning genes from multigene familiesâ€”perhaps the most challenging test for whole-genome alignments. our whole-genome multiple alignments are available through the vista browser at .
1742	3994145	article	computers \\& education	\N	\N	\N	6	52	3	2009	apr	2009-02-02 11:20:26	\N	{itunes} university and the classroom: can podcasts replace professors?	{itunes} university, a website with downloadable educational podcasts, can provide students the opportunity to obtain professors' lectures when students are unable to attend class. to determine the effectiveness of audio lectures in higher education, undergraduate general psychology students participated in one of two conditions. in the lecture condition, participants listened to a 25-min lecture given in person by a professor using {powerpoint} slides. copies of the slides were given to aid note-taking. in the podcast condition, participants received a podcast of the same lecture along with the {powerpoint} handouts. participants in both conditions were instructed to keep a running log of study time and activities used in preparing for an exam. one week from the initial session students returned to take an exam on lecture content. results indicated that students in the podcast condition who took notes while listening to the podcast scored significantly higher than the lecture condition. the impact of mobile learning on classroom performance is discussed.
1743	4000622	article	bmc bioinformatics	\N	\N	\N	\N	10	1	2009	\N	2009-02-03 11:51:04	\N	comparison of small n statistical tests of differential expression applied to microarrays.	background: dna microarrays provide data for genome wide patterns of expression between observation classes. microarray studies often have small samples sizes, however, due to cost constraints or specimen availability. this can lead to poor random error estimates and inaccurate statistical tests of differential expression. we compare the performance of the standard t-test, fold change, and four small n statistical test methods designed to circumvent these problems. we report results of various normalization methods for empirical microarray data and of various random error models for simulated data. results: three empirical bayes methods (cybert, brb, and limma t-statistics) were the most effective statistical tests across simulated and both 2-colour cdna and affymetrix experimental data. the cybert regularized t-statistic in particular was able to maintain expected false positive rates with simulated data showing high variances at low gene intensities, although at the cost of low true positive rates. the local pooled error (lpe) test introduced a bias that lowered false positive rates below theoretically expected values and had lower power relative to the top performers. the standard two-sample t-test and fold change were also found to be sub-optimal for detecting differentially expressed genes. the generalized log transformation was shown to be beneficial in improving results with certain data sets, in particular high variance cdna data. conclusion: pre-processing of data influences performance and the proper combination of pre-processing and statistical testing is necessary for obtaining the best results. all three empirical bayes methods assessed in our study are good choices for statistical tests for small n microarray studies for both affymetrix and cdna data. choice of method for a particular study will depend on software and normalization preferences.
1744	4015011	article	science	\N	\N	american association for the advancement of science	2	323	5915	2009	feb	2009-02-06 09:10:26	\N	computational social science	10.1126/science.1167742
1745	4037235	article	wsdm	\N	\N	\N	\N	\N	\N	-1	\N	2009-02-11 23:58:44	\N	classifying tags using open content resources	tagging has emerged as a popular means to annotate on-line objects such as bookmarks, photos and videos. tags vary in semantic meaning and can describe different aspects of a media object. tags describe the content of the media as well as locations, dates, people and other associated meta-data. being able to automatically classify tags into semantic categories allows us to understand better the way users annotate media objects and to build tools for viewing and browsing the media objects. in this paper we present a generic method for classifying tags using third party open content resources, such as wikipedia and the open directory. our method uses structural patterns that can be extracted from resource meta-data. we describe the implementation of our method on wikipedia using wordnet categories as our classification schema and ground truth. two structural patterns found in wikipedia are used for training and classification: categories and templates. we apply our system to classifying flickr tags. compared to a wordnet baseline our method increases the coverage of the flickr vocabulary by 115%. we can classify many important entities that are not covered by wordnet, such as,  london eye, big island, ronaldinho, geocaching  and  wii .
1746	4042280	article	proceedings of the national academy of sciences of the united states of america	\N	\N	\N	5	106	9	2009	mar	2009-02-13 08:20:24	\N	ab initio construction of a eukaryotic transcriptome by massively parallel {mrna} sequencing.	defining the transcriptome, the repertoire of transcribed regions encoded in the genome, is a challenging experimental task. current approaches, relying on sequencing of {ests} or {cdna} libraries, are expensive and labor-intensive. here, we present a general approach for ab initio discovery of the complete transcriptome of the budding yeast, based only on the unannotated genome sequence and millions of short reads from a single massively parallel sequencing run. using novel algorithms, we automatically construct a highly accurate transcript catalog. our approach automatically and fully defines 86\\% of the genes expressed under the given conditions, and discovers 160 previously undescribed transcription units of 250 bp or longer. it correctly demarcates the 5' and 3' {utr} boundaries of 86 and 77\\% of expressed genes, respectively. the method further identifies 83\\% of known splice junctions in expressed genes, and discovers 25 previously uncharacterized introns, including 2 cases of condition-dependent intron retention. our framework is applicable to poorly understood organisms, and can lead to greater understanding of the transcribed elements in an explored genome.
1747	4044723	article	plos genetics	\N	\N	public library of science	\N	5	2	2009	feb	2009-02-13 18:32:29	\N	a groupwise association test for rare mutations using a weighted sum statistic.	resequencing is an emerging tool for identification of rare disease-associated mutations. rare mutations are difficult to tag with {snp} genotyping, as genotyping studies are designed to detect common variants. however, studies have shown that genetic heterogeneity is a probable scenario for common diseases, in which multiple rare mutations together explain a large proportion of the genetic basis for the disease. thus, we propose a weighted-sum method to jointly analyse a group of mutations in order to test for groupwise association with disease status. for example, such a group of mutations may result from resequencing a gene. we compare the proposed weighted-sum method to alternative methods and show that it is powerful for identifying disease-associated genes, both on simulated and encode data. using the weighted-sum method, a resequencing study can identify a disease-associated gene with an overall population attributable risk ({par}) of 2\\%, even when each individual mutation has much lower {par}, using 1,000 to 7,000 affected and unaffected individuals, depending on the underlying genetic model. this study thus demonstrates that resequencing studies can identify important genetic associations, provided that specialised analysis methods, such as the weighted-sum method, are used.
1748	4050881	inproceedings	\N	\N	\N	\N	\N	\N	\N	-1	\N	2009-02-14 04:51:13	\N	toward a unified ontology of cloud computing	progress of research efforts in a novel tech- nology is contingent on having a rigorous organization of its knowledge domain and a comprehensive understanding of  all  the  relevant  components  of  this  technology  and their relationships. cloud computing is one contemporary technology in which the research community has recently embarked. manifesting itself as the descendant of several other computing research areas such as service-oriented architecture, distributed and grid computing, and virtual- ization, cloud computing inherits their advancements and limitations. towards the end-goal of a thorough comprehension of the field of cloud computing, and a more rapid adoption from the scientific community, we propose in this paper an ontology of this area which demonstrates a dissection of the cloud into five main layers, and illustrates their inter- relations as well as their inter-dependency on preceding technologies. the contribution of this paper lies in being one of the first attempts to establish a detailed ontology of the cloud. better comprehension of the technology would enable the community to design more efficient portals and gateways for the cloud, and facilitate the adoption of this novel computing approach in scientific environments. in turn, this will assist the scientific community to expedite its contributions and insights into this evolving computing field.
1749	4067975	article	faseb j.	\N	\N	\N	\N	\N	\N	2003	sep	2009-02-18 17:33:03	\N	modulation of dopamine transporter function by \\&alpha;-synuclein is altered by impairment of cell adhesion and by induction of oxidative stress	human -synuclein accumulates in dopaminergic neurons as intraneuronal inclusions, lewy bodies, which are characteristic of idiopathic parkinson's disease ({pd}). here, we suggest that modulation of the functional activity of the dopamine transporter ({dat}) by -synuclein may be a key factor in the preferential degeneration of mesencephalic dopamine ({da})-synthesizing neurons in {pd}. in cotransfected ltk-, {hek} 293, and {sk}-{n-mc} cells, -synuclein induced a 35\\% decrease in [{3h}]{da} uptake. biotinylated {dat} levels were decreased by 40\\% in cotransfected cells relative to cells expressing only {dat}. {dat} was colocalized with -synuclein in mesencephalic neurons and cotransfected ltk- cells. coimmunoprecipitation studies showed the existence of a complex between -synuclein and {dat}, in specific rat brain regions and cotransfected cells, through specific amino acid motifs of both proteins. the attenuation of {dat} function by -synuclein was cytoprotective, because {da}-mediated oxidative stress and cell death were reduced in cotransfected cells. the neurotoxin {mpp}+ (1-methyl-4-phenylpyridinium), oxidative stress, or impairment of cell adhesion ablated the -synuclein-mediated inhibition of {dat} activity, which caused increased uptake of {da} and increased biotinylated {dat} levels, in both mesencephalic neurons and cotransfected cells. these studies suggest a novel normative role for -synuclein in regulating {da} synaptic availability and homeostasis, which is relevant to the pathophysiology of {pd}. key words: parkinson's disease  synucleinopathies  {mpp}+  neurodegeneration  lewy bodies 10.1096/fj.03-0152fje
1750	4073078	article	science (new york, n.y.)	\N	\N	\N	3	323	5917	2009	feb	2009-02-20 04:59:55	\N	stress-inducible regulation of heat shock factor 1 by the deacetylase {sirt1}.	heat shock factor 1 (hsf1) is essential for protecting cells from protein-damaging stress associated with misfolded proteins and regulates the insulin-signaling pathway and aging. here, we show that human hsf1 is inducibly acetylated at a critical residue that negatively regulates dna binding activity. activation of the deacetylase and longevity factor sirt1 prolonged hsf1 binding to the heat shock promoter hsp70 by maintaining hsf1 in a deacetylated, dna-binding competent state. conversely, down-regulation of sirt1 accelerated the attenuation of the heat shock response (hsr) and release of hsf1 from its cognate promoter elements. these results provide a mechanistic basis for the requirement of hsf1 in the regulation of life span and establish a role for sirt1 in protein homeostasis and the hsr. 10.1126/science.1165946
1751	4075274	article	plos comput biol	\N	\N	public library of science	\N	5	2	2009	feb	2009-02-20 19:17:41	\N	allosteric communication occurs via networks of tertiary and quaternary motions in proteins	allosteric proteins bind an effector molecule at one site resulting in a functional change at a second site. we hypothesize that allosteric communication in proteins relies upon networks of quaternary (collective, rigid-body) and tertiary (residue–residue contact) motions. we argue that cyclic topology of these networks is necessary for allosteric communication. an automated algorithm identifies rigid bodies from the displacement between the inactive and the active structures and constructs  ” quaternary networks” from these rigid bodies and the substrate and effector ligands. we then integrate quaternary networks with a coarse-grained representation of contact rearrangements to form  ” global communication networks” ({gcns}). the {gcn} reveals allosteric communication among all substrate and effector sites in 15 of 18 multidomain and multimeric proteins, while tertiary and quaternary networks exhibit such communication in only 4 and 3 of these proteins, respectively. furthermore, in 7 of the 15 proteins connected by the {gcn}, 50\\% or more of the substrate-effector paths via the {gcn} are  ” interdependent” paths that do not exist via either the tertiary or the quaternary network. substrate-effector  ” pathways” typically are not linear but rather consist of polycyclic networks of rigid bodies and clusters of rearranging residue contacts. these results argue for broad applicability of allosteric communication based on structural changes and demonstrate the utility of the {gcn}. global communication networks may inform a variety of experiments on allosteric proteins as well as the design of allostery into non-allosteric proteins. allosteric regulation is a major mechanism of control in many biological processes, including cell signaling, gene regulation, and metabolic regulation, and malfunctioning allosteric proteins are often involved in cancer and other diseases. in allostery, an effector-binding signal transmits over a long distance through the protein structure, resulting in a functional change at a second site. while many three-dimensional structures of allosteric proteins have been solved, the allosteric communication mechanism is usually not obvious from the motions between inactive and active state structures. in addition, allosteric structural transitions involve both small-scale motions at the level of amino acid residues and large-scale motions at the level of domains. here, to address allosteric mechanisms, we transform the aforementioned protein motions into a multi-scale  ” global communication network” ({gcn}) representation from which substrate-effector pathways and other important allosteric communication properties can be identified. the {gcn} accounts for substrate-effector pathways in 15 of 18 proteins surveyed, and the {gcn} reveals that allostery often depends on linkage between the small- and the large-scale motions. this work will inform a wide variety of experiments investigating allostery, and it proposes concepts for engineering allostery into non-allosteric proteins.
1752	4087454	article	nature nanotechnology	\N	\N	nature publishing group	5	4	4	2009	feb	2009-02-23 17:03:17	\N	continuous base identification for single-molecule nanopore {dna} sequencing	a single-molecule method for sequencing {dna} that does not require fluorescent labelling could reduce costs and increase sequencing speeds. an exonuclease enzyme might be used to cleave individual nucleotide molecules from the {dna}, and when coupled to an appropriate detection system, these nucleotides could be identified in the correct order. here, we show that a protein nanopore with a covalently attached adapter molecule can continuously identify unlabelled nucleoside 5'-monophosphate molecules with accuracies averaging 99.8\\%. methylated cytosine can also be distinguished from the four standard {dna} bases: guanine, adenine, thymine and cytosine. the operating conditions are compatible with the exonuclease, and the kinetic data show that the nucleotides have a high probability of translocation through the nanopore and, therefore, of not being registered twice. this highly accurate tool is suitable for integration into a system for sequencing nucleic acids and for analysing epigenetic modifications.
1753	4087856	inproceedings	\N	proceedings of the 2008 acm conference on recommender systems	recsys	acm	7	\N	\N	2008	\N	2009-02-23 21:17:52	new york, ny, usa	personalized, interactive tag recommendation for flickr	we study the problem of personalized, interactive tag recommendation for flickr: while a user enters/selects new tags for a particular picture, the system suggests related tags to her, based on the tags that she or other people have used in the past along with (some of) the tags already entered. the suggested tags are dynamically updated with every additional tag entered/selected. we describe a new algorithm, called hybrid, which can be applied to this problem, and show that it outperforms previous algorithms. it has only a single tunable parameter, which we found to be very robust. apart from this new algorithm and its detailed analysis, our main contributions are (i) a clean methodology which leads to conservative performance estimates, (ii) showing how classical classification algorithms can be applied to this problem, (iii) introducing a new cost measure, which captures the effort of the whole tagging process, (iv) clearly identifying, when purely local schemes (using only a user's tagging history) can or cannot be improved by global schemes (using everybody's tagging history).
1754	4089107	article	genome biology	\N	\N	\N	\N	10	2	2009	feb	2009-02-24 08:14:10	\N	{pemer}: a computational framework with simulation-based error models for inferring genomic structural variants from massive paired-end sequencing data.	personal-genomics endeavors, such as the 1000 genomes project, are generating maps of genomic structural variants by analyzing ends of massively sequenced genome fragments. to process these we developed {paired-end} mapper ({pemer}; http://sv.gersteinlab.org/pemer). this comprises an analysis pipeline, compatible with several next-generation sequencing platforms; simulation-based error models, yielding confidence-values for each structural variant; and a back-end database. the simulations demonstrated high structural variant reconstruction efficiency for {pemer}'s coverage-adjusted multi-cutoff scoring-strategy and showed its relative insensitivity to base-calling errors.
1755	4091149	inproceedings	\N	www	\N	\N	\N	\N	\N	2009	\N	2009-02-24 07:50:13	\N	mapping the world's photos	we investigate how to organize a large collection of geotagged photos, working with a dataset of about 35 million images collected from flickr. our approach combines content analysis based on text tags and image data with structural analysis based on geospatial data. we use the spatial distribution of where people take photos to define a relational structure between the photos that are taken at popular places. we then study the interplay between this structure and the content, using classification methods for predicting such locations from visual, textual and temporal features of the photos. we find that visual and temporal features improve the ability to estimate the location of a photo, compared to using just textual features. we illustrate using these techniques to organize a large photo collection, while also revealing various interesting properties about popular cities and landmarks at a global scale.
1756	4096561	article	briefings in bioinformatics	\N	\N	oxford university press	17	10	3	2009	may	2009-02-25 05:46:30	\N	a roadmap of clustering algorithms: finding a match for a biomedical application	clustering is ubiquitously applied in bioinformatics with hierarchical clustering and k-means partitioning being the most popular methods. numerous improvements of these two clustering methods have been introduced, as well as completely different approaches such as grid-based, density-based and model-based clustering. for improved bioinformatics analysis of data, it is important to match clusterings to the requirements of a biomedical application. in this article, we present a set of desirable clustering features that are used as evaluation criteria for clustering algorithms. we review 40 different clustering algorithms of all approaches and datatypes. we compare algorithms on the basis of desirable clustering features, and outline algorithms' benefits and drawbacks as a basis for matching them to biomedical applications.
1757	4122655	article	portal: libraries and the academy	\N	\N	the johns hopkins university press	19	9	1	2009	\N	2009-03-03 09:31:56	\N	google scholar search performance: comparative recall and precision	this paper presents a comparative evaluation of google scholar and 11 other bibliographic databases (academic search elite, ageline, articlefirst, econlit, geobase, medline, pais international, popline, social sciences abstracts, social sciences citation index, and socindex), focusing on search performance within the multidisciplinary field of later-life migration. the results of simple keyword searches are evaluated with reference to a set of 155 relevant articles identified in advance. in terms of both recall and precision, google scholar performs better than most of the subscription databases. this finding, based on a rigorous evaluation procedure, is contrary to the impressions of many early reviewers. the paper concludes with a discussion of a new approach to document relevance in educational settings--an approach that accounts for the instructors' goals as well as the students' assessments of relevance. (contains 4 tables, 2 figures, and 29 notes.)
1758	4129082	article	plos biol	\N	\N	public library of science	\N	7	3	2009	mar	2009-03-03 20:24:07	\N	comparative functional analysis of the caenorhabditis elegans and drosophila melanogaster proteomes	the nematode caenorhabditis elegans is a popular model system in genetics, not least because a majority of human disease genes are conserved in c. elegans . to generate a comprehensive inventory of its expressed proteome, we performed extensive shotgun proteomics and identified more than half of all predicted c. elegans proteins. this allowed us to confirm and extend genome annotations, characterize the role of operons in c. elegans , and semiquantitatively infer abundance levels for thousands of proteins. furthermore, for the first time to our knowledge, we were able to compare two animal proteomes ( c. elegans and drosophila melanogaster ). we found that the abundances of orthologous proteins in metazoans correlate remarkably well, better than protein abundance versus transcript abundance within each organism or transcript abundances across organisms; this suggests that changes in transcript abundance may have been partially offset during evolution by opposing changes in protein abundance.
1759	4133229	article	plos one	\N	\N	public library of science	\N	4	3	2009	mar	2009-03-04 22:13:05	\N	big genomes facilitate the comparative identification of regulatory elements	the identification of regulatory sequences in animal genomes remains a significant challenge. comparative genomic methods that use patterns of evolutionary conservation to identify non-coding sequences with regulatory function have yielded many new vertebrate enhancers. however, these methods have not contributed significantly to the identification of regulatory sequences in sequenced invertebrate taxa. we demonstrate here that this differential success, which is often attributed to fundamental differences in the nature of vertebrate and invertebrate regulatory sequences, is instead primarily a product of the relatively small size of sequenced invertebrate genomes. we sequenced and compared loci involved in early embryonic patterning from four species of true fruit flies (family tephritidae) that have genomes four to six times larger than those of drosophila melanogaster. unlike in drosophila, where virtually all non-coding {dna} is highly conserved, blocks of conserved non-coding sequence in tephritids are flanked by large stretches of poorly conserved sequence, similar to what is observed in vertebrate genomes. we tested the activities of nine conserved non-coding sequences flanking the even-skipped gene of the teprhitid ceratis capitata in transgenic d. melanogaster embryos, six of which drove patterns that recapitulate those of known d. melanogaster enhancers. in contrast, none of the three non-conserved tephritid non-coding sequences that we tested drove expression in d. melanogaster embryos. based on the landscape of non-coding conservation in tephritids, and our initial success in using conservation in tephritids to identify d. melanogaster regulatory sequences, we suggest that comparison of tephritid genomes may provide a systematic means to annotate the non-coding portion of the d. melanogaster genome. we also propose that large genomes be given more consideration in the selection of species for comparative genomics projects, to provide increased power to detect functional non-coding {dnas} and to provide a less biased view of the evolution and function of animal genomes.
1760	4141636	article	nature genetics	\N	\N	\N	\N	\N	\N	2009	mar	2009-03-06 09:56:56	\N	intrinsic variability of gene expression encoded in nucleosome positioning sequences.	variation in gene expression is an essential material for biological diversity among single cells, individuals and populations or species. here we show that expression variability is an intrinsic property that persists at those different levels. each promoter seems to have a unique capacity to respond to external signals that can be environmental, genetic or even stochastic. our investigation into nucleosome organization of variably responding promoters revealed a commonly positioned nucleosome at a critical regulatory region where most transcription start sites and {tata} elements are located, a deviation from typical nucleosome-free status. the nucleotide sequences in this region of variable promoters showed a high propensity for {dna} bending and a periodic distribution of particular dinucleotides, encoding preferences for {dna}-nucleosome interaction. variable expression is likely to occur during removal of this nucleosome for gene activation. this is a unique example of how promoter sequences intrinsically encode regulatory flexibility, which is vital for biological processes such as adaptation, development and evolution.
1761	4154311	inproceedings	\N	proceedings of www09	\N	\N	\N	\N	\N	2009	\N	2009-03-09 08:47:16	\N	learning to tag	social tagging provides valuable and crucial information for large-scale web image retrieval. it is ontology-free and easy to obtain; however, irrelevant tags frequently appear, and users typically will not tag all semantic objects in the image, which is also called semantic loss. to avoid noises and compensate for the semantic loss, tag recommendation is proposed in literature. however, current recommendation simply ranks the related tags based on the single modality of tag co-occurrence on the whole dataset, which ignores other modalities, such as visual correlation. this paper proposes a multi-modality recommendation based on both tag and visual correlation, and formulates the tag recommendation as a learning problem. each modality is used to generate a ranking feature, and rankboost algorithm is applied to learn an optimal combination of these ranking features from different modalities. experiments on flickr data demonstrate the effectiveness of this learning-based multi-modality recommendation strategy.
1762	4164691	article	brief bioinform	\N	\N	\N	10	10	4	2009	jul	2009-03-22 23:13:22	\N	computational systems biology of the cell cycle	one of the early success stories of computational systems biology was the work done on cell-cycle regulation. the earliest mathematical descriptions of cell-cycle control evolved into very complex, detailed computational models that describe the regulation of cell division in many different cell types. on the way these models predicted several dynamical properties and unknown components of the system that were later experimentally verified/identified. still, research on this field is far from over. we need to understand how the core cell-cycle machinery is controlled by internal and external signals, also in yeast cells and in the more complex regulatory networks of higher eukaryotes. furthermore, there are many computational challenges what we face as new types of data appear thanks to continuing advances in experimental techniques. we have to deal with cell-to-cell variations, revealed by single cell measurements, as well as the tremendous amount of data flowing from high throughput machines. we need new computational concepts and tools to handle these data and develop more detailed, more precise models of cell-cycle regulation in various organisms. here we review past and present of computational modeling of cell-cycle regulation, and discuss possible future directions of the field. 10.1093/bib/bbp005
1763	4170452	article	physical review e	\N	\N	american physical society	\N	80	1	2009	jul	2009-03-13 10:26:40	\N	line graphs, link partitions, and overlapping communities	in this paper, we use a partition of the links of a network in order to uncover its community structure. this approach allows for communities to overlap at nodes so that nodes may be in more than one community. we do this by making a node partition of the line graph of the original network. in this way we show that any algorithm that produces a partition of nodes can be used to produce a partition of links. we discuss the role of the degree heterogeneity and propose a weighted version of the line graph in order to account for this.
1764	4182419	article	trends in ecology \\& evolution	\N	\N	\N	8	24	4	2009	apr	2009-03-16 14:47:44	\N	harnessing genomics for evolutionary insights	next-generation {dna} sequencing technologies can generate unprecedented amounts of genomic data, even for non-model organisms. here we describe how these new technologies have facilitated recent key advances in ecology and evolutionary biology, and highlight several outstanding ecological and evolutionary questions that are distinctly suited to the innovations they provide. importantly, using these technologies to their full potential requires careful experimental design and critical consideration of several caveats associated with them. although several significant challenges remain to be resolved before the integration of next-generation sequencing technologies into single-investigator research programs, we argue that they will soon transform ecology and evolution by fundamentally changing the ranges and types of questions that can be addressed.
1765	4186671	article	health affairs	\N	\N	\N	7	28	2	2009	mar	2009-03-17 15:45:08	\N	take two aspirin and tweet me in the morning: how twitter, facebook, and other social media are reshaping health care	if you want a glimpse of what health care could look like a few years from now, consider  ” hello health,” the brooklyn-based primary care practice that is fast becoming an emblem of modern medicine. a paperless, concierge practice that eschews the limitations of insurance-based medicine, hello health is popular and successful, largely because of the powerful and cost-effective communication tools it employs: web-based social media. indeed, across the health care industry, from large hospital networks to patient support groups, new media tools like weblogs, instant messaging platforms, video chat, and social networks are reengineering the way doctors and patients interact.
1766	4199626	article	briefings in bioinformatics	\N	\N	\N	15	10	2	2009	mar	2009-03-20 17:00:50	\N	semantic web for integrated network analysis in biomedicine	the semantic web technology enables integration of heterogeneous data on the world wide web by making the semantics of data explicit through formal ontologies. in this article, we survey the feasibility and state of the art of utilizing the semantic web technology to represent, integrate and analyze the knowledge in various biomedical networks. we introduce a new conceptual framework, semantic graph mining, to enable researchers to integrate graph mining with ontology reasoning in network data analysis. through four case studies, we demonstrate how semantic graph mining can be applied to the analysis of disease-causal genes, gene ontology category cross-talks, drug efficacy analysis and herb–drug interactions analysis.
1767	4202153	article	science	\N	\N	american association for the advancement of science	5	324	5925	2009	apr	2009-03-21 00:46:00	\N	optical deconstruction of parkinsonian neural circuitry	deep brain stimulation ({dbs}) is a therapeutic option for intractable neurological and psychiatric disorders, including parkinson's disease and major depression. because of the heterogeneity of brain tissues where electrodes are placed, it has been challenging to elucidate the relevant target cell types or underlying mechanisms of {dbs}. we used optogenetics and solid-state optics to systematically drive or inhibit an array of distinct circuit elements in freely moving parkinsonian rodents and found that therapeutic effects within the subthalamic nucleus can be accounted for by direct selective stimulation of afferent axons projecting to this region. in addition to providing insight into {dbs} mechanisms, these results demonstrate an optical approach for dissection of disease circuitry and define the technological toolbox needed for systematic deconstruction of disease circuits by selectively controlling individual components.
1768	4202177	inproceedings	\N	www 2009,	\N	\N	\N	\N	\N	2009	apr	2009-03-21 00:59:37	\N	evaluating similarity measures for emergent semantics of social tagging	social bookmarking systems and their emergent information structures, known as folksonomies, are increasingly important data sources for semantic web applications. a key question for harvesting semantics from these systems is how to extend and adapt traditional notions of similarity to folksonomies, and which measures are best suited for applications such as navigation support, semantic search, and ontology learning. here we build an evaluation framework to compare various general folksonomy-based similarity measures derived from established information-theoretic, statistical, and practical measures. our framework deals generally and symmetrically with users, tags, and resources. for evaluation purposes we focus on similarity among tags and resources, considering different ways to aggregate annotations across users. after comparing how tag similarity measures predict user-created tag relations, we provide an external grounding by user-validated semantic proxies based on {wordnet} and the open directory. we also investigate the issue of scalability. we find that mutual information with distributional micro-aggregation across users yields the highest accuracy, but is not scalable; per-user projection with collaborative aggregation provides the best scalable approach via incremental computations. the results are consistent across resource and tag similarity.
1769	4214142	article	briefings in bioinformatics	\N	\N	\N	14	10	3	2009	may	2009-03-24 12:06:41	\N	an introduction to artificial neural networks in bioinformatics—application to complex microarray and mass spectrometry datasets in cancer studies	applications of genomic and proteomic technologies have seen a major increase, resulting in an explosion in the amount of highly dimensional and complex data being generated. subsequently this has increased the effort by the bioinformatics community to develop novel computational approaches that allow for meaningful information to be extracted. this information must be of biological relevance and thus correlate to disease phenotypes of interest. artificial neural networks are a form of machine learning from the field of artificial intelligence with proven pattern recognition capabilities and have been utilized in many areas of bioinformatics. this is due to their ability to cope with highly dimensional complex datasets such as those developed by protein mass spectrometry and {dna} microarray experiments. as such, neural networks have been applied to problems such as disease classification and identification of biomarkers. this review introduces and describes the concepts related to neural networks, the advantages and caveats to their use, examples of their applications in mass spectrometry and microarray research (with a particular focus on cancer studies), and illustrations from recent literature showing where neural networks have performed well in comparison to other machine learning methods. this should form the necessary background knowledge and information enabling researchers with an interest in these methodologies, but not necessarily from a machine learning background, to apply the concepts to their own datasets, thus maximizing the information gain from these complex biological systems.
1770	4214865	article	acm comput. surv.	\N	\N	acm	30	41	2	2009	feb	2009-03-24 16:52:07	new york, ny, usa	web page classification: features and algorithms	classification of web page content is essential to many tasks in web information retrieval such as maintaining web directories and focused crawling. the uncontrolled nature of web content presents additional challenges to web page classification as compared to traditional text classification, but the interconnected nature of hypertext also provides features that can assist the process. as we review work in web page classification, we note the importance of these web-specific features and algorithms, describe state-of-the-art practices, and track the underlying assumptions behind the use of information from neighboring pages.
1771	4233160	article	nature genetics	\N	\N	nature publishing group	2	41	4	2009	apr	2009-03-27 18:21:49	\N	human mutation rate associated with {dna} replication timing.	eukaryotic {dna} replication is highly stratified, with different genomic regions shown to replicate at characteristic times during s phase. here we observe that mutation rate, as reflected in recent evolutionary divergence and human nucleotide diversity, is markedly increased in later-replicating regions of the human genome. all classes of substitutions are affected, suggesting a generalized mechanism involving replication time-dependent {dna} damage. this correlation between mutation rate and regionally stratified replication timing may have substantial evolutionary implications.
1772	4267889	article	science	\N	\N	\N	4	324	5923	2009	apr	2009-04-03 04:42:58	\N	a bipedal {dna} brownian motor with coordinated legs	a substantial challenge in engineering molecular motors is designing mechanisms to coordinate the motion between multiple domains of the motor so as to bias random thermal motion. for bipedal motors, this challenge takes the form of coordinating the movement of the biped's legs so that they can move in a synchronized fashion. to address this problem, we have constructed an autonomous {dna} bipedal walker that coordinates the action of its two legs by cyclically catalyzing the hybridization of metastable {dna} fuel strands. this process leads to a chemically ratcheted walk along a directionally polar {dna} track. by covalently cross-linking aliquots of the walker to its track in successive walking states, we demonstrate that this brownian motor can complete a full walking cycle on a track whose length could be extended for longer walks. we believe that this study helps to uncover principles behind the design of unidirectional devices that can function without intervention. this device should be able to fulfill roles that entail the performance of useful mechanical work on the nanometer scale. 10.1126/science.1170336
1773	4281711	article	\N	\N	\N	\N	\N	\N	\N	2009	sep	2009-04-07 10:27:53	\N	cosmology of the lifshitz universe	we study the ultraviolet complete non-relativistic theory recently proposed by horava. after introducing a lifshitz scalar for a general background, we analyze the cosmology of the model in lorentzian and euclidean signature. vacuum solutions are found and it is argued the existence of non-singular bouncing profiles. we find a general qualitative agreement with both the picture of causal dynamical triangulations and quantum einstein gravity. however, inflation driven by a lifshitz scalar field on a classical background might not produce a scale-invariant spectrum when the principle of detailed balance is assumed.
1774	4293788	article	bioinformatics	\N	\N	oxford university press	1	25	12	2009	jun	2009-04-09 09:10:58	\N	{webgbrowse}—a web server for {gbrowse}	summary: the generic genome browser ({gbrowse}) is one of the most widely used tools for visualizing genomic features along a reference sequence. however, the installation and configuration of {gbrowse} is not trivial for biologists. we have developed a web server, {webgbrowse} that allows users to upload genome annotation in the {gff3} format, configure the display of each genomic feature by simply using a web browser and visualize the configured genomic features with the integrated {gbrowse} software.
1775	4295674	article	science	\N	\N	american association for the advancement of science	2	324	5924	2009	apr	2009-04-10 02:17:17	\N	trapping moving targets with small molecules	structure-based drug design traditionally uses static protein models as inspirations for focusing on  ” active” site targets. allosteric regulation of biological macromolecules, however, is affected by both conformational and dynamic properties of the protein or protein complex and can potentially lead to more avenues for therapeutic development. we discuss the advantages of searching for molecules that conformationally trap a macromolecule in its inactive state. although multiple methodologies exist to probe protein dynamics and ligand binding, our current discussion highlights the use of nuclear magnetic resonance spectroscopy in the drug discovery and design process.
1776	4298633	article	j. proteome res.	\N	\N	american chemical society	5	8	6	2009	jun	2009-04-10 18:34:17	\N	low cost, scalable proteomics data analysis using amazon's cloud computing services and open source search algorithms	one of the major difficulties for many laboratories setting up proteomics programs has been obtaining and maintaining the computational infrastructure required for the analysis of the large flow of proteomics data. we describe a system that combines distributed cloud computing and open source software to allow laboratories to set up scalable virtual proteomics analysis clusters without the investment in computational hardware or software licensing fees. additionally, the pricing structure of distributed computing providers, such as amazon web services, allows laboratories or even individuals to have large-scale computational resources at their disposal at a very low cost per run. we provide detailed step-by-step instructions on how to implement the virtual proteomics analysis clusters as well as a list of current available preconfigured amazon machine images containing the {omssa} and {x!tandem} search algorithms and sequence databases on the medical college of wisconsin proteomics center web site (http://proteomics.mcw.edu/vipdac).
1777	4340822	misc	\N	\N	\N	\N	\N	\N	\N	2009	apr	2009-04-17 05:30:27	\N	an introduction to quantum error correction and {fault-tolerant} quantum computation	quantum states are very delicate, so it is likely some sort of quantum error correction will be necessary to build reliable quantum computers. the theory of quantum error-correcting codes has some close ties to and some striking differences from the theory of classical error-correcting codes. many quantum codes can be described in terms of the stabilizer of the codewords. the stabilizer is a finite abelian group, and allows a straightforward characterization of the error-correcting properties of the code. the stabilizer formalism for quantum codes also illustrates the relationships to classical coding theory, particularly classical codes over gf(4), the finite field with four elements. to build a quantum computer which behaves correctly in the presence of errors, we also need a theory of fault-tolerant quantum computation, instructing us how to perform quantum gates on qubits which are encoded in a quantum error-correcting code. the threshold theorem states that it is possible to create a quantum computer to perform an arbitrary quantum computation provided the error rate per physical gate or time step is below some constant threshold value.
1778	4381383	article	briefings in bioinformatics	\N	\N	\N	11	10	3	2009	may	2009-04-23 02:21:11	\N	{2d} molecular graphics: a flattened world of chemistry and biology.	molecular graphics provides an intuitive way for representation, modeling and analysis of complex chemical and biological systems. it is now widely used in the theoretical chemistry, structural biology, molecular modeling and drug design communities. traditional molecular graphics techniques mainly dedicate to showing molecular architectures at three-dimensional ({3d}) level. however, in some occasions the two-dimensional ({2d}) representation of molecular configurations, profiles, behaviors and interactions may be more readily acceptable for audiences, especially when we need to describe abstract information in a straightforward way or to present numerous data in schematic diagrams. in recent years, {2d} representation methods/tools have been developed rapidly for various purposes, ranging from the aesthetic depiction of atomic arrangement for small organic molecules to schematic layout of complicated nonbonding network across the biomolecular binding interfaces, and have received considerable interest in the fields of chemistry, biology and medicine. in this article we first propose the term of {2d} molecular graphics to cover the spectrum of {2d} representing chemical and biological systems, we also give a comprehensive review on the methods, tools and applications of {2d} molecular graphics.
1779	4381405	article	plos one	\N	\N	public library of science	\N	4	4	2009	apr	2009-04-23 02:42:13	\N	assembling the marine metagenome, one cell at a time	the difficulty associated with the cultivation of most microorganisms and the complexity of natural microbial assemblages, such as marine plankton or human microbiome, hinder genome reconstruction of representative taxa using cultivation or metagenomic approaches. here we used an alternative, single cell sequencing approach to obtain high-quality genome assemblies of two uncultured, numerically significant marine microorganisms. we employed fluorescence-activated cell sorting and multiple displacement amplification to obtain hundreds of micrograms of genomic {dna} from individual, uncultured cells of two marine flavobacteria from the gulf of maine that were phylogenetically distant from existing cultured strains. shotgun sequencing and genome finishing yielded 1.9 mbp in 17 contigs and 1.5 mbp in 21 contigs for the two flavobacteria, with estimated genome recoveries of about 91\\% and 78\\%, respectively. only 0.24\\% of the assembling sequences were contaminants and were removed from further analysis using rigorous quality control. in contrast to all cultured strains of marine flavobacteria, the two single cell genomes were excellent global ocean sampling ({gos}) metagenome fragment recruiters, demonstrating their numerical significance in the ocean. the geographic distribution of {gos} recruits along the northwest atlantic coast coincided with ocean surface currents. metabolic reconstruction indicated diverse potential energy sources, including biopolymer degradation, proteorhodopsin photometabolism, and hydrogen oxidation. compared to cultured relatives, the two uncultured flavobacteria have small genome sizes, few non-coding nucleotides, and few paralogous genes, suggesting adaptations to narrow ecological niches. these features may have contributed to the abundance of the two taxa in specific regions of the ocean, and may have hindered their cultivation. we demonstrate the power of single cell {dna} sequencing to generate reference genomes of uncultured taxa from a complex microbial community of marine bacterioplankton. a combination of single cell genomics and metagenomics enabled us to analyze the genome content, metabolic adaptations, and biogeography of these taxa.
1780	4404756	article	trends in genetics	\N	\N	\N	6	25	5	2009	may	2009-04-26 03:28:21	\N	how confident can we be that orthologs are similar, but paralogs differ?	homologous genes are classified into orthologs and paralogs, depending on whether they arose by speciation or duplication. it is widely assumed that orthologs share similar functions, whereas paralogs are expected to diverge more from each other. but does this assumption hold up on further examination? we present evidence that orthologs and paralogs are not so different in either their evolutionary rates or their mechanisms of divergence. we emphasize the importance of appropriately designed studies to test models of gene evolution between orthologs and between paralogs. thus, functional change between orthologs might be as common as between paralogs, and future studies should be designed to test the impact of duplication against this alternative model.
1781	4447374	article	science	\N	\N	american association for the advancement of science	9	324	5930	2009	may	2009-05-01 08:59:42	\N	the genetic structure and history of africans and african americans	africa is the source of all modern humans, but characterization of genetic variation and of relationships among populations across the continent has been enigmatic. we studied 121 african populations, four african american populations, and 60 {non-african} populations for patterns of variation at 1327 nuclear microsatellite and insertion/deletion markers. we identified 14 ancestral population clusters in africa that correlate with self-described ethnicity and shared cultural and/or linguistic properties. we observed high levels of mixed ancestry in most populations, reflecting historical migration events across the continent. our data also provide evidence for shared ancestry among geographically diverse hunter-gatherer populations (khoesan speakers and pygmies). the ancestry of african americans is predominantly from {niger-kordofanian} (\\~{}71\\%), european (\\~{}13\\%), and other african (\\~{}8\\%) populations, although admixture levels varied considerably among individuals. this study helps tease apart the complex evolutionary history of africans and african americans, aiding both anthropological and genetic epidemiologic studies.
1782	4462135	article	embo reports	\N	\N	nature publishing group	5	7	7	2006	jul	2009-05-04 17:52:25	\N	the gut flora as a forgotten organ.	the intestinal microflora is a positive health asset that crucially influences the normal structural and functional development of the mucosal immune system. mucosal immune responses to resident intestinal microflora require precise control and an immunosensory capacity for distinguishing commensal from pathogenic bacteria. in genetically susceptible individuals, some components of the flora can become a liability and contribute to the pathogenesis of various intestinal disorders, including inflammatory bowel diseases. it follows that manipulation of the flora to enhance the beneficial components represents a promising therapeutic strategy. the flora has a collective metabolic activity equal to a virtual organ within an organ, and the mechanisms underlying the conditioning influence of the bacteria on mucosal homeostasis and immune responses are beginning to be unravelled. an improved understanding of this hidden organ will reveal secrets that are relevant to human health and to several infectious, inflammatory and neoplastic disease processes.
1783	4486449	inproceedings	\N	recsys	\N	acm	1	\N	\N	2008	\N	2009-05-07 13:41:41	new york, ny, usa	context-aware recommender systems	the importance of contextual information has been recognized by researchers and practitioners in many disciplines, including e-commerce personalization, information retrieval, ubiquitous and mobile computing, data mining, marketing, and management. while a substantial amount of research has already been performed in the area of recommender systems, most existing approaches focus on recommending the most relevant items to users without taking into account any additional contextual information, such as time, location, or the company of other people (e.g., for watching movies or dining out). in this chapter we argue that relevant contextual information does matter in recommender systems and that it is important to take this information into account when providing recommendations. we discuss the general notion of context and how it can be modeled in recommender systems. furthermore, we introduce three different algorithmic paradigms â€“ contextual pre-filtering, post-filtering, and modeling â€“ for incorporating contextual information into the recommendation process, discuss the possibilities of combining several context-aware recommendation techniques into a single unifying approach, and provide a case study of one such combined approach. finally, we present additional capabilities for context-aware recommenders and discuss important and promising directions for future research.
1784	4510581	article	molecular biology and evolution	\N	\N	oxford university press	9	26	8	2009	aug	2009-05-13 18:28:20	\N	{indelible}: a flexible simulator of biological sequence evolution	many methods exist for reconstructing phylogenies from molecular sequence data, but few phylogenies are known and can be used to check their efficacy. simulation remains the most important approach to testing the accuracy and robustness of phylogenetic inference methods. however, current simulation programs are limited, especially concerning realistic models for simulating insertions and deletions. we implement a portable and flexible application, named {indelible}, for generating nucleotide, amino acid and codon sequence data by simulating insertions and deletions (indels) as well as substitutions. indels are simulated under several models of indel-length distribution. the program implements a rich repertoire of substitution models, including the general unrestricted model and nonstationary nonhomogeneous models of nucleotide substitution, mixture, and partition models that account for heterogeneity among sites, and codon models that allow the nonsynonymous/synonymous substitution rate ratio to vary among sites and branches. with its many unique features, {indelible} should be useful for evaluating the performance of many inference methods, including those for multiple sequence alignment, phylogenetic tree inference, and ancestral sequence, or genome reconstruction.
1785	4525735	article	bmc bioinformatics	\N	\N	\N	\N	10	Suppl 5	2009	\N	2009-05-16 09:17:39	\N	{bowiki}: an ontology-based wiki for annotation of data and integration of knowledge in biology	{motivation}:ontology development and the annotation of biological data using ontologies are time-consuming exercises that currently require input from expert curators. open, collaborative platforms for biological data annotation enable the wider scientific community to become involved in developing and maintaining such resources. however, this openness raises concerns regarding the quality and correctness of the information added to these knowledge bases. the combination of a collaborative web-based platform with logic-based approaches and semantic web technology can be used to address some of these challenges and {concerns.results}:we have developed the {bowiki}, a web-based system that includes a biological core ontology. the core ontology provides background knowledge about biological types and relations. against this background, an automated reasoner assesses the consistency of new information added to the knowledge base. the system provides a platform for research communities to integrate information and annotate data {collaboratively.availability}:the {bowiki} and supplementary material is available at http://www.bowiki.net/. the source code is available under the {gnu} {gpl} from {http://onto.eva.mpg.de/trac/bowiki}.
1786	4532761	book	\N	\N	\N	o'reilly media	\N	\N	\N	2009	jul	2009-05-17 21:05:39	\N	beautiful data: the stories behind elegant data solutions	in this insightful book, you'll learn from the best data practitioners in the field just how wide-ranging -- and beautiful -- working with data can be. join 39 contributors as they explain how they developed simple and elegant solutions on projects ranging from the mars lander to a radiohead video. with beautiful data, you will: explore the opportunities and challenges involved in working with the vast number of datasets made available by the web learn how to visualize trends in urban crime, using maps and data mashups discover the challenges of designing a data processing system that works within the constraints of space travel learn how crowdsourcing and transparency have combined to advance the state of drug research understand how new data can automatically trigger alerts when it matches or overlaps pre-existing data learn about the massive infrastructure required to create, capture, and process dna data  that's only small sample of what you'll find in beautiful data. for anyone who handles data, this is a truly fascinating book. contributors include: nathan yau jonathan follett and matt holm j.m. hughes raghu ramakrishnan, brian cooper, and utkarsh srivastava jeff hammerbacher jason dykes and jo wood jeff jonas and lisa sokol jud valeski alon halevy and jayant madhavan aaron koblin and valdean klump michal migurski jeff heer coco krumme peter norvig matt wood and ben blackburne jean-claude bradley, rajarshi guha, andrew lang, pierre lindenbaum, cameron neylon, antony williams, and egon willighagen lukas biewald and brendan o'connor hadley wickham, deborah swayne, and david poole andrew gelman, jonathan p. kastellec, and yair ghitza toby segaran
1787	4537934	article	bmc bioinformatics	\N	\N	\N	\N	10	Suppl 5	2009	\N	2009-05-18 12:29:28	\N	issues in learning an ontology from text	:ontology construction for any domain is a labour intensive and complex process. any methodology that can reduce the cost and increase efficiency has the potential to make a major impact in the life sciences. this paper describes an experiment in ontology construction from text for the animal behaviour domain. our objective was to see how much could be done in a simple and relatively rapid manner using a corpus of journal papers. we used a sequence of pre-existing text processing steps, and here describe the different choices made to clean the input, to derive a set of terms and to structure those terms in a number of hierarchies. we describe some of the challenges, especially that of focusing the ontology appropriately given a starting point of a heterogeneous {corpus.results}:using mainly automated techniques, we were able to construct an 18055 term ontology-like structure with 73\\% recall of animal behaviour terms, but a precision of only 26\\%. we were able to clean unwanted terms from the nascent ontology using lexico-syntactic patterns that tested the validity of term inclusion within the ontology. we used the same technique to test for subsumption relationships between the remaining terms to add structure to the initially broad and shallow structure we generated. all outputs are available at {http://thirlmere.aston.ac.uk/\\~{}kiffer/animalbehaviour/.conclusion}:we present a systematic method for the initial steps of ontology or structured vocabulary construction for scientific domains that requires limited human effort and can make a contribution both to ontology learning and maintenance. the method is useful both for the exploration of a scientific domain and as a stepping stone towards formally rigourous ontologies. the filtering of recognised terms from a heterogeneous corpus to focus upon those that are the topic of the ontology is identified to be one of the main challenges for research in ontology learning.
1788	4544030	article	bioinformatics	bioinformatics	\N	oxford university press	6	25	14	2009	jul	2009-05-19 00:33:55	\N	hierarchical hidden markov model with application to joint analysis of {chip}-chip and {chip}-seq data	motivation: chromatin immunoprecipitation ({chip}) experiments followed by array hybridization, or {chip}-chip, is a powerful approach for identifying transcription factor binding sites ({tfbs}) and has been widely used. recently, massively parallel sequencing coupled with {chip} experiments ({chip}-seq) has been increasingly used as an alternative to {chip}-chip, offering cost-effective genome-wide coverage and resolution up to a single base pair. for many well-studied {tfs}, both {chip}-seq and {chip}-chip experiments have been applied and their data are publicly available. previous analyses have revealed substantial technology-specific binding signals despite strong correlation between the two sets of results. therefore, it is of interest to see whether the two data sources can be combined to enhance the detection of {tfbs}.results: in this work, hierarchical hidden markov model ({hhmm}) is proposed for combining data from {chip}-seq and {chip}-chip. in {hhmm}, inference results from individual {hmms} in {chip}-seq and {chip}-chip experiments are summarized by a higher level {hmm}. simulation studies show the advantage of {hhmm} when data from both technologies co-exist. analysis of two well-studied {tfs}, {nrsf} and {ccctc}-binding factor ({ctcf}), also suggests that {hhmm} yields improved {tfbs} identification in comparison to analyses using individual data sources or a simple merger of the {two.availability}: source code for the software {chipmeta} is freely available for download at {http://www.umich.edu/?hwchoi/hhmmsoftware}.zip, implemented in c and supported on {linux.contact}: ghoshd@psu.edu; {qin@umich.edusupplementary} information: supplementary data are available at bioinformatics online.
1789	4544032	article	bioinformatics	\N	\N	oxford university press	6	25	14	2009	jul	2009-05-19 00:37:13	\N	fast and accurate short read alignment with {burrows–wheeler} transform	motivation: the enormous amount of short reads generated by the new {dna} sequencing technologies call for the development of fast and accurate read alignment programs. a first generation of hash table-based methods has been developed, including {maq}, which is accurate, feature rich and fast enough to align short reads from a single individual. however, {maq} does not support gapped alignment for single-end reads, which makes it unsuitable for alignment of longer reads where indels may occur frequently. the speed of {maq} is also a concern when the alignment is scaled up to the resequencing of hundreds of individuals.
1790	4545636	inproceedings	\N	proceedings of the 27th international conference on human factors in computing systems	chi	acm	9	\N	\N	2009	\N	2009-05-19 17:38:25	new york, ny, usa	make new friends, but keep the old: recommending people on social networking sites	this paper studies people recommendations designed to help users find known, offline contacts and discover new friends on social networking sites. we evaluated four recommender algorithms in an enterprise social networking site using a personalized survey of 500 users and a field study of 3,000 users. we found all algorithms effective in expanding users' friend lists. algorithms based on social network information were able to produce better-received recommendations and find more known contacts for users, while algorithms using similarity of user-created content were stronger in discovering new friends. we also collected qualitative feedback from our survey users and draw several meaningful design implications.
1791	4673368	article	bioinformatics	\N	\N	oxford university press	8	25	12	2009	jun	2009-05-29 05:18:52	\N	{keller}: estimating time-varying interactions between genes	motivation: gene regulatory networks underlying temporal processes, such as the cell cycle or the life cycle of an organism, can exhibit significant topological changes to facilitate the underlying dynamic regulatory functions. thus, it is essential to develop methods that capture the temporal evolution of the regulatory networks. these methods will be an enabling first step for studying the driving forces underlying the dynamic gene regulation circuitry and predicting the future network structures in response to internal and external stimuli.
1792	4674775	article	science (new york, n.y.)	\N	\N	american association for the advancement of science	4	324	5931	2009	may	2009-05-29 08:40:45	\N	the computation of social behavior.	neuroscientists are beginning to advance explanations of social behavior in terms of underlying brain mechanisms. two distinct networks of brain regions have come to the fore. the first involves brain regions that are concerned with learning about reward and reinforcement. these same reward-related brain areas also mediate preferences that are social in nature even when no direct reward is expected. the second network focuses on regions active when a person must make estimates of another person's intentions. however, it has been difficult to determine the precise roles of individual brain regions within these networks or how activities in the two networks relate to one another. some recent studies of reward-guided behavior have described brain activity in terms of formal mathematical models; these models can be extended to describe mechanisms that underlie complex social exchange. such a mathematical formalism defines explicit mechanistic hypotheses about internal computations underlying regional brain activity, provides a framework in which to relate different types of activity and understand their contributions to behavior, and prescribes strategies for performing experiments under strong control. 10.1126/science.1169694
1793	4680712	article	science (new york, n.y.)	\N	\N	american association for the advancement of science	3	324	5931	2009	may	2009-06-02 00:27:12	\N	high-frequency, long-range coupling between prefrontal and visual cortex during attention.	electrical recordings in humans and monkeys show attentional enhancement of evoked responses and gamma synchrony in ventral stream cortical areas. does this synchrony result from intrinsic activity in visual cortex or from inputs from other structures? using paired recordings in the frontal eye field ({fef}) and area v4, we found that attention to a stimulus in their joint receptive field leads to enhanced oscillatory coupling between the two areas, particularly at gamma frequencies. this coupling appeared to be initiated by {fef} and was time-shifted by about 8 to 13 milliseconds across a range of frequencies. considering the expected conduction and synaptic delays between the areas, this time-shifted coupling at gamma frequencies may optimize the postsynaptic impact of spikes from one area upon the other, improving cross-area communication with attention.
1794	4680784	article	plos comput biol	\N	\N	public library of science	\N	5	5	2009	may	2009-05-30 00:32:40	\N	brain anatomical network and intelligence	intuitively, higher intelligence might be assumed to correspond to more efficient information transfer in the brain, but no direct evidence has been reported from the perspective of brain networks. in this study, we performed extensive analyses to test the hypothesis that individual differences in intelligence are associated with brain structural organization, and in particular that higher scores on intelligence tests are related to greater global efficiency of the brain anatomical network. we constructed binary and weighted brain anatomical networks in each of 79 healthy young adults utilizing diffusion tensor tractography and calculated topological properties of the networks using a graph theoretical method. based on their {iq} test scores, all subjects were divided into general and high intelligence groups and significantly higher global efficiencies were found in the networks of the latter group. moreover, we showed significant correlations between {iq} scores and network properties across all subjects while controlling for age and gender. specifically, higher intelligence scores corresponded to a shorter characteristic path length and a higher global efficiency of the networks, indicating a more efficient parallel information transfer in the brain. the results were consistently observed not only in the binary but also in the weighted networks, which together provide convergent evidence for our hypothesis. our findings suggest that the efficiency of brain structural organization may be an important biological basis for intelligence.
1795	4688553	article	the lancet	\N	\N	\N	11	373	9678	2009	jun	2009-06-04 14:20:36	\N	aspirin in the primary and secondary prevention of vascular disease: collaborative meta-analysis of individual participant data from randomised trials	{summarybackgroundlow}-dose aspirin is of definite and substantial net benefit for many people who already have occlusive vascular disease. we have assessed the benefits and risks in primary {prevention.methodswe} undertook meta-analyses of serious vascular events (myocardial infarction, stroke, or vascular death) and major bleeds in six primary prevention trials (95 000 individuals at low average risk, 660 000 person-years, 3554 serious vascular events) and 16 secondary prevention trials (17 000 individuals at high average risk, 43 000 person-years, 3306 serious vascular events) that compared long-term aspirin versus control. we report intention-to-treat analyses of first events during the scheduled treatment {period.findingsin} the primary prevention trials, aspirin allocation yielded a 12\\% proportional reduction in serious vascular events (0·51\\% aspirin vs 0·57\\% control per year, p=0·0001), due mainly to a reduction of about a fifth in non-fatal myocardial infarction (0·18\\% vs 0·23\\% per year, p<0·0001). the net effect on stroke was not significant (0·20\\% vs 0·21\\% per year, p=0·4: haemorrhagic stroke 0·04\\% vs 0·03\\%, p=0·05; other stroke 0·16\\% vs 0·18\\% per year, p=0·08). vascular mortality did not differ significantly (0·19\\% vs 0·19\\% per year, p=0·7). aspirin allocation increased major gastrointestinal and extracranial bleeds (0·10\\% vs 0·07\\% per year, p<0·0001), and the main risk factors for coronary disease were also risk factors for bleeding. in the secondary prevention trials, aspirin allocation yielded a greater absolute reduction in serious vascular events (6·7\\% vs 8·2\\% per year, p<0.0001), with a non-significant increase in haemorrhagic stroke but reductions of about a fifth in total stroke (2·08\\% vs 2·54\\% per year, p=0·002) and in coronary events (4·3\\% vs 5·3\\% per year, p<0·0001). in both primary and secondary prevention trials, the proportional reductions in the aggregate of all serious vascular events seemed similar for men and {women.interpretationin} primary prevention without previous disease, aspirin is of uncertain net value as the reduction in occlusive events needs to be weighed against any increase in major bleeds. further trials are in {progress.fundinguk} medical research council, british heart foundation, cancer research {uk}, and the european community biomed programme.
1796	4743344	article	nucleic acids research	nucl. acids res.	\N	oxford university press	5	37	suppl 2	2009	jul	2009-06-04 12:11:40	\N	pscan: finding over-represented transcription factor binding site motifs in sequences from co-regulated or co-expressed genes	the first step in gene expression, transcription, is modulated by the interaction of transcription factors with their corresponding binding sites on the {dna} sequence. pscan is a software tool that scans a set of sequences (e.g. promoters) from co-regulated or co-expressed genes with motifs describing the binding specificity of known transcription factors and assesses which motifs are significantly over- or under-represented, providing thus hints on which transcription factors could be common regulators of the genes studied, together with the location of their candidate binding sites in the sequences. pscan does not resort to comparisons with orthologous sequences and experimental results show that it compares favorably to other tools for the same task in terms of false positive predictions and computation time. the website is free and open to all users and there is no login requirement. address: http://www.beaconlab.it/pscan.
1797	4750782	article	science	\N	\N	american association for the advancement of science	3	324	5932	2009	jun	2009-06-05 13:13:44	\N	late pleistocene demography and the appearance of modern human behavior	the origins of modern human behavior are marked by increased symbolic and technological complexity in the archaeological record. in western eurasia this transition, the upper paleolithic, occurred about 45,000 years ago, but many of its features appear transiently in southern africa about 45,000 years earlier. we show that demography is a major determinant in the maintenance of cultural complexity and that variation in regional subpopulation density and/or migratory activity results in spatial structuring of cultural skill accumulation. genetic estimates of regional population size over time show that densities in early upper paleolithic europe were similar to those in {sub-saharan} africa when modern behavior first appeared. demographic factors can thus explain geographic variation in the timing of the first appearance of modern behavior without invoking increased cognitive capacity.
1798	4774373	article	bioinformatics	\N	\N	oxford university press	1	25	16	2009	aug	2009-06-08 07:51:37	\N	detecting {snps} and estimating allele frequencies in clonal bacterial populations by sequencing pooled {dna}	summary: here, we present a method for estimating the frequencies of {snp} alleles present within pooled samples of {dna} using high-throughput short-read sequencing. the method was tested on real data from six strains of the highly monomorphic pathogen salmonella paratyphi a, sequenced individually and in a pool. a variety of read mapping and quality-weighting procedures were tested to determine the optimal parameters, which afforded ?80\\% sensitivity of {snp} detection and strong correlation with true {snp} frequency at poolwide read depth of 40×, declining only slightly at read depths 20–40×.
1799	4774507	article	brief bioinform	brief bioinform	\N	\N	8	10	4	2009	jul	2009-06-08 08:59:59	\N	approaches to neuroscience data integration	as the number of neuroscience databases increases, the need for neuroscience data integration grows. this paper reviews and compares several approaches, including the neuroscience database gateway ({ndg}), neuroscience information framework ({nif}) and entrez neuron, which enable neuroscience database annotation and integration. these approaches cover a range of activities spanning from registry, discovery and integration of a wide variety of neuroscience data sources. they also provide different user interfaces for browsing, querying and displaying query results. in entrez neuron, for example, four different facets or tree views (neuron, neuronal property, gene and drug) are used to hierarchically organize concepts that can be used for querying a collection of ontologies. the facets are also used to define the structure of the query results. 10.1093/bib/bbp029
1800	4867928	article	bioinformatics	bioinformatics	\N	oxford university press	5	25	21	2009	nov	2009-06-16 09:55:04	\N	de novo transcriptome assembly with {abyss}	motivation: whole transcriptome shotgun sequencing data from non-normalized samples offer unique opportunities to study the metabolic states of organisms. one can deduce gene expression levels using sequence coverage as a surrogate, identify coding changes or discover novel isoforms or transcripts. especially for discovery of novel events, de novo assembly of transcriptomes is desirable.
1801	4882841	book	\N	\N	\N	o'reilly media	\N	\N	\N	2009	jun	2009-06-17 18:56:46	\N	hadoop: the definitive guide	hadoop: the definitive guide helps you harness the power of your data. ideal for processing large datasets, the apache hadoop framework is an open source implementation of the mapreduce algorithm on which google built its empire. this comprehensive resource demonstrates how to use hadoop to build reliable, scalable, distributed systems: programmers will find details for analyzing large datasets, and administrators will learn how to set up and run hadoop clusters. complete with case studies that illustrate how hadoop solves specific problems, this book helps you:  use the hadoop distributed file system (hdfs) for storing large datasets, and run distributed computations over those datasets using mapreduce become familiar with hadoop's data and i/o building blocks for compression, data integrity, serialization, and persistence discover common pitfalls and advanced features for writing real-world mapreduce programs design, build, and administer a dedicated hadoop cluster, or run hadoop in the cloud use pig, a high-level query language for large-scale data processing take advantage of hbase, hadoop's database for structured and semi-structured data learn zookeeper, a toolkit of coordination primitives for building distributed systems  if you have lots of data -- whether it's gigabytes or petabytes -- hadoop is the perfect solution. hadoop: the definitive guide is the most thorough book available on the subject. "now you have the opportunity to learn about hadoop from a master-not only of the technology, but also of common sense and plain talk." -- doug cutting, hadoop founder, yahoo!
1802	4891923	article	j. comput. chem.	\N	\N	wiley subscription services, inc., a wiley company	6	31	2	2010	jan	2009-06-18 15:49:45	department of molecular biology, the scripps research institute, la jolla, california	{autodock} vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading	{autodock} vina, a new program for molecular docking and virtual screening, is presented. {autodock} vina achieves an approximately two orders of magnitude speed-up compared with the molecular docking software previously developed in our lab ({autodock} 4), while also significantly improving the accuracy of the binding mode predictions, judging by our tests on the training set used in {autodock} 4 development. further speed-up is achieved from parallelism, by using multithreading on multicore machines. {autodock} vina automatically calculates the grid maps and clusters the results in a way transparent to the user. {\\copyright} 2009 wiley periodicals, inc. j comput chem 2010
1803	4892239	article	molecular systems biology	\N	\N	nature publishing group	\N	5	\N	2009	jun	2009-06-18 14:13:44	\N	backup in gene regulatory networks explains differences between binding and knockout results.	the complementarity of gene expression and {protein-dna} interaction data led to several successful models of biological systems. however, recent studies in multiple species raise doubts about the relationship between these two datasets. these studies show that the overwhelming majority of genes bound by a particular transcription factor ({tf}) are not affected when that factor is knocked out. here, we show that this surprising result can be partially explained by considering the broader cellular context in which {tfs} operate. factors whose functions are not backed up by redundant paralogs show a fourfold increase in the agreement between their bound targets and the expression levels of those targets. in addition, we show that incorporating protein interaction networks provides physical explanations for knockout effects. new double knockout experiments support our conclusions. our results highlight the robustness provided by redundant {tfs} and indicate that in the context of diverse cellular systems, binding is still largely functional.
1804	4909050	article	journal of informetrics	\N	\N	\N	14	3	3	2009	jul	2009-06-19 17:03:54	\N	untangling the web of {e-research}: towards a sociology of online knowledge	{e-research} is a rapidly growing research area, both in terms of publications and in terms of funding. in this article we argue that it is necessary to reconceptualize the ways in which we seek to measure and understand {e-research} by developing a sociology of knowledge based on our understanding of how science has been transformed historically and shifted into online forms. next, we report data which allows the examination of {e-research} through a variety of traces in order to begin to understand how knowledge in the realm of {e-research} has been and is being constructed. these data indicate that {e-research} has had a variable impact in different fields of research. we argue that only an overall account of the scale and scope of {e-research} within and between different fields makes it possible to identify the organizational coherence and diffuseness of {e-research} in terms of its socio-technical networks, and thus to identify the contributions of {e-research} to various research fronts in the online production of knowledge.
1805	4923889	book	\N	\N	\N	polity	\N	\N	\N	2009	jun	2009-06-22 15:43:33	\N	{youtube}: online video and participatory culture	{youtube} is one of the most well-known and widely discussed sites ofparticipatory media in the contemporary online environment, and it is thefirst genuinely mass-popular platform for user-created video. in this timelyand comprehensive introduction to how {youtube} is being used and why itmatters, burgess and green discuss the ways that it relates to widertransformations in culture, society and the {economy.the} book critically examines the public debates surrounding the site,demonstrating how it is central to struggles for authority and control in thenew media environment. drawing on a range of theoretical sources and empiricalresearch, the authors discuss how {youtube} is being used by the mediaindustries, by audiences and amateur producers, and by particular communitiesof interest, and the ways in which these uses challenge existing ideas aboutcultural 'production' and '{consumption'.rich} with both concrete examples and featuring specially commissioned chaptersby henry jenkins and john hartley, the book is essential reading for anyoneinterested in the contemporary and future implications of online media. itwill be particularly valuable for students and scholars in media,communication and cultural studies.
1806	4970791	article	future generation computer systems	\N	\N	elsevier science publishers b. v.	17	25	6	2009	jun	2009-06-26 10:46:30	amsterdam, the netherlands, the netherlands	cloud computing and emerging {it} platforms: vision, hype, and reality for delivering computing as the 5th utility	with the significant advances in information and communications technology ({ict}) over the last half century, there is an increasingly perceived vision that computing will one day be the 5th utility (after water, electricity, gas, and telephony). this computing utility, like all other four existing utilities, will provide the basic level of computing service that is considered essential to meet the everyday needs of the general community. to deliver this vision, a number of computing paradigms have been proposed, of which the latest one is known as cloud computing. hence, in this paper, we define cloud computing and provide the architecture for creating clouds with market-oriented resource allocation by leveraging technologies such as virtual machines ({vms}). we also provide insights on market-based resource management strategies that encompass both customer-driven service management and computational risk management to sustain service level agreement ({sla})-oriented resource allocation. in addition, we reveal our early thoughts on interconnecting clouds for dynamically creating global cloud exchanges and markets. then, we present some representative cloud platforms, especially those developed in industries, along with our current work towards realizing market-oriented resource allocation of clouds as realized in aneka enterprise cloud technology. furthermore, we highlight the difference between high performance computing ({hpc}) workload and internet-based services workload. we also describe a meta-negotiation infrastructure to establish global cloud exchanges and markets, and illustrate a case study of harnessing 'storage clouds' for high performance content delivery. finally, we conclude with the need for convergence of competing {it} paradigms to deliver our 21st century vision.
1807	5038173	article	genome research	\N	\N	cold spring harbor laboratory press	8	19	9	2009	sep	2009-07-02 02:14:47	\N	{jbrowse}: a next-generation genome browser.	we describe an open source, portable, {javascript}-based genome browser, {jbrowse}, that can be used to navigate genome annotations over the web. {jbrowse} helps preserve the user's sense of location by avoiding discontinuous transitions, instead offering smoothly animated panning, zooming, navigation, and track selection. unlike most existing genome browsers, where the genome is rendered into images on the webserver and the role of the client is restricted to displaying those images, {jbrowse} distributes work between the server and client and therefore uses significantly less server overhead than previous genome browsers. we report benchmark results empirically comparing server- and client-side rendering strategies, review the architecture and design considerations of {jbrowse}, and describe a simple wiki plug-in that allows users to upload and share annotation tracks.
1808	5044135	article	plos comput biol	\N	\N	public library of science	\N	5	7	2009	jul	2009-07-03 11:17:11	\N	the gene ontology's reference genome project: a unified framework for functional annotation across species	the gene ontology ({go}) is a collaborative effort that provides structured vocabularies for annotating the molecular function, biological role, and cellular location of gene products in a highly systematic way and in a species-neutral manner with the aim of unifying the representation of gene function across different organisms. each contributing member of the {go} consortium independently associates {go} terms to gene products from the organism(s) they are annotating. here we introduce the reference genome project, which brings together those independent efforts into a unified framework based on the evolutionary relationships between genes in these different organisms. the reference genome project has two primary goals: to increase the depth and breadth of annotations for genes in each of the organisms in the project, and to create data sets and tools that enable other genome annotation efforts to infer {go} annotations for homologous genes in their organisms. in addition, the project has several important incidental benefits, such as increasing annotation consistency across genome databases, and providing important improvements to the {go}'s logical structure and biological content. biological research is increasingly dependent on the availability of well-structured representations of biological data with detailed, accurate descriptions provided by the curators of the data repositories. the reference genome project's goal is to provide comprehensive functional annotation for the genomes of human as well as eleven organisms that are important models in biomedical research. to achieve this, we have developed an approach that superposes experimentally-based annotations onto the leaves of phylogenetic trees and then we manually annotate the function of the common ancestors, predicated on the assumption that the ancestors possessed the experimentally determined functions that are held in common at these leaves, and that these functions are likely to be conserved in all other descendents of each family.
1809	5057521	article	cell	\N	\N	\N	9	137	7	2009	jun	2009-07-31 01:50:22	\N	a synthetic genetic edge detection program.	edge detection is a signal processing algorithm common in artificial intelligence and image recognition programs. we have constructed a genetically encoded edge detection algorithm that programs an isogenic community of e. coli to sense an image of light, communicate to identify the light-dark edges, and visually present the result of the computation. the algorithm is implemented using multiple genetic circuits. an engineered light sensor enables cells to distinguish between light and dark regions. in the dark, cells produce a diffusible chemical signal that diffuses into light regions. genetic logic gates are used so that only cells that sense light and the diffusible signal produce a positive output. a mathematical model constructed from first principles and parameterized with experimental measurements of the component circuits predicts the performance of the complete program. quantitatively accurate models will facilitate the engineering of more complex biological behaviors and inform bottom-up studies of natural genetic regulatory networks.
1810	5148747	inproceedings	\N	www	\N	acm	9	\N	\N	2009	\N	2009-07-14 16:09:21	new york, ny, usa	tagommenders: connecting users to items through tags	tagging has emerged as a powerful mechanism that enables users to find, organize, and understand online entities. recommender systems similarly enable users to efficiently navigate vast collections of items. algorithms combining tags with recommenders may deliver both the automation inherent in recommenders, and the flexibility and conceptual comprehensibility inherent in tagging systems. in this paper we explore tagommenders, recommender algorithms that predict users' preferences for items based on their inferred preferences for tags. we describe tag preference inference algorithms based on users' interactions with tags and movies, and evaluate these algorithms based on tag preference ratings collected from 995 {movielens} users. we design and evaluate algorithms that predict users' ratings for movies based on their inferred tag preferences. our tag-based algorithms generate better recommendation rankings than state-of-the-art algorithms, and they may lead to flexible recommender systems that leverage the characteristics of items users find most important.
1811	5175610	article	nature reviews. genetics	nat rev genet	\N	nature publishing group	13	10	8	2009	aug	2009-07-17 08:16:31	\N	mechanisms of change in gene copy number.	deletions and duplications of chromosomal segments (copy number variants, {cnvs}) are a major source of variation between individual humans and are an underlying factor in human evolution and in many diseases, including mental illness, developmental disorders and cancer. {cnvs} form at a faster rate than other types of mutation, and seem to do so by similar mechanisms in bacteria, yeast and humans. here we review current models of the mechanisms that cause copy number variation. non-homologous end-joining mechanisms are well known, but recent models focus on perturbation of {dna} replication and replication of non-contiguous {dna} segments. for example, cellular stress might induce repair of broken replication forks to switch from high-fidelity homologous recombination to non-homologous repair, thus promoting copy number change.
1812	5183264	article	journal of biology	\N	\N	\N	\N	8	6	2009	\N	2009-07-16 17:48:04	\N	search for a 'tree of life' in the thicket of the phylogenetic forest.	comparative genomics has revealed extensive horizontal gene transfer among prokaryotes, a development that is often considered to undermine the 'tree of life' concept. however, the possibility remains that a statistical central trend still exists in the phylogenetic 'forest of life'. a comprehensive comparative analysis of a 'forest' of 6,901 phylogenetic trees for prokaryotic genes revealed a consistent phylogenetic signal, particularly among 102 nearly universal trees, despite high levels of topological inconsistency, probably due to horizontal gene transfer. horizontal transfers seemed to be distributed randomly and did not obscure the central trend. the nearly universal trees were topologically similar to numerous other trees. thus, the nearly universal trees might reflect a significant central tendency, although they cannot represent the forest completely. however, topological consistency was seen mostly at shallow tree depths and abruptly dropped at the level of the radiation of archaeal and bacterial phyla, suggesting that early phases of evolution could be non-tree-like (biological big bang). simulations of evolution under compressed cladogenesis or biological big bang yielded a better fit to the observed dependence between tree inconsistency and phylogenetic depth for the compressed cladogenesis model. horizontal gene transfer is pervasive among prokaryotes: very few gene trees are fully consistent, making the original tree of life concept obsolete. a central trend that most probably represents vertical inheritance is discernible throughout the evolution of archaea and bacteria, although compressed cladogenesis complicates unambiguous resolution of the relationships between the major archaeal and bacterial clades.
1813	5195365	article	bioinformatics	\N	\N	\N	6	25	15	2009	jun	2009-07-20 04:54:48	\N	rapid detection, classification and accurate alignment of up to a million or more related protein sequences	motivation: the patterns of sequence similarity and divergence present within functionally-diverse, evolutionarily-related proteins contain implicit information about corresponding biochemical similarities and differences. a first step toward accessing such information is to statistically analyze these patterns, which, in turn, requires that one first identify and accurately align a very large set of protein se-quences. ideally, the set should include many distantly-related, functionally divergent subgroups.  because it is extremely difficult, if not impossible for fully automated methods to align such sequences correctly, researchers often resort to manual curation based on de-tailed structural and biochemical information. however, multiply-aligning vast numbers of sequences in this way is clearly impracti-cal. results: this problem is addressed using {multiply-aligned} profiles for global alignment of protein sequences ({mapgaps}). the {mapgaps} program uses a set of multiply-aligned profiles both as a query to detect and classify related sequences and as a template to multiply-align the sequences. it relies on {karlin-altschul} statistics for sensitivity and on {psi}-{blast} (and other) heuristics for speed. using as input a carefully curated multiple-profile alignment for p-loop {gtpases}, {mapgaps} correctly aligned weakly conserved sequence motifs within 33 distantly related {gtpases} of known structure. by comparison, the sequence- and structurally-based alignment methods hmmalign and {promals3d} misaligned at least 11 and 23 of these regions, respectively. when applied to a dataset of 65 million protein sequences, {mapgaps} identified, classified and aligned (with comparable accuracy) nearly half a million putative p-loop {gtpase} sequences. availability: a c++ implementation of {mapgaps} is available at {http://mapgaps.igs.umaryland.edu.supplementary} information: supplementary data are available at bioinformatics online.
1814	5249602	article	commun. acm	\N	\N	acm	8	52	8	2009	aug	2009-07-24 11:08:58	new york, ny, usa	the pathologies of big data	scale up your datasets enough and your apps come undone. what are the typical problems and where do the bottlenecks surface?
1815	5296845	article	molecular systems biology	\N	\N	nature publishing group	\N	5	1	2009	jul	2009-07-29 01:28:12	\N	pathway databases and tools for their exploitation: benefits, current limitations and challenges	in past years, comprehensive representations of cell signalling pathways have been developed by manual curation from literature, which requires huge effort and would benefit from information stored in databases and from automatic retrieval and integration methods. once a reconstruction of the network of interactions is achieved, analysis of its structural features and its dynamic behaviour can take place. mathematical modelling techniques are used to simulate the complex behaviour of cell signalling networks, which ultimately sheds light on the mechanisms leading to complex diseases or helps in the identification of drug targets. a variety of databases containing information on cell signalling pathways have been developed in conjunction with methodologies to access and analyse the data. in principle, the scenario is prepared to make the most of this information for the analysis of the dynamics of signalling pathways. however, are the knowledge repositories of signalling pathways ready to realize the systems biology promise? in this article we aim to initiate this discussion and to provide some insights on this issue.
1816	5308512	article	bioinformatics (oxford, england)	\N	\N	\N	10	25	19	2009	oct	2009-07-31 11:39:28	\N	upcoming challenges for multiple sequence alignment methods in the high-throughput era.	this review focuses on recent trends in multiple sequence alignment tools. it describes the latest algorithmic improvements including the extension of consistency-based methods to the problem of template-based multiple sequence alignments. some results are presented suggesting that template-based methods are significantly more accurate than simpler alternative methods. the validation of existing methods is also discussed at length with the detailed description of recent results and some suggestions for future validation strategies. the last part of the review addresses future challenges for multiple sequence alignment methods in the genomic era, most notably the need to cope with very large sequences, the need to integrate large amounts of experimental data, the need to accurately align non-coding and non-transcribed sequences and finally, the need to integrate many alternative methods and approaches.  contact: cedric.notredame@crg.es 10.1093/bioinformatics/btp452
1817	5321670	article	proceedings of the national academy of sciences	\N	\N	\N	5	106	30	2009	jul	2009-08-01 17:33:13	\N	replica exchange with nonequilibrium switches	we introduce a replica exchange (parallel tempering) method in which attempted configuration swaps are generated using nonequilibrium work simulations. by effectively increasing phase space overlap, this approach mitigates the need for many replicas. we illustrate our method by using a model system and show that it is able to achieve the computational efficiency of ordinary replica exchange, using fewer replicas.
1818	5362916	article	proceedings of the national academy of sciences	\N	\N	national academy of sciences	5	106	31	2009	aug	2009-08-04 23:45:26	\N	scaling laws of human interaction activity	even though people in our contemporary technological society are depending on communication, our understanding of the underlying laws of human communicational behavior continues to be poorly understood. here we investigate the communication patterns in 2 social internet communities in search of statistical laws in human interaction activity. this research reveals that human communication networks dynamically follow scaling laws that may also explain the observed trends in economic growth. specifically, we identify a generalized version of gibrat's law of social activity expressed as a scaling law between the fluctuations in the number of messages sent by members and their level of activity. gibrat's law has been essential in understanding economic growth patterns, yet without an underlying general principle for its origin. we attribute this scaling law to long-term correlation patterns in human activity, which surprisingly span from days to the entire period of the available data of more than 1 year. further, we provide a mathematical framework that relates the generalized version of gibrat's law to the long-term correlated dynamics, which suggests that the same underlying mechanism could be the source of gibrat's law in economics, ranging from large firms, research and development expenditures, gross domestic product of countries, to city population growth. these findings are also of importance for designing communication networks and for the understanding of the dynamics of social systems in which communication plays a role, such as economic markets and political systems.
1819	5369385	article	bioinformatics	\N	\N	\N	1	25	19	2009	oct	2009-08-05 08:23:39	\N	{shortread}: a bioconductor package for input, quality assessment and exploration of high-throughput sequence data	summary: {shortread} is a package for input, quality assessment, manipulation and output of high-throughput sequencing data. {shortread} is provided in the r and bioconductor environments, allowing ready access to additional facilities for advanced statistical analysis, data transformation, visualization and integration with diverse genomic {resources.availability} and implementation: this package is implemented in r and available at the bioconductor web site; the package contains a 'vignette' outlining typical work {flows.contact}: mtmorgan@fhcrc.org
1820	5395168	article	isme	\N	\N	\N	\N	\N	\N	2009	jul	2009-08-07 21:44:22	\N	systematic artifacts in metagenomes from complex microbial communities	metagenomics is providing an unprecedented view of the taxonomic diversity, metabolic potential and ecological role of microbial communities in biomes as diverse as the mammalian gastrointestinal tract, the marine water column and soils. however, we have found a systematic error in metagenomes generated by 454-based pyrosequencing that leads to an overestimation of gene and taxon abundance; between 11% and 35% of sequences in a typical metagenome are artificial replicates. here we document the error in several published and original datasets and offer a web-based solution (http://microbiomes.msu.edu/replicates) for identifying and removing these artifacts.
1821	5403625	article	\N	\N	\N	\N	\N	\N	\N	2009	nov	2009-08-10 12:26:28	\N	opinion and community formation in coevolving networks	in human societies, opinion formation is mediated by social interactions, consequently taking place on a network of relationships and at the same time influencing the structure of the network and its evolution. to investigate this coevolution of opinions and social interaction structure, we develop a dynamic agent-based network model by taking into account short range interactions like discussions between individuals, long range interactions like a sense for overall mood modulated by the attitudes of individuals, and external field corresponding to outside influence. moreover, individual biases can be naturally taken into account. in addition, the model includes the opinion-dependent link-rewiring scheme to describe network topology coevolution with a slower time scale than that of the opinion formation. with this model, comprehensive numerical simulations and mean field calculations have been carried out and they show the importance of the separation between fast and slow time scales resulting in the network to organize as well-connected small communities of agents with the same opinion.
1822	5405522	article	journal of cell science	j cell sci	\N	\N	7	122	16	2009	aug	2009-08-10 18:37:33	\N	biological pathways as communicating computer systems	time and cost are the enemies of cell biology. the number of experiments required to rigorously dissect and comprehend a pathway of even modest complexity is daunting. methods are needed to formulate biological pathways in a machine-analysable fashion, which would automate the process of considering all possible experiments in a complex pathway and identify those that command attention. in this essay, we describe a method that is based on the exploitation of computational tools that were originally developed to analyse reactive communicating computer systems such as mobile phones and web browsers. in this approach, the biological process is articulated as an executable computer program that can be interrogated using methods that were developed to analyse complex software systems. using case studies of the {fgf}, {mapk} and {delta/notch} pathways, we show that the application of this technology can yield interesting insights into the behaviour of signalling pathways, which have subsequently been corroborated by experimental data.
1823	5413127	article	foundations and trends in information retrieval	\N	\N	\N	\N	3	3	2009	\N	2009-08-11 01:48:01	\N	learning to rank for information retrieval	learning to rank for information retrieval (ir) is a task to automatically construct a ranking model using training data, such that the model can sort new objects according to their degrees of relevance, preference, or importance. many ir problems are by nature ranking problems, and many ir technologies can be potentially enhanced by using learning-to-rank techniques. the objective of this tutorial is to give an introduction to this research direction. specifically, the existing learning-to-rank algorithms are reviewed and categorized into three approaches: the pointwise, pairwise, and listwise approaches. the advantages and disadvantages with each approach are analyzed, and the relationships between the loss functions used in these approaches and ir evaluation measures are discussed. then the empirical evaluations on typical learning-to-rank methods are shown, with the letor collection as a benchmark dataset, which seems to suggest that the listwise approach be the most effective one among all the approaches. after that, a statistical ranking theory is introduced, which can describe different learning-to-rank algorithms, and be used to analyze their query-level generalization abilities. at the end of the tutorial, we provide a summary and discuss potential future work on learning to rank.
1824	5415605	article	nature methods	\N	\N	nature publishing group	4	6	9	2009	sep	2009-08-11 14:53:44	\N	{breakdancer}: an algorithm for high-resolution mapping of genomic structural variation.	detection and characterization of genomic structural variation are important for understanding the landscape of genetic variation in human populations and in complex diseases such as cancer. recent studies demonstrate the feasibility of detecting structural variation using next-generation, short-insert, paired-end sequencing reads. however, the utility of these reads is not entirely clear, nor are the analysis methods with which accurate detection can be achieved. the algorithm {breakdancer} predicts a wide variety of structural variants including insertion-deletions (indels), inversions and translocations. we examined {breakdancer}'s performance in simulation, in comparison with other methods and in analyses of a sample from an individual with acute myeloid leukemia and of samples from the 1,000 genomes trio individuals. {breakdancer} sensitively and accurately detected indels ranging from 10 base pairs to 1 megabase pair that are difficult to detect via a single conventional approach.
1825	5433696	misc	\N	\N	\N	\N	\N	\N	\N	2009	sep	2009-08-14 04:02:23	\N	effective field theory, past and future	this is a written version of the opening talk at the 6th international workshop on chiral dynamics, at the university of bern, switzerland, july 6, 2009, to be published in the proceedings of the workshop. in it, i reminisce about the early development of effective field theories of the strong interactions, comment briefly on some other applications of effective field theories, and then take up the idea that the standard model and general relativity are the leading terms in an effective field theory. finally, i cite recent calculations that suggest that the effective field theory of gravitation and matter is asymptotically safe.
1826	5451459	article	nature	\N	\N	macmillan publishers limited. all rights reserved	2	460	7259	2009	aug	2009-08-17 20:04:19	\N	demonstration of a spaser-based nanolaser	one of the most rapidly growing areas of physics and nanotechnology focuses on plasmonic effects on the nanometre scale, with possible applications ranging from sensing and biomedicine to imaging and information technology1, 2. however, the full development of nanoplasmonics is hindered by the lack of devices that can generate coherent plasmonic fields. it has been proposed3 that in the same way as a laser generates stimulated emission of coherent photons, a â€˜spaserâ€™ could generate stimulated emission of surface plasmons (oscillations of free electrons in metallic nanostructures) in resonating metallic nanostructures adjacent to a gain medium. but attempts to realize a spaser face the challenge of absorption loss in metal, which is particularly strong at optical frequencies. the suggestion4, 5, 6 to compensate loss by optical gain in localized and propagating surface plasmons has been implemented recently7, 8, 9, 10 and even allowed the amplification of propagating surface plasmons in open paths11. still, these experiments and the reported enhancement of the stimulated emission of dye molecules in the presence of metallic nanoparticles12, 13, 14 lack the feedback mechanism present in a spaser. here we show that 44-nm-diameter nanoparticles with a gold core and dye-doped silica shell allow us to completely overcome the loss of localized surface plasmons by gain and realize a spaser. and in accord with the notion that only surface plasmon resonances are capable of squeezing optical frequency oscillations into a nanoscopic cavity to enable a true nanolaser15, 16, 17, 18, we show that outcoupling of surface plasmon oscillations to photonic modes at a wavelength of 531â€‰nm makes our system the smallest nanolaser reported to dateâ€”and to our knowledge the first operating at visible wavelengths. we anticipate that now it has been realized experimentally, the spaser will advance our fundamental understanding of nanoplasmonics and the development of practical applications.
1827	5467858	article	nat rev genet	\N	\N	nature publishing group	11	10	9	2009	sep	2009-08-18 17:40:33	\N	genetics in geographically structured populations: defining, estimating and interpreting {fst}	wright's f-statistics, and especially {f(st}), provide important insights into the evolutionary processes that influence the structure of genetic variation within and among populations, and they are among the most widely used descriptive statistics in population and evolutionary genetics. estimates of {f(st}) can identify regions of the genome that have been the target of selection, and comparisons of {f(st}) from different parts of the genome can provide insights into the demographic history of populations. for these reasons and others, {f(st}) has a central role in population and evolutionary genetics and has wide applications in fields that range from disease association mapping to forensic science. this review clarifies how {f(st}) is defined, how it should be estimated, how it is related to similar statistics and how estimates of {f(st}) should be interpreted.
1828	5489191	article	bioinformatics	bioinformatics	\N	oxford university press	6	25	20	2009	oct	2009-08-20 08:33:56	\N	{targetminer}: {microrna} target prediction with systematic identification of tissue-specific negative examples	motivation: prediction of {microrna} ({mirna}) target {mrnas} using machine learning approaches is an important area of research. however, most of the methods suffer from either high false positive or false negative rates. one reason for this is the marked deficiency of negative examples or {mirna} non-target pairs. systematic identification of non-target {mrnas} is still not addressed properly, and therefore, current machine learning approaches are compelled to rely on artificially generated negative examples for training.
1829	5490410	article	j comput biol.	\N	\N	\N	11	16	7	2009	jul	2009-08-20 16:25:51	\N	parametric complexity of sequence assembly: theory and applications to next generation sequencing	in recent years, a flurry of new dna sequencing technologies have altered the landscape of genomics, providing a vast amount of sequence information at a fraction of the costs that were previously feasible. the task of assembling these sequences into a genome has, however, still remained an algorithmic challenge that is in practice answered by heuristic solutions. in order to design better assembly algorithms and exploit the characteristics of sequence data from new technologies, we need an improved understanding of the parametric complexity of the assembly problem. in this article, we provide a first theoretical study in this direction, exploring the connections between repeat complexity, read lengths, overlap lengths and coverage in determining the "hard" instances of the assembly problem. our work suggests at least two ways in which existing assemblers can be extended in a rigorous fashion, in addition to delineating directions for future theoretical investigations.
1830	5649721	article	\N	\N	\N	\N	\N	\N	\N	2009	dec	2009-08-26 14:59:38	\N	cooperative behavior cascades in human social networks	theoretical models suggest that social networks influence the evolution ofcooperation, but to date there have been few experimental studies.observational data suggest that a wide variety of behaviors may spread in humansocial networks, but subjects in such studies can choose to befriend peoplewith similar behaviors, posing difficulty for causal inference. here, weexploit a seminal set of laboratory experiments that originally showed thatvoluntary costly punishment can help sustain cooperation. in these experiments,subjects were randomly assigned to a sequence of different groups in order toplay a series of single-shot public goods games with strangers; this featureallowed us to draw networks of interactions to explore how cooperative anduncooperative behavior spreads from person to person to person. we show that,in both an ordinary public goods game and in a public goods game withpunishment, focal individuals are influenced by fellow group members'contribution behavior in future interactions with other individuals who werenot a party to the initial interaction. furthermore, this influence persistsfor multiple periods and spreads up to three degrees of separation (from personto person to person to person). the results suggest that each additionalcontribution a subject makes to the public good in the first period is tripledover the course of the experiment by other subjects who are directly orindirectly influenced to contribute more as a consequence. these are the firstresults to show experimentally that cooperative behavior cascades in humansocial networks.
1831	5655355	article	nucleic acids research	\N	\N	\N	\N	37	19	2009	oct	2009-08-26 23:40:09	\N	{subpathwayminer}: a software package for flexible identification of pathways	with the development of high-throughput experimental techniques such as microarray, mass spectrometry and large-scale mutagenesis, there is an increasing need to automatically annotate gene sets and identify the involved pathways. although many pathway analysis tools are developed, new tools are still needed to meet the requirements for flexible or advanced analysis purpose. here, we developed an r-based software package ({subpathwayminer}) for flexible pathway identification. {subpathwayminer} facilitates sub-pathway identification of metabolic pathways by using pathway structure information. additionally, {subpathwayminer} also provides more flexibility in annotating gene sets and identifying the involved pathways (entire pathways and sub-pathways): (i) {subpathwayminer} is able to provide the most up-to-date pathway analysis results for users; (ii) {subpathwayminer} supports multiple species (?100 eukaryotes, 714 bacteria and 52 archaea) and different gene identifiers (entrez gene {ids}, {ncbi}-gi {ids}, {uniprot} {ids}, {pdb} {ids}, etc.) in the {kegg} {gene} database; (iii) the system is quite efficient in cooperating with other r-based tools in biology. {subpathwayminer} is freely available at {http://cran.r-project.org/web/packages/subpathwayminer}/.
1832	5657848	article	genome biology	\N	\N	\N	\N	10	8	2009	\N	2009-08-27 16:28:25	\N	assisted assembly: how to improve a de novo genome assembly by using related species	we describe a new assembly algorithm, where a genome assembly with low sequence coverage, either throughout the genome or locally, due to cloning bias, is considerably improved through an assisting process via a related genome. we show that the information provided by aligning the whole-genome shotgun reads of the target against a reference genome can be used to substantially improve the quality of the resulting assembly.
1833	5700677	article	genome biology	\N	\N	\N	\N	10	9	2009	sep	2009-09-11 13:42:20	\N	{mrna} expression profiles show differential regulatory effects of {micrornas} between estrogen receptor-positive and estrogen receptor-negative breast cancer	{background}:recent studies have shown that the regulatory effect of {micrornas} can be investigated by examining expression changes of their target genes. given this, it is useful to define an overall metric of regulatory effect for a specific {microrna} and see how this changes across different {conditions.results}:here, we define a regulatory effect score ({re}-score) to measure the inhibitory effect of a {microrna} in a sample, essentially the average difference in expression of its targets versus non-targets. then we compare the {re}-scores of various {micrornas} between two breast cancer subtypes: estrogen receptor positive ({er}+) and negative ({er}-). we applied this approach to five microarray breast cancer datasets and found that the expression of target genes of most {micrornas} was more repressed in {er}- than {er}+; that is, {micrornas} appear to have higher {re}-scores in {er}- breast cancer. these results are robust to the {microrna} target prediction method. to interpret these findings, we analyzed the level of {microrna} expression in previous studies and found that higher {microrna} expression was not always accompanied by higher inhibitory effects. however, several key {microrna} processing genes, especially ago2 and dicer, were differentially expressed between {er}- and {er}+ breast cancer, which may explain the different regulatory effects of {micrornas} in these two breast cancer {subtypes.conclusions}:the {re}-score is a promising indicator to measure {micrornas}' inhibitory effects. most {micrornas} exhibit higher {re}-scores in {er}- than in {er}+ samples, suggesting that they have stronger inhibitory effects in {er}- breast cancers.
1834	5705840	article	bmc bioinformatics	\N	\N	\N	\N	10	1	2009	\N	2009-09-02 10:32:43	\N	comparative study of gene set enrichment methods	{background}:the analysis of high-throughput gene expression data with respect to sets of genes rather than individual genes has many advantages. a variety of methods have been developed for assessing the enrichment of sets of genes with respect to differential expression. in this paper we provide a comparative study of four of these methods: fisher's exact test, gene set enrichment analysis ({gsea}), {random-sets} ({rs}), and gene list analysis with prediction accuracy ({glapa}). the first three methods use associative statistics, while the fourth uses predictive statistics. we first compare all four methods on simulated data sets to verify that fisher's exact test is markedly worse than the other three approaches. we then validate the other three methods on seven real data sets with known genetic perturbations and then compare the methods on two cancer data sets where our a priori knowledge is {limited.results}:the simulation study highlights that none of the three method outperforms all others consistently. {gsea} and {rs} are able to detect weak signals of deregulation and they perform differently when genes in a gene set are both differentially up and down regulated. {glapa} is more conservative and large differences between the two phenotypes are required to allow the method to detect differential deregulation in gene sets. this is due to the fact that the enrichment statistic in {glapa} is prediction error which is a stronger criteria than classical two sample statistic as used in {rs} and {gsea}. this was reflected in the analysis on real data sets as {gsea} and {rs} were seen to be significant for particular gene sets while {glapa} was not, suggesting a small effect size. we find that the rank of gene set enrichment induced by {glapa} is more similar to {rs} than {gsea}. more importantly, the rankings of the three methods share significant {overlap.conclusion}:the three methods considered in our study recover relevant gene sets known to be deregulated in the experimental conditions and pathologies analyzed. there are differences between the three methods and {gsea} seems to be more consistent in finding enriched gene sets, although no method uniformly dominates over all data sets. our analysis highlights the deep difference existing between associative and predictive methods for detecting enrichment and the use of both to better interpret results of pathway analysis. we close with suggestions for users of gene set methods.
1835	5706026	article	jama	\N	\N	\N	7	302	9	2009	\N	2009-09-02 12:13:17	\N	comparison of registered and published primary outcomes in randomized controlled trials	context: as of 2005, the international committee of medical journal editors required investigators to register their trials prior to participant enrollment as a precondition for publishing the trial's findings in member journals. objective: to assess the proportion of registered trials with results recently published in journals with high impact factors; to compare the primary outcomes specified in trial registries with those reported in the published articles; and to determine whether primary outcome reporting bias favored significant outcomes. data sources and study selection: medline via pubmed was searched for reports of randomized controlled trials (rcts) in 3 medical areas (cardiology, rheumatology, and gastroenterology) indexed in 2008 in the 10 general medical journals and specialty journals with the highest impact factors. data extraction: for each included article, we obtained the trial registration information using a standardized data extraction form. results: of the 323 included trials, 147 (45.5%) were adequately registered (ie, registered before the end of the trial, with the primary outcome clearly specified). trial registration was lacking for 89 published reports (27.6%), 45 trials (13.9%) were registered after the completion of the study, 35 (10.8%) were registered with no or an unclear description of the primary outcome, 39 (12%) were registered with no or an unclear description of the primary outcome, and 3 (0.9%) were registered after the completion of the study and had an unclear description of the primary outcome. among articles with trials adequately registered, 31% (46 of 147) showed some evidence of discrepancies between the outcomes registered and the outcomes published. the influence of these discrepancies could be assessed in only half of them and in these statistically significant results were favored in 82.6% (19 of 23). conclusion: comparison of the primary outcomes of rcts registered with their subsequent publication indicated that selective outcome reporting is prevalent.
1836	5738683	article	\N	\N	\N	\N	\N	\N	\N	2011	jan	2009-09-08 03:28:52	\N	the information paradox: a pedagogical introduction	the black hole information paradox is a very poorly understood problem. it is often believed that hawking's argument is not precisely formulated, and a more careful accounting of naturally occurring quantum corrections will allow the radiation process to become unitary. we show that such is not the case, by proving that small corrections to the leading order hawking computation cannot remove the entanglement between the radiation and the hole. we formulate hawking's argument as a `theorem': assuming `traditional' physics at the horizon and usual assumptions of locality we will be forced into mixed states or remnants. we also argue that one cannot explain away the problem by invoking ads/cft duality. we conclude with recent results on the quantum physics of black holes which show the the interior of black holes have a `fuzzball' structure. this nontrivial structure of microstates resolves the information paradox, and gives a qualitative picture of how classical intuition can break down in black hole physics.
1837	5738975	article	bioinformatics	bioinformatics	\N	\N	1	25	21	2009	nov	2009-09-08 08:24:40	\N	updates to the {rmap} short-read mapping software	summary: we report on a major new version of the {rmap} software for mapping reads from short-read sequencing technology. general improvements to accuracy and space requirements are included, along with novel functionality. included in the {rmap} software package are tools for mapping paired-end reads, mapping using more sophisticated use of quality scores, collecting ambiguous mapping locations and mapping bisulfite-treated {reads.availability}: the applications described in this note are available for download at http://www.cmb.usc.edu/people/andrewds/rmap and are distributed as open source software under the {gplv3}.0. the software has been tested on linux and {os} x {platforms.contact}: andrewds@usc.edu; {mzhang@cshl.eduthe} {rmap} algorithm was introduced by (smith et al., 2008) as one of the earliest available programs for mapping reads from the illumina second-generation sequencing technology. one important contribution of {rmap} was to incorporate the use of quality scores directly into the mapping process: read positions with too low a quality score were not considered while mapping, and that quality score cutoff could be adjusted by the user. subsequently, numerous mapping algorithm have appeared (langmead et al., 2009; {li,h}. et al., 2008; {li,r}. et al., 2008; lin et al., 2008; schatz, 2009; yanovsky et al., 2008), with improvements in both efficiency and breadth of functionality (e.g. ability to map paired-end reads; integrated {snp} calling). investigators requiring solutions to mapping problems now have many options. as new applications of short-read sequencing emerge, many variations on the analysis task of read mapping emerge. diversity in performance characteristics of existing mapping tools becomes potentially {valuable.we} report the first major update to {rmap}. the basic algorithmic framework in {rmap} is still to preprocess reads and scan the genome, but several modifications have been made and much additional functionality has been included. importantly, {rmap} has a memory footprint that depends on the number of reads being mapped. this feature allows {rmap} to be used effectively in cluster environments with commodity nodes, because partitioning the reads allows natural parallelizations with linear reduction in memory requirements per processor core {used.included} in this release of the {rmap} software package is functionality for mapping paired-end reads, making more sophisticated use of quality scores, collecting mapping locations for ambiguously mapping reads and mapping bisulfite-treated reads.
1838	5777260	article	pattern recognition letters	\N	\N	elsevier science inc.	15	31	8	2010	sep	2009-12-14 14:45:43	new york, ny, usa	data clustering: 50 years beyond k-means	organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. as an example, a common scheme of scientific classification puts organisms into a system of ranked taxa: domain, kingdom, phylum, class, etc. cluster analysis is the formal study of methods and algorithms for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. the absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). the aim of clustering is to find structure in data and is therefore exploratory in nature. clustering has a long and rich history in a variety of scientific fields. one of the most popular and simple clustering algorithms, k-means, was first published in 1955. in spite of the fact that k-means was proposed over 50 years ago and thousands of clustering algorithms have been published since then, k-means is still widely used. this speaks to the difficulty in designing a general purpose clustering algorithm and the ill-posed problem of clustering. we provide a brief overview of clustering, summarize well known clustering methods, discuss the major challenges and key issues in designing clustering algorithms, and point out some of the emerging and useful research directions, including semi-supervised clustering, ensemble clustering, simultaneous feature selection during data clustering, and large scale data clustering.
1839	5779456	article	genome research	\N	\N	\N	13	19	10	2009	oct	2009-09-13 19:14:22	\N	comparative genomics of protoploid saccharomycetaceae.	our knowledge of yeast genomes remains largely dominated by the extensive studies on saccharomyces cerevisiae and the consequences of its ancestral duplication, leaving the evolution of the entire class of hemiascomycetes only partly explored. we concentrate here on five species of saccharomycetaceae, a large subdivision of hemiascomycetes, that we call "protoploid" because they diverged from the s. cerevisiae lineage prior to its genome duplication. we determined the complete genome sequences of three of these species: kluyveromyces (lachancea) thermotolerans and saccharomyces (lachancea) kluyveri (two members of the newly described lachancea clade), and zygosaccharomyces rouxii. we included in our comparisons the previously available sequences of kluyveromyces lactis and ashbya (eremothecium) gossypii. despite their broad evolutionary range and significant individual variations in each lineage, the five protoploid saccharomycetaceae share a core repertoire of approximately 3300 protein families and a high degree of conserved synteny. synteny blocks were used to define gene orthology and to infer ancestors. far from representing minimal genomes without redundancy, the five protoploid yeasts contain numerous copies of paralogous genes, either dispersed or in tandem arrays, that, altogether, constitute a third of each genome. ancient, conserved paralogs as well as novel, lineage-specific paralogs were identified.
1840	5802477	article	nature neuroscience	\N	\N	nature publishing group	1	12	10	2009	sep	2009-09-18 15:19:54	\N	selective suppression of hippocampal ripples impairs spatial memory	sharp wave–ripple ({spw}-r) complexes in the hippocampus-entorhinal cortex are believed to be important for transferring labile memories from the hippocampus to the neocortex for long-term storage. we found that selective elimination of {spw}-rs during post-training consolidation periods resulted in performance impairment in rats trained on a hippocampus-dependent spatial memory task. our results provide evidence for a prominent role of hippocampal {spw}-rs in memory consolidation.
1841	5832760	article	nature	\N	\N	nature publishing group	5	461	7263	2009	sep	2009-09-23 20:19:06	\N	reconstructing indian population history	india has been underrepresented in genome-wide surveys of human variation. we analyse 25 diverse groups in india to provide strong evidence for two ancient populations, genetically divergent, that are ancestral to most indians today. one, the 'ancestral north indians' ({ani}), is genetically close to middle easterners, central asians, and europeans, whereas the other, the 'ancestral south indians' ({asi}), is as distinct from {ani} and east asians as they are from each other. by introducing methods that can estimate ancestry without accurate ancestral populations, we show that {ani} ancestry ranges from 39–71\\% in most indian groups, and is higher in traditionally upper caste and {indo-european} speakers. groups with only {asi} ancestry may no longer exist in mainland india. however, the indigenous andaman islanders are unique in being {asi}-related groups without {ani} ancestry. allele frequency differences between groups in india are larger than in europe, reflecting strong founder effects whose signatures have been maintained for thousands of years owing to endogamy. we therefore predict that there will be an excess of recessive diseases in india, which should be possible to screen and map genetically.
1842	5833633	article	bmc bioinformatics	\N	\N	\N	\N	10	1	2009	sep	2009-09-24 06:51:10	\N	{sswap}: a simple semantic web architecture and protocol for semantic web services	{background}:{sswap} (simple semantic web architecture and protocol; pronounced "swap") is an architecture, protocol, and platform for using reasoning to semantically integrate heterogeneous disparate data and services on the web. {sswap} was developed as a hybrid semantic web services technology to overcome limitations found in both pure web service technologies and pure semantic web {technologies.results}:there are currently over 2400 resources published in {sswap}. approximately two dozen are custom-written services for {qtl} (quantitative trait loci) and mapping data for legumes and grasses (grains). the remaining are wrappers to nucleic acids research database and web server entries. as an architecture, {sswap} establishes how clients (users of data, services, and ontologies), providers (suppliers of data, services, and ontologies), and discovery servers (semantic search engines) interact to allow for the description, querying, discovery, invocation, and response of semantic web services. as a protocol, {sswap} provides the vocabulary and semantics to allow clients, providers, and discovery servers to engage in semantic web services. the protocol is based on the {w3c}-sanctioned first-order description logic language {owl} {dl}. as an open source platform, a discovery server running at http://sswap.info webcite (as in to "swap info") uses the description logic reasoner pellet to integrate semantic resources. the platform hosts an interactive guide to the protocol at http://sswap.info/protocol.jsp webcite, developer tools at http://sswap.info/developer.jsp webcite, and a portal to third-party ontologies at http://sswapmeet.sswap.info webcite (a "swap {meet").conclusion}:{sswap} addresses the three basic requirements of a semantic web services architecture (i.e., a common syntax, shared semantic, and semantic discovery) while addressing three technology limitations common in distributed service systems: i.e., i) the fatal mutability of traditional interfaces, ii) the rigidity and fragility of static subsumption hierarchies, and iii) the confounding of content, structure, and presentation. {sswap} is novel by establishing the concept of a canonical yet mutable {owl} {dl} graph that allows data and service providers to describe their resources, to allow discovery servers to offer semantically rich search engines, to allow clients to discover and invoke those resources, and to allow providers to respond with semantically tagged data. {sswap} allows for a mix-and-match of terms from both new and legacy third-party ontologies in these graphs.
1843	5841463	misc	\N	\N	\N	\N	\N	\N	\N	2010	dec	2009-09-25 22:39:39	\N	finding structure with randomness: probabilistic algorithms for constructing approximate matrix decompositions	low-rank matrix approximations, such as the truncated singular value decomposition and the rank-revealing qr decomposition, play a central role in data analysis and scientific computing. this work surveys recent research which demonstrates that \\emph{randomization} offers a powerful tool for performing low-rank matrix approximation. these techniques exploit modern computational architectures more fully than classical methods and open the possibility of dealing with truly massive data sets. in particular, these techniques offer a route toward principal component analysis (pca) for petascale data.   this paper presents a modular framework for constructing randomized algorithms that compute partial matrix decompositions. these methods use random sampling to identify a subspace that captures most of the action of a matrix. the input matrix is then compressed - either explicitly or implicitly - to this subspace, and the reduced matrix is manipulated deterministically to obtain the desired low-rank factorization. in many cases, this approach beats its classical competitors in terms of accuracy, speed, and robustness. these claims are supported by extensive numerical experiments and a detailed error analysis.
1844	5870912	article	genome research	\N	\N	\N	9	19	11	2009	nov	2009-10-02 07:26:23	\N	coexpression network based on natural variation in human gene expression reveals gene interactions and functions.	genes interact in networks to orchestrate cellular processes. analysis of these networks provides insights into gene interactions and functions. here, we took advantage of normal variation in human gene expression to infer gene networks, which we constructed using correlations in expression levels of more than 8.5 million gene pairs in immortalized b cells from three independent samples. the resulting networks allowed us to identify biological processes and gene functions. among the biological pathways, we found processes such as translation and glycolysis that co-occur in the same subnetworks. we predicted the functions of poorly characterized genes, including {chchd2} and {tmem111}, and provided experimental evidence that {tmem111} is part of the endoplasmic reticulum-associated secretory pathway. we also found that {ifih1}, a susceptibility gene of type 1 diabetes, interacts with {yes1}, which plays a role in glucose transport. furthermore, genes that predispose to the same diseases are clustered nonrandomly in the coexpression network, suggesting that networks can provide candidate genes that influence disease susceptibility. therefore, our analysis of gene coexpression networks offers information on the role of human genes in normal and disease processes.
1845	5876649	article	physical review e	\N	\N	american physical society	\N	81	4	2010	apr	2009-10-02 13:47:56	\N	the performance of modularity maximization in practical contexts	although widely used in practice, the behavior and accuracy of the popular module identification technique called modularity maximization is not well understood. here, we present a broad and systematic characterization of its performance in practical situations. first, we generalize and clarify the recently identified resolution limit phenomenon. second, we show that the modularity function q exhibits extreme degeneracies: that is, the modularity landscape admits an exponential number of distinct high-scoring solutions and does not typically exhibit a clear global maximum. third, we derive the limiting behavior of the maximum modularity q_max for infinitely modular networks, showing that it depends strongly on the size of the network and the number of module-like subgraphs it contains. finally, using three real-world examples of metabolic networks, we show that the degenerate solutions can fundamentally disagree on the composition of even the largest modules. together, these results significantly extend and clarify our understanding of this popular method. in particular, they explain why so many heuristics perform well in practice at finding high-scoring partitions, why these heuristics can disagree on the composition of the identified modules, and how the estimated value of q_max should be interpreted. further, they imply that the output of any modularity maximization procedure should be interpreted cautiously in scientific contexts. we conclude by discussing avenues for mitigating these behaviors, such as combining information from many degenerate solutions or using generative models.
1846	5933057	article	bmc systems biology	\N	\N	\N	\N	3	1	2009	\N	2009-10-13 08:51:22	\N	how to identify essential genes from molecular networks?	{background}:the prediction of essential genes from molecular networks is a way to test the understanding of essentiality in the context of what is known about the network. however, the current knowledge on molecular network structures is incomplete yet, and consequently the strategies aimed to predict essential genes are prone to uncertain predictions. we propose that simultaneously evaluating different network structures and different algorithms representing gene essentiality (centrality measures) may identify essential genes in networks in a reliable {fashion.results}:by simultaneously analyzing 16 different centrality measures on 18 different reconstructed metabolic networks for saccharomyces cerevisiae, we show that no single centrality measure identifies essential genes from these networks in a statistically significant way; however, the combination of at least 2 centrality measures achieves a reliable prediction of most but not all of the essential genes. no improvement is achieved in the prediction of essential genes when 3 or 4 centrality measures were {combined.conclusion}:the method reported here describes a reliable procedure to predict essential genes from molecular networks. our results show that essential genes may be predicted only by combining centrality measures, revealing the complex nature of the function of essential genes.
1847	5938416	article	genome research	\N	\N	cold spring harbor laboratory press	9	19	11	2009	nov	2009-10-14 08:00:13	\N	windshield splatter analysis with the galaxy metagenomic pipeline	how many species inhabit our immediate surroundings? a straightforward collection technique suitable for answering this question is known to anyone who has ever driven a car at highway speeds. the windshield of a moving vehicle is subjected to numerous insect strikes and can be used as a collection device for representative sampling. unfortunately the analysis of biological material collected in that manner, as with most metagenomic studies, proves to be rather demanding due to the large number of required tools and considerable computational infrastructure. in this study, we use organic matter collected by a moving vehicle to design and test a comprehensive pipeline for phylogenetic profiling of metagenomic samples that includes all steps from processing and quality control of data generated by next-generation sequencing technologies to statistical analyses and data visualization. to the best of our knowledge, this is also the first publication that features a live online supplement providing access to exact analyses and workflows used in the article.
1848	6008522	article	molecular biology and evolution	\N	\N	oxford university press	3	27	2	2010	feb	2009-10-26 07:46:59	\N	{seaview} version 4: a multiplatform graphical user interface for sequence alignment and phylogenetic tree building	we present {seaview} version 4, a multiplatform program designed to facilitate multiple alignment and phylogenetic tree building from molecular sequence data through the use of a graphical user interface. {seaview} version 4 combines all the functions of the widely used programs {seaview} (in its previous versions) and phylo\\_win, and expands them by adding network access to sequence databases, alignment with arbitrary algorithm, maximum-likelihood tree building with {phyml}, and display, printing, and copy-to-clipboard of rooted or unrooted, binary or multifurcating phylogenetic trees. in relation to the wide present offer of tools and algorithms for phylogenetic analyses, {seaview} is especially useful for teaching and for occasional users of such software. {seaview} is freely available at http://pbil.univ-lyon1.fr/software/seaview.
1849	6054102	article	nat meth	\N	\N	nature publishing group	3	6	11s	2009	nov	2009-11-01 22:30:56	\N	next-generation gap	via @mndoci : there is a growing gap between the generation of massively parallel sequencing output and the ability to process and analyze the resulting data. new users are left to navigate a bewildering maze of base calling, alignment, assembly and analysis tools with often incomplete documentation and no idea how to compare and validate their outputs. bridging this gap is essential, or the coveted $1,000 genome will come with a $20,000 analysis price tag.
1850	6055159	article	bmc bioinformatics	\N	\N	\N	\N	10	1	2009	oct	2009-11-02 04:17:49	\N	prediction of hot spot residues at protein-protein interfaces by combining machine learning and energy-based methods	{background}:alanine scanning mutagenesis is a powerful experimental methodology for investigating the structural and energetic characteristics of protein complexes. individual amino-acids are systematically mutated to alanine and changes in free energy of binding ({deltadeltag}) measured. several experiments have shown that protein-protein interactions are critically dependent on just a few residues ("hot spots") at the interface. hot spots make a dominant contribution to the free energy of binding and if mutated they can disrupt the interaction. as mutagenesis studies require significant experimental efforts, there is a need for accurate and reliable computational methods. such methods would also add to our understanding of the determinants of affinity and specificity in protein-protein {recognition.results}:we present a novel computational strategy to identify hot spot residues, given the structure of a complex. we consider the basic energetic terms that contribute to hot spot interactions, i.e. van der waals potentials, solvation energy, hydrogen bonds and coulomb electrostatics. we treat them as input features and use machine learning algorithms such as support vector machines and gaussian processes to optimally combine and integrate them, based on a set of training examples of alanine mutations. we show that our approach is effective in predicting hot spots and it compares favourably to other available methods. in particular we find the best performances using transductive support vector machines, a semi-supervised learning scheme. when hot spots are defined as those residues for which {deltadeltag} [greater than or equal to] 2 kcal/mol, our method achieves a precision and a recall respectively of 56\\% and {65\\%.conclusion}:we have developed an hybrid scheme in which energy terms are used as input features of machine learning models. this strategy combines the strengths of machine learning and energy-based methods. although so far these two types of approaches have mainly been applied separately to biomolecular problems, the results of our investigation indicate that there are substantial benefits to be gained by their integration.
1851	6056800	article	nature	nature	\N	macmillan publishers limited. all rights reserved	6	462	7270	2009	nov	2009-11-02 16:30:03	\N	predicting new molecular targets for known drugs	although drugs are intended to be selective, at least some bind to several physiological targets, explaining side effects and efficacy. because many drug-target combinations exist, it would be useful to explore possible interactions computationally. here we compared 3,665 us food and drug administration (fda)-approved and investigational drugs against hundreds of targets, defining each target by its ligands. chemical similarities between drugs and ligand sets predicted thousands of unanticipated associations. thirty were tested experimentally, including the antagonism of the beta(1) receptor by the transporter inhibitor prozac, the inhibition of the 5-hydroxytryptamine (5-ht) transporter by the ion channel drug vadilex, and antagonism of the histamine h(4) receptor by the enzyme inhibitor rescriptor. overall, 23 new drug-target associations were confirmed, five of which were potent (<100 nm). the physiological relevance of one, the drug n,n-dimethyltryptamine (dmt) on serotonergic receptors, was confirmed in a knockout mouse. the chemical similarity approach is systematic and comprehensive, and may suggest side-effects and new indications for many drugs.
1852	6058169	inproceedings	\N	proceedings of the third acm conference on recommender systems	recsys	acm	3	\N	\N	2009	\N	2009-11-03 03:10:38	new york, ny, usa	using twitter to recommend real-time topical news	recommending news stories to users, based on their preferences, has long been a favourite domain for recommender systems research. in this paper, we describe a novel approach to news recommendation that harnesses real-time micro-blogging activity, from a service such as twitter, as the basis for promoting news stories from a user's favourite {rss} feeds. a preliminary evaluation is carried out on an implementation of this technique that shows promising results.
1853	6064489	article	j. am. chem. soc.	journal of the american chemical society	\N	american chemical society	8	131	42	2009	oct	2009-11-03 20:37:09	\N	energetics of displacing water molecules from protein binding sites: consequences for ligand optimization	a strategy in drug design is to consider enhancing the affinity of lead molecules with structural modifications that displace water molecules from a protein binding site. because success of the approach is uncertain, clarification of the associated energetics was sought in cases where similar structural modifications yield qualitatively different outcomes. specifically, free-energy perturbation calculations were carried out in the context of monte carlo statistical mechanics simulations to investigate ligand series that feature displacement of ordered water molecules in the binding sites of scytalone dehydratase, {p38-?map} kinase, and {egfr} kinase. the change in affinity for a ligand modification is found to correlate with the ease of displacement of the ordered water molecule. however, as in the {egfr} example, the binding affinity may diminish if the free-energy increase due to the removal of the bound water molecule is not more than compensated by the additional interactions of the water-displacing moiety. for accurate computation of the effects of ligand modifications, a complete thermodynamic analysis is shown to be needed. it requires identification of the location of water molecules in the protein?ligand interface and evaluation of the free-energy changes associated with their removal and with the introduction of the ligand modification. direct modification of the ligand in free-energy calculations is likely to trap the ordered molecule and provide misleading guidance for lead optimization.
1854	6095667	article	bmc bioinformatics	\N	\N	\N	\N	10 Suppl 14	Suppl 14	2009	\N	2009-11-10 18:51:15	\N	scratchpads: a data-publishing framework to build, share and manage information on the diversity of life.	{background}: natural history science is characterised by a single immense goal (to document, describe and synthesise all facets pertaining to the diversity of life) that can only be addressed through a seemingly infinite series of smaller studies. the discipline's failure to meaningfully connect these small studies with natural history's goal has made it hard to demonstrate the value of natural history to a wider scientific community. digital technologies provide the means to bridge this gap. {results}: we describe the system architecture and template design of "scratchpads", a data-publishing framework for groups of people to create their own social networks supporting natural history science. scratchpads cater to the particular needs of individual research communities through a common database and system architecture. this is flexible and scalable enough to support multiple networks, each with its own choice of features, visual design, and constituent data. our data model supports web services on standardised data elements that might be used by related initiatives such as {gbif} and the encyclopedia of life. a scratchpad allows users to organise data around user-defined or imported ontologies, including biological classifications. automated semantic annotation and indexing is applied to all content, allowing users to navigate intuitively and curate diverse biological data, including content drawn from third party resources. a system of archiving citable pages allows stable referencing with unique identifiers and provides credit to contributors through normal citation processes. {conclusion}: our framework http://scratchpads.eu/ currently serves more than 1,100 registered users across 100 sites, spanning academic, amateur and citizen-science audiences. these users have generated more than 130,000 nodes of content in the first two years of use. the template of our architecture may serve as a model to other research communities developing data publishing frameworks outside biodiversity research.
1855	6097099	article	plos one	plos one	\N	public library of science	\N	4	11	2009	nov	2009-11-11 08:32:57	\N	{bfast}: an alignment tool for large scale genome resequencing	the new generation of massively parallel {dna} sequencers, combined with the challenge of whole human genome resequencing, result in the need for rapid and accurate alignment of billions of short {dna} sequence reads to a large reference genome. speed is obviously of great importance, but equally important is maintaining alignment accuracy of short reads, in the 25–100 base range, in the presence of errors and true biological variation. we introduce a new algorithm specifically optimized for this task, as well as a freely available implementation, {bfast}, which can align data produced by any of current sequencing platforms, allows for user-customizable levels of speed and accuracy, supports paired end data, and provides for efficient parallel and multi-threaded computation on a computer cluster. the new method is based on creating flexible, efficient whole genome indexes to rapidly map reads to candidate alignment locations, with arbitrary multiple independent indexes allowed to achieve robustness against read errors and sequence variants. the final local alignment uses a {smith-waterman} method, with gaps to support the detection of small indels. we compare {bfast} to a selection of large-scale alignment tools - {blat}, {maq}, {shrimp}, and {soap} - in terms of both speed and accuracy, using simulated and real-world datasets. we show {bfast} can achieve substantially greater sensitivity of alignment in the context of errors and true variants, especially insertions and deletions, and minimize false mappings, while maintaining adequate speed compared to other current methods. we show {bfast} can align the amount of data needed to fully resequence a human genome, one billion reads, with high sensitivity and accuracy, on a modest computer cluster in less than 24 hours. {bfast} is available at http://bfast.sourceforge.net.
1856	6109252	article	\N	\N	\N	\N	\N	\N	\N	2009	nov	2009-11-13 12:51:42	\N	revisiting date and party hubs: novel approaches to role assignment in protein interaction networks	the idea of "date" and "party" hubs has been influential in the study of protein-protein interaction networks. date hubs display low co-expression with their partners, whilst party hubs have high co-expression. it was proposed that party hubs are local coordinators whereas date hubs are global connectors. here, we show that the reported importance of date hubs to network connectivity can in fact be attributed to a tiny subset of them. crucially, these few, extremely central, hubs do not display particularly low expression correlation, undermining the idea of a link between this quantity and hub function. the date/party distinction was originally motivated by an approximately bimodal distribution of hub co-expression; we show that this feature is not always robust to methodological changes. additionally, topological properties of hubs do not in general correlate with co-expression. however, we find significant correlations between interaction centrality and the functional similarity of the interacting proteins. we suggest that thinking in terms of a date/party dichotomy for hubs in protein interaction networks is not meaningful, and it might be more useful to conceive of roles for protein-protein interactions rather than for individual proteins.
1857	6141998	article	nature	\N	\N	macmillan publishers limited. all rights reserved	4	462	7271	2009	nov	2009-11-18 19:53:22	\N	systems-level dynamic analyses of fate change in murine embryonic stem cells	molecular regulation of embryonic stem cell (esc) fate involves a coordinated interaction between epigenetic1, 2, 3, 4, transcriptional5, 6, 7, 8, 9, 10 and translational11, 12 mechanisms. it is unclear how these different molecular regulatory mechanisms interact to regulate changes in stem cell fate. here we present a dynamic systems-level study of cell fate change in murine escs following a well-defined perturbation. global changes in histone acetylation, chromatin-bound rna polymerase ii, messenger rna (mrna), and nuclear protein levels were measured over 5â€‰days after downregulation of nanog, a key pluripotency regulator13, 14, 15. our data demonstrate how a single genetic perturbation leads to progressive widespread changes in several molecular regulatory layers, and provide a dynamic view of information flow in the epigenome, transcriptome and proteome. we observe that a large proportion of changes in nuclear protein levels are not accompanied by concordant changes in the expression of corresponding mrnas, indicating important roles for translational and post-translational regulation of esc fate. gene-ontology analysis across different molecular layers indicates that although chromatin reconfiguration is important for altering cell fate, it is preceded by transcription-factor-mediated regulatory events. the temporal order of gene expression alterations shows the order of the regulatory network reconfiguration and offers further insight into the gene regulatory network. our studies extend the conventional systems biology approach to include many molecular species, regulatory layers and temporal series, and underscore the complexity of the multilayer regulatory mechanisms responsible for changes in protein expression that determine stem cell fate.
1858	6172123	article	nucleic acids research	\N	\N	\N	8	38	suppl 1	2010	jan	2009-11-20 10:05:52	\N	gene expression atlas at the european bioinformatics institute	the gene expression atlas (http://www.ebi.ac.uk/gxa) is an added-value database providing information about gene expression in different cell types, organism parts, developmental stages, disease states, sample treatments and other biological/experimental conditions. the content of this database derives from curation, re-annotation and statistical analysis of selected data from the {arrayexpress} archive of functional genomics data. a simple interface allows the user to query for differential gene expression either (i) by gene names or attributes such as gene ontology terms, or (ii) by biological conditions, e.g. diseases, organism parts or cell types. the gene queries return the conditions where expression has been reported, while condition queries return which genes are reported to be expressed in these conditions. a combination of both query types is possible. the query results are ranked using various statistical measures and by how many independent studies in the database show the particular gene-condition association. currently, the database contains information about more than 200 000 genes from nine species and almost 4500 biological conditions studied in over 30 000 assays from over 1000 independent studies.
1859	6174642	article	proteins	\N	\N	\N	16	78	5	2010	apr	2009-11-20 17:19:13	european bioinformatics institute, wellcome trust genome campus, hinxton, cambridgeshire cb10 1sd, united kingdom; swiss federal institute of technology zurich, institute of molecular systems biology, wolfgang-pauli strasse 16, 8106 zurich, switzerland; john innes centre, computational and system biology, norwich research park, norwich nr7 7uh, united kingdom; istituto italiano di tecnologia (iit), drug discovery and development, via morego 30, 16163 genoa, italy	on the diversity of physicochemical environments experienced by identical ligands in binding pockets of unrelated proteins.	most function prediction methods that identify cognate ligands from binding site analyses work on the assumption of molecular complementarity. these approaches build on the conjectured complementarity of geometrical and physicochemical properties between ligands and binding sites so that similar binding sites will bind similar ligands. we found that this assumption does not generally hold for protein-ligand interactions and observed that it is not the chemical composition of ligand molecules that dictates the complementarity between protein and ligand molecules, but that the ligand's share within the functional mechanism of a protein determines the degree of complementarity. here, we present for a set of cognate ligands a descriptive analysis and comparison of the physicochemical properties that each ligand experiences in various nonhomologous binding pockets. the comparisons in each ligand set reveal large variations in their experienced physicochemical properties, suggesting that the same ligand can bind to distinct physicochemical environments. in some protein ligand complexes, the variation was found to correlate with the electrochemical characteristic of ligand molecules, whereas in others it was disclosed as a prerequisite for the biochemical function of the protein. to achieve binding, proteins were observed to engage in subtle balancing acts between electrostatic and hydrophobic interactions to generate stabilizing free energies of binding. for the presented analysis, a new method for scoring hydrophobicity from molecular environments was developed showing high correlations with experimental determined desolvation energies. the presented results highlight the complexities of molecular recognition and underline the challenges of computational structural biology in developing methods to detect these important subtleties.
1860	6221233	article	plos computational biology	plos comput biol	\N	public library of science	\N	5	11	2009	nov	2009-11-27 08:42:32	\N	a high-throughput screening approach to discovering good forms of biologically inspired visual representation.	while many models of biological object recognition share a common set of "broad-stroke" properties, the performance of any one model depends strongly on the choice of parameters in a particular instantiation of that model--e.g., the number of units per layer, the size of pooling kernels, exponents in normalization operations, etc. since the number of such parameters (explicit or implicit) is typically large and the computational cost of evaluating one particular parameter set is high, the space of possible model instantiations goes largely unexplored. thus, when a model fails to approach the abilities of biological visual systems, we are left uncertain whether this failure is because we are missing a fundamental idea or because the correct "parts" have not been tuned correctly, assembled at sufficient scale, or provided with enough training. here, we present a high-throughput approach to the exploration of such parameter sets, leveraging recent advances in stream processing hardware (high-end nvidia graphic cards and the playstation 3's ibm cell processor). in analogy to high-throughput screening approaches in molecular biology and genetics, we explored thousands of potential network architectures and parameter instantiations, screening those that show promising object recognition performance for further analysis. we show that this approach can yield significant, reproducible gains in performance across an array of basic object recognition tasks, consistently outperforming a variety of state-of-the-art purpose-built vision systems from the literature. as the scale of available computational power continues to expand, we argue that this approach has the potential to greatly accelerate progress in both artificial vision and our understanding of the computational underpinning of biological vision.
1861	6259589	article	proceedings of the national academy of sciences	proceedings of the national academy of sciences	\N	\N	5	106	48	2009	dec	2009-12-02 02:12:38	\N	transcription factors mediate long-range enhancer\\^{a}??promoter interactions	we examined how remote enhancers establish physical communication with target promoters to activate gene transcription in response to environmental signals. although the natural {ifn}-\\^{i}² enhancer is located immediately upstream of the core promoter, it also can function as a classical enhancer element conferring virus infection-dependent activation of heterologous promoters, even when it is placed several kilobases away from these promoters. we demonstrated that the remote {ifn}-\\^{i}² enhancer \\^{a}??loops out\\^{a}?? the intervening {dna} to reach the target promoter. these chromatin loops depend on sequence-specific transcription factors bound to the enhancer and the promoter and thus can explain the specificity observed in enhancer\\^{a}??promoter interactions, especially in complex genetic loci. transcription factor binding sites scattered between an enhancer and a promoter can work as decoys trapping the enhancer in nonproductive loops, thus resembling insulator elements. finally, replacement of the transcription factor binding sites involved in {dna} looping with those of a heterologous prokaryotic protein, the \\^{i}» repressor, which is capable of loop formation, rescues enhancer function from a distance by re-establishing enhancer\\^{a}??promoter loop formation.
1862	6285341	article	reviews of modern physics	\N	\N	american physical society	22	81	4	2009	dec	2009-12-03 09:34:48	\N	colloquium: statistical mechanics of money, wealth, and income	this colloquium reviews statistical models for money, wealth, and income distributions developed in the econophysics literature since the late 1990s. by analogy with the {boltzmann-gibbs} distribution of energy in physics, it is shown that the probability distribution of money is exponential for certain classes of models with interacting economic agents. alternative scenarios are also reviewed. data analysis of the empirical distributions of wealth and income reveals a two-class distribution. the majority of the population belongs to the lower class, characterized by the exponential ("thermal") distribution, whereas a small fraction of the population in the upper class is characterized by the power-law ("superthermal") distribution. the lower part is very stable, stationary in time, whereas the upper part is highly dynamical and out of equilibrium.
1863	6346056	article	procedia - social and behavioral sciences	\N	\N	\N	4	1	1	2009	\N	2009-12-09 19:59:41	\N	to use or not to use web 2.0 in higher education?	web 2.0 has been, during the last years, one of the most fashionable words for a whole range of evolutions regarding the internet. although it was identified by the current analysts as the key technology for the next decade, the actors from the educational field do not really know what web 2.0 means. since the author started to explore and use web 2.0 technologies in her own development/improvement, she has been intrigued by their potential and, especially, by the possibility of integrating them in education and in particular in the teaching activity. the purpose of this paper is both to promote scholarly inquiry about the need of a new type a pedagogy (web 2.0 based) and the development / adoption of best practice in teaching and learning with web 2.0 in higher education ({he}). the article main objectives are: • to introduce theoretical aspects of using web 2.0 technologies in higher education • to present models of integrating web 2.0 technologies in teaching, learning and assessment • to identify the potential benefits of these technologies as well as to highlight some of the problematic issues / barriers encountered, surrounding the pedagogical use of web 2.0 in higher education • to propose an agenda for future research, and to develop pedagogy 2.0 scenarios for {he} sector.
1864	6387600	article	proceedings of the national academy of sciences of the united states of america	\N	\N	national academy of sciences	5	106	50	2009	dec	2009-12-15 21:11:35	\N	a minimal sequence code for switching protein structure and function.	we present here a structural and mechanistic description of how a protein changes its fold and function, mutation by mutation. our approach was to create 2 proteins that (i) are stably folded into 2 different folds, (ii) have 2 different functions, and (iii) are very similar in sequence. in this simplified sequence space we explore the mutational path from one fold to another. we show that an {igg}-binding, 4beta+alpha fold can be transformed into an albumin-binding, 3-alpha fold via a mutational pathway in which neither function nor native structure is completely lost. the stabilities of all mutants along the pathway are evaluated, key high-resolution structures are determined by {nmr}, and an explanation of the switching mechanism is provided. we show that the conformational switch from 4beta+alpha to 3-alpha structure can occur via a single amino acid substitution. on one side of the switch point, the 4beta+alpha fold is >90\\% populated ({ph} 7.2, 20 degrees c). a single mutation switches the conformation to the 3-alpha fold, which is >90\\% populated ({ph} 7.2, 20 degrees c). we further show that a bifunctional protein exists at the switch point with affinity for both {igg} and albumin.
1865	6390470	article	nature	\N	\N	macmillan publishers limited. all rights reserved	3	463	7278	2009	dec	2009-12-17 07:37:08	\N	three-dimensional structure determination from a single view	the ability to determine the structure of matter in three dimensions has profoundly advanced our understanding of nature. traditionally, the most widely used schemes for three-dimensional ({3d}) structure determination of an object are implemented by acquiring multiple measurements over various sample orientations, as in the case of crystallography and tomography1, 2, or by scanning a series of thin sections through the sample, as in confocal microscopy3. here we present a {3d} imaging modality, termed ankylography (derived from the greek words ankylos meaning 'curved' and graphein meaning 'writing'), which under certain circumstances enables complete {3d} structure determination from a single exposure using a monochromatic incident beam. we demonstrate that when the diffraction pattern of a finite object is sampled at a sufficiently fine scale on the ewald sphere, the {3d} structure of the object is in principle determined by the {2d} spherical pattern. we confirm the theoretical analysis by performing {3d} numerical reconstructions of a sodium silicate glass structure at 2?\\aa{} resolution, and a single poliovirus at 2–3?nm resolution, from {2d} spherical diffraction patterns alone. using diffraction data from a soft x-ray laser, we also provide a preliminary demonstration that ankylography is experimentally feasible by obtaining a {3d} image of a test object from a single {2d} diffraction pattern. with further development, this approach of obtaining complete {3d} structure information from a single view could find broad applications in the physical and life sciences.
1866	6401146	article	nature reviews. genetics	\N	\N	nature publishing group	12	11	1	2010	jan	2009-12-19 10:13:22	\N	{rna} processing and its regulation: global insights into biological networks.	in recent years views of eukaryotic gene expression have been transformed by the finding that enormous diversity can be generated at the {rna} level. advances in technologies for characterizing {rna} populations are revealing increasingly complete descriptions of {rna} regulation and complexity; for example, through alternative splicing, alternative polyadenylation and {rna} editing. new biochemical strategies to map {protein-rna} interactions in vivo are yielding transcriptome-wide insights into mechanisms of {rna} processing. these advances, combined with bioinformatics and genetic validation, are leading to the generation of functional {rna} maps that reveal the rules underlying {rna} regulation and networks of biologically coherent transcripts. together these are providing new insights into molecular cell biology and disease.
1867	6411489	article	science	\N	\N	\N	4	326	5960	2009	dec	2009-12-19 14:25:49	\N	stepwise modification of a modular enhancer underlies adaptation in a drosophila population	the evolution of cis regulatory elements (enhancers) of developmentally regulated genes plays a large role in the evolution of animal morphology. however, the mutational path of enhancer evolution--the number, origin, effect, and order of mutations that alter enhancer function--has not been elucidated. here, we localized a suite of substitutions in a modular enhancer of the ebony locus responsible for adaptive melanism in a ugandan drosophila population. we show that at least five mutations with varied effects arose recently from a combination of standing variation and new mutations and combined to create an allele of large phenotypic effect. we underscore how enhancers are distinct macromolecular entities, subject to fundamentally different, and generally more relaxed, functional constraints relative to protein sequences. 10.1126/science.1178357
1868	6434100	article	plos comput biol	\N	\N	public library of science	\N	5	12	2009	dec	2009-12-24 05:57:41	\N	a quick guide for developing effective bioinformatics programming skills	bioinformatics programming skills are becoming a necessity across many facets of biology and medicine, owed in part to the continuing explosion of biological data aggregation and the complexity and scale of questions now being addressed through modern bioinformatics. although many are now receiving formal training in bioinformatics through various university degree and certificate programs, this training is often focused strongly on bioinformatics methodology, leaving many important and practical aspects of bioinformatics to self-education and experience. the following set of guidelines distill several key principals of effective bioinformatics programming, which the authors learned through insights gained across many years of combined experience developing popular bioinformatics software applications and database systems in both academic and commercial settings [1]â€“[6]. successful adoption of these principals will serve both beginner and experienced bioinformaticians alike in career development and pursuit of professional and scientific goals.
1869	6456651	article	psychological science in the public interest	\N	\N	sage publications	14	9	3	2008	dec	2009-12-28 21:41:55	university of california, san diego;  washington university in st. louis;  university of south florida;  university of california, los angeles	learning styles: concepts and evidence	the term  ” learning styles” refers to the concept that individuals differ in regard to what mode of instruction or study is most effective for them. proponents of learning-style assessment contend that optimal instruction requires diagnosing individuals' learning style and tailoring instruction accordingly. assessments of learning style typically ask people to evaluate what sort of information presentation they prefer (e.g., words versus pictures versus speech) and/or what kind of mental activity they find most engaging or congenial (e.g., analysis versus listening), although assessment instruments are extremely diverse. the most common—but not the only—hypothesis about the instructional relevance of learning styles is the meshing hypothesis, according to which instruction is best provided in a format that matches the preferences of the learner (e.g., for a  ” visual learner,” emphasizing visual presentation of {information).the} learning-styles view has acquired great influence within the education field, and is frequently encountered at levels ranging from kindergarten to graduate school. there is a thriving industry devoted to publishing learning-styles tests and guidebooks for teachers, and many organizations offer professional development workshops for teachers and educators built around the concept of learning {styles.the} authors of the present review were charged with determining whether these practices are supported by scientific evidence. we concluded that any credible validation of learning-styles-based instruction requires robust documentation of a very particular type of experimental finding with several necessary criteria. first, students must be divided into groups on the basis of their learning styles, and then students from each group must be randomly assigned to receive one of multiple instructional methods. next, students must then sit for a final test that is the same for all students. finally, in order to demonstrate that optimal learning requires that students receive instruction tailored to their putative learning style, the experiment must reveal a specific type of interaction between learning style and instructional method: students with one learning style achieve the best educational outcome when given an instructional method that differs from the instructional method producing the best outcome for students with a different learning style. in other words, the instructional method that proves most effective for students with one learning style is not the most effective method for students with a different learning {style.our} review of the literature disclosed ample evidence that children and adults will, if asked, express preferences about how they prefer information to be presented to them. there is also plentiful evidence arguing that people differ in the degree to which they have some fairly specific aptitudes for different kinds of thinking and for processing different types of information. however, we found virtually no evidence for the interaction pattern mentioned above, which was judged to be a precondition for validating the educational applications of learning styles. although the literature on learning styles is enormous, very few studies have even used an experimental methodology capable of testing the validity of learning styles applied to education. moreover, of those that did use an appropriate method, several found results that flatly contradict the popular meshing {hypothesis.we} conclude therefore, that at present, there is no adequate evidence base to justify incorporating learning-styles assessments into general educational practice. thus, limited education resources would better be devoted to adopting other educational practices that have a strong evidence base, of which there are an increasing number. however, given the lack of methodologically sound studies of learning styles, it would be an error to conclude that all possible versions of learning styles have been tested and found wanting; many have simply not been tested at all. further research on the use of learning-styles assessment in instruction may in some cases be warranted, but such research needs to be performed appropriately.
1870	6463427	article	science	\N	\N	american association for the advancement of science	2	327	5961	2010	jan	2010-01-01 00:23:53	\N	the rate and molecular spectrum of spontaneous mutations in arabidopsis thaliana	to take complete advantage of information on within-species polymorphism and divergence from close relatives, one needs to know the rate and the molecular spectrum of spontaneous mutations. to this end, we have searched for de novo spontaneous mutations in the complete nuclear genomes of five arabidopsis thaliana mutation accumulation lines that had been maintained by single-seed descent for 30 generations. we identified and validated 99 base substitutions and 17 small and large insertions and deletions. our results imply a spontaneous mutation rate of 7 × 10?9 base substitutions per site per generation, the majority of which are {g:c}?{a:t} transitions. we explain this very biased spectrum of base substitution mutations as a result of two main processes: deamination of methylated cytosines and ultraviolet light–induced mutagenesis.
1871	6531882	article	genome biology	\N	\N	\N	\N	11	1	2010	jan	2010-01-14 11:50:49	\N	{netpath}: a public resource of curated signal transduction pathways.	we have developed {netpath} as a resource of curated human signaling pathways. as an initial step, {netpath} provides detailed maps of a number of immune signaling pathways, which include approximately 1,600 reactions annotated from the literature and more than 2,800 instances of transcriptionally regulated genes - all linked to over 5,500 published articles. we anticipate {netpath} to become a consolidated resource for human signaling pathways that should enable systems biology approaches.
1872	6569490	article	nature reviews. neuroscience	\N	\N	nature publishing group	11	11	2	2010	feb	2010-01-21 08:47:16	\N	the free-energy principle: a unified brain theory?	a free-energy principle has been proposed recently that accounts for action, perception and learning. this review looks at some key brain theories in the biological (for example, neural darwinism) and physical (for example, information theory and optimal control theory) sciences from the free-energy perspective. crucially, one key theme runs through each of these theories - optimization. furthermore, if we look closely at what is optimized, the same quantity keeps emerging, namely value (expected reward, expected utility) or its complement, surprise (prediction error, expected cost). this is the quantity that is optimized under the free-energy principle, which suggests that several global brain theories might be unified within a free-energy framework.
1873	6580456	article	nature protocols	\N	\N	nature publishing group	11	5	2	2010	feb	2010-01-22 17:05:03	\N	defining transcribed regions using {rna}-seq.	next-generation sequencing technologies are revolutionizing genomics research. it is now possible to generate gigabase pairs of {dna} sequence within a week without time-consuming cloning or massive infrastructure. this technology has recently been applied to the development of '{rna}-seq' techniques for sequencing {cdna} from various organisms, with the goal of characterizing entire transcriptomes. these methods provide unprecedented resolution and depth of data, enabling simultaneous quantification of gene expression, discovery of novel transcripts and exons, and measurement of splicing efficiency. we present here a validated protocol for nonstrand-specific transcriptome sequencing via {rna}-seq, describing the library preparation process and outlining the bioinformatic analysis procedure. while sample preparation and sequencing take a fairly short period of time (1-2 weeks), the downstream analysis is by far the most challenging and time-consuming aspect and can take weeks to months, depending on the experimental objectives.
1874	6581403	article	plos genetics	\N	\N	public library of science	\N	6	1	2010	jan	2010-01-23 00:56:13	\N	evolutionary mirages: selection on binding site composition creates the illusion of conserved grammars in drosophila enhancers.	the clustering of transcription factor binding sites in developmental enhancers and the apparent preferential conservation of clustered sites have been widely interpreted as proof that spatially constrained physical interactions between transcription factors are required for regulatory function. however, we show here that selection on the composition of enhancers alone, and not their internal structure, leads to the accumulation of clustered sites with evolutionary dynamics that suggest they are preferentially conserved. we simulated the evolution of idealized enhancers from drosophila melanogaster constrained to contain only a minimum number of binding sites for one or more factors. under this constraint, mutations that destroy an existing binding site are tolerated only if a compensating site has emerged elsewhere in the enhancer. overlapping sites, such as those frequently observed for the activator bicoid and repressor krÃƒÂ¼ppel, had significantly longer evolutionary half-lives than isolated sites for the same factors. this leads to a substantially higher density of overlapping sites than expected by chance and the appearance that such sites are preferentially conserved. because d. melanogaster (like many other species) has a bias for deletions over insertions, sites tended to become closer together over time, leading to an overall clustering of sites in the absence of any selection for clustered sites. since this effect is strongest for the oldest sites, clustered sites also incorrectly appear to be preferentially conserved. following speciation, sites tend to be closer together in all descendent species than in their common ancestors, violating the common assumption that shared features of species' genomes reflect their ancestral state. finally, we show that selection on binding site composition alone recapitulates the observed number of overlapping and closely neighboring sites in real d. melanogaster enhancers. thus, this study calls into question the common practice of inferring Ã¢â‚¬Å“ cis -regulatory grammarsÃ¢â‚¬ï¿½ from the organization and evolutionary dynamics of developmental enhancers.
1875	6617284	article	proceedings of the national academy of sciences of the united states of america	\N	\N	national academy of sciences	5	107	7	2010	feb	2010-02-03 10:20:38	\N	histone modification levels are predictive for gene expression.	histones are frequently decorated with covalent modifications. these histone modifications are thought to be involved in various chromatin-dependent processes including transcription. to elucidate the relationship between histone modifications and transcription, we derived quantitative models to predict the expression level of genes from histone modification levels. we found that histone modification levels and gene expression are very well correlated. moreover, we show that only a small number of histone modifications are necessary to accurately predict gene expression. we show that different sets of histone modifications are necessary to predict gene expression driven by high {cpg} content promoters ({hcps}) or low {cpg} content promoters ({lcps}). quantitative models involving {h3k4me3} and {h3k79me1} are the most predictive of the expression levels in {lcps}, whereas {hcps} require {h3k27ac} and {h4k20me1}. finally, we show that the connections between histone modifications and gene expression seem to be general, as we were able to predict gene expression levels of one cell type using a model trained on another one.
1876	6628158	article	genome biology	\N	\N	\N	\N	11	2	2010	feb	2010-02-05 08:48:48	\N	gene ontology analysis for {rna}-seq: accounting for selection bias	we present {goseq}, an application for performing gene ontology ({go}) analysis on {rna}-seq data. {go} analysis is widely used to reduce complexity and highlight biological processes in genome-wide expression studies, but standard methods give biased results on {rna}-seq data due to over-detection of differential expression for long and highly expressed transcripts. application of {goseq} to a prostate cancer data set shows that {goseq} dramatically changes the results, highlighting categories more consistent with the known biology.
1877	6644901	article	proceedings of the national academy of sciences	\N	\N	national academy of sciences	5	107	9	2010	feb	2010-02-09 08:41:26	\N	ultrahigh-throughput screening in drop-based microfluidics for directed evolution	the explosive growth in our knowledge of genomes, proteomes, and metabolomes is driving ever-increasing fundamental understanding of the biochemistry of life, enabling qualitatively new studies of complex biological systems and their evolution. this knowledge also drives modern biotechnologies, such as molecular engineering and synthetic biology, which have enormous potential to address urgent problems, including developing potent new drugs and providing environmentally friendly energy. many of these studies, however, are ultimately limited by their need for even-higher-throughput measurements of biochemical reactions. we present a general ultrahigh-throughput screening platform using drop-based microfluidics that overcomes these limitations and revolutionizes both the scale and speed of screening. we use aqueous drops dispersed in oil as picoliter-volume reaction vessels and screen them at rates of thousands per second. to demonstrate its power, we apply the system to directed evolution, identifying new mutants of the enzyme horseradish peroxidase exhibiting catalytic rates more than 10 times faster than their parent, which is already a very efficient enzyme. we exploit the ultrahigh throughput to use an initial purifying selection that removes inactive mutants; we identify ?100 variants comparable in activity to the parent from an initial population of ?107. after a second generation of mutagenesis and high-stringency screening, we identify several significantly improved mutants, some approaching diffusion-limited efficiency. in total, we screen ?108 individual enzyme reactions in only 10 h, using < 150 {?l} of total reagent volume; compared to state-of-the-art robotic screening systems, we perform the entire assay with a 1,000-fold increase in speed and a 1-million-fold reduction in cost.
1878	6646501	article	genome biology	\N	\N	\N	\N	11	2	2010	\N	2010-02-09 16:19:54	\N	2x genomes--depth does matter.	we argue that it will remain difficult to differentiate artifacts from true changes in modes and tempo of genome evolution until there is better homogeneity in both taxon sampling and high-coverage sequencing. this is important for broadening the utility of full genome data to the community of evolutionary biologists, whose interests go well beyond widely conserved physiologies and developmental patterns as they seek to understand the generative mechanisms underlying biological diversity.
1879	6651681	article	nature	\N	\N	nature publishing group	5	463	7282	2010	feb	2010-02-10 21:02:28	\N	ancient human genome sequence of an extinct {palaeo-eskimo}	we report here the genome sequence of an ancient human. obtained from \\~{}4,000-year-old permafrost-preserved hair, the genome represents a male individual from the first known culture to settle in greenland. sequenced to an average depth of 20×, we recover 79\\% of the diploid genome, an amount close to the practical limit of current sequencing technologies. we identify 353,151 high-confidence single-nucleotide polymorphisms ({snps}), of which 6.8\\% have not been reported previously. we estimate raw read contamination to be no higher than 0.8\\%. we use functional {snp} assessment to assign possible phenotypic characteristics of the individual that belonged to a culture whose location has yielded only trace human remains. we compare the high-confidence {snps} to those of contemporary populations to find the populations most closely related to the individual. this provides evidence for a migration from siberia into the new world some 5,500 years ago, independent of that giving rise to the modern native americans and inuit.
1880	6652871	article	bioinformatics	\N	\N	\N	6	26	7	2010	apr	2010-02-11 09:07:38	\N	assigning roles to {dna} regulatory motifs using comparative genomics	motivation: transcription factors ({tfs}) are crucial during the lifetime of the cell. their functional roles are defined by the genes they regulate. uncovering these roles not only sheds light on the {tf} at hand but puts it into the context of the complete regulatory {network.results}: here, we present an alignment- and threshold-free comparative genomics approach for assigning functional roles to {dna} regulatory motifs. we incorporate our approach into the gomo algorithm, a computational tool for detecting associations between a user-specified {dna} regulatory motif [expressed as a position weight matrix ({pwm})] and gene ontology ({go}) terms. incorporating multiple species into the analysis significantly improves gomo's ability to identify {go} terms associated with the regulatory targets of {tfs}. including three comparative species in the process of predicting {tf} roles in saccharomyces cerevisiae and homo sapiens increases the number of significant predictions by 75 and 200\\%, respectively. the predicted {go} terms are also more specific, yielding deeper biological insight into the role of the {tf}. adjusting motif (binding) affinity scores for individual sequence composition proves to be essential for avoiding false positive associations. we describe a novel {dna} sequence-scoring algorithm that compensates a thermodynamic measure of {dna}-binding affinity for individual sequence base composition. gomo's prediction accuracy proves to be relatively insensitive to how promoters are defined. because gomo uses a threshold-free form of gene set analysis, there are no free parameters to tune. biologists can investigate the potential roles of {dna} regulatory motifs of interest using gomo via the web ({http://meme.nbcr.net).contact}: {t.bailey@uq.edu.ausupplementary} information: supplementary data are available at bioinformatics online.
1881	6678060	article	nature	\N	\N	nature publishing group	4	463	7283	2010	feb	2010-02-17 18:58:27	\N	complete khoisan and bantu genomes from southern africa	the genetic structure of the indigenous hunter-gatherer peoples of southern africa, the oldest known lineage of modern human, is important for understanding human diversity. studies based on mitochondrial1 and small sets of nuclear markers2 have shown that these hunter-gatherers, known as khoisan, san, or bushmen, are genetically divergent from other humans1, 3. however, until now, fully sequenced human genomes have been limited to recently diverged populations4, 5, 6, 7, 8. here we present the complete genome sequences of an indigenous hunter-gatherer from the kalahari desert and a bantu from southern africa, as well as protein-coding regions from an additional three hunter-gatherers from disparate regions of the kalahari. we characterize the extent of whole-genome and exome diversity among the five men, reporting 1.3 million novel {dna} differences genome-wide, including 13,146 novel amino acid variants. in terms of nucleotide substitutions, the bushmen seem to be, on average, more different from each other than, for example, a european and an asian. observed genomic differences between the hunter-gatherers and others may help to pinpoint genetic adaptations to an agricultural lifestyle. adding the described variants to current databases will facilitate inclusion of southern africans in medical research efforts, particularly when family and medical histories can be correlated with genome-wide data.
1882	6678285	article	nature	\N	\N	nature publishing group	5	463	7283	2010	feb	2010-02-17 20:35:17	\N	variability in gene expression underlies incomplete penetrance	the phenotypic differences between individual organisms can often be ascribed to underlying genetic and environmental variation. however, even genetically identical organisms in homogeneous environments vary, indicating that randomness in developmental processes such as gene expression may also generate diversity. to examine the consequences of gene expression variability in multicellular organisms, we studied intestinal specification in the nematode caenorhabditis elegans in which wild-type cell fate is invariant and controlled by a small transcriptional network. mutations in elements of this network can have indeterminate effects: some mutant embryos fail to develop intestinal cells, whereas others produce intestinal precursors. by counting transcripts of the genes in this network in individual embryos, we show that the expression of an otherwise redundant gene becomes highly variable in the mutants and that this variation is subjected to a threshold, producing an {on}/{off} expression pattern of the master regulatory gene of intestinal differentiation. our results demonstrate that mutations in developmental networks can expose otherwise buffered stochastic variability in gene expression, leading to pronounced phenotypic variation.
1883	6714609	article	bioinformatics (oxford, england)	\N	\N	\N	6	26	7	2010	apr	2010-02-23 09:01:13	\N	{go}-bayes: gene ontology-based overrepresentation analysis using a bayesian approach.	motivation: a typical approach for the interpretation of high-throughput experiments, such as gene expression microarrays, is to produce groups of genes based on certain criteria (e.g. genes that are differentially expressed). to gain more mechanistic insights into the underlying biology, overrepresentation analysis (ora) is often conducted to investigate whether gene sets associated with particular biological functions, for example, as represented by gene ontology (go) annotations, are statistically overrepresented in the identified gene groups. however, the standard ora, which is based on the hypergeometric test, analyzes each go term in isolation and does not take into account the dependence structure of the go-term hierarchy. results: we have developed a bayesian approach (go-bayes) to measure overrepresentation of go terms that incorporates the go dependence structure by taking into account evidence not only from individual go terms, but also from their related terms (i.e. parents, children, siblings, etc.). the bayesian framework borrows information across related go terms to strengthen the detection of overrepresentation signals. as a result, this method tends to identify sets of closely related go terms rather than individual isolated go terms. the advantage of the go-bayes approach is demonstrated with a simulation study and an application example.
1884	6714735	article	journal of librarianship and information science	\N	\N	\N	16	42	1	2010	mar	2010-02-23 10:03:23	\N	thirty years of information literacy (1977—2007): a terminological, conceptual and statistical analysis	over the last three decades, promotion of information literacy has become one of the main goals of librarians and academics. as the emergence of information technologies has raised new challenges and roles for users, information literacy has shifted from the concept of simple training to the provision of the skills and competencies that are critical to the improved use of information. a terminological, conceptual and statistical analysis of the main subjects related to information literacy, as well as its evolution over the last 30 years, is provided with the aim of illustrating how information literacy has been progressively incorporated into the library and academic fields.
1885	6728249	article	bioinformatics	\N	\N	oxford university press	1	26	8	2010	apr	2010-02-26 06:33:11	\N	{bionet}: an {r-package} for the functional analysis of biological networks	motivation: increasing quantity and quality of data in transcriptomics and interactomics create the need for integrative approaches to network analysis. here, we present a comprehensive r-package for the analysis of biological networks including an exact and a heuristic approach to identify functional modules.
1886	6746962	article	bioinformatics	\N	\N	\N	1	26	8	2010	apr	2010-03-02 07:25:56	\N	{scriptree}: scripting phylogenetic graphics	summary: there is a large amount of tools for interactive display of phylogenetic trees. however, there is a shortage of tools for the automation of tree rendering. scripting phylogenetic graphics would enable the saving of graphical analyses involving numerous and complex tree handling operations and would allow the automation of repetitive tasks. {scriptree} is a tool intended to fill this gap. it is an interpreter to be used in batch mode. phylogenetic graphics instructions, related to tree rendering as well as tree annotation, are stored in a text file and processed in a sequential {way.availability}: {scriptree} can be used online or downloaded at www.scriptree.org, under the {gpl} {license.implementation}: {scriptree}, written in {tcl/tk}, is a cross-platform application available for windows and unix-like systems including {os} x. it can be used either as a stand-alone package or included in a bioinformatic pipeline and linked to a {http} {server.contact}: chevenet@ird.fr
1887	6756679	article	genome biology	genome biology	\N	biomed central	8	11	3	2010	mar	2010-03-03 11:51:17	\N	a scaling normalization method for differential expression analysis of {rna}-seq data.	the fine detail provided by sequencing-based transcriptome surveys suggests that {rna}-seq is likely to become the platform of choice for interrogating steady state {rna}. in order to discover biologically important changes in expression, we show that normalization continues to be an essential step in the analysis. we outline a simple and effective method for performing normalization and show dramatically improved results for inferring differential expression in simulated and publicly available data sets.
1888	6758508	article	\N	\N	\N	\N	8	98	5	2010	mar	2010-03-03 21:04:57	\N	statistics and physical origins of {pk} and ionization state changes upon {protein-ligand} binding	this work investigates statistical prevalence and overall physical origins of changes in charge states of receptor proteins upon ligand binding. these changes are explored as a function of the ligand type (small molecule, protein, and nucleic acid), and distance from the binding region. standard continuum solvent methodology is used to compute, on an equal footing, {pk} changes upon ligand binding for a total of 5899 ionizable residues in 20 protein-protein, 20 protein-small molecule, and 20 protein-nucleic acid high-resolution complexes. the size of the data set combined with an extensive error and sensitivity analysis allows us to make statistically justified and conservative conclusions: in 60\\% of all protein-small molecule, 90\\% of all protein-protein, and 85\\% of all protein-nucleic acid complexes there exists at least one ionizable residue that changes its charge state upon ligand binding at physiological conditions ({ph} = 6.5). considering the most biologically relevant {ph} range of 48, the number of ionizable residues that experience substantial {pk} changes (?{pk} > 1.0) due to ligand binding is appreciable: on average, 6\\% of all ionizable residues in protein-small molecule complexes, 9\\% in protein-protein, and 12\\% in protein-nucleic acid complexes experience a substantial {pk} change upon ligand binding. these changes are safely above the statistical false-positive noise level. most of the changes occur in the immediate binding interface region, where approximately one out of five ionizable residues experiences substantial {pk} change regardless of the ligand type. however, the physical origins of the change differ between the types: in protein-nucleic acid complexes, the {pk} values of interface residues are predominantly affected by electrostatic effects, whereas in protein-protein and protein-small molecule complexes, structural changes due to the induced-fit effect play an equally important role. in protein-protein and protein-nucleic acid complexes, there is a statistically significant number of substantial {pk} perturbations, mostly due to the induced-fit structural changes, in regions far from the binding interface.
1889	6763715	inproceedings	\N	nsdi	\N	\N	\N	\N	\N	2010	apr	2010-03-04 22:41:01	san jose, ca	{mapreduce} online	mapreduce is a popular framework for data-intensive distributed computing of batch jobs. to simplify fault tolerance, many implementations of mapreduce materialize the entire output of each map and reduce task before it can be consumed. in this paper, we propose a modiï¬ed mapreduce architecture that allows data to be pipelined between operators. this extends the mapreduce programming model beyond batch processing, and can reduce completion times and improve system utilization for batch jobs as well. we present a modiï¬ed version of the hadoop mapreduce framework that supports on-line aggregation, which allows users to see "early returns" from a job as it is being computed. our hadoop online prototype (hop) also supports continuous queries, which enable mapreduce programs to be written for applications such as event monitoring and stream processing. hop retains the fault tolerance properties of hadoop and can run unmodiï¬ed user-deï¬ned mapreduce programs.
1890	6765697	article	arxiv e-prints	\N	\N	\N	\N	\N	\N	2010	mar	2010-03-05 01:05:35	\N	{rpsph}: a novel smoothed particle hydrodynamics algorithm	we suggest a novel discretisation of the momentum equation for smoothed particle hydrodynamics (sph) and show that it significantly improves the accuracy of the obtained solutions. our new formulation which we refer to as relative pressure sph, rpsph, evaluates the pressure force in respect to the local pressure. it respects newtons first law of motion and applies forces to particles only when there is a net force acting upon them. this is in contrast to standard sph which explicitly uses newtons third law of motion continuously applying equal but opposite forces between particles. rpsph does not show the unphysical particle noise, the clumping or banding instability, unphysical surface tension, and unphysical scattering of different mass particles found for standard sph. at the same time it uses fewer computational operations. and only changes a single line in existing sph codes. we demonstrate its performance on isobaric uniform density distributions, uniform density shearing flows, the kelvin-helmholtz and rayleigh-taylor instabilities, the sod shock tube, the sedov-taylor blast wave and a cosmological integration of the santa barbara galaxy cluster formation test. rpsph is an improvement these cases. the improvements come at the cost of giving up exact momentum conservation of the scheme. consequently one can also obtain unphysical solutions particularly at low resolutions.
1891	6776856	article	genome biology	\N	\N	\N	\N	11	3	2010	mar	2010-03-08 13:37:40	\N	identification and analysis of unitary pseudogenes: historic and contemporary gene losses in humans and other primates.	this analysis of unitary pseudogenes provides insights into the evolutionary constraints faced by different organisms and the timescales of functional gene loss in humans.
1892	6780233	article	bmc bioinformatics	\N	\N	\N	\N	11	1	2010	mar	2010-03-09 12:32:39	\N	mayday--integrative analytics for expression data.	background: dna microarrays have become the standard method for large scale analyses of gene expression and epigenomics. the increasing complexity and inherent noisiness of the generated data makes visual data exploration ever more important. fast deployment of new methods as well as a combination of predefined, easy to apply methods with programmer's access to the data are important requirements for any analysis framework. mayday is an open source platform with emphasis on visual data exploration and analysis. many built-in methods for clustering, machine learning and classification are provided for dissecting complex datasets. plugins can easily be written to extend mayday's functionality in a large number of ways. as java program, mayday is platform-independent and can be used as java webstart application without any installation. mayday can import data from several file formats, database connectivity is included for efficient data organization. numerous interactive visualization tools, including box plots, profile plots, principal component plots and a heatmap are available, can be enhanced with metadata and exported as publication quality vector files. results: we have rewritten large parts of mayday's core to make it more efficient and ready for future developments. among the large number of new plugins are an automated processing framework, dynamic filtering, new and efficient clustering methods, a machine learning module and database connectivity. extensive manual data analysis can be done using an inbuilt r terminal and an integrated sql querying interface. our visualization framework has become more powerful, new plot types have been added and existing plots improved. conclusions: we present a major extension of mayday, a very versatile open-source framework for efficient micro array data analysis designed for biologists and bioinformaticians. most everyday tasks are already covered. the large number of available plugins as well as the extension possibilities using compiled plugins and ad-hoc scripting allow for the rapid adaption of mayday also to very specialized data exploration. mayday is available at http://microarray-analysis.org.
1893	6787348	article	genome research	\N	\N	cold spring harbor laboratory press	14	20	4	2010	apr	2010-03-10 05:26:36	\N	integrative analysis of the melanoma transcriptome	an international, peer-reviewed genome sciences journal featuring outstanding original research that offers novel insights into the biology of all organisms
1894	6791333	article	genome biology	\N	\N	\N	\N	11	3	2010	mar	2010-03-10 11:41:23	\N	{xgap}: a uniform and extensible data model and software platform for genotype and phenotype experiments.	we present an extensible software model for the genotype and phenotype community, {xgap}. readers can download a standard {xgap} (http://www.xgap.org) or auto-generate a custom version using {molgenis} with programming interfaces to r-software and web-services or user interfaces for biologists. {xgap} has simple load formats for any type of genotype, epigenotype, transcript, protein, metabolite or other phenotype data. current functionality includes tools ranging from {eqtl} analysis in mouse to genome-wide association studies in humans.
1895	6807250	article	science	\N	\N	american association for the advancement of science	3	328	5978	2010	apr	2010-03-11 18:05:53	\N	analysis of genetic inheritance in a family quartet by {whole-genome} sequencing	we analyzed the whole-genome sequences of a family of four, consisting of two siblings and their parents. family-based sequencing allowed us to delineate recombination sites precisely, identify 70% of the sequencing errors (resulting in > 99.999% accuracy), and identify very rare single-nucleotide polymorphisms. we also directly estimated a human intergeneration mutation rate of ~1.1 ÃƒÂ— 10Ã¢ÂˆÂ’8 per position per haploid genome. both offspring in this family have two recessive disorders: miller syndrome, for which the gene was concurrently identified, and primary ciliary dyskinesia, for which causative genes have been previously identified. family-based genome analysis enabled us to narrow the candidate genes for both of these mendelian disorders to only four. our results demonstrate the value of complete genome sequencing in families.
1896	6889031	article	briefings in bioinformatics	\N	\N	oxford university press	10	12	1	2011	jan	2010-03-22 07:43:16	\N	a guide to web tools to prioritize candidate genes	finding the most promising genes among large lists of candidate genes has been defined as the gene prioritization problem. it is a recurrent problem in genetics in which genetic conditions are reported to be associated with chromosomal regions. in the last decade, several different computational approaches have been developed to tackle this challenging task. in this study, we review 19 computational solutions for human gene prioritization that are freely accessible as web tools and illustrate their differences. we summarize the various biological problems to which they have been successfully applied. ultimately, we describe several research directions that could increase the quality and applicability of the tools. in addition we developed a website (http://www.esat.kuleuven.be/gpp) containing detailed information about these and other tools, which is regularly updated. this review and the associated website constitute together a guide to help users select a gene prioritization strategy that suits best their needs.
1897	6892271	article	\N	\N	\N	\N	\N	\N	\N	2010	mar	2010-03-23 02:02:44	\N	analogies in theoretical physics	analogies have had and continue to have an important role in the development of theoretical physics. they may start from similarities of physical concepts followed by similarities in the mathematical formalization or it may be a purely mathematical aspect to suggest the development of analogous physical concepts. more often a subtle non obvious interplay between these levels is involved. in this paper i will discuss two cases sufficiently intricate to illustrate some ways of how analogies work. the first topic is the introduction of spontaneous symmetry breaking in particle physics. the second one is the use of the renormalization group in the theory of critical phenomena and its statistical interpretation.
1898	6931396	article	genome research	\N	\N	cold spring harbor laboratory press	9	20	6	2010	jun	2010-03-31 08:12:26	\N	regulatory divergence in drosophila revealed by {mrna}-seq.	10.1101/gr.102491.109 the regulation of gene expression is critical for organismal function and is an important source of phenotypic diversity between species. understanding the genetic and molecular mechanisms responsible for regulatory divergence is therefore expected to provide insight into evolutionary change. using deep sequencing, we quantified total and allele-specific mrna expression levels genome-wide in two closely related  species ( and ) and their f hybrids. we show that 78% of expressed genes have divergent expression between species, and that - and -regulatory divergence affects 51% and 66% of expressed genes, respectively, with 35% of genes showing evidence of both. this is a relatively larger contribution of -regulatory divergence than was expected based on prior studies, and may result from the unique demographic history of . genes with antagonistic - and -regulatory changes were more likely to be misexpressed in hybrids, consistent with the idea that such regulatory changes contribute to hybrid incompatibilities. in addition, -regulatory differences contributed more to divergent expression of genes that showed additive rather than nonadditive inheritance. a correlation between sequence similarity and the conservation of -regulatory activity was also observed that appears to be a general feature of regulatory evolution. finally, we examined regulatory divergence that may have contributed to the evolution of a specific traitâ€”divergent feeding behavior in . overall, this study illustrates the power of mrna sequencing for investigating regulatory evolution, provides novel insight into the evolution of gene expression in , and reveals general trends that are likely to extend to other species.
1899	6941278	article	bmc bioinformatics	\N	\N	\N	\N	11	1	2010	apr	2010-04-02 07:00:13	\N	{genemesh}: a web-based microarray analysis tool for relating differentially expressed genes to {mesh} terms.	background: an important objective of dna microarray-based gene expression experimentation is determining inter-relationships that exist between differentially expressed genes and biological processes, molecular functions, cellular components, signaling pathways, physiologic processes and diseases. results: here we describe genemesh, a web-based program that facilitates analysis of dna microarray gene expression data. genemesh relates genes in a query set to categories available in the medical subject headings (mesh) hierarchical index. the interface enables hypothesis driven relational analysis to a specific mesh subcategory (e.g., cardiovascular system, genetic processes, immune system diseases etc.) or unbiased relational analysis to broader mesh categories (e.g., anatomy, biological sciences, disease etc.). genes found associated with a given mesh category are dynamically linked to facilitate tabular and graphical depiction of entrez gene information, gene ontology information, kegg metabolic pathway diagrams and intermolecular interaction information. expression intensity values of groups of genes that cluster in relation to a given mesh category, gene ontology or pathway can be displayed as heat maps of z score-normalized values. genemesh operates on gene expression data derived from a number of commercial microarray platforms including affymetrix, agilent and illumina. conclusions: genemesh is a versatile web-based tool for testing and developing new hypotheses through relating genes in a query set (e.g., differentially expressed genes from a dna microarray experiment) to descriptors making up the hierarchical structure of the national library of medicine controlled vocabulary thesaurus, mesh. the system further enhances the discovery process by providing links between sets of genes associated with a given mesh category to a rich set of html linked tabular and graphic information including entrez gene summaries, gene ontologies, intermolecular interactions, overlays of genes onto kegg pathway diagrams and heatmaps of expression intensity values. genemesh is freely available online at http://proteogenomics.musc.edu/genemesh/.
1900	6976308	article	plos one	\N	\N	public library of science	\N	5	4	2010	apr	2010-04-08 08:13:06	\N	"positive" results increase down the hierarchy of the sciences.	the hypothesis of a hierarchy of the sciences with physical sciences at the top, social sciences at the bottom, and biological sciences in-between is nearly 200 years old. this order is intuitive and reflected in many features of academic life, but whether it reflects the "hardness" of scientific research--i.e., the extent to which research questions and results are determined by data and theories as opposed to non-cognitive factors--is controversial. this study analysed 2434 papers published in all disciplines and that declared to have tested a hypothesis. it was determined how many papers reported a "positive" (full or partial) or "negative" support for the tested hypothesis. if the hierarchy hypothesis is correct, then researchers in "softer" sciences should have fewer constraints to their conscious and unconscious biases, and therefore report more positive outcomes. results confirmed the predictions at all levels considered: discipline, domain and methodology broadly defined. controlling for observed differences between pure and applied disciplines, and between papers testing one or several hypotheses, the odds of reporting a positive result were around 5 times higher among papers in the disciplines of psychology and psychiatry and economics and business compared to space science, 2.3 times higher in the domain of social sciences compared to the physical sciences, and 3.4 times higher in studies applying behavioural and social methodologies on people compared to physical and chemical studies on non-biological material. in all comparisons, biological studies had intermediate values. these results suggest that the nature of hypotheses tested and the logical and methodological rigour employed to test them vary systematically across disciplines and fields, depending on the complexity of the subject matter and possibly other factors (e.g., a field's level of historical and/or intellectual development). on the other hand, these results support the scientific status of the social sciences against claims that they are completely subjective, by showing that, when they adopt a scientific approach to discovery, they differ from the natural sciences only by a matter of degree.
1901	6983695	article	genome research	\N	\N	cold spring harbor laboratory press	12	20	6	2010	jun	2010-04-09 09:39:11	\N	multiplexed massively parallel {selex} for characterization of human transcription factor binding specificities.	the genetic code-the binding specificity of all transfer-rnas--defines how protein primary structure is determined by dna sequence. dna also dictates when and where proteins are expressed, and this information is encoded in a pattern of specific sequence motifs that are recognized by transcription factors. however, the dna-binding specificity is only known for a small fraction of the approximately 1400 human transcription factors (tfs). we describe here a high-throughput method for analyzing transcription factor binding specificity that is based on systematic evolution of ligands by exponential enrichment (selex) and massively parallel sequencing. the method is optimized for analysis of large numbers of tfs in parallel through the use of affinity-tagged proteins, barcoded selection oligonucleotides, and multiplexed sequencing. data are analyzed by a new bioinformatic platform that uses the hundreds of thousands of sequencing reads obtained to control the quality of the experiments and to generate binding motifs for the tfs. the described technology allows higher throughput and identification of much longer binding profiles than current microarray-based methods. in addition, as our method is based on proteins expressed in mammalian cells, it can also be used to characterize dna-binding preferences of full-length proteins or proteins requiring post-translational modifications. we validate the method by determining binding specificities of 14 different classes of tfs and by confirming the specificities for nfatc1 and rfx3 using chip-seq. our results reveal unexpected dimeric modes of binding for several factors that were thought to preferentially bind dna as monomers.
1902	7010698	article	journal of biomedical informatics	\N	\N	\N	9	43	5	2010	oct	2010-04-13 12:56:41	\N	{openflydata}: an exemplar data web integrating gene expression data on the fruit fly drosophila melanogaster	motivation: integrating heterogeneous data across distributed sources is a major requirement for in silico bioinformatics supporting translational research. for example, genome-scale data on patterns of gene expression in the fruit fly drosophila melanogaster are widely used in functional genomic studies in many organisms to inform candidate gene selection and validate experimental results. however, current data integration solutions tend to be heavy weight, and require significant initial and ongoing investment of effort. development of a common web-based data integration infrastructure (a.k.a. data web), using semantic web standards, promises to alleviate these difficulties, but little is known about the feasibility, costs, risks or practical means of migrating to such an infrastructure. results: we describe the development of {openflydata}, a proof-of-concept system integrating gene expression data on d. melanogaster, combining semantic web standards with light-weight approaches to web programming based on web 2.0 design patterns. to support researchers designing and validating functional genomic studies, {openflydata} includes user-facing search applications providing intuitive access to and comparison of gene expression data from {flyatlas}, the {bdgp} in situ database, and {flyted}, using data from {flybase} to expand and disambiguate gene names. {openflydata}'s services are also openly accessible, and are available for reuse by other bioinformaticians and application developers. semi-automated methods and tools were developed to support labour- and knowledge-intensive tasks involved in deploying {sparql} services. these include methods for generating ontologies and {relational-to-rdf} mappings for relational databases, which we illustrate using the {flybase} chado database schema; and methods for mapping gene identifiers between databases. the advantages of using semantic web standards for biomedical data integration are discussed, as are open issues. in particular, although the performance of open source {sparql} implementations is sufficient to query gene expression data directly from user-facing applications such as web-based data fusions (a.k.a. mashups), we found open {sparql} endpoints to be vulnerable to denial-of-service-type problems, which must be mitigated to ensure reliability of services based on this standard. these results are relevant to data integration activities in translational bioinformatics. availability: the gene expression search applications and {sparql} endpoints developed for {openflydata} are deployed at http://openflydata.org. {flyui}, a library of {javascript} widgets providing re-usable user-interface components for drosophila gene expression data, is available at http://flyui.googlecode.com. software and ontologies to support transformation of data from {flybase}, {flyatlas}, {bdgp} and {flyted} to {rdf} are available at http://openflydata.googlecode.com. {sparqlite}, an implementation of the {sparql} protocol, is available at http://sparqlite.googlecode.com. all software is provided under the {gpl} version 3 open source license.
1903	7013715	article	bioinformatics (oxford, england)	\N	\N	oxford university press	6	26	10	2010	may	2010-04-13 09:36:36	\N	detection and characterization of novel sequence insertions using paired-end next-generation sequencing.	in the past few years, human genome structural variation discovery has enjoyed increased attention from the genomics research community. many studies were published to characterize short insertions, deletions, duplications and inversions, and associate copy number variants ({cnvs}) with disease. detection of new sequence insertions requires sequence data, however, the 'detectable' sequence length with read-pair analysis is limited by the insert size. thus, longer sequence insertions that contribute to our genetic makeup are not extensively researched. we present {novelseq}: a computational framework to discover the content and location of long novel sequence insertions using paired-end sequencing data generated by the next-generation sequencing platforms. our framework can be built as part of a general sequence analysis pipeline to discover multiple types of genetic variation ({snps}, structural variation, etc.), thus it requires significantly less-computational resources than de novo sequence assembly. we apply our methods to detect novel sequence insertions in the genome of an anonymous donor and validate our results by comparing with the insertions discovered in the same genome using various sources of sequence data. the implementation of the {novelseq} pipeline is available at http://compbio.cs.sfu.ca/strvar.htm eee@gs.washington.edu; cenk@cs.sfu.ca
1904	7020706	article	nature	\N	\N	nature publishing group	6	464	7291	2010	apr	2010-04-15 02:01:15	\N	genome remodelling in a basal-like breast cancer metastasis and xenograft	massively parallel {dna} sequencing technologies provide an unprecedented ability to screen entire genomes for genetic changes associated with tumour progression. here we describe the genomic analyses of four {dna} samples from an {african-american} patient with basal-like breast cancer: peripheral blood, the primary tumour, a brain metastasis and a xenograft derived from the primary tumour. the metastasis contained two de novo mutations and a large deletion not present in the primary tumour, and was significantly enriched for 20 shared mutations. the xenograft retained all primary tumour mutations and displayed a mutation enrichment pattern that resembled the metastasis. two overlapping large deletions, encompassing {ctnna1}, were present in all three tumour samples. the differential mutation frequencies and structural variation patterns in metastasis and xenograft compared with the primary tumour indicate that secondary tumours may arise from a minority of cells within the primary tumour.
1905	7021637	article	plos one	\N	\N	public library of science	\N	5	4	2010	apr	2010-04-15 12:09:13	\N	{biotorrents}: a file sharing service for scientific data	the transfer of scientific data has emerged as a significant challenge, as datasets continue to grow in size and demand for open access sharing increases. current methods for file transfer do not scale well for large files and can cause long transfer times. in this study we present {biotorrents}, a website that allows open access sharing of scientific data and uses the popular {bittorrent} peer-to-peer file sharing technology. {biotorrents} allows files to be transferred rapidly due to the sharing of bandwidth across multiple institutions and provides more reliable file transfers due to the built-in error checking of the file sharing technology. {biotorrents} contains multiple features, including keyword searching, category browsing, {rss} feeds, torrent comments, and a discussion forum. {biotorrents} is available at http://www.biotorrents.net.
1906	7046614	article	nature	\N	\N	nature publishing group, a division of macmillan publishers limited. all rights reserved.	3	465	7299	2010	jun	2010-04-21 08:26:24	\N	putting brain training to the test	â€˜brain trainingâ€™, or the goal of improved cognitive function through the regular use of computerized tests, is a multimillion-pound industry1, yet in our view scientific evidence to support its efficacy is lacking. modest effects have been reported in some studies of older individuals2, 3 and preschool children4, and video-game players outperform non-players on some tests of visual attention5. however, the widely held belief that commercially available computerized brain-training programs improve general cognitive function in the wider population in our opinion lacks empirical support. the central question is not whether performance on cognitive tests can be improved by training, but rather, whether those benefits transfer to other untrained tasks or lead to any general improvement in the level of cognitive functioning. here we report the results of a six-week online study in which 11,430 participants trained several times each week on cognitive tasks designed to improve reasoning, memory, planning, visuospatial skills and attention. although improvements were observed in every one of the cognitive tasks that were trained, no evidence was found for transfer effects to untrained tasks, even when those tasks were cognitively closely related.
1907	7063829	article	science	\N	\N	\N	3	328	5977	2010	apr	2010-04-22 21:05:19	\N	complexity and diversity	the mechanisms for the origin and maintenance of biological diversity are not fully understood. it is known that frequency-dependent selection, generating advantages for rare types, can maintain genetic variation and lead to speciation, but in models with simple phenotypes (that is, low-dimensional phenotype spaces), frequency dependence needs to be strong to generate diversity. however, we show that if the ecological properties of an organism are determined by multiple traits with complex interactions, the conditions needed for frequency-dependent selection to generate diversity are relaxed to the point where they are easily satisfied in high-dimensional phenotype spaces. mathematically, this phenomenon is reflected in properties of eigenvalues of quadratic forms. because all living organisms have at least hundreds of phenotypes, this casts the potential importance of frequency dependence for the origin and maintenance of diversity in a new light. 10.1126/science.1187468
1908	7093894	article	bioinformatics (oxford, england)	\N	\N	\N	7	26	10	2010	may	2010-04-27 19:22:51	\N	meta-analysis for pathway enrichment analysis when combining multiple genomic studies.	motivation: many pathway analysis (or gene set enrichment analysis) methods have been developed to identify enriched pathways under different biological states within a genomic study. as more and more microarray datasets accumulate, meta-analysis methods have also been developed to integrate information among multiple studies. currently, most meta-analysis methods for combining genomic studies focus on biomarker detection and meta-analysis for pathway analysis has not been systematically pursued. results: we investigated two approaches of meta-analysis for pathway enrichment (mape) by combining statistical significance across studies at the gene level (mape_g) or at the pathway level (mape_p). simulation results showed increased statistical power of meta-analysis approaches compared to a single study analysis and showed complementary advantages of mape_g and mape_p under different scenarios. we also developed an integrated method (mape_i) that incorporates advantages of both approaches. comprehensive simulations and applications to real data on drug response of breast cancer cell lines and lung cancer tissues were evaluated to compare the performance of three mape variations. mape_p has the advantage of not requiring gene matching across studies. when mape_g and mape_p show complementary advantages, the hybrid version of mape_i is generally recommended. availability: http://www.biostat.pitt.edu/bioinfo/ contact: ctseng@pitt.edu supplementary information: supplementary data are available at bioinformatics online.
1909	7118071	article	international journal of neuroscience	\N	\N	informa clin med	26	119	11	2009	jan	2010-05-03 06:55:06	\N	dyslexic children show {short-term} memory deficits in phonological storage and serial rehearsal: an {fmri} study	doi: 10.1080/00207450903139671 dyslexia is primarily associated with a phonological processing deficit. however, the clinical manifestation also includes a reduced verbal working memory ({wm}) span. it is unclear whether this {wm} impairment is caused by the phonological deficit or a distinct {wm} deficit. the main aim of this study was to investigate neuronal activation related to phonological storage and rehearsal of serial order in {wm} in a sample of 13-year-old dyslexic children compared with age-matched nondyslexic children. a sequential verbal {wm} task with two tasks was used. in the letter probe task, the probe consisted of a single letter and the judgment was for the presence or absence of that letter in the prior sequence of six letters. in the sequence probe ({sp}) task, the probe consisted of all six letters and the judgment was for a match of their serial order with the temporal order in the prior sequence. group analyses as well as single-subject analysis were performed with the statistical parametric mapping software {spm2}. in the letter probe task, the dyslexic readers showed reduced activation in the left precentral gyrus ({ba6}) compared to control group. in the sequence probe task, the dyslexic readers showed reduced activation in the prefrontal cortex and the superior parietal cortex ({ba7}) compared to the control subjects. our findings suggest that a verbal {wm} impairment in dyslexia involves an extended neural network including the prefrontal cortex and the superior parietal cortex. reduced activation in the left {ba6} in both the letter probe and sequence probe tasks may be caused by a deficit in phonological processing. however, reduced bilateral activation in the {ba7} in the sequence probe task only could indicate a distinct working memory deficit in dyslexia associated with temporal order processing.
1910	7128750	article	genetics	\N	\N	genetics society of america	11	185	2	2010	jun	2010-05-05 14:18:54	\N	statistical design and analysis of {rna} sequencing data	next-generation sequencing technologies are quickly becoming the preferred approach for characterizing and quantifying entire genomes. even though data produced from these technologies are proving to be the most informative of any thus far, very little attention has been paid to fundamental design aspects of data collection and analysis, namely sampling, randomization, replication, and blocking. we discuss these concepts in an {rna} sequencing framework. using simulations we demonstrate the benefits of collecting replicated {rna} sequencing data according to well known statistical designs that partition the sources of biological and technical variation. examples of these designs and their corresponding models are presented with the goal of testing differential expression.
1911	7192460	article	nat rev genet	\N	\N	nature publishing group	4	11	6	2010	jun	2010-05-18 23:09:19	\N	missing heritability and strategies for finding the underlying causes of complex disease	although recent genome-wide studies have provided valuable insights into the genetic basis of human disease, they have explained relatively little of the heritability of most complex traits, and the variants identified through these studies have small effect sizes. this has led to the important and hotly debated issue of where the 'missing heritability' of complex diseases might be found. here, seven leading geneticists offer their opinion about where this heritability is likely to lie, what this could tell us about the underlying genetic architecture of common diseases and how this could inform research strategies for uncovering genetic risk factors.
1912	7203908	article	plos genet	\N	\N	public library of science	\N	6	5	2010	may	2010-05-21 09:26:48	\N	{gc}-biased evolution near human accelerated regions	regions of the genome that have been the target of positive selection specifically along the human lineage are of special importance in human biology. we used high throughput sequencing combined with methods to enrich human genomic samples for particular targets to obtain the sequence of 22 chromosomal samples at high depth in 40 kb neighborhoods of 49 previously identified 100\\^{a}€ ” 400 bp elements that show evidence for human accelerated evolution. in addition to selection, the pattern of nucleotide substitutions in several of these elements suggested an historical bias favoring the conversion of weak (a or t) alleles into strong (g or c) alleles. here we found strong evidence in the derived allele frequency spectra of many of these 40 kb regions for ongoing weak-to-strong fixation bias. comparison of the nucleotide composition at polymorphic loci to the composition at sites of fixed substitutions additionally reveals the signature of historical weak-to-strong fixation bias in a subset of these regions. most of the regions with evidence for historical bias do not also have signatures of ongoing bias, suggesting that the evolutionary forces generating weak-to-strong bias are not constant over time. to investigate the role of selection in shaping these regions, we analyzed the spatial pattern of polymorphism in our samples. we found no significant evidence for selective sweeps, possibly because the signal of such sweeps has decayed beyond the power of our tests to detect them. together, these results do not rule out functional roles for the observed changes in these regions\\^{a}€”indeed there is good evidence that the first two are functional elements in humans\\^{a}€”but they suggest that a fixation process (such as biased gene conversion) that is biased at the nucleotide level, but is otherwise selectively neutral, could be an important evolutionary force at play in them, both historically and at present.
1913	7209028	article	mathematics and computers in simulation	\N	\N	\N	11	54	4-5	2000	dec	2010-05-24 13:42:59	\N	from computational science to internetics: integration of science with computer science	we describe how our world dominated by science and scientists has been changed and will be revolutionized by technologies moving with internet time. computers have always been well-used tools but in the beginning only the science counted and little credit or significance was attached to any computing activities associated with scientific research. some 20 years ago, this started to change and the area of computational science gathered support with the {nsf} supercomputer centers playing a critical role. however, this vision has stalled over the last 5 years with information technology increasing in importance. the holy grail of computational science — scalable parallel computing — is still important but is just one supporting component of the internet revolution. we discuss the emergence of the field of internetics — bridging computer science and all application areas whether simulation or information based. internetics is an exciting field, which seems complete and rich enough to be a lasting interdisciplinary area. physics and other core science and engineering disciplines used to attract the very best minds but now their popularity is declining. we describe curricula initiatives that can reinvigorate these fields. this curricula turmoil must be addressed by our education infrastructure whose professorial staff find it hard to develop courses to satisfy student and employer interests in times of such rapid change. distance education is very relevant as it can be used to disseminate expertise to students and teachers in these new areas. all of this has implications for our educational institutions, which could be quite profound.
1914	7220035	article	bmc bioinformatics	\N	\N	\N	\N	11	1	2010	may	2010-05-27 01:51:50	\N	an {escience}-bayes strategy for analyzing omics data.	background: the omics fields promise to revolutionize our understanding of biology and biomedicine. however, their potential is compromised by the challenge to analyze the huge datasets produced. analysis of omics data is plagued by the curse of dimensionality, resulting in imprecise estimates of model parameters and performance. moreover, the integration of omics data with other data sources is difficult to shoehorn into classical statistical models. this has resulted in ad hoc approaches to address specific problems. results: we present a general approach to omics data analysis that alleviates these problems. by combining escience and bayesian methods, we retrieve scientific information and data from multiple sources and coherently incorporate them into large models. these models improve the accuracy of predictions and offer new insights into the underlying mechanisms. this "escience-bayes" approach is demonstrated in two proof-of-principle applications, one for breast cancer prognosis prediction from transcriptomic data and one for protein-protein interaction studies based on proteomic data. conclusions: bayesian statistics provide the flexibility to tailor statistical models to the complex data structures in omics biology as well as permitting coherent integration of multiple data sources. however, bayesian methods are in general computationally demanding and require specification of possibly thousands of prior distributions. escience can help us overcome these difficulties. the escience-bayes thus approach permits us to fully leverage on the advantages of bayesian methods, resulting in models with improved predictive performance that gives more information about the underlying biological system.
1915	7224150	article	genome research	\N	\N	cold spring harbor laboratory press	8	20	9	2010	sep	2010-05-28 04:52:41	\N	assembly of large genomes using second-generation sequencing.	second-generation sequencing technology can now be used to sequence an entire human genome in a matter of days and at low cost. sequence read lengths, initially very short, have rapidly increased since the technology first appeared, and we now are seeing a growing number of efforts to sequence large genomes de novo from these short reads. in this perspective, we describe the issues associated with short-read assembly, the different types of data produced by second-gen sequencers, and the latest assembly algorithms designed for these data. we also review the genomes that have been assembled recently from short reads and make recommendations for sequencing strategies that will yield a high-quality assembly.
1916	7237686	article	journal of information science	\N	\N	sage publications	22	36	4	2010	jun	2010-06-03 00:21:19	\N	researchers' e-journal use and information seeking behaviour	this paper presents the results of the second phase of a research information network study, which sought to establish the impact of e-journals on the scholarly behaviour of researchers in the {uk}. the first phase of the project was a deep log analysis of the usage and information seeking behaviour of researchers in connection with the {sciencedirect} and oxford journals databases. this paper reports on the second phase, which sought to explain and provide context for the deep log data by taking the questions raised by the quantitative study to the research community via interview, questionnaire and observation. nine major research institutions took part, six subjects were covered and the behaviour of about 1400 people was analyzed. findings show that academic journals have become central to all disciplines and that the e-form is the prime means of access. most importantly the study demonstrates that computer usage logs provide an accurate picture of online behaviour. high levels of gateway service use point to the re-intermediating of the broken chain between publisher and reader.
1917	7262603	article	bioinformatics	\N	\N	oxford university press	7	26	12	2010	jun	2010-06-07 09:04:14	\N	{pathtext}: a text mining integrator for biological pathway visualizations	motivation: metabolic and signaling pathways are an increasingly important part of organizing knowledge in systems biology. they serve to integrate collective interpretations of facts scattered throughout literature. biologists construct a pathway by reading a large number of articles and interpreting them as a consistent network, but most of the models constructed currently lack direct links to those articles. biologists who want to check the original articles have to spend substantial amounts of time to collect relevant articles and identify the sections relevant to the pathway. furthermore, with the scientific literature expanding by several thousand papers per week, keeping a model relevant requires a continuous curation effort. in this article, we present a system designed to integrate a pathway visualizer, text mining systems and annotation tools into a seamless environment. this will enable biologists to freely move between parts of a pathway and relevant sections of articles, as well as identify relevant papers from large text bases. the system, {pathtext}, is developed by systems biology institute, okinawa institute of science and technology, national centre for text mining (university of manchester) and the university of tokyo, and is being used by groups of biologists from these locations.
1918	7262609	article	nat genet	\N	\N	nature publishing group	3	42	7	2010	jul	2010-06-07 09:10:24	\N	transposable elements have rewired the core regulatory network of human embryonic stem cells	detection of new genomic control elements is critical in understanding transcriptional regulatory networks in their entirety. we studied the genome-wide binding locations of three key regulatory proteins ({pou5f1}, also known as {oct4}; {nanog}; and {ctcf}) in human and mouse embryonic stem cells. in contrast to {ctcf}, we found that the binding profiles of {oct4} and {nanog} are markedly different, with only \\~{}5\\% of the regions being homologously occupied. we show that transposable elements contributed up to 25\\% of the bound sites in humans and mice and have wired new genes into the core regulatory network of embryonic stem cells. these data indicate that species-specific transposable elements have substantially altered the transcriptional circuitry of pluripotent stem cells.
1919	7343719	article	briefings in bioinformatics	\N	\N	oxford university press	7	11	6	2010	nov	2010-06-19 22:59:34	\N	bioinformatics training: a review of challenges, actions and support requirements	as bioinformatics becomes increasingly central to research in the molecular life sciences, the need to train non-bioinformaticians to make the most of bioinformatics resources is growing. here, we review the key challenges and pitfalls to providing effective training for users of bioinformatics services, and discuss successful training strategies shared by a diverse set of bioinformatics trainers. we also identify steps that trainers in bioinformatics could take together to advance the state of the art in current training practices. the ideas presented in this article derive from the first trainer networking session held under the auspices of the {eu}-funded {sling} integrating activity, which took place in november 2009.
1920	7351537	article	journal of biomedical semantics	\N	\N	biomed central ltd	\N	1	Suppl 1	2010	jun	2010-06-23 13:33:51	\N	{cito}, the citation typing ontology	{cito}, the citation typing ontology, is an ontology for describing the nature of reference citations in scientific research articles and other scholarly works, both to other such publications and also to web information resources, and for publishing these descriptions on the semantic web. citation are described in terms of the factual and rhetorical relationships between citing publication and cited publication, the in-text and global citation frequencies of each cited work, and the nature of the cited work itself, including its publication and peer review status. this paper describes {cito} and illustrates its usefulness both for the annotation of bibliographic reference lists and for the visualization of citation networks. the latest version of {cito}, which this paper describes, is {cito} version 1.6, published on 19 march 2010. {cito} is written in the web ontology language {owl}, uses the namespace http://purl.org/net/cito/, and is available from http://purl.org/net/cito/. this site uses content negotiation to deliver to the user an {owldoc} web version of the ontology if accessed via a web browser, or the {owl} ontology itself if accessed from an ontology management tool such as prot\\'{e}g\\'{e} 4 (http://protege.stanford.edu/). collaborative work is currently under way to harmonize {cito} with other ontologies describing bibliographies and the rhetorical structure of scientific discourse.
1921	7465908	article	genome biology	\N	\N	\N	\N	11	7	2010	jul	2010-07-12 14:41:59	\N	long noncoding {rna} genes: conservation of sequence and brain expression among diverse amniotes.	background:long considered to be the building block of life, it is now apparent that protein is only one of many functional products generated by the eukaryotic genome. indeed, more of the human genome is transcribed into noncoding sequence than into protein-coding sequence. nevertheless, whilst we have developed a deep understanding of the relationships between evolutionary constraint and function for protein-coding sequence, little is known about these relationships for non-coding transcribed sequence. this dearth of information is partially attributable to a lack of established non-protein-coding (ncrna) orthologs among birds and mammals within sequence and expression databases.results:here, we performed a multi-disciplinary study of four highly conserved and brain-expressed transcripts selected from a list of mouse long intergenic noncoding (lncrna) loci that generally show pronounced evolutionary constraint within their putative promoter regions and across exon-intron boundaries. we identify some of the first lncrna orthologs present in birds (chicken), marsupial (opossum), and eutherian mammals (mouse), and investigate whether they exhibit conservation of brain expression. in contrast to conventional protein-coding genes, the sequences, transcriptional start sites, exon structures, and lengths for these non-coding genes are all highly variable.conclusions:the biological relevance of lncrnas would be highly questionable if they were limited to closely-related phyla. instead, their preservation across diverse amniotes, their residual apparent conservation in exon structure, and similarities in their pattern of brain expression during embryonic and early postnatal stages together indicate that these are functional rna molecules, of which some have roles in vertebrate brain development.
1922	7470021	article	science (new york, n.y.)	\N	\N	\N	5	329	5992	2010	aug	2010-07-21 09:33:12	\N	high-resolution analysis of parent-of-origin allelic expression in the mouse brain.	genomic imprinting results in preferential expression of the paternal or maternal allele of certain genes. we have performed a genome-wide characterization of imprinting in the mouse embryonic and adult brain. this approach uncovered parent-of-origin allelic effects of more than 1300 loci. we identified parental bias in the expression of individual genes and of specific transcript isoforms, with differences between brain regions. many imprinted genes are expressed in neural systems associated with feeding and motivated behaviors, and parental biases preferentially target genetic pathways governing metabolism and cell adhesion. we observed a preferential maternal contribution to gene expression in the developing brain and a major paternal contribution in the adult brain. thus, parental expression bias emerges as a major mode of epigenetic regulation in the brain.
1923	7493416	article	genome biology	\N	\N	\N	\N	11	7	2010	jul	2010-07-15 16:23:31	\N	quantifying the mechanisms of domain gain in animal proteins.	background: protein domains are protein regions that are shared among different proteins and are frequently functionally and structurally independent from the rest of the protein. novel domain combinations have a major role in evolutionary innovation. however, the relative contributions of the different molecular mechanisms that underlie domain gains in animals are still unknown. by using animal gene phylogenies we were able to identify a set of high confidence domain gain events and by looking at their coding dna investigate the causative mechanisms. results: here we show that the major mechanism for gains of new domains in metazoan proteins is likely to be gene fusion through joining of exons from adjacent genes, possibly mediated by non-allelic homologous recombination. retroposition and insertion of exons into ancestral introns through intronic recombination are, in contrast to previous expectations, only minor contributors to domain gains and have accounted for less than 1% and 10% of high confidence domain gain events, respectively. additionally, exonization of previously non-coding regions appears to be an important mechanism for addition of disordered segments to proteins. we observe that gene duplication has preceded domain gain in at least 80% of the gain events. conclusions: the interplay of gene duplication and domain gain demonstrates an important mechanism for fast neofunctionalization of genes.
1924	7511505	article	science	\N	\N	american association for the advancement of science	4	329	5989	2010	jul	2010-07-19 10:10:54	\N	computational design of an enzyme catalyst for a stereoselective bimolecular {diels-alder} reaction	the {diels-alder} reaction is a cornerstone in organic synthesis, forming two carbon-carbon bonds and up to four new stereogenic centers in one step. no naturally occurring enzymes have been shown to catalyze bimolecular {diels-alder} reactions. we describe the de novo computational design and experimental characterization of enzymes catalyzing a bimolecular {diels-alder} reaction with high stereoselectivity and substrate specificity. x-ray crystallography confirms that the structure matches the design for the most active of the enzymes, and binding site substitutions reprogram the substrate specificity. designed stereoselective catalysts for carbon-carbon bond-forming reactions should be broadly useful in synthetic chemistry.
1925	7515324	article	bmc bioinformatics	\N	\N	\N	\N	11	1	2010	jul	2010-07-20 00:38:20	\N	integration and visualization of systems biology data in context of the genome.	background:high-density tiling arrays and new sequencing technologies are generating rapidly increasing volumes of transcriptome and protein-dna interaction data. visualization and exploration of this data is critical to understanding the regulatory logic encoded in the genome by which the cell dynamically affects its physiology and interacts with its environment.results:the gaggle genome browser is a cross-platform desktop program for interactively visualizing high-throughput data in the context of the genome. important features include dynamic panning and zooming, keyword search and open interoperability through the gaggle framework. users may bookmark locations on the genome with descriptive annotations and share these bookmarks with other users. the program handles large sets of user-generated data using an in-process database and leverages the facilities of sql and the r environment for importing and manipulating data.a key aspect of the gaggle genome browser is interoperability. by connecting to the gaggle framework, the genome browser joins a suite of interconnected bioinformatics tools for analysis and visualization with connectivity to major public repositories of sequences, interactions and pathways. to this flexible environment for exploring and combining data, the gaggle genome browser adds the ability to visualize diverse types of data in relation to its coordinates on the genome.conclusions:genomic coordinates function as a common key by which disparate biological data types can be related to one another. in the gaggle genome browser, heterogeneous data are joined by their location on the genome to create information-rich visualizations yielding insight into genome organization, transcription and its regulation and, ultimately, a better understanding of the mechanisms that enable the cell to dynamically respond to its environment.
1926	7577931	article	genome research	\N	\N	cold spring harbor laboratory press	8	20	10	2010	oct	2010-08-07 09:53:53	\N	optimization of de novo transcriptome assembly from next-generation sequencing data.	transcriptome analysis has important applications in many biological fields. however, assembling a transcriptome without a known reference remains a challenging task requiring algorithmic improvements. we present two methods for substantially improving transcriptome de novo assembly. the first method relies on the observation that the use of a single k-mer length by current de novo assemblers is suboptimal to assemble transcriptomes where the sequence coverage of transcripts is highly heterogeneous. we present the multiple-k method in which various k-mer lengths are used for de novo transcriptome assembly. we demonstrate its good performance by assembling de novo a published next-generation transcriptome sequence data set of aedes aegypti, using the existing genome to check the accuracy of our method. the second method relies on the use of a reference proteome to improve the de novo assembly. we developed the scaffolding using translation mapping (stm) method that uses mapping against the closest available reference proteome for scaffolding contigs that map onto the same protein. in a controlled experiment using simulated data, we show that the stm method considerably improves the assembly, with few errors. we applied these two methods to assemble the transcriptome of the non-model catfish loricaria gr. cataphracta. using the multiple-k and stm methods, the assembly increases in contiguity and in gene identification, showing that our methods clearly improve quality and can be widely used. the new methods were used to assemble successfully the transcripts of the core set of genes regulating tooth development in vertebrates, while classic de novo assembly failed.
1927	7730675	article	genome research	\N	\N	\N	8	20	10	2010	oct	2010-08-29 15:17:05	\N	reshaping the gut microbiome with bacterial transplantation and antibiotic intake.	the intestinal microbiota consists of over 1000 species, which play key roles in gut physiology and homeostasis. imbalances in the composition of this bacterial community can lead to transient intestinal dysfunctions and chronic disease states. understanding how to manipulate this ecosystem is thus essential for treating many disorders. in this study, we took advantage of recently developed tools for deep sequencing and phylogenetic clustering to examine the long-term effects of exogenous microbiota transplantation combined with and without an antibiotic pretreatment. in our rat model, deep sequencing revealed an intestinal bacterial diversity exceeding that of the human gut by a factor of two to three. the transplantation produced a marked increase in the microbial diversity of the recipients, which stemmed from both capture of new phylotypes and increase in abundance of others. however, when transplantation was performed after antibiotic intake, the resulting state simply combined the reshaping effects of the individual treatments (including the reduced diversity from antibiotic treatment alone). therefore, lowering the recipient bacterial load by antibiotic intake prior to transplantation did not increase establishment of the donor phylotypes, although some dominant lineages still transferred successfully. remarkably, all of these effects were observed after 1 mo of treatment and persisted after 3 mo. overall, our results indicate that the indigenous gut microbial composition is more plastic that previously anticipated. however, since antibiotic pretreatment counterintuitively interferes with the establishment of an exogenous community, such plasticity is likely conditioned more by the altered microbiome gut homeostasis caused by antibiotics than by the primary bacterial loss.
1928	7811569	article	plos genet	\N	\N	public library of science	\N	6	9	2010	sep	2010-09-11 03:45:46	\N	the characterization of twenty sequenced human genomes	we present the analysis of twenty human genomes to evaluate the prospects for identifying rare functional variants that contribute to a phenotype of interest. we sequenced at high coverage ten  ” case” genomes from individuals with severe hemophilia a and ten  ” control” genomes. we summarize the number of genetic variants emerging from a study of this magnitude, and provide a proof of concept for the identification of rare and highly-penetrant functional variants by confirming that the cause of hemophilia a is easily recognizable in this data set. we also show that the number of novel single nucleotide variants ({snvs}) discovered per genome seems to stabilize at about 144,000 new variants per genome, after the first 15 individuals have been sequenced. finally, we find that, on average, each genome carries 165 homozygous protein-truncating or stop loss variants in genes representing a diverse set of pathways. we report here the nearly complete genomic sequence of 20 different individuals, determined using  ” next-generation” sequencing technologies. we use these data to characterize the type of genetic variation carried by humans in a sample of this size, which is to our knowledge the largest set of unrelated genomic sequences that have been reported. we summarize different categories of variation in each genome, and in total across all 20 of the genomes, finding a surprising number of variants predicted to reduce or remove the proteins encoded by many different genes. this work provides important fundamental information about the scope of human genetic variation, and suggests ways to further explore the relationship between these genetic variants and human disease.